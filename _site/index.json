{
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [1.1.5] - 2023-09-28 Fixed Long warning popping up when user starts playmode while editing a prefab that contains NavMesh components (NAVB-47) [1.1.4] - 2023-06-14 Fixed Published the missing API reference documentation for the new properties that were made available in 1.1.0 [1.1.3] - 2023-04-13 Changed Remove some unnecessary files from the package [1.1.2] - 2023-04-03 Changed The AI Navigation overlay in the scene view remembers which sections have been collapsed Updated a large part of the documentation to reflect the current functionality [1.1.1] - 2022-10-21 Changed Clarified the information text displayed by the NavMesh Updater [1.1.0-pre.2] - 2022-08-09 Changed The Dungeon scene included in the package samples now uses tile prefabs that contain a NavMeshSurface component instead of the NavMeshPrefabInstance script. The Drop Plank scene included in the package samples now has a NavMeshSurface component and the NavMeshSurfaceUpdater script on the geometry, as well as the DynamicNavMeshObject script on the Plank prefab for dynamically updating the NavMesh when new Planks are instantiated. The offset when instantiating Planks in the Drop Plank scene has been reduced. The Sliding Window Infinite and the Sliding Window Terrain scenes included in the package samples now use the NavMeshSurfaceVolumeUpdater script instead of the LocalNavMeshBuilder and NavMeshSourceTag scripts for dynamically updating the NavMesh. The Modify Mesh scene included in the package samples now uses a NavMeshSurface component on the Mesh Tool for dynamically updating the NavMesh instead of the LocalNavMeshBuilder and NavMeshSourceTag scripts. The MeshTool script now uses the Update() method of NavMeshSurface for updating the NavMesh whenever the mesh is modified. Fixed The Drop Plank scene included in the package samples now destroys instantiated Planks that have fallen off the edge. Missing agent type references in the samples. Removed The NavMeshPrefabInstance and NavMeshPrefabInstanceEditor scripts from the package samples were removed. The prefab editing scene 7b_dungeon_tile_prefabs from the package samples was removed. The tiles can now be edited directly as prefabs. The LocalNavMeshBuilder and NavMeshSourceTag scripts from the package samples were removed. [1.1.0-pre.1] - 2022-04-27 Added NavMeshSurface supports links generation. NavMeshSurface supports HeightMesh baking. New package Navigation window adapting the obsolete Unity Navigation window functionalities to the package workflow. Changed NavMeshSurface is using the Background Tasks window to report the baking progress Minimum supported version is increased to Unity 2022.2 [1.0.0-exp.4] - 2021-07-19 Changed Documentation updated with changes from Unity manual Test scripts moved into namespaces Unity.AI.Navigation.Tests and Unity.AI.Navigation.Editor.Tests [1.0.0-exp.3] - 2021-06-16 Fixed An assembly definition in the package sample was referencing an invalid AsmDef [1.0.0-exp.2] - 2021-05-19 Fixed Baking a NavMeshSurface with a bounding volume was not detecting the geometry nearby the bounds (1027006) Changed New note in the documentation about the bounding volume of a NavMeshSurface [1.0.0-exp.1] - 2021-04-06 This is the first release of the AI Navigation package. It contains the scripts that were previously known as NavMeshComponents and it adds a few improvements. Fixed Disabling a NavMeshLink component in the Editor does not remove the link Added New minRegionArea property in NavMeshSurface that prevents small isolated patches from being built in the NavMesh Documentation for the new minRegionArea property Changed Documentation updated Script namespaces changed to Unity.AI.Navigation.* The license has changed. The folder structure has changed in accordance to the requirements of the Unity standards for packages."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutAgents.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutAgents.html",
    "title": "About NavMesh agents | FSM Unity Framework",
    "keywords": "About NavMesh agents The NavMesh agent is a GameObject that is represented by an upright cylinder whose size is specified by the Radius and Height properties. The cylinder moves with the GameObject, but remains upright even if the GameObject rotates. The shape of the cylinder is used to detect and respond to collisions with other agents and obstacles. When the anchor point of the GameObject is not at the base of the cylinder, use the Base Offset property to specify the height difference. The height and radius of the cylinder are specified in the Navigation window and the NavMesh Agent component properties of the individual agents. Navigation window settings describe how all the NavMesh Agents collide and avoid static world geometry. To keep memory on budget and CPU load at a reasonable level, you can only specify one size in the bake settings. NavMesh Agent component properties values describe how the agent collides with moving obstacles and other agents. Typically you set the size of the agent with the same values in both places. However, you might, give a heavy soldier a larger radius, so that other agents leave more space around your soldier. Otherwise, your soldier avoids the environment in the same manner as the other agents. Additional resources Create a NavMesh Agent NavMesh Agent component reference NavMesh Agent scripting reference"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutHeightMesh.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutHeightMesh.html",
    "title": "HeightMesh | FSM Unity Framework",
    "keywords": "HeightMesh The HeightMesh allows for the character to be placed more accurately on the ground. While navigating, the NavMesh Agent is constrained on the surface of the NavMesh. Since the NavMesh is an approximation of the walkable space, some features are evened out when the NavMesh is being built. For example, stairs may appear as a slope in the NavMesh. Without the HeightMesh the characters are moved along the approximate NavMesh surface. If your game requires accurate placement of the agent, you should enable the HeightMesh option when you build the NavMesh. Note that building the HeightMesh will take up memory and processing at runtime, and it will lengthen the total time of baking the NavMesh."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutObstacles.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AboutObstacles.html",
    "title": "About NavMesh Obstacles | FSM Unity Framework",
    "keywords": "About NavMesh Obstacles NavMesh Obstacles can affect the NavMesh Agent’s navigation during the game in two ways: Obstructing When Carve is not enabled, the default behavior of the NavMesh Obstacle is similar to that of a [Collider][2]. NavMesh Agents try to avoid [collisions][3] with the NavMesh Obstacle, and when close, they collide with the NavMesh Obstacle. Obstacle avoidance behaviour is very basic, and has a short radius. As such, the NavMesh Agent might not be able to find its way around in an environment cluttered with NavMesh Obstacles. This mode is best used in cases where the obstacle is constantly moving (for example, a vehicle or player character). Carving When Carve is enabled, the obstacle carves a hole in the NavMesh when stationary. When moving, the obstacle is an obstruction. When a hole is carved into the NavMesh, the pathfinder is able to navigate the NavMesh Agent around locations cluttered with obstacles, or find another route if the current path gets blocked by an obstacle. It’s good practice to turn on carving for NavMesh Obstacles that generally block navigation but can be moved by the player or other game events like explosions (for example, crates or barrels). Logic for moving NavMesh Obstacles Unity treats the NavMesh Obstacle as moving when it has moved more than the distance set by the Carve > Move Threshold. When the NavMesh Obstacle moves, the carved hole also moves. However, to reduce CPU overhead, the hole is only recalculated when necessary. The result of this calculation is available in the next frame update. The recalculation logic has two options: Only carve when the NavMesh Obstacle is stationary Carve when the NavMesh Obstacle has moved Only carve when the NavMesh Obstacle is stationary This is the default behavior. To enable it, tick the NavMesh Obstacle component’s Carve Only Stationary checkbox. In this mode, when the NavMesh Obstacle moves, the carved hole is removed. When the NavMesh Obstacle has stopped moving and has been stationary for more than the time set by Carving Time To Stationary, it is treated as stationary and the carved hole is updated again. While the NavMesh Obstacle is moving, the NavMesh Agents avoid it using collision avoidance, but don’t plan paths around it. Carve Only Stationary is generally the best choice in terms of performance, and is a good match when the [GameObject][4] associated with the NavMesh Obstacle is controlled by physics. Carve when the NavMesh Obstacle has moved To enable this mode, untick the NavMesh Obstacle component’s Carve Only Stationary checkbox. When this is unticked, the carved hole is updated when the obstacle has moved more than the distance set by Carving Move Threshold. This mode is useful for large, slowly moving obstacles (for example, a tank that is being avoided by infantry). Note: When using NavMesh query methods, you should take into account that there is a one-frame delay between changing a NavMesh Obstacle and the effect that change has on the NavMesh. Additional Resources Creating a NavMesh Obstacle - Guidance on creating NavMesh Obstacles. Inner Workings of the Navigation System - Learn more about how NavMesh Obstacles are used as part of navigation. NavMesh Obstacle scripting reference - Full description of the NavMesh Obstacle scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AreasAndCosts.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/AreasAndCosts.html",
    "title": "Navigation Areas and Costs | FSM Unity Framework",
    "keywords": "Navigation Areas and Costs The Navigation Areas define how difficult it is to walk across a specific area, the lower cost areas will be preferred during path finding. In addition each NavMesh Agent has an Area Mask which can be used to specify on which areas the agent can move. In the above example the area types are used for two common use cases: Water area is made more costly to walk by assigning it a higher cost, to deal with a scenario where walking on shallow water is slower. Door area is made accessible by specific characters, to create a scenario where humans can walk through doors, but zombies cannot. The area type can be assigned to every object that is included in the NavMesh baking, in addition, each Off-Mesh Link has a property to specify the area type. Pathfinding Cost In a nutshell, the cost allows you to control which areas the pathfinder favors when finding a path. For example, if you set the cost of an area to 3.0, traveling across that area is considered to be three times longer than alternative routes. To fully understand how the cost works, let’s take a look at how the pathfinder works. Nodes and links visited during pathfinding. Unity uses A* to calculate the shortest path on the NavMesh. A* works on a graph of connected nodes. The algorithm starts from the nearest node to the path start and visits the connect nodes until the destination is reached. Since the Unity navigation representation is a mesh of polygons, the first thing the pathfinder needs to do is to place a point on each polygon, which is the location of the node. The shortest path is then calculated between these nodes. The yellow dots and lines in the above picture shows how the nodes and links are placed on the NavMesh, and in which order they are traversed during the A*. The cost to move between two nodes depends on the distance to travel and the cost associated with the area type of the polygon under the link, that is, distance * cost. In practice this means, that if the cost of an area is 2.0, the distance across such polygon will appear to be twice as long. The A* algorithm requires that all costs must be larger than 1.0. The effect of the costs on the resulting path can be hard to tune, especially for longer paths. The best way to approach costs is to treat them as hints. For example, if you want the agents to not to use Off-Mesh Links too often, you could increase their cost. But it can be challenging to tune a behavior where the agents to prefer to walk on sidewalks. Another thing you may notice on some levels is that the pathfinder does not always choose the very shortest path. The reason for this is the node placement. The effect can be noticeable in scenarios where big open areas are next to tiny obstacles, which results navigation mesh with very big and small polygons. In such cases the nodes on the big polygons may get placed anywhere in the big polygon and from the pathfinder’s point of view it looks like a detour. The cost per area type can be set globally in the Areas tab, or you can override them per agent using a script. Area Types The area types are specified in the Navigation Window’s Areas tab. There are 29 custom types, and 3 built-in types: Walkable, Not Walkable, and Jump. Walkable is a generic area type which specifies that the area can be walked on. Not Walkable is a generic area type which prevents navigation. It is useful for cases where you want to mark certain object to be an obstacle, but without getting NavMesh on top of it. Jump is an area type that is assigned to all auto-generated Off-Mesh Links. If several objects of different area types are overlapping, the resulting navmesh area type will generally be the one with the highest index. There is one exception however: Not Walkable always takes precedence. Which can be helpful if you need to block out an area. Area Mask Each agent has an Area Mask which describes which areas it can use when navigating. The area mask can be set in the agent properties, or the bitmask can be manipulated using a script at runtime. The area mask is useful when you want only certain types characters to be able to walk through an area. For example, in a zombie evasion game, you could mark the area under each door with a Door area type, and uncheck the Door area from the zombie character’s Area Mask. Additional resources Create a NavMesh - Workflow to create a NavMesh NavMeshAgent.areaMask - Script API to set areaMask for an agent. NavMeshAgent.SetAreaCost() - Script API to set area cost for an agent."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CouplingAnimationAndNavigation.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CouplingAnimationAndNavigation.html",
    "title": "Coupling Animation and Navigation | FSM Unity Framework",
    "keywords": "Coupling Animation and Navigation The goal of this document is to guide you to setup navigating humanoid characters to move using the navigation system. We’ll be using Unity’s built-in systems for animation and navigation along with custom scripting to achieve this. It’s assumed you’re familiar with the basics of Unity and the Mecanim animation system. An example project is available — so you don’t have add scripts or set up animations and animation controller from scratch: NavigationAnimation_53.zip Works with Unity 5.3+ Creating the Animation Controller To get a responsive and versatile animation controller — covering a wide range of movements — we need a set of animations moving in different directions. This is sometimes referred to as a strafe-set. In addition to the move animations we need an animation for the standing character. We proceed by arranging the strafe-set in a 2D blend tree — choose blend type: 2D Simple Directional and place animations using Compute Positions > Velocity XZ For blending control we add two float parameters velx and vely, and assign them to the blend tree. Here we’ll be placing 7 run animations — each with a different velocity. In addition to the forwards (+ left/right) and backwards (+ left/right) we also use an animation clip for running on the spot. The latter is highlighted in the center of the 2D blend map below. The reason for having an animation running on the spot is two-fold, firstly it preserves the style of running when blended with the other animations. Secondly the animation prevents foot-sliding when blending. Then we add the idle animation clip in it’s own node (Idle). We now have two discrete animation states that we couple with 2 transitions. To control the switch between the moving and idle states we add a boolean control parameter move. Then disable the Has Exit Time property on the transitions. This allows the transition to trigger at any time during the animation. Transition time should be set to around 0.10 second to get a responsive transition. Now place the new created animation controller on the character you want to move. Press play and select the character in the Hierarchy window. You can now manually control the animation values in the Animator window and change the move state and velocity. The next step is to create other means of controlling the animation parameters. Navigation Control Place a NavMeshAgent component on the character and adjust the radius, height and to match the character - additionally change the speed property to match the maximum speed in the animation blend tree. Create a NavMesh for the Scene you’ve placed the character in. Next we need to tell the character where to navigate to. This typically is very specific to the application. Here we choose a click to move behavior — the character moved to the point in the world where the user has clicked on the screen. // ClickToMove.cs using UnityEngine; using UnityEngine.AI; [RequireComponent (typeof (NavMeshAgent))] public class ClickToMove : MonoBehaviour { RaycastHit hitInfo = new RaycastHit(); NavMeshAgent agent; void Start () { agent = GetComponent<NavMeshAgent> (); } void Update () { if(Input.GetMouseButtonDown(0)) { Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition); if (Physics.Raycast(ray.origin, ray.direction, out hitInfo)) agent.destination = hitInfo.point; } } } Pressing play now — and clicking around in the scene — you’ll see the character move around in the scene. However — the animations don’t match the movement at all. We need to communicate the state and velocity of the agent to the animation controller. To transfer the velocity and state info from the agent to the animation controller we will add another script. // LocomotionSimpleAgent.cs using UnityEngine; using UnityEngine.AI; [RequireComponent (typeof (NavMeshAgent))] [RequireComponent (typeof (Animator))] public class LocomotionSimpleAgent : MonoBehaviour { Animator anim; NavMeshAgent agent; Vector2 smoothDeltaPosition = Vector2.zero; Vector2 velocity = Vector2.zero; void Start () { anim = GetComponent<Animator> (); agent = GetComponent<NavMeshAgent> (); // Don’t update position automatically agent.updatePosition = false; } void Update () { Vector3 worldDeltaPosition = agent.nextPosition - transform.position; // Map 'worldDeltaPosition' to local space float dx = Vector3.Dot (transform.right, worldDeltaPosition); float dy = Vector3.Dot (transform.forward, worldDeltaPosition); Vector2 deltaPosition = new Vector2 (dx, dy); // Low-pass filter the deltaMove float smooth = Mathf.Min(1.0f, Time.deltaTime/0.15f); smoothDeltaPosition = Vector2.Lerp (smoothDeltaPosition, deltaPosition, smooth); // Update velocity if time advances if (Time.deltaTime > 1e-5f) velocity = smoothDeltaPosition / Time.deltaTime; bool shouldMove = velocity.magnitude > 0.5f && agent.remainingDistance > agent.radius; // Update animation parameters anim.SetBool(\"move\", shouldMove); anim.SetFloat (\"velx\", velocity.x); anim.SetFloat (\"vely\", velocity.y); GetComponent<LookAt>().lookAtTargetPosition = agent.steeringTarget + transform.forward; } void OnAnimatorMove () { // Update position to agent position transform.position = agent.nextPosition; } } This script deserves a little explanation. It’s placed on the character — which has an Animator and a NavMeshAgent component attached — as well as the click to move script above. First the script tells the agent not to update the character position automatically. We handle the position update that last in the script. The orientation is updated by the agent. The animation blend is controlled by reading the agent velocity. It is transformed into a relative velocity (based on character orientation) — and then smoothed. The transformed horizontal velocity components are then passed to the Animator and additionally the state switching between idle and moving is controlled by the speed (i.e. velocity magnitude). In the OnAnimatorMove() callback we update the position of the character to match the NavMeshAgent. Playing the scene again gives show that animation matches the movement to as close as possible. Improving the Quality of the Navigating Character To improve the quality of the animated and navigating character we will explore a couple of options. Looking Having the character to look and turn towards points of interest is important to convey attention and anticipation. We’ll use the animation systems lookat API. This calls for another script. // LookAt.cs using UnityEngine; using System.Collections; [RequireComponent (typeof (Animator))] public class LookAt : MonoBehaviour { public Transform head = null; public Vector3 lookAtTargetPosition; public float lookAtCoolTime = 0.2f; public float lookAtHeatTime = 0.2f; public bool looking = true; private Vector3 lookAtPosition; private Animator animator; private float lookAtWeight = 0.0f; void Start () { if (!head) { Debug.LogError(\"No head transform - LookAt disabled\"); enabled = false; return; } animator = GetComponent<Animator> (); lookAtTargetPosition = head.position + transform.forward; lookAtPosition = lookAtTargetPosition; } void OnAnimatorIK () { lookAtTargetPosition.y = head.position.y; float lookAtTargetWeight = looking ? 1.0f : 0.0f; Vector3 curDir = lookAtPosition - head.position; Vector3 futDir = lookAtTargetPosition - head.position; curDir = Vector3.RotateTowards(curDir, futDir, 6.28f*Time.deltaTime, float.PositiveInfinity); lookAtPosition = head.position + curDir; float blendTime = lookAtTargetWeight > lookAtWeight ? lookAtHeatTime : lookAtCoolTime; lookAtWeight = Mathf.MoveTowards (lookAtWeight, lookAtTargetWeight, Time.deltaTime/blendTime); animator.SetLookAtWeight (lookAtWeight, 0.2f, 0.5f, 0.7f, 0.5f); animator.SetLookAtPosition (lookAtPosition); } } Add the script to the character and assign the head property to the head transform in your characters transform hierarchy. The LookAt script has no notion of navigation control — so to control where to look we go back to the LocomotionSimpleAgent.cs script and add a couple of lines to control the looking. Add the end of Update() add: LookAt lookAt = GetComponent<LookAt> (); if (lookAt) lookAt.lookAtTargetPosition = agent.steeringTarget + transform.forward; This will tell the LookAt script to set the point of interest to approximately the next corner along the path or — if no corners — to the end of the path. Try it out. Animation Driven Character using Navigation The character has so far been controlled completely by the position dictated by the agent. This ensures that the avoidance of other characters and obstacles translates directly to the character position. However it may lead to foot-sliding if the animation doesn’t cover the proposed velocity. Here we’ll relax the constraint of the character a bit. Basically we’ll be trading the avoidance quality for animation quality. Replace the OnAnimatorMove() callback on the LocomotionSimpleAgent.cs script replace the line with the following void OnAnimatorMove () { // Update position based on animation movement using navigation surface height Vector3 position = anim.rootPosition; position.y = agent.nextPosition.y; transform.position = position; } When trying this out you may notice the that character can now drift away from the agent position (green wireframe cylinder) . You may need to limit that character animation drift. This can be done either by pulling the agent towards the character — or pull the character towards the agent position. Add the following at the end of the Update() method on the script LocomotionSimpleAgent.cs. // Pull character towards agent if (worldDeltaPosition.magnitude > agent.radius) transform.position = agent.nextPosition - 0.9f*worldDeltaPosition; Or — if you want the agent to follow the character. // Pull agent towards character if (worldDeltaPosition.magnitude > agent.radius) agent.nextPosition = transform.position + 0.9f*worldDeltaPosition; What works best very much depends on the specific use-case. Conclusion We have set up a character that moves using the navigation system and animates accordingly. Tweaking the numbers of blend time, look-at weights etc. can improve the looks — and is a good way to further explore this setup."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMesh.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMesh.html",
    "title": "Create a NavMesh | FSM Unity Framework",
    "keywords": "Create a NavMesh You need to create a NavMesh to define an area of your scene within which a character can navigate intelligently. To create a NavMesh do the following: Select the scene geometry where you want to add the NavMesh. In the Inspector window, click Add Component. Select Navigation > NavMesh Surface. In the NavMesh Surface component, specify the necessary settings. For details on the available settings, refer to NavMesh Surface component. When you are finished, click Bake. The NavMesh is generated and displayed in the scene as a blue overlay on the underlying scene geometry whenever the Navigation window is open and visible. Additional resources Navigation window Creating a NavMeshAgent NavMesh Surface component Navigation Areas and Costs"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMeshAgent.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMeshAgent.html",
    "title": "Creating a NavMesh Agent | FSM Unity Framework",
    "keywords": "Creating a NavMesh Agent Once you have a NavMesh baked for your level it is time to create a character which can navigate the scene. We’re going to build our prototype agent from a cylinder and set it in motion. This is done using a NavMesh Agent component and a simple script. First let’s create the character: Create a cylinder: GameObject > 3D Object > Cylinder. The default cylinder dimensions (height 2 and radius 0.5) are good for a humanoid shaped agent, so we will leave them as they are. Add a NavMesh Agent component: Component > Navigation > NavMesh Agent. Now you have simple NavMesh Agent set up ready to receive commands! When you start to experiment with a NavMesh Agent, you most likely are going to adjust its dimensions for your character size and speed. The NavMesh Agent component handles both the pathfinding and the movement control of a character. In your scripts, navigation can be as simple as setting the desired destination point - the NavMesh Agent can handle everything from there on. // MoveTo.cs using UnityEngine; using UnityEngine.AI; public class MoveTo : MonoBehaviour { public Transform goal; void Start () { NavMeshAgent agent = GetComponent<NavMeshAgent>(); agent.destination = goal.position; } } Next we need to build a simple script which allows you to send your character to the destination specified by another Game Object, and a Sphere which will be the destination to move to: Create a new C# script (MoveTo.cs) and replace its contents with the above script. Assign the MoveTo script to the character you’ve just created. Create a sphere, this will be the destination the agent will move to. Move the sphere away from the character to a location that is close to the NavMesh surface. Select the character, locate the MoveTo script, and assign the Sphere to the Goal property. Press Play; you should see the agent navigating to the location of the sphere. To sum it up, in your script, you will need to get a reference to the NavMesh Agent component and then to set the agent in motion, you just need to assign a position to its destination property. The Navigation How Tos will give you further examples on how to solve common gameplay scenarios with the NavMesh Agent. Additional resources Navigation HowTos - common use cases for NavMesh Agent, with source code. Inner Workings of the Navigation System - learn more about path following. NavMesh Agent component reference – full description of all the NavMeshAgent properties. NavMesh Agent scripting reference - full description of the NavMeshAgent scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMeshObstacle.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateNavMeshObstacle.html",
    "title": "Creating a NavMesh Obstacle | FSM Unity Framework",
    "keywords": "Creating a NavMesh Obstacle NavMesh Obstacle components can be used to describe obstacles the agents should avoid while navigating. For example the agents should avoid physics controlled objects, such as crates and barrels while moving. We’re going to add a crate to block the pathway at the top of the level. First create a cube to depict the crate: Game Object > 3D Object > Cube. Move the cube to the platform at the top, the default size of the cube is good for a crate so leave it as it is. Add a NavMesh Obstacle component to the cube. Choose Add Component from the inspector and choose Navigation > NavMesh Obstacle. Set the shape of the obstacle to box, changing the shape will automatically fit the center and size to the render mesh. Add a Rigid body to the obstacle. Choose Add Component from the inspector and choose Physics > Rigid Body. Finally turn on the Carve setting from the NavMesh Obstacle inspector so that the agent knows to find a path around the obstacle. Now we have a working crate that is physics controlled, and which the AI knows how to avoid while navigating. Additional resources Inner Workings of the Navigation System - learn more about how obstacles are used as part of navigation. NavMesh Obstacle component reference – full description of all the NavMesh Obstacle properties. NavMesh Obstacle scripting reference - full description of the NavMesh Obstacle scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateOffMeshLink.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/CreateOffMeshLink.html",
    "title": "Creating an Off-mesh Link | FSM Unity Framework",
    "keywords": "Creating an Off-mesh Link Off-Mesh Links are used to create paths crossing outside the walkable navigation mesh surface. For example, jumping over a ditch or a fence, or opening a door before walking through it, can be all described as Off-mesh links. We’re going to add an Off-Mesh Link component to describe a jump from the upper platform to the ground. First create two cylinders: Game Object > 3D Object > cylinder. You can scale the cylinders to (0.1, 0.5, 0.1) to make it easier to work with them. Move the first cylinder at the edge of the top platform, close to the NavMesh surface. Place the second cylinder on the ground, close to the NavMesh, at the location where the link should land. Select the first cylinder cylinder and add an Off-Mesh Link component to it. Choose Add Component from the inspector and choose Navigation > OffMesh Link. Assign the first cylinder in the Start field and the second cylinder in the End field. Now you have a functioning Off-Mesh Link set up! If the path via the off-mesh link is shorter than via walking along the Navmesh, the off-mesh link will be used. You can use any game object in the Scene to hold the Off-Mesh link component, for example a fence prefab could contain the off-mesh link component. Similarly you can use any game object with a Transform as the start and end marker. The NavMesh bake process can detect and create common jump-across and drop-down links automatically. Take a look at the Building Off-Mesh Links Automatically for more details. Details If the agent does not traverse an OffMesh link make sure that both end points are connected correctly. A properly connected end point should show a circle around the access point. Another common cause is that the Navmesh Agent’s Area Mask does not have the OffMesh Link’s area included. Additional resources Navigation HowTos - common use cases for NavMesh Agent, with source code. OffMesh Link component reference – full description of all the Off-Mesh Link properties. Off-Mesh Link scripting reference - full description of the Off-Mesh Link scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Glossary.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Glossary.html",
    "title": "Glossary | FSM Unity Framework",
    "keywords": "Glossary Animation blend tree Used for continuous blending between similar Animation Clips based on float Animation Parameters. Animation clip Animation data that can be used for animated characters or simple animations. An animation clip is one piece of motion, such as (one specific instance of) “Idle”, “Walk” or “Run”. Animation parameters Used to communicate between scripting and the Animator Controller. Some parameters can be set in scripting and used by the controller, while other parameters are based on Custom Curves in Animation Clips and can be sampled using the scripting API. Animator window The window where the Animator Controller is visualized and edited. Colliders An invisible shape that is used to handle physical collisions for an object. A collider doesn’t need to be exactly the same shape as the object’s mesh - a rough approximation is often more efficient and indistinguishable in gameplay. Collision A collision occurs when the physics engine detects that the colliders of two GameObjects make contact or overlap, and at least one has a Rigidbody component and is in motion. GameObject The fundamental object in Unity scenes, which can represent characters, props, scenery, cameras, waypoints, and more. Inspector A Unity window that displays information about the currently selected GameObject, asset or project settings, allowing you to inspect and edit the values. Mesh The main graphics primitive of Unity. Meshes make up a large part of your 3D worlds. Unity supports triangulated or Quadrangulated polygon meshes. Nurbs, Nurms, Subdiv surfaces must be converted to polygons. NavMesh A mesh that Unity generates to approximate the walkable areas and obstacles in your environment for path finding and AI-controlled navigation. Prefab An asset type that allows you to store a GameObject complete with components and properties. The prefab acts as a template from which you can create new object instances in the scene. Rigidbody A component that allows a GameObject to be affected by simulated gravity and other forces. Root motion Motion of character’s root node, whether it’s controlled by the animation itself or externally. Scenes A Scene contains the environments and menus of your game. Think of each unique Scene file as a unique level. In each Scene, you place your environments, obstacles, and decorations, essentially designing and building your game in pieces. Scripts A piece of code that allows you to create your own Components, trigger game events, modify Component properties over time and respond to user input in any way you like. Terrains The landscape in your scene. A Terrain GameObject adds a large flat plane to your scene and you can use the Terrain’s Inspector window to create a detailed landscape. Unity unit The unit size used in Unity projects. By default, 1 Unity unit is 1 meter. To use a different scale, set the Scale Factor in the Import Settings when importing assets."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/HeightMesh.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/HeightMesh.html",
    "title": "Building a HeightMesh for Accurate Character Placement | FSM Unity Framework",
    "keywords": "Building a HeightMesh for Accurate Character Placement Height mesh allows you to place your character more accurately on the walkable surfaces. While navigating, the NavMesh Agent is constrained on the surface of the NavMesh. Since the NavMesh is an approximation of the walkable space, some features are evened out when the NavMesh is being built. For example, stairs may appear as a slope in the NavMesh. If your game requires accurate placement of the agent, you should enable HeightMesh building when you bake the NavMesh. The setting can be found under the Advanced settings in Navigation window. Note that building HeightMesh will take up memory and processing at runtime, and it will take a little longer to bake the NavMesh. Additional resources Building a NavMesh – workflow for NavMesh baking."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/MixingComponents.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/MixingComponents.html",
    "title": "Using NavMesh Agent with Other Components | FSM Unity Framework",
    "keywords": "Using NavMesh Agent with Other Components You can use NavMesh Agent, NavMesh Obstacle, and Off Mesh Link components with other Unity components too. Here’s a list of dos and don’ts when mixing different components together. NavMesh Agent and Physics You don’t need to add physics colliders to NavMesh Agents for them to avoid each other That is, the navigation system simulates agents and their reaction to obstacles and the static world. Here the static world is the baked NavMesh. If you want a NavMesh Agent to push around physics objects or use physics triggers: Add a Collider component (if not present) Add Rigidbody component Turn on kinematic (Is Kinematic) - this is important! Kinematic means that the rigid body is controlled by something else than the physics simulation If both NavMesh Agent and Rigidbody (non-kinematic) are active at the same time, you have race condition Both components may try to move the agent at the same which leads to undefined behavior You can use a NavMesh Agent to move e.g. a player character, without physics Set players agent’s avoidance priority to a small number (high priority), to allow the player to brush through crowds Move the player agent using NavMeshAgent.velocity, so that other agents can predict the player movement to avoid the player. NavMesh Agent and Animator NavMesh Agent and Animator with Root Motion can cause race condition Both components try to move the transform each frame Two possible solutions Information should always flow in one direction Either agent moves the character and animations follows Or animation moves the character based on simulated result Otherwise you’ll end up having a hard to debug feedback loop Animation follows agent Use the NavMeshAgent.velocity as input to the Animator to roughly match the agent’s movement to the animations Robust and simple to implement, will result in foot sliding where animations cannot match the velocity Agent follows animation Disable NavMeshAgent.updatePosition and NavMeshAgent.updateRotation to detach the simulation from the game objects locations Use the difference between the simulated agent’s position (NavMeshAgent.nextPosition) and animation root (Animator.rootPosition) to calculate controls for the animations See Coupling Animation and Navigation for more details NavMesh Agent and NavMesh Obstacle Do not mix well! Enabling both will make the agent trying to avoid itself If carving is enabled in addition, the agent tries to constantly remap to the edge of the carved hole, even more erroneous behavior ensues Make sure only one of them are active at any given time Deceased state, you may turn off the agent and turn on the obstacle to force other agents to avoid it Alternatively you can use priorities to make certain agents to be avoided more NavMesh Obstacle and Physics If you want physics controlled object to affect NavMesh Agent’s behavior Add NavMesh Obstacle component to the object which agent should be aware of, this allows the avoidance system to reason about the obstacle If a game object has a Rigidbody and a NavMesh Obstacle attached, the obstacle’s velocity is obtained from the Rigidbody automatically This allows NavMesh Agents to predict and avoid the moving obstacle"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavAgentPatrol.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavAgentPatrol.html",
    "title": "Making an Agent Patrol Between a Set of Points | FSM Unity Framework",
    "keywords": "Making an Agent Patrol Between a Set of Points Many games feature NPCs that patrol automatically around the playing area. The navigation system can be used to implement this behaviour but it is slightly more involved than standard pathfinding - merely using the shortest path between two points makes for a limited and predictable patrol route. You can get a more convincing patrol pattern by keeping a set of key points that are “useful” for the NPC to pass through and visiting them in some kind of sequence. For example, in a maze, you might place the key patrol points at junctions and corners to ensure the agent checks every corridor. For an office building, the key points might be the individual offices and other rooms. A maze with key patrol points marked The ideal sequence of patrol points will depend on the way you want the NPCs to behave. For example, a robot would probably just visit the points in a methodical order while a human guard might try to catch the player out by using a more random pattern. The simple behaviour of the robot can be implemented using the code shown below. The patrol points are supplied to the script using a public array of Transforms. This array can be assigned from the inspector using GameObjects to mark the points’ positions. The GotoNextPoint function sets the destination point for the agent (which also starts it moving) and then selects the new destination that will be used on the next call. As it stands, the code cycles through the points in the sequence they occur in the array but you can easily modify this, say by using Random.Range to choose an array index at random. In the Update function, the script checks how close the agent is to the destination using the remainingDistance property. When this distance is very small, a call to GotoNextPoint is made to start the next leg of the patrol. // Patrol.cs using UnityEngine; using UnityEngine.AI; using System.Collections; public class Patrol : MonoBehaviour { public Transform[] points; private int destPoint = 0; private NavMeshAgent agent; void Start () { agent = GetComponent<NavMeshAgent>(); // Disabling auto-braking allows for continuous movement // between points (ie, the agent doesn't slow down as it // approaches a destination point). agent.autoBraking = false; GotoNextPoint(); } void GotoNextPoint() { // Returns if no points have been set up if (points.Length == 0) return; // Set the agent to go to the currently selected destination. agent.destination = points[destPoint].position; // Choose the next point in the array as the destination, // cycling to the start if necessary. destPoint = (destPoint + 1) % points.Length; } void Update () { // Choose the next destination point when the agent gets // close to the current one. if (!agent.pathPending && agent.remainingDistance < 0.5f) GotoNextPoint(); } }"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavEditorPreferences.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavEditorPreferences.html",
    "title": "AI Navigation preferences reference | FSM Unity Framework",
    "keywords": "AI Navigation preferences reference Use the AI Navigation preferences to specify how the navigation meshes (NavMesh and HeightMesh) display in the Scene view. The AI Navigation preferences are located in the Preferences window. To open the AI Navigation preferences, do the following: In the main menu, go to Edit > Preferences. Select AI Navigation. The following table describes the controls available in the AI Navigation preferences tab. Control Description Selected Surfaces Opacity Specify the opacity of the displayed meshes (NavMesh and HeightMesh) for NavMesh Surface instances that are part of the current selection hierarchy. Unselected Surfaces Opacity Specify the opacity of displayed meshes (NavMesh and HeightMesh) for NavMesh Surface instances that are outside of the current selection hierarchy. Height Mesh Color Set the color used to display the HeightMesh. Reset to Defaults Set all the NavMesh Visualization Settings parameters to their default value. Note: The NavMesh is represented in the colors described in the Areas tab of the Navigation window. That color palette cannot be modified. Additional resources Preferences window AI Navigation overlay"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavHowTos.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavHowTos.html",
    "title": "Navigation How-Tos | FSM Unity Framework",
    "keywords": "Navigation How-Tos The following topics describe a set of techniques, and include code samples, to implement common tasks in navigation. As with all code in our documentation, you are free to use these samples for any purpose without crediting Unity. Topic Description Tell a NavMeshAgent to Move to a Destination Set a destination for a NavMesh agent. Move an Agent to a Position Clicked by the Mouse Use a mouse click to set the destination for a NavMesh agent. Make an Agent Patrol Between a Set of Points Set patrol points for a NavMesh agent. Couple Animation and Navigation Integrate animation into your navigation."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavInnerWorkings.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavInnerWorkings.html",
    "title": "Inner Workings of the Navigation System | FSM Unity Framework",
    "keywords": "Inner Workings of the Navigation System When you want to intelligently move characters in your game (or agents as they are called in AI circles), you have to solve two problems: how to reason about the level to find the destination, then how to move there. These two problems are tightly coupled, but quite different in nature. The problem of reasoning about the level is more global and static, in that it takes into account the whole scene. Moving to the destination is more local and dynamic, it only considers the direction to move and how to prevent collisions with other moving agents. Walkable Areas The navigation system needs its own data to represent the walkable areas in a game scene. The walkable areas define the places in the scene where the agent can stand and move. In Unity the agents are described as cylinders. The walkable area is built automatically from the geometry in the scene by testing the locations where the agent can stand. Then the locations are connected to a surface laying on top of the scene geometry. This surface is called the navigation mesh (NavMesh for short). The NavMesh stores this surface as convex polygons. Convex polygons are a useful representation, since we know that there are no obstructions between any two points inside a polygon. In addition to the polygon boundaries, we store information about which polygons are neighbours to each other. This allows us to reason about the whole walkable area. Finding Paths To find path between two locations in the scene, we first need to map the start and destination locations to their nearest polygons. Then we start searching from the start location, visiting all the neighbours until we reach the destination polygon. Tracing the visited polygons allows us to find the sequence of polygons which will lead from the start to the destination. A common algorithm to find the path is A* (pronounced “A star”), which is what Unity uses. Following the Path The sequence of polygons which describe the path from the start to the destination polygon is called a corridor. The agent will reach the destination by always steering towards the next visible corner of the corridor. If you have a simple game where only one agent moves in the scene, it is fine to find all the corners of the corridor in one swoop and animate the character to move along the line segments connecting the corners. When dealing with multiple agents moving at the same time, they will need to deviate from the original path when avoiding each other. Trying to correct such deviations using a path consisting of line segments soon becomes very difficult and error prone. Since the agent movement in each frame is quite small, we can use the connectivity of the polygons to fix up the corridor in case we need to take a little detour. Then we quickly find the next visible corner to steer towards. Avoiding Obstacles The steering logic takes the position of the next corner and based on that figures out a desired direction and speed (or velocity) needed to reach the destination. Using the desired velocity to move the agent can lead to collision with other agents. Obstacle avoidance chooses a new velocity which balances between moving in the desired direction and preventing future collisions with other agents and edges of the navigation mesh. Unity is using reciprocal velocity obstacles (RVO) to predict and prevent collisions. Moving the Agent Finally after steering and obstacle avoidance the final velocity is calculated. In Unity the agents are simulated using a simple dynamic model, which also takes into account acceleration to allow more natural and smooth movement. At this stage you can feed the velocity from the simulated agent to the animation system to move the character using root motion, or let the navigation system take care of that. Once the agent has been moved using either method, the simulated agent location is moved and constrained to NavMesh. This last small step is important for robust navigation. Global and Local One of the most important things to understand about navigation is the difference between global and local navigation. Global navigation is used to find the corridor across the world. Finding a path across the world is a costly operation requiring quite a lot of processing power and memory. The linear list of polygons describing the path is a flexible data structure for steering, and it can be locally adjusted as the agent’s position moves. Local navigation tries to figure out how to efficiently move towards the next corner without colliding with other agents or moving objects. Two Cases for Obstacles Many applications of navigation require other types of obstacles rather than just other agents. These could be the usual crates and barrels in a shooter game, or vehicles. The obstacles can be handled using local obstacle avoidance or global pathfinding. When an obstacle is moving, it is best handled using local obstacles avoidance. This way the agent can predictively avoid the obstacle. When the obstacle becomes stationary, and can be considered to block the path of all agents, the obstacles should affect the global navigation, that is, the navigation mesh. Changing the NavMesh is called carving. The process detects which parts of the obstacle touches the NavMesh and carves holes into the NavMesh. This is computationally expensive operation, which is yet another compelling reason, why moving obstacles should be handled using collision avoidance. Local collision avoidance can be often used to steer around sparsely scattered obstacles too. Since the algorithm is local, it will only consider the next immediate collisions, and cannot steer around traps or handle cases where the an obstacle blocks a path. These cases can be solved using carving. Describing Off-mesh Links The connections between the NavMesh polygons are described using links inside the pathfinding system. Sometimes it is necessary to let the agent navigate across places which are not walkable, for example, jumping over a fence, or traversing through a closed door. These cases need to know the location of the action. These actions can be annotated using Off-Mesh Links which tell the pathfinder that a route exists through the specified link. This link can be later accessed when following the path, and the special action can be executed."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshAgent.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshAgent.html",
    "title": "NavMesh Agent component reference | FSM Unity Framework",
    "keywords": "NavMesh Agent component reference The NavMesh Agent component allows you to create characters (agents) that avoid each other as they move through a scene. Agents use the NavMesh to navigate through the space of the game and avoid each other and other moving obstacles. You can use the scripting API of the NavMesh Agent to handle pathfinding and spatial reasoning. To use the NavMesh Agent component, add it to a GameObject: Select the GameObject that represents your agent. In the Inspector, click Add Component. Select Navigation > NavMesh Agent. The NavMesh Agent component is displayed in the Inspector window. You can use this component to create NavMesh agents. For more details, see Create a NavMesh Agent. For more information about NavMesh agents, see About NavMesh agents. The following tables describe the properties available in the NavMesh agent component. Property Description Agent type Select the type of agent you want to create. This allows the agent to move along any NavMesh created for the selected agent type. Base offset Specify the offset of the collision cylinder in relation to the transform pivot point. Steering Property Description Speed Set the maximum speed (in Unity units per second) at which the agent can move along a path. Angular Speed Set the maximum rotation speed (in degrees per second) of the agent. Acceleration Set the maximum acceleration (in Unity units per second squared). Stopping Distance Specify how close the agent can get to its destination. The agent stops when it arrives this close to the destination location. Auto Braking Specify if the agent slows down as it approaches its destination. When enabled, the agent slows down as it approaches the destination. Disable this if you want the agent to move smoothly between multiple points (for example, if the agent is a guard on patrol). Obstacle Avoidance Property Description Radius Specify the distance from the agent's center that is used to calculate collisions between the agent and other GameObjects. Height Specify the height clearance that the agent needs to pass below an obstacle that is overhead. For example, the minimum height of a doorway or tunnel. Quality Select the obstacle avoidance quality. If you have a high number of agents, you can reduce the obstacle avoidance quality to reduce performance costs. If you set obstacle avoidance quality to none, then collisions resolve, but other agents and obstacles are not actively avoided. Priority Specify how agents behave as they avoid each other. Agents avoid other agents of higher priority and ignore other agents of lower priority. The value should be in the range 0–99 where lower numbers indicate higher priority. Path Finding Property Description Auto Traverse OffMesh Link Specify whether or not the agent automatically traverses OffMesh links. When enabled, the agent automatically traverses OffMesh links. Disable Auto Traverse OffMesh Link if you want to use animation or a specific way to traverse OffMesh links. Auto Repath Specify what the agent does when it reaches the end of a partial path. When there is no path to the destination, Unity generates a partial path to the reachable location that is closest to the destination. If this property is enabled, when the agent reaches the end of a partial path it tries again to find a path to the destination. Area Mask Specify which area types the agent considers as it tries to find a path. You can select multiple options. When you prepare meshes for NavMesh baking, you can set each mesh's area type. For example, you can mark stairs with a special area type, and restrict some agent types from using the stairs. Additional resources Create a NavMesh Agent About NavMesh agents Inner Workings of the Navigation System NavMesh Agent scripting reference"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshBuildingComponents.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshBuildingComponents.html",
    "title": "NavMesh Building Components | FSM Unity Framework",
    "keywords": "NavMesh Building Components The NavMesh-building components provide you with controls that allow you to automatically generate and use NavMeshes at runtime and in the Unity Editor. Here we introduce four high level components for the navigation system: NavMeshSurface - Use for building and enabling a NavMesh surface for one type of Agent. NavMeshModifier - Use for affecting the NavMesh generation of NavMesh area types based on the transform hierarchy. NavMeshModifierVolume - Use for affecting the NavMesh generation of NavMesh area types based on volume. NavMeshLink - Use for connecting the same or different NavMesh surfaces for one type of Agent."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshLink.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshLink.html",
    "title": "NavMeshLink | FSM Unity Framework",
    "keywords": "NavMeshLink NavMesh Link creates a navigable link between two locations that use NavMeshes. This link can be from point to point or it can span a gap, in which case the Agent uses the nearest location along the entry edge to cross the link. You must use a NavMesh Link to connect different NavMesh Surfaces. To use the NavMesh Link component, navigate to GameObject > AI > NavMesh Link. Parameters Property Description Agent Type Specify the Agent type that can use the link. Start Point Specify the start point of the link, relative to the GameObject. Defined by XYZ coordinates. End Point Specify the end point of the link, relative to the GameObject. Defined by XYZ coordinates. Align Transform To Points Clicking this button moves the GameObject at the link’s center point and aligns the transform’s forward axis with the end point. Cost Modifier When the cost modifier value is non-negative the cost of moving over the NavMeshLink is equivalent to the cost modifier value times the Euclidean distance between NavMeshLink end points. Bidirectional With this checkbox selected, NavMesh Agents traverse the NavMesh Link both ways (from the start point to the end point, and from the end point back to the start point). When this checkbox is cleared, the NavMesh Link only functions one-way (from the start point to the end point only). Area Type The area type of the NavMesh Link (this affects pathfinding costs). - Walkable (this is the default option) - Not Walkable - Jump Connecting multiple NavMesh Surfaces together If you want an Agent to move between multiple NavMesh Surfaces in a Scene, they must be connected using a NavMesh Link. In the example Scene above, the blue and red NavMeshes are defined in different NavMesh Surfaces, with a NavMesh link connecting them. Note that when connecting NavMesh Surfaces: You can connect NavMesh Surfaces using multiple NavMesh Links. Both the NavMesh Surfaces and the NavMesh Link must have same Agent type. The NavMesh Link’s start and end point must only be on one NavMesh Surface - be careful if you have multiple NavMeshes at the same location. If you are loading a second NavMesh Surface and you have unconnected NavMesh Links in the first Scene, check that they do not connect to any unwanted NavMesh Surfaces."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshModifier.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshModifier.html",
    "title": "NavMesh Modifier component reference | FSM Unity Framework",
    "keywords": "NavMesh Modifier component reference Use the NavMesh Modifier component to adjust the behavior of a GameObject when the NavMesh is baked at runtime. The NavMesh Modifier component affects the NavMesh during the generation process only. This means the NavMesh is updated to reflect any changes to NavMesh Modifier components when you bake the NavMesh. Use the available properties to specify changes in behavior and any limits to those changes. To use the NavMesh Modifier component, add it to a GameObject as follows: Select the GameObject whose effect on the NavMesh you want to modify. In the Inspector, select Add Component, then select Navigation > NavMesh Modifier. The NavMesh Modifier component is displayed in the Inspector window. The NavMesh Modifier can also affect the NavMesh generation process hierarchically. This means that the GameObject the component is attached to, as well as all its children, are affected. In addition, you can place another NavMesh Modifier further down the hierarchy to override the NavMesh Modifier that is further up the hierarchy. To apply the NavMesh Modifier hierarchically, select the Apply To Children property. Note: The NavMesh Modifier component replaces the legacy Navigation Static setting which you could enable from the Objects tab of the Navigation window and the Static flags dropdown on the GameObject. The NavMesh Modifier component is available for baking at runtime, whereas the Navigation Static flags were available in the Editor only. The following table describes the properties available in the NavMesh Modifier component. Property Description Mode Specify whether to consider or ignore the affected GameObject(s). Add or Modify Object Consider the affected GameObject(s) when building the NavMesh. Remove Object Ignore the affected object(s) when building the NavMesh for the specified agent type. Affected Agents Specify which agents the NavMesh Modifier affects. For example, you can choose to have certain obstacles be ignored by specific agents. All Modify the behavior of all agents. None Exclude all agents from the modified behavior. Apply to Children Apply the configuration to the child hierarchy of the GameObject. To override this component's influence further down the hierarchy, add another NavMesh Modifier component. Override Area Change the area type for the affected GameObject(s). If you want to change the area type, select the checkbox then select the new area type in the Area Type dropdown. If you do not want to change the area type, clear the checkbox. Area Type Select the new area type you want to apply from the dropdown. Override Generate Links Force the NavMesh bake process to either include or ignore the affected GameObject(s) when you generate links. Generate Links Specify whether or not to include the affected GameObject(s) when you generate links. To include the GameObject(s) when you generate links in the NavMesh bake process, select this checkbox. To ignore the GameObject(s) when you generate links in the NavMesh bake process, clear this checkbox. Additional resources Create a NavMesh"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshModifierVolume.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshModifierVolume.html",
    "title": "NavMeshModifierVolume | FSM Unity Framework",
    "keywords": "NavMeshModifierVolume NavMesh Modifier Volume marks a defined area as a certain type (for example, Lava or Door). Whereas NavMesh Modifier marks certain GameObjects with an area type. NavMesh Modifier Volume allows you to change an area type locally based on a specific volume. To use the NavMesh Modifier Volume component, navigate to GameObject > AI > NavMesh Modifier Volume. NavMesh Modifier Volume is useful for marking certain areas of walkable surfaces that might not be represented as separate geometry, for example danger areas. You can also use It to make certain areas non-walkable. The NavMesh Modifier Volume also affects the NavMesh generation process, meaning the NavMesh has to be updated to reflect any changes to NavMesh Modifier Volumes. Parameters Property Function Size Dimensions of the NavMesh Modifier Volume, defined by XYZ measurements. Center The center of the NavMesh Modifier Volume relative to the GameObject center, defined by XYZ measurements. Area Type Describes the area type to which the NavMesh Modifier Volume applies. - Walkable (this is the default option) - Not Walkable - Jump Affected Agents A selection of agent types that the NavMesh Modifier Volume affects. For example, you may choose to make the selected NavMesh Modifier Volume a danger zone for specific Agent types only."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshObstacle.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshObstacle.html",
    "title": "NavMesh Obstacle component reference | FSM Unity Framework",
    "keywords": "NavMesh Obstacle component reference The NavMesh Obstacle component allows you to define obstacles that NavMesh Agents should avoid as they navigate the world (for example, barrels or crates controlled by the physics system). It contains properties that allow you to define the size, shape, and behavior of the obstacle. To use the NavMesh component you need to add it to a game object as follows: Select the GameObject you want to use as an obstacle. In the Inspector select Add Component, then select Navigation > NavMesh Obstacle. The NavMesh Obstacle component is displayed in the Inspector window. You can use this component to create NavMesh obstacles. For more information, see Create a NavMesh Obstacle. For more information on NavMesh obstacles and how to use them, see About NavMesh obstacles. The following table describes the properties available in the NavMesh Obstacle component. Property Description Shape Specify the shape of the obstacle geometry. Choose whichever one best fits the shape of the object. Box Select a cube-shaped geometry for the obstacle. Capsule Select a 3D oval-shaped geometry for the obstacle. Center Specify the center of the box relative to the transform position. Size Specify the size of the box. This property is visible only when Shape is set to Box. Center Specify the center of the capsule relative to the transform position. Radius Specify the radius of the capsule. This property is visible only when Shape is set to Capsule. Height Specify the height of the capsule. This property is visible only when Shape is set to Capsule. Carve Allow the NavMesh Obstacle to create a hole in the NavMesh. When selected, the NavMesh obstacle carves a hole in the NavMesh. When deselected, the NavMesh obstacle does not carve a hole in the NavMesh. Move Threshold Set the threshold distance for updating a moving carved hole. Unity treats the NavMesh obstacle as moving when it has moved more than the distance set by the Move Threshold. This property is available only when Carve is selected. Time To Stationary Specify the time (in seconds) to wait until the obstacle is treated as stationary. This property is available only when Carve is selected. Carve Only Stationary Specify when the obstacle is carved. This property is available only when Carve is selected. Additional resources About NavMesh obstacles - Details on how to use NavMesh obstacles. Create a NavMesh Obstacle - Guidance on creating NavMesh obstacles. Inner Workings of the Navigation System - Learn more about how NavMesh Obstacles are used as part of navigation. NavMesh Obstacle scripting reference - Full description of the NavMesh Obstacle scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshSurface.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMeshSurface.html",
    "title": "NavMesh Surface | FSM Unity Framework",
    "keywords": "NavMesh Surface The NavMesh Surface component represents the walkable area for a specific NavMesh Agent type, and defines a part of the Scene where a NavMesh should be built. To use the NavMesh Surface component, navigate to GameObject > AI > NavMesh Surface. This creates an empty GameObject with a NavMesh Surface component attached to it. A Scene can contain multiple NavMesh Surfaces. You can add the NavMesh Surface component to any GameObject. This is useful for when you want to use the GameObject parenting Hierarchy to define which GameObjects contribute to the NavMesh. Parameters Property Description Agent Type The NavMesh Agent type using the NavMesh Surface. Use for bake settings and matching the NavMesh Agent to proper surfaces during pathfinding. Default Area Defines the area type generated when building the NavMesh. - Walkable (this is the default option) - Not Walkable - Jump Use the NavMesh Modifier component to modify the area type in more detail. Generate Links If this option is enabled, objects collected by the surface will be considered to generate links during the baking process. See the Links Generation section for more information. Use Geometry Select which geometry to use for baking. - Render Meshes – Use geometry from Render Meshes and Terrains. - Physics Colliders – Use geometry from Colliders and Terrains. Agents can move closer to the edge of the physical bounds of the environment with this option than they can with the Render Meshes option. NavMesh Data (Read-only) Locate the asset file where the NavMesh is stored. Clear Bake Bake a NavMesh with the current settings. Use the main settings for the NavMesh Surface component to filter the input geometry on a broad scale. Fine tune how Unity treats input geometry on a per-GameObject basis, using the NavMesh Modifier component. The baking process automatically excludes GameObjects that have a NavMesh Agent or NavMesh Obstacle. They are dynamic users of the NavMesh, and so do not contribute to NavMesh building. Object collection Property Description Collect Objects Defines which GameObjects to use for baking. - All – Use all active GameObjects (this is the default option). - Volume – Use all active GameObjects overlapping the bounding volume. Geometry outside of the bounding volume but within the agent radius is taken into account for baking. - Children – Use all active GameObjects which are children of the NavMesh Surface component, in addition to the object the component is placed on. Include Layers Define the layers on which GameObjects are included in the bake process. In addition to Collect Objects, this allows for further exclusion of specific GameObjects from the bake (for example, effects or animated characters). This is set to Everything by default, but you can toggle options on (denoted by a tick) or off individually. Advanced Settings The Advanced settings section allows you to customize the following additional parameters: Property Description Override Voxel Size Controls how accurately Unity processes the input geometry for NavMesh baking (this is a tradeoff between speed and accuracy). Select the checkbox to enable. The default is unchecked (disabled). 3 voxels per Agent radius (6 per diameter) allows the capture of narrow passages, such as doors, while maintaining a quick baking time. For big open areas, using 1 or 2 voxels per radius speeds up baking. Tight indoor spots are better suited to smaller voxels, for example 4 to 6 voxels per radius. More than 8 voxels per radius does not usually provide much additional benefit. Override Tile Size In order to make the bake process parallel and memory efficient, the Scene is divided into tiles for baking. The white lines visible on the NavMesh are tile boundaries. The default tile size is 256 voxels, which provides a good tradeoff between memory usage and NavMesh fragmentation. To change this default tile size, check this checkbox and, in the Tile Size field, enter the number of voxels you want the Tile Size to be. The smaller the tiles, the more fragmented the NavMesh is. This can sometimes cause non-optimal paths. NavMesh carving also operates on tiles. If you have a lot of obstacles, you can often speed up carving by making the tile size smaller (for example around 64 to 128 voxels). If you plan to bake the NavMesh at runtime, using a smaller tile size to keep the maximum memory usage low. Minimum Region Area Allows you to cull away the small regions disconnected from the larger NavMesh. The process that builds the NavMesh does not retain the stretches of the mesh that have a surface size smaller than the specified value. Please note that some areas may not get removed despite the Minimum Region Area parameter. The NavMesh is built in parallel as a grid of tiles. If an area straddles a tile boundary, the area is not removed. The reason for this is that the area pruning step takes place at a stage in the build process when the surrounding tiles are not accessible. Build Height Mesh Enables the creation of additional data used for determining more accurately the height at any position on the NavMesh. See the Height Mesh section for more information. This option is available starting with Unity 2022.2.0f1."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMoveToClickPoint.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMoveToClickPoint.html",
    "title": "Moving an Agent to a Position Clicked by the Mouse | FSM Unity Framework",
    "keywords": "Moving an Agent to a Position Clicked by the Mouse This script lets you choose the destination point on the NavMesh by clicking the mouse on the object’s surface. The position of the click is determined by a raycast, rather like pointing a laser beam at the object to see where it hits (see the page Rays from the Camera for a full description of this technique). Since the GetComponent function is fairly slow to execute, the script stores its result in a variable during the Start function rather than call it repeatedly in Update. // MoveToClickPoint.cs using UnityEngine; using UnityEngine.AI; public class MoveToClickPoint : MonoBehaviour { NavMeshAgent agent; void Start() { agent = GetComponent<NavMeshAgent>(); } void Update() { if (Input.GetMouseButtonDown(0)) { RaycastHit hit; if (Physics.Raycast(Camera.main.ScreenPointToRay(Input.mousePosition), out hit, 100)) { agent.destination = hit.point; } } } }"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMoveToDestination.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavMoveToDestination.html",
    "title": "Telling a NavMeshAgent to Move to a Destination | FSM Unity Framework",
    "keywords": "Telling a NavMeshAgent to Move to a Destination You can tell an agent to start calculating a path simply by setting the NavMeshAgent.destination property with the point you want the agent to move to. As soon as the calculation is finished, the agent will automatically move along the path until it reaches its destination. The following code implements a simple class that uses a GameObject to mark the target point which gets assigned to the destination property in the Start function. Note that the script assumes you have already added and configured the NavMeshAgent component from the editor. // MoveDestination.cs using UnityEngine; public class MoveDestination : MonoBehaviour { public Transform goal; void Start () { NavMeshAgent agent = GetComponent<NavMeshAgent>(); agent.destination = goal.position; } }"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationOverlay.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationOverlay.html",
    "title": "AI Navigation overlay reference | FSM Unity Framework",
    "keywords": "AI Navigation overlay reference The AI Navigation overlay allows you to control the display of NavMesh surfaces, agents, and GameObjects in the Scene view. You can use it to help you debug any issues with AI Navigation and pathfinding. The Navigation overlay docks to the lower right corner of the Scene view by default. Surfaces This section controls the way NavMesh Surface instances are displayed. The following table describes the controls available in the Surfaces section of the overlay. Control Description Show Only Selected Display only the surfaces part of the current scene selection hierarchy. You can set the opacity of the selected and non-selected surfaces in the Preferences window. For more details, refer to AI Navigation preferences. Show NavMesh Display navigation meshes for the relevant surfaces. The colors used to display this mesh are the ones defined for the area types. Show HeightMesh Display HeightMeshes (surface precise elevation information) for the relevant surfaces. Agents This section controls the displayed information for the currently selected NavMesh Agents. The following table describes the controls available in the Agents section of the overlay. Control Description Show Path Polygons Display the NavMesh polygons part of the agent's path in a darker color. Show Path Query Nodes Display the path nodes explored during the pathfinding query in yellow. Show Neighbors Display the collision avoidance neighbors (dynamic obstacles) relative to the agent. Show Walls Display the collision avoidance walls (static obstacles) for an agent. Show Avoidance Show the different positions sampled during the collision avoidance process. Obstacles This section controls the displayed information for the currently selected NavMesh Obstacles. The following table describes the controls available in the Obstacles section of the overlay. Control Description Show Carve Hull Display the convex shape that is used to carve the NavMesh. Additional resources Overlays - How to use and work with overlays."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationOverview.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationOverview.html",
    "title": "Navigation Overview | FSM Unity Framework",
    "keywords": "Navigation Overview This section provides details on how to create NavMeshes for your scene, NavMesh Agents, NavMesh Obstacles and Off-Mesh Links. It contains the following topics: Create a NavMesh Create a NavMesh Agent Create a NavMesh Obstacle Create an Off-mesh Link Using NavMesh Agent with Other Components Advanced Navigation How-tos"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationSystem.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationSystem.html",
    "title": "Navigation System in Unity | FSM Unity Framework",
    "keywords": "Navigation System in Unity This section describes the key concepts necessary to use AI Navigation in Unity. It contains the following topics: Topic Description Inner Workings of the Navigation System Understand how the different elements of the AI Navigation system work together. About Agents Learn about NavMesh agents. About Obstacles Learn about NavMesh obstacles. Navigation Areas and Costs Understand the purpose of navigation areas and why you would use one type of area over another."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationWindow.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/NavigationWindow.html",
    "title": "Navigation window reference | FSM Unity Framework",
    "keywords": "Navigation window reference Use the Navigation window to specify the types of NavMesh agents and areas used in your scenes. To get to the Navigation window, in the main menu go to Window > AI > Navigation. Agents tab The Agents tab contains properties that allow you to define the type of agents that you use in your scenes. Property Description Agent Types Select an agent type to modify. Click the \"+\" icon to add an agent type. Click the \"-\" icon to remove the currently selected agent type. Name Specify the name of the type of agent. Radius Define how close the agent center can get to a wall or a ledge. Height Specify the height of this type of agent in Unity units. Step Height Specify the maximum step height that this type of agent can climb. Max Slope Specify how steep of a ramp the agent can walk up. Type a value, in degrees, in the text box or drag the slider to adjust the value. Generated Links The following table describes the properties that define the limits of this agent type with respect to generated links. Property Description Drop Height Specify the maximum height from which this agent type can jump down. Jump Distance Specify the maximum distance of jump-across links for this agent type. Areas tab The Areas tab contains properties that allow you to specify how difficult it is to walk across the different area types used in your scenes. There are 29 custom area types, and 3 built-in area types: Walkable is a generic area type which specifies that the area can be walked on. Not Walkable is a generic area type which prevents navigation. It is useful for cases where you want to mark a certain object to be an obstacle, but you don't want to put a NavMesh on top of it. Jump is an area type that is assigned to all auto-generated OffMesh links. The following table describes the properties available on the Areas tab. Property Description Name Specify a name for the area type. Cost Specify the cost of traveling across this area. Costs are multipliers applied to the distance traveled across an area. A cost of 2 means an area is twice as difficult to cross as an area with a cost of 1. The default value is 1. Additional resources About NavMesh agents Create a NavMesh agent Navigation Areas and costs"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/OffMeshLink.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/OffMeshLink.html",
    "title": "OffMesh Link | FSM Unity Framework",
    "keywords": "OffMesh Link OffMeshLink component allows you to incorporate navigation shortcuts which cannot be represented using a walkable surface. For example, jumping over a ditch or a fence, or opening a door before walking through it, can all be described as OffMesh links. Properties Property Function Start Object describing the start location of the OffMesh Link. End Object describing the end location of the OffMesh Link. Cost Override If value is positive, use it when calculating path cost on processing a path request. Otherwise, the default cost is used (the cost of the area to which this game object belongs). If the Cost Override is set to the value 3.0, moving over the OffMesh link will be three times more expensive than moving the same distance on a default NavMesh area. The cost override becomes useful when you want to make the agents generally favor walking, but use the OffMesh link when the walk distance is clearly longer. Bi-Directional If enabled, the link can be traversed in either direction. Otherwise, it can only be traversed from Start to End. Activated Specifies if this link will used by the pathfinder (it will just be ignored if this is set to false). Auto Update Positions When enabled, the OffMesh link will be reconnected to the NavMesh when the end points move. If disabled the link will stay at its start location even if the end points are moved. Navigation Area Describes the navigation area type of the link. The area type allows you to apply a common traversal cost to similar area types and prevent certain characters from accessing the OffMesh Link based on the agent’s Area Mask. Additional resources Creating an OffMesh Link – workflow for setting up an OffMesh link. OffMesh Link scripting reference - full description of the OffMesh Link scripting API."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Reference.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Reference.html",
    "title": "Navigation user interface | FSM Unity Framework",
    "keywords": "Navigation user interface The Navigation user interface consists of the Navigation window, AI Navigation overlay, AI Navigation Editor Preferences and several components for building a NavMesh. The NavMesh building components provide you with additional controls that allow you to generate and use NavMeshes at runtime and in the Unity Editor. Topic Description Navigation window Define the types of agents and areas in your game world. AI Navigation preferences Customize the navigation debug visualization. AI Navigation overlay Display navigation debug visualization. NavMesh Agent component Define the characters that you want to navigate the game world. NavMesh Surface component Build and enable a NavMesh surface for one type of Agent. NavMesh Modifier component Adjust the behavior of a GameObject when the NavMesh is baked at runtime. NavMesh Modifier Volume component Control the generation of NavMesh area types based on volume. NavMesh Obstacle component Define moving obstacles that NavMesh Agents avoid as they navigate your game world. NavMesh Link component Connect the NavMesh surfaces for each type of agent. OffMesh Link component Create shortcuts between NavMeshes which cannot be represented by a walkable surface."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Samples.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/Samples.html",
    "title": "Navigation Samples | FSM Unity Framework",
    "keywords": "Navigation Samples The following sample scenes are included with the AI Navigation package: Multiple Agent Sizes: Demonstrates how a different radius on an agent type can change the way agents navigate through the same scene. Drop Plank: Demonstrates dynamically changing walkable paths by allowing the player to add walkable planks by pressing space. Free Orientation: Demonstrates a controllable agent that can walk on a tilted plane. Sliding Window Infinite: Demonstrates a controllable agent that can walk through a dynamically created world that gets updated to simulate infinity as the agent walks through it. The NavMesh is only built in some set bounds that follow the agent. Sliding Window Terrain: Demonstrates a controllable agent that can walk through a terrain for which the NavMesh is only generated within a set distance of the agent. Modify Mesh: Demonstrates agents walking aimlessly on planes whose mesh can be modified dynamically by the player. Dungeon: Demonstrates a controllable agent that can walk through a maze generated from pre-baked tiles that connect to each other at runtime. The link traversal animation can be modified with some presets (teleport, normal speed, parabola, curve). Height Mesh: Demonstrates two agents walking down stairs. The environment on the left uses NavMeshSurface with a Height Mesh which allows the agent to snap to each step in the stairs as it goes down. The environment on the right uses a NavMeshSurface with no Height Mesh; the agent simply slides down the stairs."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "Navigation and Pathfinding What's new Upgrade Navigation System in Unity Inner Workings of the Navigation System About Agents About Obstacles Navigation Areas and Costs Navigation overview Create a NavMesh Create a NavMesh agent Create a NavMesh obstacle Create an OffMesh link Use NavMesh Agents with other components Advanced navigation how-tos Tell a NavMesh agent to move to a destination Move an agent to a position clicked by the mouse Make an agent patrol between a set of points Couple animation and navigation Navigation interface AI Navigation editor preferences AI Navigation overlay Navigation window NavMesh Agent component NavMesh Link component NavMesh Modifier component NavMesh Modifier Volume component NavMesh Obstacle component NavMesh Surface component OffMesh Link component Navigation Samples Glossary"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/UpgradeGuide.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/UpgradeGuide.html",
    "title": "Upgrade projects for use with AI Navigation package | FSM Unity Framework",
    "keywords": "Upgrade projects for use with AI Navigation package Navigation and Pathfinding in Unity is handled by the AI Navigation package as of Unity 2022.2. If you have projects that were created with the Navigation feature in previous versions of Unity, the AI Navigation package is automatically installed and added to your project. You can then do one of the following: Continue to use your projects as they are Convert your projects to use the new package Remove old component scripts If your project uses the NavMesh Surface, NavMesh Modifier, NavMesh Modifier Volume or NavMesh Link components defined by scripts downloaded from Unity’s NavMeshComponents GitHub repository, then remove those scripts and any associated files before you add the AI Navigation package to your project. If you don’t remove these scripts, you might get conflicts and errors related to these components in the Console. The new components mirror the same behavior as the old components do in your project except when using the following components: The NavMesh Surface component now includes an option to use only the objects that have a NavMesh Modifier in the baking process. You can now specify whether or not to apply the NavMesh Modifier component to child objects in the hierarchy. Convert your project If you want to use the new package you need to convert your project(s). As part of the conversion process, the NavMesh Updater makes the following changes: Any NavMesh that was previously baked and embedded in the scene is now referenced from a NavMeshSurface component created on a new GameObject called Navigation. Any object that was marked with Navigation Static now has a NavMeshModifier component with the appropriate settings. To convert your project do the following: In the main menu go to Window > AI > NavMesh Updater. In the NavMesh Updater window, select which kind of data to convert. Click Initialize Converters to detect and display the types of data you selected. Select the data you want to convert. Click Convert Assets to complete the conversion. Create new agent types If the NavMeshes in different scenes are baked with different agent settings then you need to create new agent types to match those settings. To create the agent types do the following: In the main menu go to Window > AI > Navigation. Select Agents. Create new entries and specify the relevant settings. Assign new agent types When you have created the new agent types you then need to assign them as follows: Assign the newly created agent types to their respective NavMeshSurfaces in the Navigation created for that scene. Assign the agent types to the NavMeshAgents intended to use that NavMesh. To find the settings that were used for each existing NavMesh, select the NavMesh .asset file in the Project window. The NavMesh settings will be displayed in the Inspector."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/index.html",
    "title": "AI Navigation | FSM Unity Framework",
    "keywords": "AI Navigation The navigation system allows you to create characters that can intelligently move around the game world. These characters use navigation meshes that are created automatically from your Scene geometry. Dynamic obstacles allow you to alter the navigation of the characters at runtime, while offmesh links let you build specific actions like opening doors or jumping over gaps or down from a ledge. This section describes Unity's navigation and pathfinding systems in detail. The following table describes the main topics of the AI Navigation package documentation. Topic Description What's new See what's changed in the latest version of the AI Navigation package. Upgrade Convert your projects to work with the new navigation system. Navigation System Understand the key concepts necessary to use AI Navigation in Unity. Navigation Overview Create NavMeshes, agents, links, and obstacles with this package. Navigation Interface Learn about the interface of the Navigation components in this package. Samples Learn about the sample projects included with this package. Glossary View AI Navigation terminology definitions. Additional resources Navigation tutorial Unity Knowledge Base"
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/whats-new.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Documentation~/whats-new.html",
    "title": "What's new in AI Navigation version 1.1.1 | FSM Unity Framework",
    "keywords": "What's new in AI Navigation version 1.1.1 This is a new package release. Summary of changes in the AI Navigation package version 1.1.1 compared to functionality that existed previously. The main updates in this release include: Added New options for the NavMesh Surface to bake a HeightMesh and to generate links automatically. New option in the NavMesh Surface to use only the source objects marked with NavMesh Modifiers. New option in the NavMesh Modifier to act on child objects as well. Updated The Navigation window becomes available only when the package is installed. Removed the Bake and Object tabs from the Navigation window as they are no longer necessary. However, you can still find them in the Navigation (Obsolete) window. The Navigation Static and OffMesh Link Generation options are no longer available in the Static Editor Flags drop-down menu. However, you can still find them in the Navigation (Obsolete) window, on the Object tab. Improved a few of the included samples. For a full list of changes and updates in this version, see the AI Navigation package changelog."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.ai.navigation copyright © 2016 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects (see https://unity3d.com/legal/licenses/unity_companion_license). Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/README.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/README.html",
    "title": "About AI Navigation | FSM Unity Framework",
    "keywords": "About AI Navigation The AI Navigation package makes available the following tools for working with navigation in Unity: The Navigation window The NavMesh Updater window The AI Naigation overlay in the scene view The visualization in the scene view of the NavMeshes, links, obstacles, agents and their gizmos used for editing The authoring part of the components that exist in the com.unity.modules.ai core module (i.e. NavMeshAgent, NavMeshObstacle, OffMeshLink) Four components that comprise the high level controls for creating data for the navigation system: NavMeshSurface – for building and enabling a NavMesh surface for one agent type. NavMeshModifier – affects the NavMesh generation of NavMesh area types, based on the transform hierarchy. NavMeshModifierVolume – affects the NavMesh generation of NavMesh area types, based on volume. NavMeshLink – connects same or different NavMesh surfaces for one agent type. Using AI Navigation For detailed information on how to use the package, see the User manual. You can add it to your project by installing it from the list in the Package Manager window or by specifying its name com.unity.ai.navigation. Alternatively, you can add an entry with the package name directly in the project manifest. The package is available as of Unity 2022.2. Notice on the Change of License With effect from 8th December 2020 this package is licensed under the Unity Companion License for Unity-dependent projects. Prior to the 8th December 2020 this package was licensed under MIT and all use of the package content prior to 8th December 2020 is licensed under MIT."
  },
  "Library/PackageCache/com.unity.ai.navigation@1.1.5/Samples~/README.html": {
    "href": "Library/PackageCache/com.unity.ai.navigation@1.1.5/Samples~/README.html",
    "title": "Navigation Samples | FSM Unity Framework",
    "keywords": "Navigation Samples The Navigation Samples showcase various usages of the NavMesh. They contain eight samples: Multiple Agent Sizes: Demonstrates how a different radius on an agent type can change the way agents navigate through the same scene. Drop Plank: Demonstrates dynamically changing walkable paths by allowing the player to add walkable planks by pressing space. Free Orientation: Demonstrates a controllable agent that can walk on a tilted plane. Sliding Window Infinite: Demonstrates a controllable agent that can walk through a dynamically created world that gets updated to simulate infinity as the agent walks through it. The NavMesh is only built in some set bounds that follow the agent. Sliding Window Terrain: Demonstrates a controllable agent that can walk through a terrain for which the NavMesh is only generated within a set distance of the agent. Modify Mesh: Demonstrates agents walking aimlessly on planes whose mesh can be modified dynamically by the player. Dungeon: Demonstrates a controllable agent that can walk through a maze generated from pre-baked tiles that connect to each other at runtime. The link traversal animation can be modified with some presets (teleport, normal speed, parabola, curve). Height Mesh: Demonstrates two agents walking down stairs. The environment on the left uses NavMeshSurface with a Height Mesh which allows the agent to snap to each step in the stairs as it goes down. The environment on the right uses a NavMeshSurface with no Height Mesh; the agent simply slides down the stairs. Note that some of these samples require that the Packages/manifest.json file of your project references the following default modules: \"com.unity.modules.physics\": \"1.0.0\", \"com.unity.modules.terrain\": \"1.0.0\", \"com.unity.modules.terrainphysics\": \"1.0.0\" Introduction to NavMesh The Navigation package allows you to set up pathfinding AI in your Unity project. Two fundamental concepts of pathfinding are (1) agents and (2) world representation. An agent is a game entity that travels autonomously between two points in a scene. In Unity, a GameObject can be turned into a navigation agent by adding a NavMeshAgent component to it. World representation is what allows the pathfinding program of an agent to understand the traversable surfaces of a world. It is a simplification of a 3D world. In Unity, a traversable surface is represented as a mesh of polygons which we refer to as NavMesh. To convert some or all of the geometry in your scene into a surface that is traversable by an agent, you can use the NavMeshSurface component. However, you must also generate the data of the NavMeshSurface by using the Bake button in the Inspector. The process of baking is what actually creates a representation of the geometry in your scene that agents and their pathfinding program can understand. Whenever there are modifications done to the scene's geometry that can impact the navigation, the related NavMeshSurface component must be rebaked. The baking process is not done automatically because it can be a long process depending on the size and complexity of the input geometry. Note that baking cannot be done from the Inspector during Playmode. In order for an agent to move, it must know its destination. In Unity, the destination of a NavMeshAgent can be set through code with the destination property or the SetDestination() method. You can find an example of this in the ClickToMove script. For more information, refer to the AI Navigation package manual. Agent Types The following agent types are created and used by the samples: 1. Name: Humanoid for Navigation Sample Radius: 0.5 Height: 2.0 Step Height: 0.75 Max Slope: 45 2. Name: Ogre for Navigation Sample Radius: 1.0 Height: 2.0 Step Height: 0.4 Max Slope: 36"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.8.10] - 2023-10-25 Fixed Fixed the Burst Inspector not displaying target methods if namespace/class contained the method name. Linking libstdc++/libc++ statically on HMI platforms Fixed an issue that caused an empty variable to be returned if it is between a zero initializer Native debug information would fail to reflect the contents of parameters to functions. For native debug information, type symbols can now be referenced using :: separator between namespaces (C++ style) - E.g. Example.Type becomes Example::Type. Fixed that changing certain player build platform settings (like SDK version) would not trigger Burst to recompile Fixed invalid burst string formats leading to internal compiler error. Fixed QNX player builds on 32-bit ARM Fixed an error thrown by the Burst Inspector when opening a non-static job with special characters in its name Added Added support for System.Math functions Acosh, Asinh, Atanh, Cbrt, CopySign, Log2, SinCos, FusedMultiplyAdd, and ILogB Removed Remove all code specific to DOTS Runtime Changed Use mimalloc as our native allocator on Windows to speed up concurrently executing LLVM work Known Issues [1.8.9] - 2023-09-22 Fixed Added Removed Changed Known Issues [1.8.8] - 2023-07-24 Fixed Fixed error when trying to use direct call to a nested protected class Fixed that converting a negated unsigned type to a float would produce a mismatching value in Burst versus .Net/Mono Fixed that the Burst Inspector handled negation of unsigned types differently than .Net for static readonly fields or static constructors Fixed Burst sometimes returning wrong value for static readonly fields or static constructors. Fixed a possible source of invalid alignment, avx2 storing to stack was given a slot with the wrong alignment. Fixed System.NotImplementedException: Unimplemented Instruction Extension Tail_ error when the code contained tail-calls Fixed wrong alignment for v128 when doing an indirect access. Fix compiler crash when compiling different assemblies that define methods or types with the exact same name and namespace Fixed using Armv9 target in the Burst Inspector not formatting the assembly. Fixed that jobs wouldn't be Burst compiled for player builds with high stripping Fixed burst not being able to find external function leading to crashing the Editor Prevented Burst emitting errors even when Burst was disabled via the --burst-disable-compilation command line option Under some conditions (if the error in compilation occurred in a location that didn't have valid debug information), building a player might not generate any files, and not display any errors. Fixed uint to float conversion edge-case Fixed syntax highlight missing for some ARM instructions. Added Added support for default interface methods Added ability to support hashing against different target frameworks. Added support for string interpolation in exception messages Removed Changed Fixed a compile-time performance regression in 1.8.7 that could result in slower Burst compilation and increased memory usage during compilation Direct call is now correctly disabled for methods that are decorated with both [BurstCompile] and [UnmanagedCallersOnly] attributes (such methods shouldn't be called directly from managed code) Add support for Math.Clamp (this API is available when Api Compatibility Level is set to .NET Standard 2.1) Known Issues [1.8.7] - 2023-06-07 Added Add proper license attribution for MUSL and SLEEF libraries. Removed Changed Changed focus for initial Burst Inspector focus to actually get the search hit in focus. Fixed Fix QNX builds using the qnxInstallationPath editor build setting Fixed an issue causing source file handles to be left open (preventing saving in an ide, if in debug scripting mode and the file is used in the burst path). Fixed an issue when targeting multiple cpu architectures (e.g. SSE2 & AVX2) that under some circumstances would lead to code attempting to execute paths not designed for that cpu. Fixed an issue that caused builds to fail due to the System.Diagnostics.Tracing assembly not being found Fixed a warning that occurred when opening Burst AOT Settings while in Play Mode Fixed a hashing error that could occur when an operator overload method is used as a Burst entry point Fixed crash on linux if debug logging was enabled. Fixed \"The specified path is not of a legal form (empty)\" error Calls to methods with multiple [Conditional] attributes are now kept if any one of the conditions are met Fixed Burst implementation of IntPtr.GetHashCode() being different than .Net Fixed an issue that caused the digits and MidpointRounding parameters of Math.Round be ignored Known Issues [1.8.4] - 2023-03-20 Fixed Fixed possible deadlock when compiling after domain reload Fixed incorrect codegen when having multiple try-finally blocks inside another try-finally block (for example from foreach loops) Domain completed stall when switching between debug/release scripting modes when burst compilation is needed for items in the new domain. Fixed \"An item with the same key has already been added\" compiler error that could occur when duplicate field names were present in obfuscated assemblies Fixed \"Failed to find entry-points: Mono.Cecil.AssemblyResolutionException: Failed to resolve assembly\" error that was displayed when Burst tried to compile an assembly that had C# compilation errors Fixed code-gen issue where side-effects before a conditional throw would be ignored Burst managed breakpoints might fail to work, after a domain reload. Fixed that some changes made to versioned assemblies wouldn't get picked up and compiled by Burst Fixed line highlight and register highlight not clearing when Burst Inspector settings change. Fixed Burst compilation error relating to UnityEngine.Assertions.Assert.Fail when doing player builds with high stripping settings Fixed a BadImageFormatException error that could occur in some player builds Neon intrinsics: fixed default target CPU for Arm Mac Standalone builds Fixed MethodDecoderException when trying to call CompileFunctionPointer on a nested static method Fixed incorrect pdb path for AoT dll libraries Fixed inaccurate stacktraces when throwing an exception from Burst in specific cases Fixed \"An item with the same key has already been added\" hashing error that could occur if obfuscators changed nested type names to have the same name and different namespaces Added Add support for ChromeOS in Unity versions 2020.3 and 2019.4. Windows/ARM64 targeting support Removed Changed Changed Burst Inspector input handling so that arrow-keys can be used to select in search boxes. Made Burst Inspector's target job load asynchronous. Known Issues [1.8.3] - 2023-01-30 Added Added selection of line and highlight of selected line and selected lines register usage. FunctionPointer ()::Invoke usage is now checked and patched to ensure the calling convention is compatible with burst. Added SIMD smell test to the Burst Inspector, highlighting ARM or x86-64 SIMD instruction differently depending on whether they work for packed or scalar inputs. Added a toggle for filtering out \".Generated\" jobs from the Burst Inspector target job list. Added a Burst AOT setting for the kind of debug information generated for player builds Fixed Fixed AoT linking error on Windows Link based linkers when file paths (typically user names/home folders) contain non-ASCII characters. Fixed ARM vector registers not being highlighted. Fixed Burst Inspector sometimes throwing ArugmentOutOfRangeException when copying without color-tags from assembly that is colored. Fixes error when calling direct call method from background thread without having previously called a BurstCompiler API from the main thread Fixes \"Plain Without Debug Information\" outputting assembly with debug information. Fixed a hashing error that could occur when a struct implements a generic interface multiple times with different generic parameters An issue that could cause function pointers to point to the wrong burst function, if a domain reload occurs and a compilation started before the reload, completes soon after. Fixed bug in a small set of managed fallback versions of intrinsics, where the bitwise representation of float values would not be maintained Fixed player build error that could occur if the project contains an assembly whose name doesn't match the assembly filename Crashes on 32bit cpus when an entry point with byvalue paramaters was called, when using dispatch (multiple supported cpu targets). Fixed module verification errors when using overloaded functions as function pointers Fixed an issue the definition order of overloaded methods with function pointer parameters would decide which overload was actually being used Fixed compiler AccessViolationException that could occur when compiling two or more types with the same name but different source assemblies Burst now updates its list of assembly paths if they change, for instance - adding packages that contain precompiled assemblies. Fixed a stall that could occur at Editor shutdown Fixed BC1361 error when trying to compile large static readonly arrays. Fixed compilation error when using CompileFunctionPointer from Burst in code compiled with Roslyn on .NET 7+ Fixed a BadImageFormatException error that could occur in DOTS Runtime builds Fixed the inspector job tree view splitting jobs, with '.' in their parameters. Fixed internal compiler error when implcitly converting an array to a Span Fixed managed fallback implementation of Sse4_2.cmpestrs \"LLVM IR Optimisation Diagnostics\" tab in Burst Inspector was blank if \"Native Debug Mode Compilation\" was enabled; this is now fixed Fixed burst tree view items leading to wrong job if some jobs where hidden from view by filter or similar. Fixed \"Callee/caller attribute ABI did not match!\" error that could occur in certain player builds when calling an entry point that had at least one struct-by-value parameter Fixed namespace collision that could occur between Unity.Burst.Cecil.dll and the com.unity.nuget.mono-cecil package Enum values cast to integers in a format string previously output the enum type name; now the integer value is correctly output Fix Burst compilation on QNX Arm Fixed visual artifact in Burst Inspector, where block of enhanced code was cut at the bottom. Fixed compiler crash when invoking FunctionPointers based on a generic delegate in DOTS Runtime Fixed internal compiler error that occurred when creating debug metadata from certain obfuscated dlls Fixed \"Assertion failed on expression: \"exception == SCRIPTING_NULL\" errors and editor crash when the project path contained multi-byte Unicode characters Changed Changed burst inspector source location comments from \"===\" to either \";\" or \"#\" depending on the given assembly kind. Changed horizontal code focus in the Burst Inspector to only scroll when branches fill more than half the space Changes so target job list in the Burst Inspector is a fold-able/expandable tree view, instead of a simple list. Improved how optimisation remarks are displayed in the \"LLVM IR Optimisation Diagnostics\" tab in Burst Inspector to make them more useful Burst now only generates full debug information when \"Native Debug Mode Compilation\" and script debug information is enabled Removed Known Issues [1.8.2] - 2022-11-18 Added Changed Fixed Fixed an issue where sometimes the wrong body of an overloaded entrypoint would be used Failing to link if ; in path Fixed Burst being disabled in the Editor after changing script optimization mode (i.e. from Release to Debug or vica-versa) C# Debug information was incorrectly ignored for methods that had multiple source files. This caused native debug information to be dropped for code generated methods, and prevented the disabling of burst for such methods when a managed break point was set in Unity 2022.2 or greater (see https://docs.unity3d.com/Packages/com.unity.burst@1.8/manual/debugging-profiling-tools.html). Pointer addition of byte would incorrectly sign extend the byte, instead of zero extend. lib_burst_generated.txt was not being output. Player stripping levels higher than minimal would fail to build with burst if they used String.Formatters, String Copy, or BurstDiscard. Fixed error when building player caused by calling an entrypoint method from within other Burst-compiled code iOS/tvOS burst libraries are now using explicit min os version, as configured in player settings. Fixed Burst AOT setting \"Enable Optimizations\" not being applied in player builds Fixed player builds not being recompiled when changing only Burst AOT settings (and changing nothing else) in Unity 2022.2+ Error caused by the MonoDebuggerHandling.dll requiring VCRuntime to be installed. Removed Known Issues [1.8.1] - 2022-10-12 Added Added a custom lld wrapper, to save package space in transit and on disk. Added hover box information for assembly instructions. Changed Upgraded Burst to use LLVM Version 14.0.6 by default, bringing the latest optimization improvements from the LLVM project. Ensured our executables and libraries on macOS and Linux are stripped to reduce package size. Changed how we handle domain reloads within Burst to avoid paying a 250ms cost on each domain reload when using Burst. With the relaxation in Unity 2022.2 or newer that we can call CompileFunctionPointer from a background thread, we now use this mechanism in Burst to handle Direct Call methods, resulting in a cost saving during Domain Reload. Added a categorized index of Neon intrinsics supported in Burst to the Manual Changed the documentation so that it is super clear that exceptions in player builds cause the application to abort. Fixed Fixed a compiler crash that could occur with code that followed the pattern Debug.Log($\"{variable}\") Compiling with line only debug information could cause a compiler crash on certain platforms PDB path associated with windows player dll had the wrong filename, resulting in broken symbols. Fixed documentation issues with Neon intrinsics where the comparison operation would not match the actual one Fixed bug that could occur when swapping large structs by value Fixed \"Unable to resolve type T. Reason: Unknown.\" error when accessing a field of a struct referenced via a pointer behind a reference. Fixed some arm64 instructions not being labelled as instructions. If burst is disabled, and an assembly is changed, burst won't recompile that assembly once burst is re-enabled. Removed Known Issues [1.8.0] - 2022-09-13 Added Added experimental atomic and/or operations to Burst. Changed math.fmod in combination with a Burst job compiled with FloatPrecision.Low will now generate a more optimized low-precision version of the function. Burst now respects the checkbox \"Enable Armv9 Security Features for Arm64\" in the Player settings, making Android builds generate PAC/BTI instructions if enabled. In Burst AOT Settings, only the relevant CPU Architectures dropdowns for the current build target and architecture are now displayed The callstack of the invalid external call is now included when reporting BC1091 Changed so code is focused when branch arrows are present. Changed so Burst reported errors are not collapsible. Removed Fixed An Internal Compiler Error that could occur if a function that requires a struct ret (due to ABI) has been discarded by other logic. Fixed a bug with locally declared array variables in functions where storing null into them could cause invalid codegen. Fixed a bug in Burst player builds where sufficiently complicated Bursted code could cause a deadlock deep within LLVM. Fixed that UWP builds wouldn't respect the specified \"Target SDK Version\" and \"Visual Studio Version\" settings Fixed Burst inspector sometimes freezing when selecting between blocks. Fixed the Burst Inspector sometimes becoming unresponsive when selecting text. Fixed a race condition with the Burst log timings such that previously reported results could be included in subsequently reported timings. Fixed the managed fallbacks for bzhi and bextr to match what the native hardware instructions do. Fixed a bug in the static readonly constant expression evaluation (what we call the IL interpreter) whereby it would not truncate unsigned integers correctly. Fixed that compilation would have full debug info forced on Fixed incorrect code-gen when a function is both used normally and as a function-pointer Known Issues The PDB path associated with the Windows Player dll is incorrect, resulting in broken symbols. [1.8.0-pre.2] - 2022-08-03 Fixed Fixed hashing bug that could occur when a function pointer type is used in a method parameter Fix selection and copying of folded blocks Fixed hashing error that could occur in the presence of multiple synthesized explicit interface implementations with the same name and signature Fixed a compiler crash if users used __refvalue or __arglist in Burst. Neither of these are supported, but now we will nicely tell you via a compiler error that they aren't supported. Fixed a compiler error when trying to acquire the function pointer of a generic function from Bursted code. Fix some ARM branch instructions not being processed as such. Using a function only through a C# function pointer could cause a crash Whitespace changes in ILPP'd assemblies would not be detected. Issue where a warning could be generated about the debug information version mismatching warning: ignoring debug info with an invalid version (0) during link. Interface methods where not being hashed correctly for constrained types, which would result in burst failing to recompile code that had changed in an implementation class. Fixed a safety check bug with Span/ReadOnlySpan and Slice(start, length) where if start + length was equal to the Length of the original span, the safety check would incorrectly report an out-of-bounds access. Linking issue when exports differ only by module. Disabling Burst from the command line via --burst-disable-compilation no longer results in Burst errors when building a player for Android Corrupted binary could be produced on M1 if there was not enough space for UUID+codesign injection. ;'s in paths would cause burst to fail. Note - Also requires a fix in the Editor, so if your project has ;'s in its path, the workaround is to remove the ; from the folder name for now. Fixed error when compiling assemblies with spaces in their names Fixed access violation race condition bug Fixed a bug where static fields in generic types could in some situations be initialized with the incorrect value Fixed last line in Burst Inspector not being select-able using the mouse cursor. Fix error that occurs with a specific formulation of IL, using xx with an early out escape and unbalanced calculation stack. (Object reference not set to an instance of ... in CollectBlock.ToVisitOrder) Changed Changed burst inspector toggles to popup menus. Removed label from burst inspector popup menu into the menu itself. Used explicit namespace for UnityEditor.PackageManager.Events to avoid conflicts. Improved \"hashing\" performance. This is the part of Burst that determines whether anything significant has changed in .NET assemblies, and therefore whether that assembly to be compiled. Entry point function names weren't always included in crash callstacks; now they are Search pattern from previous job is not carried over to the new. Changed so block of 1 line cannot be folded in the Burst Inspector Added Setting a breakpoint in an attached managed debugger (Rider/VS Unity Debugger...) on a method that is burst compiled, will switch off the burst code path for that method, allowing it to be debugged as normal. Added toggle to filter Unity tests on and off. Assembly is now searchable either through CTRL + f or the contex menu associated with the inspector view. Search options include case sensitivity, whole word match, and regex. Intrinsic support for UnsafeUtility.IsNativeContainerType Added an actual definition for HPC# in the package docs. Check that calling convention is correctly set to Cdecl for functions whose addresses are taken via ldftn. Added focus on current job in the burst inspector. Added copy to burst inspector, which ignores underlying color tags. Removed Known Issues [1.8.0-pre.1] - 2022-05-06 Changed Always preserve frame pointers in Burst. This results in a neglible performance hit (less than 0.5% in benchmarks), but ensures that stack recovery for stack traces is always possible. Class libraries are now built with netstandard 2.0 The minimum Xcode version to build for iOS, iPadOS, and tvOS with Burst is now 12.0.0. Upgraded Burst to use LLVM Version 13.0.1 by default, bringing the latest optimization improvements from the LLVM project. Fixed \"error while hashing\" message that could appear during compilation Made Burst explicitly check for any compilation requests that came from AssemblyBuilder, and do not compile these with Burst. These exist outside the normal compilation pipeline, and Burst could not support them (but we now explicitly check for that case). Made Burst's ILPP 22% faster by caching dependent assemblies that the being-processed assembly uses. Changed how we process static readonly fields in static constructors such that we'll allow more computational budget per static field. This fixes the case where having too many static readonly variables in a single static constructor could fail to compile, while they would work if each was in their own static constructors. Collapsed block of code in burst inspector now shows the blocks first line of code. Upgraded Burst to use LLVM Version 14.0.0 by default, bringing the latest optimization improvements from the LLVM project. Changed the default alignment for SharedStatic's from 4 to 16. Added Branches now highlights when you hover them. Branches are clickable; directing the view to the other end of the branch when clicked. Added support for the System.Runtime.CompilerServices.IsExternalInit workaround documented here into Burst when used in 2022.1+. Enabled keyboard navigation in the right pane of the burst inspector. Added version number to debug metadata for llvm Experimental support for Armv9 SVE2 CPU target for Android Added a Target Arm64 CPU setting in Burst AOT Settings for Android Removed Removed the requirement that BurstLoader has to initialize BurstReflection during a domain reload, making BurstLoader setup 2x faster during domain reloads. Fixed Error if install in build folder is used without ever using a regular build. Fixed a performance regression with IJobParallelFor where vectorization didn't happen for cases where it previously would have. Fixed a compiler miscompile if you loaded a static readonly v128 and passed it straight to a function as an argument. Removed implicit dependencies to pre-compile binaries in CodeGen which would otherwise cause assembly resolution conflicts. Fixed a Unity 2021.2 and newer bug that manifested with UWP builds - we were using the wrong unityaot folder in the Unity editor distribution with Burst. Fixed a really subtle caching bug in the compiler where if you had a job that compiled successfully at least once, then it failed (you used managed state for instance), then you closed the editor and restarted, if the compiler threads started in precisely a strange combination then Burst might accidentally never recompile the job which failed previously. Fixed potential hang in Editor when compiling a Burst entry point method that is defined in a generic class Fix for the X.pdb: The process cannot access the file because it is being used by another process issue our users were seeing. We were taking a FileShare.Read lock, when we needed to take FileShare.ReadWrite. Fixed a bug where the compiler would reject a try/finally statement if it was the first thing in a method Fixed a performance regression affecting some vectorization in Burst 1.7+ (LLVM 12+). Inspector performance regression. Improved UWP linker error message to clarify which VS components need to be installed for UWP Fixed a bug that meant Burst was accidentally enabled in secondary Unity processes, including the asset import worker and out-of-process profiler (see changelog entry for 1.6.0-pre.1 for more context around this) Keybindings for copy and selection did not depend on OS. Right pane vertical scrollbar not always showing correctly. Inspector font style changing when entering and exiting play mode. Fixed access violation error that could occur when reading from a static readonly variable Made --burst-force-sync-compilation command-line option actually work Fixed a bug that was exposed by a Script Updater running against the Entities tests, whereby if some sort of pre-domain-reload code (some sort of teardown like thing) called into Burst, the script updater could have caused Burst to purge valid function pointers, resulting in us trying to execute a DLL location that we had already unloaded. Fixed a super rare bug whereby if you kicked off two compilations very close together (most likely when running Unity in some sort of headless build-a-player mode), Burst could throw an exception on a burst hash cache file being locked by the process. Fix a bug where if you had a long running compilation and a new compilation came in, some threads in the thread pool could (if unlucky) block trying to dirty the assembly in our Burst caching infrastructure while waiting for the compilation to complete. Fix a bug where codegen differences could occur when using a local vector variable that was being captured by reference and passed to a called function, versus when it wasn't. Fixed an exception that could occur if you had the Burst AOT Settings menu docked in the Editor, and then did a player build. Trying to change any of the Burst AOT Settings would throw an exception (unless you closed and reopened the Burst AOT Settings). Fixed a bug where we could leave background tasks around forever when we had actually completed them (could only happen if two re-compilation requests arrived close together, meaning we'd cancel the first but never report to the background tasks that we had cancelled them!). Fix the Burst link.xml output to preserve C# methods we rely on, alongside the static constructors that we preserved previously. Fixed errors when working with paths containing special characters Fixed a bug where if you used FloatMode.Fast with math.pow, where the y argument to math.pow was actually sourced from an integer, illegal codegen would be generated (LLVM would try and call out to powf from the cstdlib). Worked around an ordering issue with post-processing in 2020.3 and earlier by deferring the early compilation of script assemblies in the editor until the entire pipeline has completed. Fixed a bug in 2022.1+ where calling Debug.Log in a static constructor would result in a Burst failure. Fixed another rare case of the file-is-locked bug where the Burst IL Post Processor could incorrectly hold a file lock on a pdb. Fixed a bug when calling profiling CreateMarker on iOS, Burst could fail at runtime saying it was unable to find CreateMarker__Unmanaged. Fixed that the crc32_u64 second parameter should have been a ulong. Added a new ulong variant and marked the old long variant as [Obsolete]. Fixed a bug where using ReinterpretStore(someIndex, (ushort)someValue) could cause an internal compiler error in Burst. Fixed a potential deadlock whereby if Burst was compiling in the background (the background tasks window showed Burst in it) and a user switched from release to debug in the editor, Burst could cause a deadlock. A potential issue with the debug info mover pass, that meant it only affected the first entry point in a module Fixed hashing error that could occur with unbound generic type Fixed a bug where if you had synchronous compilation on a job, disabled Burst compilation and entered playmode, then exited playmode, and finally re-enabled Burst compilation, a hang could occur. Fixed a bug where toggling Burst enable <-> disable during a playmode execution using Burst, and then attaching the managed debugger, could cause an editor crash. Fixed a memory leak where during hashing we'd pin a GC object and never unpin and free it. Fix burst inspector sometimes stalling during loading for script reloads. Fixed a super rare bug where Burst could hit an internal error with System.InvalidOperationException: Nullable object must have a value. Fixed a regression where out parameters of C# 9.0 function pointers weren't working in Burst. Fixed internal compiler error when encountering a calli with closed generics Fixed bug in static constructor ordering in the presence of indirect dependencies between static constructors (i.e. static constructor -> static method -> static constructor) that could result in a runtime crash Added workaround for \"cannot dlopen until fork() handlers have completed\" issue seen in macOS 12.3 Fixed compiler crash when trying to dynamically call BurstCompiler.CompileFunctionPointer in Burst-compiled code Fix compiler crash when the only usage of a static field was in a formatted exception string Fixed burst inspector sometimes not rendering text or rendering text on top of other text. Fixed selection rendering off-by-one error at last line of each block. Fixed a bug with Span and ReadOnlySpan types where if the indices used were not already 32-bit signed integers, an internal compiler error would occur if running with safety checks enabled. Fixed a really convoluted bug that could manifest in Burst returning out of date cached libraries, which would manifest as random exceptions in Burst jobs/function-pointers (users deleting the BurstCache would workaround the bug). Known Issues [1.7.0-pre.2] - 2021-12-06 Changed Improved the compiler performance when doing large struct copies by detecting more cases where a load/store can be safely converted to a move-memory operation. Used BuildReport::summary::subtarget to detect headless (server) player builds on 2022.1+. Don't move pdbs out of build folder for UWP builds. Changed how we display the timings when a user has the Show Timings option enabled in the Burst menu, by cleaning up and presenting the information in a (hopefully!) clearer way. Fixed Fixed constant folding when using Hint.Likely or Hint.Unlikely intrinsics - the compiler is now able to fold these calls away entirely if the input value is constant. Fixed an internal compiler error when casting a void* to a pointer-to-vector and then access the element. One Definition Rule optimisation would break if multiple modules shared static constructors due to an issue with sharing code but not data. Fixed type initialization error, and invalid log messages about needing to add [MonoPInvokeCallback] to be compatible with IL2CPP, that could occur in a player build with Burst disabled ILPP issue for dots runtime whereby a calli patch could generate bad IL if the first instruction replaced was the target of a branch. Fixed a bug where fixed used in conjunction with Span or ReadOnlySpan would cause a compiler error. Fixed a codegen issue with Unity 2021.2 and System.Buffer.MemoryCopy. Fixed compiler crash when trying to load a generic static field Fixed \"UnityException: CompileAsyncDelegateMethod can only be called from the main thread.\" error that was logged in standalone players when the first invocation of a direct-call method was from a background thread Fix the very rare bug whereby the Burst Hash Cache files (*.bhc) will sometimes cause an exception in the editor log. Fixed the documentation to note that the System.Runtime.CompilerServices attributes [CallerLineNumber], [CallerMemberName], and [CallerFilePath] work with Burst, with the restriction that you cannot format the [CallerMemberName], and [CallerFilePath] strings yet. Fixed an issue where with optimizations disabled, using half conversions on platforms that did not natively support half could cause linker errors. Fix error when trying to Direct Call a method belonging to a private nested type Fixed some memory leaks between the C# and C++ parts of the Burst compiler, and added some CI tooling to ensure this doesn't happen again. Fixed a bug where our [BurstCompile] job finding code would not find methods in generic base classes in places where we knew the concrete-generic type (for instance struct Foo<T> { [BurstCompile] struct MyJob : IJob { void Execute() {} } }, struct Bar<T> : Foo<T> {}, and struct Haz : Bar<int> {} - we wouldn't find the concrete Foo<int>::MyJob in Burst). Fixed editor crash when trying to debug a DirectCalled method Fixed a bug whereby complicated try/finally nesting could trip up the compiler. Fixed a bug in the fixed string processing whereby we'd miscompile a fixed string that was within a struct inside a SharedStatic (depending on how it was used). Fixed a bug in the entry-point finding code whereby we wouldn't correctly resolve a nested generic struct's job if it was within a concrete generic class that was outwith the root assembly set. Added Ability to partially select and copy text in the burst inspector. Right clicking the inspector view reveals a context menu, allowing selecting all text and copying selection. Removed The button \"Copy to Clipboard\". Removed Newtonsoft.Json as a dependency Known Issues [1.7.0-pre.1] - 2021-10-21 Fixed Fixed an issue where dsym folders would be not be copied across to the DoNotShip folder when building a multi architecture build for mac os. Fixed bug that could lead to \"Failed to resolve method with name hash X and signature hash Y\" compiler error Fixed compiler error that occurred when calling BurstCompiler.CompileFunctionPointer with a delegate type that was decorated with a custom attribute Linking would fail on non-Windows platforms if the project folder contained a single-quote Fixed the \"could not find path tempburstlibs\" error message popping up when building for Android and Burst is disabled Fixed bug that could lead to incorrect compiler errors for calls to GetHashCode from a generic type Incorrect conversions between signed and unsigned vector types Detects if the simulator is the target of a player build for iOS/tvOS and disables burst, as at present this configuration is not supported by burst. [SkipLocalsInit] now correctly doesn't zero-initialize variables in a function (previously it only avoided zero-initialization of stackalloc created variables). Fixed a bug whereby sometimes some LLVM intrinsics could be incorrectly marked as unused causing invalid codegen with calls to math.acos. The cache for pdbs was becoming stale. This caused issues with wrong source information being shown in the inspector, and potentially wrong debug information being generated for bursted code in editor sessions. Missing output messages from some tools when a failure occurred. Fixed a bug with sqrt_ps for 128-bit types where it would crash the compiler. ArgumentOutOfRangeException due to _renderBlockStart and _renderBlockEnd not being probably initialized when all blocks were above the scroll position. Arrows were rendered even though they were not within the current view. Made it save the actual line numbers for code blocks in _blockLine even when the block is below the view. Removed the starting newline character when copying, and when rendering plain assembly kind. Fixed a bug where a player build that had multiple assemblies that had structs declared with the same name and same contents but different [BurstCompile] methods in them, would wrongly only pick a single struct to Burst-compile. Crash in burst module initialization if multiple modules are compiled and then linked in a different order. Fixed our platform documentation to accurately reflect the current supported platforms with Burst. Inspector menu buttons were seen as available, even though they were not supported, when viewing i.e. .NET IL code. Burst will now handle projects special characters in their project-name Static constructor sorting didn't account for dependencies within calls' IL Static constructor cyclic checks also included method calls when this is not necessary and fails on burst runtime logging code Fixed the bug @tertle found when loading a vector from a struct pointer that is marked as in. Fixed that implicitly casting a scalar half to a vector type would cause the compiler to crash Fixed a crash that could occur when loading legacy Burst AOT settings and then entering play mode Stack overflow caused by placement of alloca under certain function transforms. linker errors on macOS due to long command lines, swapped to using filelists for inputs. Fixed issue that could cause bcl.exe to fail with an exit code of 1 but not output any compilation errors Added Added support for DOTS Runtime running / loading .Net Core assemblies. Added support for System.Span<T> and System.ReadOnlySpan<T> within Bursted code. These types are not allowed as entry-point arguments. Folding/collapsing code in inspector Branch arrows (can be switched off) Automatically collapses less important blocks of disasssembly (focuses on code). Burst now generates a link.xml automatically to avoid issues with stripping causing missing symbols at runtime from static constructor usage. Removed Removed the Use Platform SDK Linker option from Burst AOT Settings for desktop platforms. Removed the player build BC1370 exception warnings as users only found them annoying. Changed Made the cost of initializing Direct Call methods for execution 33x faster during domain reload. Upgraded Burst to use LLVM Version 12.0.0 by default, bringing the latest optimization improvements from the LLVM project. Change the optimization pipeline to run the loop unroller exclusively after the loop vectorizer. This improves codegen in a lot of cases (mostly because the SLP vectorizer is unable to vectorize all the code that the loop unroller could have). Intrinsics: Neon vst1 APIs are now fully supported Made fmod and floating-point modulus use a faster algorithm to improve performance. Made the SharedStatic initialization cost during static constructor initialization time 13.3x faster. Improved iteration time by triggering Burst compilation immediately after .NET assemblies have been compiled Upgraded the minimum supported PS4 SDK to 8.00. Updated the minimum Xcode required for Burst to compile for the Apple iOS/tvOS plaforms to 12.0. Burst now waits for all threads to complete on shutdown, rather than performing a thread abort, as that could lead to a race condition with Dispose. Known Issues Burst does not work correctly when a project has a semi-colon in its name [1.6.0-pre.3] - 2021-07-27 Fixed Fixed a bug where methods with the same name and namespace, but in different assemblies, could resolve to the wrong method. Burst no longer logs a warning when opening the standalone Profiler Fixed an UnauthorizedAccessException that could occur when using Burst in players built for the macOS App Sandbox Fixed a bug that could cause an incorrect compilation error when using a primitive type as a generic argument in a static method entry point Crash due to member function debug information on tvOS. Fix documentation to make clear that ref / out parameters are supported on [BurstDiscard] methods. Fixed a NullReferenceException in the Burst compiler when multi-dimensional arrays were used. The compiler now produces a correct error message telling users that multi-dimensional arrays are not supported by Burst. Fixed DOTS Runtime Job Marshalling behaviour to properly handle marshalling generic Job types when not all closed forms of the generic type require marshalling. Fixed a Burst package warning in our editor compiler integration with respect to BuildOptions.EnableHeadlessMode. Fixed small race which could cause an unexpected exception when finishing a standalone compilation task. Building for Apple Silicon architecture on macOS would produce a universal binary, now it behaves correctly. tvOS/iOS and other statically linked platforms would fail to burst compile if the burst compiled code contained references to functions that were [DllImport(\"__Internal\")], due to a mismatch in calling convention. Fixed a bug whereby if you had $\"{too} {many} {fixed} {string} {formatted} {arguments}\" in a string formatter, Burst wouldn't be able to correctly understand how to transform this for the purposes of logging or fixed-string construction. Fixed where Unity.Burst.CompilerServices.Constant.IsConstantExpression is evaluated to be later in the compilation pipeline, to let it catch more constant expressions (for instance post-inlining). Rare non zero return code from bcl after successfully building.. Only check assembly cache when the main-thread is requesting some Burst code - meaning that kicking off eager compilation is 1.6x faster than before. stackalloc byte[] with an array initializer was previously only supported when the stackalloc size was 8 or less. Sizes greater than 8 are now supported. Fixed an error that could occur with the form \"System.InvalidOperationException: Could not find burst.initialize function in library 'SomeLibrary'\" Fixed incorrect runtime behavior that could occur when casting a pointer to a generic type Fixed a bug where stackalloc's could be wrongly hoisted out of loops. Added [Preserve] attribute to prevent stripping a compiler service call Fixed incorrect compiler error that could occur when casting a pointer to a generic type and then calling a method with generic parameters Fixed incorrect compiler error that could occur with explicit-layout structs when setting a Size smaller than the natural struct size Added Universal (Apple Silicon + X64) versions of extra build tools Add Android x86_64 and re-enable x86 support Added support for having [MarshalAs(UnmanagedType.U1)] or [MarshalAs(UnmanagedType.I1)] on a bool external function parameter. Neon intrinsics: Added vst1* experimental APIs Added a global player build setting to let users specify the default optimization choice for Burst. Native support for Apple Silicon. Added support for StructLayoutAttribute.Pack Additional notes about BurstCompiler.CompileFunctionPointer<T> regarding; avoid wrapping in another open generic method, and interoperability with IL2CPP. Removed Removed the Enable Safety Checks option for player builds, since it didn't actually enable safety checks in containers, which are editor only in Unity. Changed Changed how we link object files for iOS and tvOS platforms such that Burst will now create the object file and hand it off to XCode for linking only. Assembly-level attributes (such as [assembly: RegisterGenericJobType]) are now scanned for generic job types to compile Fixed a regression that caused eager-compilation at Editor startup to be slower than it should have been math.f16tof32 now uses hardware intrinsics where available (AVX2 / NEON). half to float or double vector conversions now produce more optimal codegen. Burst Inspector now remembers scroll position between domain reloads Changed how we schedule Burst eager compilation threads. Previously we'd spawn at most 8 of the threads, and only allow 2 to make progress while in the Editor (to ensure the editor UX/UI was as responsive as possible). Instead we now spawn number_of_cores - 1 threads at a lower thread priority, ensuring that any computing power slack can be consumed to speed up Burst compilation. On a 24 core machine this resulted in 2.5x reduction in time taken for Burst to fully compile a large project. Fixed a potential error related to duplicate symbols when calling BurstCompiler.CompileFunctionPointer from inside Burst code Improved performance of checking the cache to see if methods have already been compiled For player builds : lib_burst_generated.txt, pdbs (in non development mode) and dysm folders are now placed into a xxx_BurstDebugInformation_DoNotShip folder alongside the data folder, this is to ensure it is easy to remove the files that you should not ship with your player. Known Issues Code that previously mixed managed or non-readonly static fields with Burst compiled code will now fail to compile. [1.6.0-pre.2] - 2021-04-15 Fixed Fixed obsolete API in package code. [1.6.0-pre.1] - 2021-04-14 Changed Start 1.6 release cycle Changed how we resolve function references in the compiler to improve resolving an existing function reference by 3x. Improve how we handle generic resolution in Cecil to cache the strictly resolved generic types and save a bunch of time in the compiler. Exception strings no longer contain the entry-point name of the job/function-pointer that caused the throw. This change was required because the Burst compiler has to produce deterministic results from any given compile, which is fundamentally opposed to per-entry-point function derivations. Changed how SLEEF global variables for trig functions are pulled into Burst to reduce duplications. Changed how exceptions throw types and messages are stored in our Burst binaries to reduce binary size. Constant array data is now named after the static field it belongs to in assembly Upgraded Burst to use LLVM Version 11.0.1 by default, bringing the latest optimization improvements from the LLVM project. The Unity.Burst.Intrinsics.Common.Pause intrinsic is no longer experimental. DOTS Runtime shares the logging code path with the general case Armv8.2 Neon intrinsics are now fully supported Disable threading within the lld linker instances we use for in-editor and desktop cross compilation, because we're already threading seperate process instances of lld and it results in lot of OS context switching. Tweaked how the IL Post Processed 'direct call' Burst function pointers are compiled so that the compilation is deferred until they are needed (previously we'd enqueue them all for compilation on a domain reload). Changed Burst minimum editor version to 2019.4 Use rpmalloc as our native allocator on Windows to speed up concurrently executing LLVM work. When Burst has previously compiled a method, and neither the assembly containing that method nor any of that assembly's dependencies have changed, it was possible after a domain reload for the Mono version of the method to be used for a short time before being replaced by the Burst version. This has now been improved such that the Burst version will be used immediately. Improved iteration speed by reducing the time it takes for Burst to check if any Burst-compilable code has changed Change our link step to not use response files if the command line was smaller enough, saving the cost of the round-trip to the disk. Made half <-> float / double conversions use native hardware where possible (Arm or AVX2 targets). In order to prevent conflicts with the main Unity process, Burst is now inactive in secondary Unity processes, including the asset import worker and out-of-process profiler. This means that in those secondary processes, code that would normally be Burst-compiled will now run under Mono. In a future release of Burst, we hope to lift this restriction and allow Burst-compiled code to run in secondary Unity processes. Fixed Fixed a bug in LLVM that it would incorrectly convert some memset -> memcpy if both pointers derived from the same memory address, and where one indexed into the 0th element of the pointer. Fixed namespace issue triggering a warning in the editor. Made math.shuffle compile correctly when non-constant ShuffleComponent's are used. Fixed alignment issues associated with xxHash3 on ArmV7 (case 1288992) Fixed managed implementation of sub_ss intrinsic Fixed a bug that occurred when an explicitly laid out struct was used by a dup instruction, which caused an internal compiler error. Fixes DOTS Runtime JobProducer Bursting code to support JobProducers with multiple generic arguments, complex job wrapper and generic jobs. Fixed a bug where if a user had defined multiple implicit or explicit casts, the compiler could resolve to the wrong cast. Fixed a bug where explicitly casting from an int to IntPtr would not sign extend the value. String interpolation issues when using Dots / Tiny runtime. Fixed managed implementations of blend_epi32 and mm256_blend_epi32 intrinsics on Mono Fixed a bug where loading from a vector within a struct, that was got from a NativeArray using an indexer, would cause the compiler to crash. Fixed an issue where Burst would erroneously error on BurstCompile.CompileFunctionPointer calls when building for the DOTS Runtime. clang segmentation fault on iOS when member function debug information was emitted, it is disabled for this platform now. Intrinsics: Neon - fixed vget_low and vget_high producing suboptimal code Private [BurstCompile] methods no longer throw MethodAccessException Fixed a bug where the Burst post-processing for direct call would cause duplicate function pointers to be compiled, wasting compile time in the editor and caused an Editor launch stall. Corrected 'Enable safety checks tooltip`. Fixed a minor debug information bug where built-in types with methods (like System.Int32) would generate incorrect debug information. Fixed a very obscure bug where if you had a function-pointer that was called from another function-pointer of job, and that function-pointer happened to be compiled in a player build in the same bucket as the caller, and the no-alias cloning analysis identified that it could clone the original function-pointer to enable more aliasing optimizations, it could create a duplicate symbol error. Revert to internal linkage for Android X86 (32bit) to ensure ABI compliance. Fixed compilation errors when targeting Arm CPUs and using some of the Intel intrinsics Added PreserveAttribute to prevent the internal log from being stripped in il2cpp builds. IL Function Pointer Invoke Transformation updated to handle transforms that affect instructions that are the destination of a branch. IL Function Pointer Invoke Transformation now uses correct runtime library for dots runtime. Fixed compilation errors when targeting Intel CPUs and using some of the Arm Neon intrinsics Fixed a bug where eager-compilation could pick up out-of-date global Burst menu options for compiling. Fixed a bug where the progress bar would report double the amount of pending compile jobs if a user changed the Burst options while background compilation was going on. Fixed some intrinsics not checking target CPU against required CPU, so it was possible to use some intrinsics without an IsXXXSupported check Fixed a bug where having any [DllImport] in a class that used the Direct Call mechanism could result in an illegal CompileFunctionPointer call being produced by our post processor. Fixed an issue where if a user used a math function (like cos, sin, etc) then LLVM would preserve both the scalar and vector implementations even if they were trivially dead, causing us to inject otherwise dead functions into the resulting binary. PDB debug information for instance methods that also used struct return were incorrect. When generating Line Table only debug information, an unreachable could occur due to a missing check. Fixed the 1.5 restriction that Direct Call methods can only be called from the main thread, now they work when called from any thread. Internal Compiler Error if a call was discarded (via BurstDiscard for example), but the callsites required an ABI transform e.g. struct return. Fixed a bug with using multiple IsXXXSupported intrinsics in the same boolean condition would fail. Broken link restored for known issues with debugging and profiling. The Direct Call injected delegate now has a unique suffix to avoid type-name clashes. Dots runtime function pointer transform has been simplified, making it less brittle and fixing some bad IL generation. Fixed crashes on 32 bit windows when calling function pointers from managed code and using IL2CPP. Fixed a possible DivideByZeroException due to race condition in TermInfoDriver initialization code. Fixed a bug where the multi-CPU dispatcher (used for player builds targetting multiple CPU architectures) could end up generating invalid instructions. Gracefully handle failing to find a particular assembly in the ILPP to prevent an ICE. function calls using in modifiers on blittable structs where being treated as non blittable. crash when extracting sequence point information for error reporting/debug information generation. Direct Call extension methods that only differ on argument types are now supported (previously Burst's AssemblyLoader would complain about multiple matches). Fixed a regression where managed static fields, in static constructors that would also be compiled with Burst, could cause a compile time failure for mixing managed and unmanaged state. Added Added links to blog posts from the burst team to the Burst documentation. Intrinsics: Neon - Added support for basic vld1 APIs Can now call BurstCompiler.CompileFunctionPointer() in Burst code Add support for the C# 8.0 construct default(T) is null to Burst by transforming the generated Box + 'is the box non-null?' at compile time. Make it possible to get a pointer to UTF-8 encoded string literal data in HPC# code via StringLiteral.UTF8() Add an OptimizeFor option to [BurstCompile], allowing users to say they want fast code, small code, or fastly compiled code. Known issue with Windows Native Debuggers and Dll numbers + workarounds. Assemblies are now allowed to have an [assembly: BurstCompile()] attribute to let users specify compile options that should apply assembly wide (for instance [assembly: BurstCompile(OptimizeFor = OptimizeFor.FastCompilation)]). Automatically add [UnmanagedFunctionPointer(CallingConvention.Cdecl)] to any delegates that are used for BurstCompiler.CompileFunctionPointer<>() or error if the delegate has the attribute and it is not Cdecl. Source location metadata into hash cache. Added support for having [return: MarshalAs(UnmanagedType.U1)] or [return: MarshalAs(UnmanagedType.I1)] on a bool return external function. An additional warning about delegates being used by BurstCompiler.CompileFunctionPointer that are not decorated as expected. In most cases, Burst will automatically add the C-declaration attribute in IL Post Processing, but if the usage of CompileFunctionPointer is abstracted away behind an open generic implementation, then Burst will not be able to automatically correct the delegate declaration, and thus this warning will fire. new burst_TargetPlatform_EmbeddedLinux new AotNativeLinkEmbeddedLinux for EmbeddedLinux Added a new OptimizeFor mode Balanced. This becomes the default optimization mode, and trades off slightly lower maximum performance for much faster compile times. Added experimental half precision floating point type f16 Added experimental support for half precision floating point Arm Neon intrinsics Removed Known Issues Direct Call methods only execute using Burst after an initial execution of them on the main-thread. Notes BurstAotCompiler integration done using reflection and raw values, since the platform will only be officially available for 2021.2+ and we special customer versions (shadow branches) for 2019.4 & 2020.3. AotNativeLinkEmbeddedLinux implementation gets the toolchain from environment vars. [1.5.0-pre.2] - 2020-12-01 Added Removed Changed Fixed Fixed a failure on linux builds where libdl.so cannot be found. Known Issues [1.5.0-pre.1] - 2020-11-26 Added New intrinsics Hint.Likely, Hint.Unlikely, and Hint.Assume to let our users tell the compiler some additional information which could aid optimization. New Bmi1 and Bmi2 x86 intrinsics. These are gated on AVX2 being supported to keep the feature sets that Burst has to support small. You can now select explicit x86/x64 architecture SIMD target for Universal Windows Platform. Added Apple silicon and macOS universal binaries support to Burst. An extra alloca hoisting step to ensure that allocas that occur deep within functions are correctly allocated in the function entry block (which LLVM requires for optimization purposes). Added the missing clflush intrinsic to the SSE2 intrinsics. An optimize-for-size option to bcl to let select users focus the optimization passes to create smaller executables. Added a Unity.Burst.CompilerServices.SkipLocalsInitAttribute attribute that lets developers tell the compiler that stack-allocations do not need to be zero initialized for a given function. Added a new attribute [IgnoreWarnings] that can be specified per method, for users that really want the compiler to be quiet. Support for RDMA, crypto, dotprod Armv8.2-A Neon intrinsics An error message if attempting to BurstCompiler.CompileFunctionPointer() on a multicast delegate, since this is not supported in Burst. Burst detects removal of the burst package in 2020.2 editors and beyond, and displays a dialog asking the user to restart the editor. Added a pass that will classify and remove dead loops for improved code generation. Add support for using ValueTuple types like (int, float) from within Burst code, as long as the types do not enter or escape the Burst function boundaries. Added a new intrinsic Unity.Burst.CompilerServices.Constant.IsConstantExpression that will return true if an expression is known to be a compile-time constant in Bursted code. Added support for PlayMode / Desktop Standalone Players to load additional burst compiled libraries for use in Modding. Add support for calling Burst code directly from C# without using function pointers. In Unity 2020.2 and above, you can now call new ProfilerMarker(\"MarkerName\") from Burst code Add a compiler error if a ldobj tries to source its address to load from a non-pointer/non-reference. C# frontends should never generate this pattern, but we did see it with code generation. Fixed Fixed an issue where a function with a [return: AssumeRange(13, 42)] could lose this information during inlining. Storing into Lo64 or Hi64 would cause a compiler exception. Hitting a ldobj of a pointer-to-vector would incorrectly load the vector rather than the pointer.Burst only generates unaligned stores. Fix that the parameter to mm256_set1_epi8 should be a byte instead of a char. Fix sqrt_ss would fail because LLVM version later than 6 changed the encoding. Fixed the comi*_ss intrinsics which would generate invalid code. Pdb location for player builds is now linked relative to the final lib_burst_generated.dll, this allows the crashdump utility to access the symbols and provide better callstacks. Support negative intrinsics features checks to enable usage like if (!IsSse41Supported) return;. Clean up linker temp response files on successful build Wasm ABI issue with pointers Pause intrinsic in wasm (ignored) fmod expansion to sleef for wasm The AOT option for disabling optimizations now actually disables optimizations in player builds. Fix a bug where a static readonly variable that was a System.Guid would result in an internal compiler error. bitmask intrinsic was broken on non intel platforms When \"Enable Compilation\" was unchecked in the Burst menu, Burst was incorrectly enabled after an Editor restart. This is now fixed. Fixed a bug where a cloned function (say through no-aliasing propagation cloning) would re-create any global variables used rather than use the original variable. If the only reference to an external function was discarded, don't attempt to add it to the burst initialisation block (which caused on ICE on prior versions). Fixed a case where extracting a FixedString4096 from a parent struct could cause very slow compile times. Fixed a poor error message when a generic unsupported type (like a class or an auto-layout struct) combined with an unsupported managed array (like (int, float)[]) wouldn't give the user any context on where the code went wrong. Fixed a bug where if you used an enum argument to a function to index into a fixed array, a codegen error would occur. If targeting multiple iOS architectures, produce a combined burst library containing all architectures, this fixes \"New Build System\" on xcode version 12. Static method parameters are now validated correctly during eager-compilation Fixed permissions error when running lipo tool to combine libraries. Fixed compiler error that could occur when calling a [BurstDiscard] method with an argument that is also used elsewhere in the method Fixed an issue that could prevent the Editor from shutting down Fixed an internal compiler error when nested managed static readonly arrays were used (produces a proper Burst error instead now). Fixed a bug whereby for platforms that require us to write intermediate LLVM bitcode files, UTF paths would be incorrectly handled. Correctly marked Neon intrinsics vmovn_high_* as ArmV7 and not ArmV8 On windows, the pdb location for burst cached dll's now points to the correct path. Native debuggers attached to the Editor should now locate the symbols without requiring adding the Library/Burst/JitCache folder to the symbol search. Re-enabled BC1370 exception warnings but only for player builds. Fixed a bug whereby if you had an assembly that was guarded by UNITY_SERVER, Burst would be unable to find the assembly when Server Build was ticked. When \"Enable Compilation\" was unchecked in the Burst menu, Burst was incorrectly enabled after an Editor restart. This is now actually fixed. static readonly array with enum elements would cause the compiler to crash. Fixed managed (reference) implementation of mm256_cvttps_epi32 (case 1288563) Debug information for instance methods is now correctly scoped. This means instance variables can now be inspected correctly. Removed Removed support for XCode SDKs less than version 11.0.0. Removed support for platform SDKs that used the older LLVM 6 and 7 in the codebase to significantly simply our code and reduce the package size. Changed Minimum SDK version for iOS/tvOS increased to 13. See https://developer.apple.com/news/?id=03042020b for details. When using \"Executable Only\" build type on Universal Windows Platform, Burst will now only generate code for a single CPU architecture that you're building for. The inliner heuristics have been modified to inline less functions, but improve compile times and reduce executable size. The minimum XCode SDK required to compile for iOS/iPadOS/tvOS is now 11.0.0. We now copy the lib_burst_generated.pdb into the root of the player build (in addition to being alongside the lib_burst_generated.dll), this allows the unity crash handler to resolve the callstacks from burst code. Made Arm Neon intrinsics fully supported (removed the guarding define) Improved eager-compilation performance Improved Burst Inspector loading time Improved Burst initialization time If an argument to a BurstDiscard method is discarded, and that argument is a method call, then a warning is now generated to indicate the function call no longer happens. Changed how struct-return and indirect arguments use stack allocations to significantly reduce stack usage and improve performance in these cases. Improved the compilers ability to deduce dead memory operations (memcpy, memset, etc) to improve performance. Improved error message seen when scheduling Burst compilation during domain reload Open-generic static methods are not supported by Burst, but they were previously visible in Burst Inspector - they are now hidden In Burst Inspector, the \"Safety Checks\" checkbox now defaults to unchecked Burst Inspector no longer loses the search filter and \"Safety Checks\" option after domain reload Changed exception throws to allow more vectorization chances surrounding them. Upgraded Burst to use LLVM Version 11.0.0 by default, bringing the latest optimization improvements from the LLVM project. Eager-compilation is now cancelled when script compilation starts, to prevent spurious errors related to recompiled assemblies Strings can now be passed between methods within Burst code. Previously, string literals used for e.g. Debug.Log calls could only appear in the same method where they were used; now the string literal can be in one method, and passed to another method via a string parameter. Transitioning from burst disabled to burst enabled in editor, will perform a re-initialise of some internal state in use by Direct Call methods. Improved the performance of in-compiler hashing by 1.2x. Improved our hashing performance some more by re-using fixed-sized buffers in the compiler to improve eager-compilation / warm-cache costs by 1.25x. Improved compile time by ~37% on big projects by reworking some core compiler infrastructure. Known Issues In player builds, exceptions can report the wrong job that they were thrown from. [1.4.0-preview.4] - 2020-08-17 Fixed Fixed a bug introduced in 1.4.0-preview.3 that prevented some UnityEngine.Debug methods (such as DrawLine) from being called Fixed compiler error when explicit-layout struct contains a field which is itself an empty struct Fixed a bug that if you used more than four arguments in a function declared within another function, and then implicitly captured a few variables, Burst would map the variables wrongly. Changed Bump com.unity.mathematics to 1.2.1 version [1.4.0-preview.3] - 2020-08-06 Added VS 2017 support for platform that needs it. Added first batch of Arm Neon intrinsics. Currently, only ArmV8 (AArch64) targets are supported. The supported intrinsics include all ArmV7 and ArmV8 ones. Removed Changed In versions of Unity older than 2019.3, changing the following options in the Burst menu now requires the Editor to be restarted: Enable Compilation, Safety Checks, and Native Debug Mode Compilation. In versions of Unity older than 2019.3, previously-compiled methods will not be recompiled after changing those options, which could lead to undefined behavior where methods may or may not be compiled with the correct options. This change removes that possibility. Improved performance of \"eager-compilation\" (scheduling compilation immediately after assemblies are changed) by cancelling queued eager-compilation when entering play mode with Synchronous Compilation unchecked Improved performance of eager-compilation by not eager-compiling test assemblies Asserts that are currently discarded no longer discard arguments with potential side effects. Fixed We no longer attempt to replace the debug metadata multiple times for a given export. Fixed a subtle codegen bug that could occur when the target is an Arm or AArch64 CPU with vectors of 3 elements. Inspector slow down when scrolling/moving the window on large listings. Fixed a bug where a stfld into an element of a vector could deduce the wrong type for the underlying vector. Fixed a potential error when running the linker with a failure on lld command. If path to the package contained spaces, then native command execution could fail. This would manifiest as weird errors with 'lld' or 'vswhere' or other native tools. Fixed Debug.Log by re-enabling it when used in function pointers or jobs. Fixed errors when opening Inspector with a non-public Execute method on a job producer type Known Issues [1.4.0-preview.2] - 2020-07-01 Added Removed Changed The Burst Inspector no longer uses JIT compilation. The code it shows is now compiled the same way as for editor / player usage. Warnings are hidden in the inspector view Fixed Fixed potential error that could occur when unloading cached libraries Known Issues [1.4.0-preview.1] - 2020-06-26 Added Experimental support for tvOS Add intrinsics support for AtomicSafetyHandle.NewStaticSafetyId<T> A new option [BurstCompile(DisableSafetyChecks = true)] that allows per job or function-pointer disabling of safety checks. This allows users to have blessed code run fast always. Improve Editor experience by scheduling compilation immediately after assemblies are changed, instead of waiting until Play Mode is entered. Improved our aliasing detection to allow DynamicBuffer structs to be easily vectorizable. Added a compiler warning for any use of throwing an exception from a method not guarded by [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")]. Since exceptions in Burst are only supported in the editor, this provides a useful warning to users who may be relying on try/catch behaviors for control-flow which is not supported in final game builds. Burst compilation status is now displayed in the Background Tasks window in Unity 2020.1 and above (click the spinner in the bottom-right of the Editor to open this window). Upgraded Burst to use LLVM Version 10.0.0 by default, bringing the latest optimization improvements from the LLVM project. Add support for try/finally and using/foreach for IDisposable patterns. Add BurstCompiler.IsEnabled API. Add syntax colouring for LLVM IR and Optimized IR panels in the inspector Removed Changed Made the compiler better at constant-folding complex static readonly constructors. Bursted DOTS Runtime Jobs are now decorated with [NativePInvokeCallback] instead of [MonoPInvokeCallback] which could generate callback wrappers which could cause native code to inadvertently interop with the managed VM. The Burst menu-item Safety Checks has been changed to a modal choice of Off, On, and Force On. Force On will overwrite any user job or function-pointer with DisableSafetyChecks = true. To avoid users falling into the consistent trap of having Safety Checks set to Off, any reload of the Editor will issue a warning telling the user that Safety Checks have been reset to On. Use platform provided memory intrinsics for iOS, tvOS, WASM, and console platforms. Updated Cross Compilation Tooling To LLVM 10 The command line option --burst-disable-compilation is now disabling entirely Burst, including the AppDomain. Fixed Fixed incorrect struct layout for certain configurations of explicit-layout structs with overlapping fields Fixes a caching issue where stale cached libraries may have been used if a project was copied to a different folder, or Unity was upgraded to a newer version Burst will now error if a cpblk was used to copy into a [ReadOnly] parameter or field. Fixed a bug where the mm256_cvtepi32_ps intrinsic would crash the compiler. Fixed a bug with constant expressions that could cause a compile-time hang. Debug symbols are now output when using the native toolchain on mac. Sleef fallback to scalar float for WASM. ABI struct ret/by val for trivial aggregates for WASM is now respected. Fixed a bug with float/double vector constructors of Unity.Mathematics that take half or half vector parameters. Debug information for anonymous structs could be created partially multiple times for the same type. Filter symbol warnings to prevent them reaching logs. Fixed an issue where UNITY_DOTSPLAYER builds not building for NET_DOTS would be unable to compile do to references to UnityEngine. Fixed handling of conversion from signed integer to pointer which caused issues as discovered by Zuntatos on the forums. Allow to call [BurstCompile] functions from other [BurstCompile] functions IntPtr.Size now correctly returns int32 size (rather than UInt64) - fixes an assert. Burst package has been upgraded popup could fire erroneously under shutdown conditions. Fixed an issue preventing player builds to succeed when burst compilation is disabled. Debug symbols for function names on some platforms are no longer hashes. Job Entry point symbols should now reflect the job name and type rather than a hash in callstacks/native profilers Job entry points without symbols now use the Execute location rather than pointing to unknown/unknown Dwarf symbols from multiple modules (e.g. multithreaded AOT compilation) now have correct compilation unit information. Known Issues Output of Debug.Log is temporarily disabled in Burst Function Pointers/Jobs to avoid a deadlock on a domain reload. A fix for the Unity editor is being developed. [1.3.0-preview.12] - 2020-05-05 Fixed Fix an issue when changing the base type of an enum that would not trigger a new compilation and would keep code previously compiled, leading to potential memory corruptions or crashes. Fixed a subtle AArch64 ABI bug with struct-return's (structs that are returned via a pointer argument) that was found by our partners at Arm. Fix an issue that was preventing Debug.Log to be used from a Job in Unity 2020.1 Changed JIT cache is now cleared when changing Burst version [1.3.0-preview.11] - 2020-04-30 Fixed Fix potentially different hashes returned from BurstRuntime.GetHashCode32/64 if called from different assemblies. Fixed an issue where Burst was misidentifying F16C supporting CPUs as AVX2. SDK level bumped for MacOS to ensure notarization requests are compatable. Fixed a typo m256_cvtsi256_si32 -> mm256_cvtsi256_si32 and m256_cvtsi256_si64 -> mm256_cvtsi256_si64. The compiler is now generating a proper compiler error if a managed type used directly or indirectly with SharedStatic . Fixed a bug where implicitly stack allocated variables (var foo = new Foo();) in Burst were not being zero initialized, so any field of the variable that was not initialized during construction would have undefined values. Fix potential race condition when accessing on-disk library cache Fixed a bug where Burst was sometimes producing invalid code for iOS 11.0.3+. Added Added support for System.Threading.Volatile methods Read and Write, and for the System.Threading.Thread.MemoryBarrier method. New FMA X86 intrinsics. These are gated on AVX2 support, as our AVX2 detection requires the AVX2, FMA, and F16C features. UnsafeUtility.MemCmp now maps to a Burst optimal memory comparison path that uses vectorization. Removed Changed Known Issues [1.3.0-preview.10] - 2020-04-21 Fixed Fix negation of integer types smaller than 32 bits. Fixed a bug where optimizer generated calls to ldexp would be incorrectly deduced when deterministic floating-point was enabled. Swapped private linkage for internal linkage on functions, this fixes duplicate symbol issues on some targets. variable scopes should now encompass the whole scope. variables in parent scopes should now be present in locals windows. Native plugin location for windows has changed in 2019.3.9f1. If you are on an older version of 2019.3 you will need to upgrade for burst to work in windows standalone players. Added an error if Assert.AreEqual or Assert.AreNotEqual were called with different typed arguments. Fixed a bug where doing an explicit cast to a Unity.Mathematics vector type where the source was a scalar would fail to compile. Fix issue when converting large unsigned integer values to double or float. Fix an invalid value returned from a conditional where one type is an int32 and the other type would be a byte extended to an int32. Button layout of disassembly toolbar tweaked. Copy to clipboard now copies exactly what is shown in the inspector window (including enhancements and colours if shown) AVX2 now generates the correct AVX2 256-bit wide SLEEF functions instead of the FMA-optimized 128-bit variants. Added Anonymous types are now named in debug information. XCode/LLDB debugging of burst compiled code is now possible on macOS. Added some extra documentation about how to enable AVX/AVX2 in AOT builds, and how we gate some functionality on multiple instruction sets to reduce the combinations exposed underneath. Optimized external functions (like UnsafeUtility.Malloc) such that if they are called multiple times the function-pointer address is cached. Add support for string interpolation (e.g $\"This is a string with an {arg1} and {arg2}\"). Add support for Debug.Log(object) (e.g Debug.Log(\"Hello Log!\");). Add support for string assignment to Unity.Collections.FixedString (e.g \"FixedString128 test = \"Hello FixedString!\"). If burst detects a package update, it now prompts a restart of Unity (via dialog). The restart was always required, but could be missed/forgotten. Better error message for unsupported static readonly arrays. Link to native debugging video to Presentations section of docs. Fixes a bug where in parameters of interfaces could sometimes confuse the Burst inspector. Removed Changed iOS builds for latest xcode versions will now use LLVM version 9. Burst AOT Settings now lets you specify the exact targets you want to compile for - so you could create a player with SSE2, AVX, and AVX2 (EG. without SSE4 support if you choose to). Improve speed of opening Burst Inspector by up to 2x. Provided a better error message when structs with static readonly fields were a mix of managed/unmanaged which Burst doesn't support. Tidied up the known issues section in the docs a little. Enhanced disassembly option has been expanded to allow better control of what is shown, and allow a reduction in the amount of debug metadata shown. Load Burst Inspector asynchronously to avoid locking-up Editor. Documented restrictions on argument and return types for DllImport, internal calls, and function pointers. Known Issues [1.3.0-preview.9] - 2020-04-01 Changed Improved the compile time performance when doing UnsafeUtility.ReadArrayElement or UnsafeUtility.WriteArrayElement with large structs. Made some compile-time improvements when indirect arguments (those whose types are too big that they have to be passed by reference) that reduced our compile time by 3.61% on average. Fixed Fixed a bug where storing a default to a pointer that was generic would cause an LLVM verifier error. Fixed an obscure bug in how struct layouts that had dependencies on each other were resolved. Fixed a bug as found by @iamarugin where LLVM would introduce ldexp/ldexpf during optimizations that LLD would not be able to resolve. Fixed a bug where the compiler would not promote sub-integer types to integers when doing scalar-by-vector math (like multiplies). Added Variable scopes are now constructed for debug information. A new setting to Burst AOT Settings that allows debug symbols to be generated even in a non development standalone build. Removed Known Issues [1.3.0-preview.8] - 2020-03-24 Added Double math builtins in Unity.Mathematics now use double vector implementations from SLEEF. Fixed a bug with lzcnt, tzcnt, and countbits which when called with long types could produce invalid codegen. New F16C X86 intrinsics. These are gated on AVX2 support, as our AVX2 detection requires the AVX2, FMA, and F16C features. Add user documentation about generic jobs and restrictions. Add new experimental compiler intrinsics Loop.ExpectVectorized() and Loop.ExpectNotVectorized() that let users express assumptions about loop vectorization, and have those assumptions validated at compile-time.Enabled with UNITY_BURST_EXPERIMENTAL_LOOP_INTRINSICS. Changed Changed how Unity.Mathematics functions behave during loop vectorization and constant folding to substantially improve code generation. Our SSE4.2 support was implicitly dependent on the POPCNT extended instruction set, but this was not reflected in our CPU identification code. This is now fixed so that SSE4.2 is gated on SSE4.2 and POPCNT support. The popcnt intrinsics now live in their own static class Unity.Burst.Intrinsics.Popcnt to match the new F16C intrinsics. Deferred when we load the SLEEF builtins to where they are actually used, decreasing compile time with Burst by 4.29% on average. Fixed Fix an issue where a generic job instance (e.g MyGenericJob<int>) when used through a generic argument of a method or type would not be detected by the Burst compiler when building a standalone player. [DlIimport(\"__Internal\")] for iOS now handled correctly. Fixes crashes when using native plugins on iOS. Removed Known Issues [1.3.0-preview.7] - 2020-03-16 Added Added additional diagnostic for tracking Visual Studio location failures. Added an override to bypass link.exe discovery under certain conditions. Added a ldloc -> stloc optimization which improves compile times. More documentation on function pointers, specifically some performance considerations to be aware of when using them. Removed Changed Updated tools used for determining Visual Studio locations. Fixed Embedded Portable PDB handling improved. Fixed a case where our load/store optimizer would inadvertently combine a load/store into a cpblk where there were intermediate memory operations that should have been considered. Fixed a bug where the no-alias analysis would, through chains of complicated pointer math, deduce that a no-alias return (like from UnsafeUtility.Malloc) would not alias with itself. No longer log missing MonoPInvokeCallbackAttribute when running tests. Known Issues [1.3.0-preview.6] - 2020-03-12 Added Experimental support for Prefetch, allowing users to request from the memory subsystem pointer addresses they intend to hit next. This functionality is guarded by the UNITY_BURST_EXPERIMENTAL_PREFETCH_INTRINSIC preprocessor define. Fixed Fix SSE maxps intrinsic would emit maxss [1.3.0-preview.5] - 2020-03-11 Fixed MemCpy and MemSet performance regression in Burst 1.3.0.preview.4 (as was spotted by @tertle) has been fixed. Fix a crash when loading assembly with PublicKeyToken starting with a digit. Better handling of MonoPInvokeCallbackAttribute: no check for the namespace, don't print message on Mono builds. Changed Improved error message for typeof usage. [1.3.0-preview.4] - 2020-03-02 Added Debug information for types. Debug information for local variables. Debug information for function parameters. Support for fixed statements. These are useful when interacting with fixed buffers in structs, to get at the pointer data underneath. A fast-math optimization for comparisons that benefits the BurstBenchmarks that nxrightthere has put together. DOTS Runtime Jobs will now generate both MarshalToBurst and MarshalFromBurst functions when job structs in .Net builds are not blittable. DOTS Runtime Job Marshalling generation is now controllable via the commandline switch --generate-job-marshalling-methods. Removed Changed Made it clear that the Burst aliasing intrinsics are tied to optimizations being enabled for a compilation. Restore unwind information for all builds. Print a info message if compiling a function pointer with missing MonoPInvokeCallback attribute (this can lead to runtime issues on IL2CPP with Burst disabled). The message will be converted to a warning in future releases. Fixed Fixed an issue where DOTS Runtime generated job marshalling functiosn may throw a FieldAccessException when scheduling private and internal job structs. Fix a bug that prevented entry point method names (and their declaring type names) from having a leading underscore. vector/array/pointer debug data now utilizes the correct size information. DOTS Runtime will now only generate job marshaling functions on Windows, as all other platforms rely on Mono which does not require job marshalling. ldobj / stobj of large structs being copied to stack-allocated variables could cause compile-time explosions that appeared to the user like the compiler had hung. Worked around these by turning them into memcpy's underneath in LLVM. Don't always use latest tool chain on certain platforms. Fix a crash when compiling job or function pointer that was previously cached, then unloaded, then reloaded. Fixed compiler error in array element access when index type is not Int32. Fix set1_xxx style x86 intrinsics generated compile time errors. Known Issues Native debugger feature is only available on windows host platform at the moment. [1.3.0-preview.3] - 2020-02-12 Changed Changed how the inliner chooses to inline functions to give the compiler much more say over inlining decisions based on heuristics. Updated AOT requirements to be clearer about cross platform support. Added 1.3.0-preview.1 added support for desktop cross compilation, but the changelog forgot to mention it. Removed Fixed Documentation for the command line options to unity contained extra - Burst now exclusively uses the <project>/Temp/Burst folder for any temporary files it requires during compilation. Fix a regression that could break usage of native plugins. Known Issues [1.3.0-preview.2] - 2020-02-10 Fixed Fix the error Burst failed to compile the function pointer Int32 DoGetCSRTrampoline() that could happen when loading a project using Burst with Burst disabled. [1.3.0-preview.1] - 2020-02-04 Added Enabled lower precision variants for pow, sin, cos, log, log2, log10, exp, exp2, and exp10 when BurstPrecision.Low is specified. Add CPU minimum and maximum target for desktop platforms Standalone Player builds. Append a newline between IRPassDiagnostic messages, fixes pass diagnostics readability in the inspector. Add a new attribute [AssumeRange] that lets users tag function parameters and returns of an integer type with a constrained range that the value is allowed to inhabit. NativeArray.Length and NativeSlice.Length have automatic detection that the property is always positive. This assumption feeds into the optimizer and can produce better codegen. Enabled support for DOTS Runtime SharedStatics. Due to the nature of DOTS Runtime, only the generic versions of SharedStatic.GetOrCreate<TContext> are supported. Add a new intrinsic Unity.Burst.Intrinsics.Common.Pause() which causes a thread pause to occur for the current thread. This is useful for spin-locks to stop over contention on the lock. Add some new Burst aliasing deductions to substantially improve the aliasing detection in the compiler, resulting in better codegen. Add syntax colouring to WASM. Add IsCreated to the FunctionPointer class to allow checks on whether a given function pointer has a valid (non null) pointer within it. Add AVX2 intrinsics Add some missing intrinsics from SSE, SSE2 and AVX Added explicit X86 intrinsics from SSE-AVX2. AVX and AVX2 CPU targets are now available for x64 AOT builds. Allow handle structs (structs with a single pointer/integer in them) to be inside another struct as long as they are the single member, as these require no ABI pain. Added support for Interlocked.Read. Added a new intrinsic Common.umul128 which lets you get the low and high components of a 64-bit multiplication. This is especially useful for things like large hash creation. Menu option to allow all burst jobs to be more easily debugged in a native debugger. Removed Changed Upgraded Burst to use LLVM Version 9.0.1 by default, bringing the latest optimization improvements from the LLVM project. Upgraded Burst to use SLEEF 3.4.1, bringing the latest performance improvements to mathematics functions as used in Burst. Improved Burst performance in the Editor by caching compiled libraries on-disk, meaning that in subsequent runs of the Editor, assemblies that haven't changed won't be recompiled. Update the documentation of CompileSynchronously to advise against any general use of setting CompileSynchronously = true. Take the Unity.Burst.CompilerServices.Aliasing intrinsics out of experimental. These intrinsics form part of our strategy to give users more insight into how the compiler understands their code, by producing compiler errors when user expectations are not met. Questions like 'Does A alias with B?' can now be definitively answered for developers. See the Aliasing Checks section of the Burst documentation for information. Align disassembly instruction output in Inspector (x86/x64 only). Renamed m128 to v128. Renamed m256 to v256. BurstCompile(Debug=true), now modifies the burst code generator (reducing some optimisations) in order to allow a better experience in debugging in a native debugger. Fixed Fix a bug where floating-point != comparisons were using a stricter NaN-aware comparison than was required. Fix inspector for ARMV7_NEON target. Fix some issues with Burst AOT Settings, including changing the settings to be Enable rather than Disable. Fix an issue where WASM was being incorrectly shown in the disassembly view. Fixed an issue where if the Unity.Entities.StaticTypeRegistry assembly wasn't present in a build, Burst would throw a NullReferenceException. Fix issue with type conversion in m128/m256 table initializers. Fix inspector source line information (and source debug information) from being lost depending on inlining. Fix occasional poor code generation for on stack AVX2 variables. Fix xor_ps was incorrectly downcoded. Fix reference version of AVX2 64-bit variable shifts intrinsics. Fix reference version of SSE4.2 cmpestrz. Fix bitwise correctness issue with SSE4.2/AVX explicit rounding in CEIL mode for negative numbers that round to zero (was not correctly computing negative zero like the h/w). Fix calls to SHUFFLE, SHUFFLE_PS and similar macro-like functions would not work in non-entrypoint functions. Source location information was offset by one on occasions. Debug metadata is now tracked on branch/switch instructions. Fix poor error reporting when intrinsic immediates were not specified as literals. Fix basic loads and stores (using explicit calls) were not unaligned and sometimes non-temporal when they shouldn't be. Removed the <>c__DisplayClass_ infix that was inserted into every Entities.ForEach in the Burst inspector to clean up the user experience when searching for Entities.ForEach jobs. Fix background compile errors accessing X86 MXCSR from job threads. Fix possible ExecutionEngineException when resolving external functions. Fix linker output not being propagated through to the Editor console. Known Issues [1.2.0-preview.9] - 2019-11-06 Fix compilation requests being lost when using asynchronous compilation. Prevent Burst compilation being toggled on while in play mode, either via \"Enable Compilation\" menu item or programmatically - was previously technically possible but produced unpredictable results. [1.2.0-preview.8] - 2019-11-01 Fix a NullReferenceException happening in a call stack involving CecilExtensions.IsDelegate(...). [1.2.0-preview.7] - 2019-10-30 Many improvements to the Inspector: New assembly syntax colorization! Fix issue with menu settings being modified when opening the Inspector. Make compile targets left pane resizable. Fix vertical scrollbar size. Add automatic refresh when selecting a target to compile. Fix an issue where ref readonly of a struct type, returned from a function, would cause a compiler crash. Add support for Interlocked.Exchange and Interlocked.CompareExchange for float and double arguments. Fix bug preventing iOS builds from working, if burst is disabled in AOT Settings. [1.2.0-preview.6] - 2019-10-16 New multi-threaded compilation support when building a standalone player. Improve BurstCompiler.CompileFunctionPointer to compile asynchronously function pointers in the Editor. Improve of error codes and messages infrastructure. Upgraded Burst to use LLVM Version 8.0.1 by default, bringing the latest optimization improvements from the LLVM project. Fix issue with libtinfo5 missing on Linux. Fix possible NullReferenceException when an entry point function is calling another empty function. Fix an exception occurring while calculating the size of a struct with indirect dependencies to itself. Fix potential failure when loading MDB debugging file. Fix linker issue with folder containing spaces. Fix issue with package validation by removing ifdef around namespaces. Fix issue with an internal compiler exception related to an empty stack. [1.2.0-preview.5] - 2019-09-23 Fix crashing issue during the shutdown of the editor. [1.2.0-preview.4] - 2019-09-20 Fix a logging issue on shutdown. [1.2.0-preview.3] - 2019-09-20 Fix potential logging of an error while shutting down the editor. [1.2.0-preview.2] - 2019-09-20 New multi-threaded compilation of jobs/function pointers in the editor. Improve caching of compiled jobs/function pointers. Fix a caching issue where some jobs/function pointers would not be updated in the editor when updating their code. Fix an issue where type initializers with interdependencies were not executed in the correct order. Fix an issue with Failed to resolve assembly Windows, Version=255.255.255.255... when building for Xbox One. Fix compilation error on ARM32 when calling an external function. Fix an issue with function pointers that would generate invalid code if a non-blittable type is used in a struct passed by ref. Fix an issue with function pointers that would generate invalid code in case containers/pointers passed to the function are memory aliased. Report a compiler error if a function pointer is trying to be compiled without having the [BurstCompile] attribute on the method and owning type. [1.2.0-preview.1] - 2019-09-09 Fix assembly caching issue, cache usage now conservative (Deals with methods that require resolving multiple assemblies prior to starting the compilation - generics). Fix Mac OS compatibility of Burst (10.10 and up) - fixes undefined symbol futimens. [1.1.3-preview.3] - 2019-09-02 Query android API target level from player settings when building android standalone players. Add calli opcode support to support bindings to native code. [1.1.3-preview.2] - 2019-08-29 Fix to allow calling [BurstDiscard] functions from static constructors. Correctly error if a DLLImport function uses a struct passed by value, but allow handle structs (structs with a single pointer/integer in them) as these require no ABI pain. Upgraded Burst to use LLVM Version 8 by default, bringing the latest optimisation improvements from the LLVM project. Added support for multiple LLVM versions, this does increase the package size, however it allows us to retain compatability with platforms that still require older versions of LLVM. Fix bug in assembly caching, subsequent runs should now correctly use cached jit code as appropriate. Add support for Lumin platform [1.1.3-preview.1] - 2019-08-26 Add support for use of the MethodImpl(MethodImplOptions.NoOptimization) on functions. Fix an issue whereby static readonly vector variables could not be constructed unless using the constructor whose number of elements matched the width of the vector. Fix an issue whereby static readonly vector variables could not be struct initialized. Improve codegen for structs with explicit layout and overlapping fields. Fix a bug causing SSE4 instructions to be run on unsupported processors. Fix an issue where storing a pointer would fail as our type normalizer would cast the pointer to an i8. Begin to add Burst-specific aliasing information by instructing LLVM on our stack-allocation and global variables rules. [1.1.2] - 2019-07-26 Fix an issue where non-readonly static variable would not fail in Burst while they are not supported. Fix issue with char comparison against an integer. Add partial support for C# char type. Improve codegen for struct layout with simple explicit layout. Fix NullReferenceException when using a static variable with a generic declaring type. Fix issue with stackalloc not clearing the allocated stack memory as it is done in .NET CLR. [1.1.1] - 2019-07-11 Fix a compiler error when using a vector type as a generic argument of a NativeHashMap container. Disable temporarily SharedStatic/Execution mode for current 2019.3 alpha8 and before. Fix detection of Android NDK for Unity 2019.3. Update documentation for known issues. [1.1.0] - 2019-07-09 Fix detection of Android NDK for Unity 2019.3. Update documentation for known issues. [1.1.0-preview.4] - 2019-07-05 Burst will now report a compilation error when writing to a [ReadOnly] container/variable. Fix regression with nested generics resolution for interface calls. Fix issue for UWP with Burst generating non appcert compliant binaries. Fix issue when reading/writing vector types to a field of an explicit layout. Fix build issue on iOS, use only hash names for platforms with clang toolchain to mitigate issues with long names in LLVM IR. Allow calls to intrinsic functions (e.g System.Math.Log) inside static constructors. Improve performance when detecting if a method needs to be recompiled at JIT time. Fix an issue with explicit struct layout and vector types. [1.1.0-preview.3] - 2019-06-28 Fix issue with generic resolution that could fail. Add support for readonly static data through generic instances. Add internal support for SharedStatic<T> for TypeManager. Add intrinsic support for math.bitmask. [1.1.0-preview.2] - 2019-06-20 Fix issue where uninitialized values would be loaded instead for native containers containing big structs. Fix issue where noalias analysis would fail for native containers containing big structs. Fix issue when calling \"internal\" methods that take bool parameters. Add support for MethodImplOptions.AggressiveInlining to force inlining. Fix issue in ABITransform that would cause compilation errors with certain explicit struct layouts. Disable debug information generation for PS4 due to IR compatability issue with latest SDK. Implemented an assembly level cache for JIT compilation to improve iteration times in the Editor. Implement a hard cap on the length of symbols to avoid problems for platforms that ingest IR for AOT. Add support for FunctionPointer<T> usable from Burst Jobs via BurstCompiler.CompileFunctionPointer<T>. Add BurstCompiler.Options to allow to control/enable/disable Burst jobs compilation/run at runtime. Add BurstRuntime.GetHashCode32<T> and GetHashCode64<T> to allow to generate a hash code for a specified time from a Burst job. [1.0.0] - 2019-04-16 Release stable version. [1.0.0-preview.14] - 2019-04-15 Bump to mathematics 1.0.1 Fix android ndk check on windows when using the builtin toolchain. Fix crash when accessing a field of a struct with an explicit layout through an embedded struct. Fix null pointer exception on building for android if editor version is less than 2019.1. Workaround IR compatibility issue with AOT builds on IOS. [1.0.0-preview.13] - 2019-04-12 Fix linker error on symbol $___check_bounds already defined. Fix StructLayout Explicit size calculation and backing storage. [1.0.0-preview.12] - 2019-04-09 Fix crash when accessing a NativeArray and performing in-place operations (e.g nativeArray[i] += 121;). [1.0.0-preview.11] - 2019-04-08 Improve error logging for builder player with Burst. Fix NullReferenceException when storing to a field which is a generic type. [1.0.0-preview.10] - 2019-04-05 Update known issues in the user manual. Improve user manual documentation about debugging, [BurstDiscard] attribute, CPU architectures supported... Fix an issue where Burst callbacks could be sent to the editor during shutdowns, causing an editor crash. Improve error messages for external tool chains when building for AOT. [1.0.0-preview.9] - 2019-04-03 Fix an auto-vectorizer issue not correctly detecting the safe usage of NativeArray access when performing in-place operations (e.g nativeArray[i] += 121;). Add support for dynamic dispatch of functions based on CPU features available at runtime. Fix issue when running SSE4 instructions on a pre-SSE4 CPU. Fix write access to NativeArray<bool>. Remove dependencies to C runtime for Windows/Linux build players (for lib_burst_generated.so/.dll). Updated API documentation. Update User manual. Static link some libraries into the Burst llvm wrapper to allow better support for some linux distros. [1.0.0-preview.8] - 2019-03-28 Fix for iOS symbol names growing too long, reduced footprint of function names via pretty printer and a hash. [1.0.0-preview.7] - 2019-03-28 Burst will now only generate debug information for AOT when targeting a Development Build. Added support for locating the build tools (standalone) for generating AOT builds on windows, without having to install Visual Studio complete. Fix Log Timings was incorrectly being passed along to AOT builds, causing them to fail. Fix editor crash if Burst aborted compilation half way through (because editor was being closed). Fix issue with job compilation that could be disabled when using the Burst inspector. Fix issue with spaces in certain paths (e.g. ANDROID_NDK_ROOT) when building for AOT. Restore behavior of compiling ios projects from windows with Burst, (Burst does not support cross compiling for ios) - we still generate a valid output project, but with no Burst code. Add support for Android embedded NDK. Fix issue where certain control flow involving object construction would crash the compiler in release mode. [1.0.0-preview.6] - 2019-03-17 Fix invalid codegen with deep nested conditionals. Fix issue with Burst menu \"Enable Compilation\" to also disable cache jobs. Improve handling of PS4 toolchain detection. [1.0.0-preview.5] - 2019-03-16 Fix regression with JIT caching that was not properly recompiling changed methods. Remove NativeDumpFlags from public API. Remove usage of PropertyChangingEventHandler to avoid conflicts with custom Newtonsoft.Json. Fix issue when a job could implement multiple job interfaces (IJob, IJobParallelFor...) but only the first one would be compiled. [1.0.0-preview.4] - 2019-03-15 Fix \"Error while verifying module: Invalid bitcast\" that could happen with return value in the context of deep nested conditionals. Fix support for AOT compilation with float precision/mode. Fix fast math for iOS/PS4. Fix issue with double not using optimized intrinsics for scalars. Fix issue when loading a MDB file was failing when building a standalone player. Fix no-alias analysis that would be disabled in a standalone player if only one of the method was failing. Fix bug with explicit layout struct returned as a pointer by a property but creating an invalid store. Change FloatPrecision.Standard defaulting from FloatPrecision.High (ULP1) to FloatPrecision.Medium (ULP3.5). [1.0.0-preview.3] - 2019-03-14 Fix compilation issue with uTiny builds. [1.0.0-preview.2] - 2019-03-13 Fix no-alias warning spamming when building a standalone player. Improve the layout of the options/buttons for the inspector so that they at least attempt to layout better when the width is too small for all the buttons. Fix formatting of error messages so the Unity Console can correctly parse the location as a clickable item (Note however it does not appear to allow double clicking on absolute paths). Change Burst menu to Jobs/Burst. Improve order of menu items. Fix for AOTSettings bug related to StandaloneWindows vs StandaloneWindows64. [1.0.0-preview.1] - 2019-03-11 Fix regression when resolving the type of generic used in a field. Fix linker for XboxOne, UWP. Fix performance codegen when using large structs. Fix codegen when a recursive function is involved with platform dependent ABI transformations. [0.2.4-preview.50] - 2019-02-27 Fix meta file conflict. Fix changelog format. [0.2.4-preview.49] - 2019-02-27 Move back com.unity.burst.experimental for function pointers support, but use internal modifier for this API. Restructure package for validation. [0.2.4-preview.48] - 2019-02-26 Move back com.unity.burst.experimental for function pointers support, but use internal modifier for this API. [0.2.4-preview.47] - 2019-02-26 Fix an issue during publish stage which was preventing to release the binaries. [0.2.4-preview.46] - 2019-02-26 iOS player builds now use static linkage (to support TestFlight) - Minimum supported Unity versions are 2018.3.6f1 or 2019.1.0b4. Fix a warning in Burst AOT settings. Enable forcing synchronous job compilation from menu. [0.2.4-preview.45] - 2019-02-07 Disable Burst AOT settings support for unity versions before 2019.1. [0.2.4-preview.44] - 2019-02-06 Fix incorrect conversions when performing subtraction with enums and floats. Fix compatability issue with future unity versions. Fix bug with ldfld bitcast on structs with explicit layouts. Guard against an issue resolving debug locations if the scope is global. [0.2.4-preview.43] - 2019-02-01 Add preliminary support for Burst AOT settings in the player settings. Move BurstCompile (delegate/function pointers support) from com.unity.burst package to com.unity.burst.experimental package. Fix issue with stackalloc allocating a pointer size for the element type resulting in possible StackOverflowException. Add support for disabling Burst compilation from Unity editor with the command line argument --burst-disable-compilation . Add support for forcing synchronous compilation from Unity editor with the command line argument --burst-force-sync-compilation. Fix a compiler crash when generating debugging information. Fix invalid codegen involving ternary operator [0.2.4-preview.42] - 2019-01-22 Fix a compilation error when implicit/explicit operators are used returning different type for the same input type. [0.2.4-preview.41] - 2019-01-17 Fix codegen issue with Interlocked.Decrement that was instead performing an increment. Fix codegen issue for an invalid layout of struct with nested recursive pointer references. Fix for Fogbugz case : https://fogbugz.unity3d.com/f/cases/1109514/. Fix codegen issue with ref bool on a method argument creating a compiler exception. [0.2.4-preview.40] - 2018-12-19 Fix bug when a write to a pointer type of an argument of a generic function. Breaking change of API: Accuracy -> FloatPrecision, and Support => FloatMode. Add FloatMode.Deterministic mode with early preview of deterministic mathematical functions. Fix bug with fonts in inspector being incorrectly reloaded. [0.2.4-preview.39] - 2018-12-06 Add preview support for readonly static arrays typically used for LUT. Fix an issue with generics incorrectly being resolved in certain situations. Fix ARM32/ARM64 compilation issues for some instructions. Fix ARM compilation issues on UWP. Fix issue with math.compress. Add support for ldnull for storing a managed null reference to a ref field (e.g for DisposeSentinel). [0.2.4-preview.38] - 2018-11-17 Fix issue when converting an unsigned integer constant to a larger unsigned integer (e.g (ulong)uint.MaxValue). Fix crash in editor when IRAnalysis can return an empty string . Fix potential crash of Cecil when reading symbols from assembly definition. [0.2.4-preview.37] - 2018-11-08 Fix a crash on Linux and MacOS in the editor with dlopen crashing when trying to load burst-llvm (linux). [0.2.4-preview.36] - 2018-11-08 Fix a crash on Linux and MacOS in the editor with dlopen crashing when trying to load burst-llvm (mac). [0.2.4-preview.35] - 2018-10-31 Try to fix a crash on macosx in the editor when a job is being compiled by Burst at startup time. Fix Burst accidentally resolving reference assemblies. Add support for Burst for ARM64 when building UWP player. [0.2.4-preview.34] - 2018-10-12 Fix compiler exception with an invalid cast that could occur when using pinned variables (e.g int32& resolved to int32** instead of int32*). [0.2.4-preview.33] - 2018-10-10 Fix a compiler crash with methods incorrectly being marked as external and throwing an exception related to ABI. [0.2.4-preview.32] - 2018-10-04 Fix codegen and linking errors for ARM when using mathematical functions on plain floats. Add support for vector types GetHashCode. Add support for DllImport (only compatible with Unity 2018.2.12f1+ and 2018.3.0b5+). Fix codegen when converting uint to int when used in a binary operation. [0.2.4-preview.31] - 2018-09-24 Fix codegen for fmodf to use inline functions instead. Add extended disassembly output to the Burst inspector. Fix generic resolution through de-virtualize methods. Fix bug when accessing float3.zero. Prevents static constructors being considered intrinsics. Fix NoAlias attribute checking when generics are used. [0.2.4-preview.30] - 2018-09-11 Fix IsValueType throwing a NullReferenceException in case of using generics. Fix discovery for Burst inspector/AOT methods inheriting from IJobProcessComponentData or interfaces with generics. Add [NoAlias] attribute. Improved codegen for csum. Improved codegen for abs(int). Improved codegen for abs on floatN/doubleN. [0.2.4-preview.29] - 2018-09-07 Fix issue when calling an explicit interface method not being matched through a generic constraint. Fix issue with or/and binary operation on a bool returned by a function. [0.2.4-preview.28] - 2018-09-05 Fix a compilation issue when storing a bool returned from a function to a component of a bool vector. Fix AOT compilation issue with a duplicated dictionary key. Fix settings of ANDROID_NDK_ROOT if it is not setup in Unity Editor. [0.2.4-preview.27] - 2018-09-03 Improve detection of jobs within nested generics for AOT/Burst inspector. Fix compiler bug of comparison of a pointer to null pointer. Fix crash compilation of sincos on ARM (neon/AARCH64). Fix issue when using a pointer to a VectorType resulting in an incorrect access of a vector type. Add support for doubles (preview). Improve AOT compiler error message/details if the compiler is failing before the linker. [0.2.4-preview.26] - 2018-08-21 Added support for cosh, sinh and tanh. [0.2.4-preview.25] - 2018-08-16 Fix warning in unity editor. [0.2.4-preview.24] - 2018-08-15 Improve codegen of math.compress. Improve codegen of math.asfloat/asint/asuint. Improve codegen of math.csum for int4. Improve codegen of math.count_bits. Support for lzcnt and tzcnt intrinsics. Fix AOT compilation errors for PS4 and XboxOne. Fix an issue that could cause wrong code generation for some unsafe ptr operations. [0.2.4-preview.23] - 2018-07-31 Fix bug with switch case to support not only int32. [0.2.4-preview.22] - 2018-07-31 Fix issue with pointers comparison not supported. Fix a StackOverflow exception when calling an interface method through a generic constraint on a nested type where the declaring type is a generic. Fix an issue with EntityCommandBuffer.CreateEntity/AddComponent that could lead to ArgumentException/IndexOutOfRangeException. [0.2.4-preview.21] - 2018-07-25 Correct issue with Android AOT compilation being unable to find the NDK. [0.2.4-preview.20] - 2018-07-05 Prepare the user documentation for a public release. [0.2.4-preview.19] - 2018-07-02 Fix compilation error with generics when types are coming from different assemblies. [0.2.4-preview.18] - 2018-06-26 Add support for subtracting pointers. [0.2.4-preview.17] - 2018-06-25 Bump only to force a new version pushed. [0.2.4-preview.16] - 2018-06-25 Fix AOT compilation errors. [0.2.4-preview.15] - 2018-06-25 Fix crash for certain access to readonly static variable. Fix StackOverflowException when using a generic parameter type into an interface method. [0.2.4-preview.14] - 2018-06-23 Fix an issue with package structure that was preventing Burst to work in Unity. [0.2.4-preview.13] - 2018-06-22 Add support for Burst timings menu. Improve codegen for sin/cos. Improve codegen when using swizzles on vector types. Add support for sincos intrinsic. Fix AOT deployment. [0.2.4-preview.12] - 2018-06-13 Fix a bug in codegen that was collapsing methods overload of System.Threading.Interlocked to the same method. [0.2.4-preview.11] - 2018-06-05 Fix exception in codegen when accessing readonly static fields from different control flow paths. [0.2.4-preview.10] - 2018-06-04 Fix a potential stack overflow issue when a generic parameter constraint on a type is also referencing another generic parameter through a generic interface constraint Update to latest Unity.Mathematics: Fix order of parameters and codegen for step functions. [0.2.4-preview.9] - 2018-05-29 Fix bug when casting an IntPtr to an enum pointer that was causing an invalid codegen exception. [0.2.4-preview.8] - 2018-05-24 Breaking change: Move Unity.Jobs.Accuracy/Support to Unity.Burst. Deprecate ComputeJobOptimizationAttribute in favor of BurstCompileAttribute. Fix bug when using enum with a different type than int. Fix bug with IL stind that could lead to a memory corruption. [0.2.4-preview.7] - 2018-05-22 Add support for nested structs in SOA native arrays. Add support for arbitrary sized elements in full SOA native arrays. Fix bug with conversion from signed/unsigned integers to signed numbers (integers & floats). Add support for substracting pointers at IL level. Improve codegen with pointers arithmetic to avoid checking for overflows. [0.2.4-preview.6] - 2018-05-11 Remove bool1 from mathematics and add proper support in Burst. Add support for ARM platforms in the Burst inspector UI. [0.2.4-preview.5] - 2018-05-09 Add support for readonly static fields. Add support for stackalloc. Fix potential crash on MacOSX when using memset is used indirectly. Fix crash when trying to write to a bool1*. Fix bug with EnableBurstCompilation checkbox not working in Unity Editor. [0.2.4-preview.4] - 2018-05-03 Fix an issue on Windows with DllNotFoundException occurring when trying to load burst-llvm.dll from a user profile containing unicode characters in the folder path. Fix an internal compiler error occurring with IL dup instruction. [0.2.4-preview.3] - 2018-05-03 Add support for struct with an explicit layout. Fix noalias regression (that was preventing the auto-vectorizer to work correctly on basic loops). 0.2.3 (21 March 2018) Improve error messages for static field access. Improve collecting of compilable job by trying to collect concrete job type instances (issue #23). 0.2.2 (19 March 2018) Improve error messages in case using is or as cast in C#. Improve error messages if a static delegate instance is used. Fix codegen error when converting a byte/ushort to a float."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Burst Getting started C# language support HPC# overview Static read-only fields and static constructor support String support Calling Burst compiled code Function pointers C#/.NET type support C#/.NET System namespace support DllImport and internal calls SharedStatic struct Burst instrinsics Burst intrinsics Common class Processor specific SIMD extensions Arm Neon intrinsics reference Editor reference Burst menu Burst Inspector Burst compilation Compilation overview Synchronous compilation BurstCompile attribute Assembly level BurstCompile BurstDiscard attribute Generic jobs Compilation warnings Building your project Burst AOT Player Settings reference Optimization Debugging and profiling tools Loop vectorization optimization Memory aliasing NoAlias attribute Aliasing and the job system AssumeRange attribute Hint intrinsic Constant intrinsic SkipLocalsInit attribute Modding support"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing-job-system.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing-job-system.html",
    "title": "Aliasing and the job system | FSM Unity Framework",
    "keywords": "Aliasing and the job system Unity's job system infrastructure has some limitations on what can alias within a job struct: Structs attributed with [NativeContainer] (for example, NativeArray and NativeSlice) that are members of a job struct don't alias. Job struct members with the [NativeDisableContainerSafetyRestriction] attribute can alias with other members. This is because this attribute explicitly opts in to this kind of aliasing. Pointers to structs attributed with [NativeContainer] can't appear in other structs attributed with [NativeContainer]. For example, you can't have a NativeArray<NativeSlice<T>>. The following example job shows how these limitations work in practice: [BurstCompile] private struct MyJob : IJob { public NativeArray<float> a; public NativeArray<float> b; public NativeSlice<int> c; [NativeDisableContainerSafetyRestriction] public NativeArray<byte> d; public void Execute() { ... } } a, b, and c don't alias with each other. d can alias with a, b, or c. Tip If you're used to working with C/C++'s Type Based Alias Analysis (TBAA), then you might assume that because d has a different type from a, b, or c, it shouldn't alias. However, in C#, pointers don't have any assumptions that pointing to a different type results in no aliasing. This is why d is assumed to alias with a, b, or c."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing-noalias.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing-noalias.html",
    "title": "NoAlias attribute | FSM Unity Framework",
    "keywords": "NoAlias attribute Use the [NoAlias] attribute to give Burst additional information on the aliasing of pointers and structs. In most use cases, you won't need to use the [NoAlias] attribute. You don't need to use it with [NativeContainer] attributed structs, or with fields in job structs. This is because the Burst compiler infers the no-alias information. The [NoAlias] attribute is exposed so that you can construct complex data structures where Burst can't infer the aliasing. If you use the [NoAlias] attribute on a pointer that could alias with another, it might result in undefined behavior and make it hard to track down bugs. You can use this attribute in the following ways: On a function parameter it signifies that the parameter doesn't alias with any other parameter to the function. On a struct field it signifies that the field doesn't alias with any other field of the struct. On a struct it signifies that the address of the struct can't appear within the struct itself. On a function return value it signifies that the returned pointer doesn't alias with any other pointer returned from the same function. NoAlias function parameter The following is an example of aliasing: int Foo(ref int a, ref int b) { b = 13; a = 42; return b; } For this, Burst produces the following assembly: mov dword ptr [rdx], 13 mov dword ptr [rcx], 42 mov eax, dword ptr [rdx] ret This means that Burst does the following: Stores 13 into b. Stores 42 into a. Reloads the value from b to return it. Burst has to reload b because it doesn't know whether a and b are backed by the same memory or not. Add the [NoAlias] attribute to the code to change this: int Foo([NoAlias] ref int a, ref int b) { b = 13; a = 42; return b; } For this, Burst produces the following assembly: mov dword ptr [rdx], 13 mov dword ptr [rcx], 42 mov eax, 13 ret In this case, the load from b has been replaced with moving the constant 13 into the return register. NoAlias struct field The following example is the same as the previous, but applied to a struct: struct Bar { public NativeArray<int> a; public NativeArray<float> b; } int Foo(ref Bar bar) { bar.b[0] = 42.0f; bar.a[0] = 13; return (int)bar.b[0]; } For this, Burst produces the following assembly: mov rax, qword ptr [rcx + 16] mov dword ptr [rax], 1109917696 mov rcx, qword ptr [rcx] mov dword ptr [rcx], 13 cvttss2si eax, dword ptr [rax] ret In this case, Burst does the following: Loads the address of the data in b into rax. Stores 42 into it (1109917696 is 0x42280000, which is 42.0f). Loads the address of the data in a into rcx. Stores 13 into it. Reloads the data in b and converts it to an integer for returning. If you know that the two NativeArrays aren't backed by the same memory, you can change the code to the following: struct Bar { [NoAlias] public NativeArray<int> a; [NoAlias] public NativeArray<float> b; } int Foo(ref Bar bar) { bar.b[0] = 42.0f; bar.a[0] = 13; return (int)bar.b[0]; } If you attribute both a and b with [NoAlias] it tells Burst that they don't alias with each other within the struct, which produces the following assembly: mov rax, qword ptr [rcx + 16] mov dword ptr [rax], 1109917696 mov rax, qword ptr [rcx] mov dword ptr [rax], 13 mov eax, 42 ret This means that Burst can return the integer constant 42. NoAlias struct Burst assumes that the pointer to a struct doesn't appear within the struct itself. However, there are cases where this isn't true: unsafe struct CircularList { public CircularList* next; public CircularList() { // The 'empty' list just points to itself. next = this; } } Lists are one of the few structures where it's normal to have the pointer to the struct accessible from somewhere within the struct itself. The following example indicates where [NoAlias] on a struct can help: unsafe struct Bar { public int i; public void* p; } float Foo(ref Bar bar) { *(int*)bar.p = 42; return ((float*)bar.p)[bar.i]; } This produces the following assembly: mov rax, qword ptr [rcx + 8] mov dword ptr [rax], 42 mov rax, qword ptr [rcx + 8] mov ecx, dword ptr [rcx] movss xmm0, dword ptr [rax + 4*rcx] ret In this case, Burst: Loads p into rax. Stores 42 into p. Loads p into rax again. Loads i into ecx. Returns the index into p by i. In this situation, Burst loads p twice. This is because it doesn't know if p points to the address of the struct bar. Once it stores 42 into p it has to reload the address of p from bar, which is a costly operation. Add [NoAlias] to prevent this: [NoAlias] unsafe struct Bar { public int i; public void* p; } float Foo(ref Bar bar) { *(int*)bar.p = 42; return ((float*)bar.p)[bar.i]; } This produces the following assembly: mov rax, qword ptr [rcx + 8] mov dword ptr [rax], 42 mov ecx, dword ptr [rcx] movss xmm0, dword ptr [rax + 4*rcx] ret In this situation, Burst only loads the address of p once, because [NoAlias] tells it that p can't be the pointer to bar. NoAlias function return Some functions can only return a unique pointer. For instance, malloc only returns a unique pointer. In this case, [return:NoAlias] gives some useful information to Burst. Important Only use [return: NoAlias] on functions that are guaranteed to produce a unique pointer. For example, with bump-allocations, or with things like malloc. Burst aggressively inlines functions for performance considerations, so with small functions, Burst inlines them into their parents to produce the same result without the attribute. The following example uses a bump allocator backed with a stack allocation: // Only ever returns a unique address into the stackalloc'ed memory. // We've made this no-inline because Burst will always try and inline // small functions like these, which would defeat the purpose of this // example [MethodImpl(MethodImplOptions.NoInlining)] unsafe int* BumpAlloc(int* alloca) { int location = alloca[0]++; return alloca + location; } unsafe int Func() { int* alloca = stackalloc int[128]; // Store our size at the start of the alloca. alloca[0] = 1; int* ptr1 = BumpAlloc(alloca); int* ptr2 = BumpAlloc(alloca); *ptr1 = 42; *ptr2 = 13; return *ptr1; } This produces the following assembly: push rsi push rdi push rbx sub rsp, 544 lea rcx, [rsp + 36] movabs rax, offset memset mov r8d, 508 xor edx, edx call rax mov dword ptr [rsp + 32], 1 movabs rbx, offset \"BumpAlloc(int* alloca)\" lea rsi, [rsp + 32] mov rcx, rsi call rbx mov rdi, rax mov rcx, rsi call rbx mov dword ptr [rdi], 42 mov dword ptr [rax], 13 mov eax, dword ptr [rdi] add rsp, 544 pop rbx pop rdi pop rsi ret The key things that Burst does: Has ptr1 in rdi. Has ptr2 in rax. Stores 42 into ptr1. Stores 13 into ptr2. Loads ptr1 again to return it. If you add the [return: NoAlias] attribute: [MethodImpl(MethodImplOptions.NoInlining)] [return: NoAlias] unsafe int* BumpAlloc(int* alloca) { int location = alloca[0]++; return alloca + location; } unsafe int Func() { int* alloca = stackalloc int[128]; // Store our size at the start of the alloca. alloca[0] = 1; int* ptr1 = BumpAlloc(alloca); int* ptr2 = BumpAlloc(alloca); *ptr1 = 42; *ptr2 = 13; return *ptr1; } It produces the following assembly: push rsi push rdi push rbx sub rsp, 544 lea rcx, [rsp + 36] movabs rax, offset memset mov r8d, 508 xor edx, edx call rax mov dword ptr [rsp + 32], 1 movabs rbx, offset \"BumpAlloc(int* alloca)\" lea rsi, [rsp + 32] mov rcx, rsi call rbx mov rdi, rax mov rcx, rsi call rbx mov dword ptr [rdi], 42 mov dword ptr [rax], 13 mov eax, 42 add rsp, 544 pop rbx pop rdi pop rsi ret In this case, Burst doesn't reload ptr2, and moves 42 into the return register."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/aliasing.html",
    "title": "Memory aliasing | FSM Unity Framework",
    "keywords": "Memory aliasing Memory aliasing is a way to tell Burst how your code uses data. This can improve and optimize the performance of your application. Memory aliasing happens when locations in the memory overlap each other. The following documentation outlines the difference between memory aliasing, and no memory aliasing. The following example shows a job that copies data from an input array to an output array: [BurstCompile] private struct CopyJob : IJob { [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public void Execute() { for (int i = 0; i < Input.Length; i++) { Output[i] = Input[i]; } } } No memory aliasing If the arrays Input and Output don't overlap, which means that their respective memory location doesn't overlap, the code returns the following result after running this job on a sample input/output: Memory with no aliasing If Burst is noalias aware, it can work at the scalar level to optimize the previous scalar loop. It does this through a process called vectorizing, where it rewrites the loop to process elements in a small batch. For example, Burst could work at vector level in 4 by 4 elements: Memory with no aliasing vectorized Memory aliasing If the Output array overlaps the Input array by one element (for example Output[0] points to Input[1]), then this means that the memory is aliasing. This gives the following result when you run CopyJob without the auto vectorizer: Memory with aliasing If Burst isn't aware of the memory aliasing, it tries to auto vectorize the loop, which results in the following: Memory with aliasing and invalid vectorized code The result of this code is invalid and might lead to bugs if Burst can't identify them. Generated code In the CopyJob example, there is an x64 assembly targeted at AVX2 in its loop. The instruction vmovups moves 8 floats, so a single auto vectorized loop moves 4 × 8 floats, which equals 32 floats copied per loop iteration, instead of just one: .LBB0_4: vmovups ymm0, ymmword ptr [rcx - 96] vmovups ymm1, ymmword ptr [rcx - 64] vmovups ymm2, ymmword ptr [rcx - 32] vmovups ymm3, ymmword ptr [rcx] vmovups ymmword ptr [rdx - 96], ymm0 vmovups ymmword ptr [rdx - 64], ymm1 vmovups ymmword ptr [rdx - 32], ymm2 vmovups ymmword ptr [rdx], ymm3 sub rdx, -128 sub rcx, -128 add rsi, -32 jne .LBB0_4 test r10d, r10d je .LBB0_8 The following example shows the same Burst compiled loop, but Burst's aliasing is artificially disabled: .LBB0_2: mov r8, qword ptr [rcx] mov rdx, qword ptr [rcx + 16] cdqe mov edx, dword ptr [rdx + 4*rax] mov dword ptr [r8 + 4*rax], edx inc eax cmp eax, dword ptr [rcx + 8] jl .LBB0_2 The result is entirely scalar and runs approximately 32 times slower than the highly optimized, vectorized variant that the original alias analysis produces. Function cloning For function calls where Burst knows about the aliasing between parameters to the function, Burst can infer the aliasing. It can then propagate this onto the called function to improve optimization: [MethodImpl(MethodImplOptions.NoInlining)] int Bar(ref int a, ref int b) { a = 42; b = 13; return a; } int Foo() { var a = 53; var b = -2; return Bar(ref a, ref b); } The assembly for Bar would be: mov dword ptr [rcx], 42 mov dword ptr [rdx], 13 mov eax, dword ptr [rcx] ret This is because Burst doesn't know the aliasing of a and b within the Bar function. This is in line with what other compiler technologies do with this code snippet. Burst is smarter than this though. Through a process of function cloning, Burst creates a copy of Bar where it knows that the aliasing properties of a and b don't alias. It then replaces the original call to Bar with a call to the copy. This results in the following assembly: mov dword ptr [rcx], 42 mov dword ptr [rdx], 13 mov eax, 42 ret In this scenario, Burst doesn't perform the second load from a. Aliasing checks Because aliasing is key to Burst's ability to optimize for performance, there are some aliasing intrinsics: Unity.Burst.CompilerServices.Aliasing.ExpectAliased expects that the two pointers do alias, and generates a compiler error if not. Unity.Burst.CompilerServices.Aliasing.ExpectNotAliased expects that the two pointers don't alias, and generates a compiler error if not. An example: using static Unity.Burst.CompilerServices.Aliasing; [BurstCompile] private struct CopyJob : IJob { [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public unsafe void Execute() { // NativeContainer attributed structs (like NativeArray) cannot alias with each other in a job struct! ExpectNotAliased(Input.getUnsafePtr(), Output.getUnsafePtr()); // NativeContainer structs cannot appear in other NativeContainer structs. ExpectNotAliased(in Input, in Output); ExpectNotAliased(in Input, Input.getUnsafePtr()); ExpectNotAliased(in Input, Output.getUnsafePtr()); ExpectNotAliased(in Output, Input.getUnsafePtr()); ExpectNotAliased(in Output, Output.getUnsafePtr()); // But things definitely alias with themselves! ExpectAliased(in Input, in Input); ExpectAliased(Input.getUnsafePtr(), Input.getUnsafePtr()); ExpectAliased(in Output, in Output); ExpectAliased(Output.getUnsafePtr(), Output.getUnsafePtr()); } } These checks only run when optimizations are enabled, because proper aliasing deduction is intrinsically linked to the optimizer's ability to see through functions via inlining."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/building-aot-settings.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/building-aot-settings.html",
    "title": "Burst AOT Player Settings reference | FSM Unity Framework",
    "keywords": "Burst AOT Player Settings reference To control Burst's AOT compilation, use the Player Settings window (Edit > Player Settings > Burst AOT Settings). These settings override the Burst settings in the Jobs menu when you make a build of your project. Setting Function Target Platform Displays the current platform. To change the platform, go to File > Build Settings. You can set these Player Settings per platform. Enable Burst Compilation Enable this setting to turn Burst compilation on. Disable this setting to deactivate Burst compilation for the selected platform. Enable Optimizations Enable this setting to activate Burst optimizations. Force Debug Information Enable this setting to make Burst generate debug information. This adds debug symbols to your project, even in release builds of your project, so that when you load it in a debugger you can see file and line information. Use Platform SDK Linker (Windows, macOS, and Linux builds only) Disables cross compilation support. When you enable this setting, you must use platform-specific tools for your target platform. Only enable this setting for debugging purposes. For more information, see Platforms with cross compilation disabled. Target 32Bit CPU Architectures (Displayed if the architecture is supported) Select the CPU architectures that you want to use for 32 bit builds. By default, SSE2 and SSE4 are selected. Target 64Bit CPU Architectures (Displayed if the architecture is supported) Select the CPU architectures that you want to use for 64-bit builds. By default, SSE2 and SSE4 are selected. Target Arm 64Bit CPU Architectures (Displayed if the architecture is supported) Select the CPU architectures that you want to use for Arm 64-bit builds. By default, ARMV8A is selected. Optimize For Select which optimization settings to compile Burst code for. For more information see OptimizeFor. Performance Optimizes the job to run as fast as possible. Size Optimizes to make the code generation as small as possible. Fast Compilation Compiles code as fast as possible, with minimal optimization. Burst doesn't perform any vectorization, inlining, or loop optimizations. Balanced (Default) Optimizes for code that runs fast but keeps compile time as low as possible. Disabled Warnings Specify a semi-colon separated list of Burst warning numbers to disable the warnings for a player build. Unity shares this setting across all platforms. This can be useful if you want to ignore specific compilation warnings while testing your application. The CPU Architecture setting is only supported for Windows, macOS, Linux and Android. Unity builds a Player that supports the CPU architectures you've selected. Burst generates a special dispatch into the module, so that the code generated detects the CPU the target platform uses and selects the appropriate CPU architecture at runtime. Optimize For setting Note Any OptimizeFor setting is the global default optimization setting for any Burst job or function-pointer. If any assembly level BurstCompile, or a specific Burst job or function-pointer has an OptimizeFor setting, it overrides the global optimization setting for those jobs. To control how Burst optimizes your code, use the Optimize For setting in the Editor, or use the OptimizeFor field: [BurstCompile(OptimizeFor = OptimizeFor.FastCompilation)] public struct MyJob : IJob { // ... }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/building-projects.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/building-projects.html",
    "title": "Building your project | FSM Unity Framework",
    "keywords": "Building your project When you build your project, Burst compiles your code, then creates a single dynamic library, and puts it into the Plugins folder for the platform you're targeting. For example, on Windows, the path is Data/Plugins/lib_burst_generated.dll. Note This is different if your target platform is iOS. Instead, Unity generates a static library because of Apple's submission requirements for TestFlight. The job system runtime loads the generated library the first time a Burst compiled method is invoked. To control Burst's AOT compilation, use the settings in the Burst AOT Settings section of the Player Settings window (Edit > Player Settings > Burst AOT Settings). For more information, see Burst AOT Settings reference. Platforms without cross compilation If you're compiling for a non-desktop platform, then Burst compilation requires specific platform compilation tools (similar to IL2CPP). By default, desktop platforms (macOS, Linux, Windows) don't need external toolchain support, unless you enable the Use Platform SDK Linker setting in the Burst AOT Settings. The table below lists the level of support for AOT compilation on each platform. If you select an invalid target (one with missing tools, or unsupported), Unity doesn't use Burst compilation, which might lead it to fail, but Unity still builds the target without Burst optimizations. Note Burst supports cross-compilation between desktop platforms (macOS/Linux/Windows) by default. Host Editor platform Target Player platform Supported CPU architectures External toolchain requirements Windows Windows x86 (SSE2, SSE4) x64 (SSE2, SSE4, AVX, AVX2) None Windows Universal Windows Platform x86 (SSE2, SSE4) x64 (SSE2, SSE4, AVX, AVX2) ARM32 (Thumb2, Neon32) ARMV8 AARCH64 Note: A UWP build always compiles all four targets. Visual Studio 2017 Universal Windows Platform Development Workflow C++ Universal Platform Tools Windows Android x86 SSE2 ARMV7 (Thumb2, Neon32) ARMV8 AARCH64 (ARMV8A, ARMV8A_HALFFP, ARMV9A) Android NDK Important: Use the Android NDK that you install through Unity Hub (via Add Component). Burst falls back to the one that the ANDROID_NDK_ROOT environment variable specifies if the Unity external tools settings aren't configured. Windows Magic Leap ARMV8 AARCH64 You must install the Lumin SDK via the Magic Leap Package Manager and configured in the Unity Editor's External Tools Preferences. Windows Xbox One x64 SSE4 Microsoft GDK Windows Xbox Series x64 AVX2 Microsoft GDK Windows PlayStation 4 x64 SSE4 Minimum PS4 SDK version 8.00 Windows PlayStation 5 x64 AVX2 Minimum PS5 SDK version 2.00 Windows Nintendo Switch ARMV8 AARCH64 None macOS macOS x64 (SSE2, SSE4, AVX, AVX2), Apple Silicon None macOS iOS ARM32 Thumb2/Neon32, ARMV8 AARCH64 Xcode with command line tools installed (xcode-select --install) macOS Android x86 SSE2 ARMV7 (Thumb2, Neon32) ARMV8 AARCH64 (ARMV8A, ARMV8A_HALFFP, ARMV9A) Android NDK Important: Use the Android NDK that you install through Unity Hub (via Add Component). Burst falls back to the one that the ANDROID_NDK_ROOT environment variable specifies if the Unity external tools settings aren't configured. macOS Magic Leap ARMV8 AARCH64 You must install the Lumin SDK via the Magic Leap Package Manager and configured in the Unity Editor's External Tools Preferences. Linux Linux x64 (SSE2, SSE4, AVX, AVX2) None The maximum target CPU is hardcoded per platform. For standalone builds that target desktop platforms (Windows/Linux/macOS) you can choose the supported targets via the Burst AOT Settings Projects that don't use Burst Some projects can't use Burst as the compiler: iOS projects from the Windows Editor Android projects from the Linux Editor Xcode projects generated from the Create Xcode Project option Multiple Burst targets When Burst compiles multiple target platforms during a build, it has to perform separate compilations. For example, if you want to compile X64_SSE2 and X64_SSE4, the Burst has to do two separate compilations to generate code for each of the targets you choose. To keep the combinations of targets to a minimum, Burst target platforms require multiple processor instruction sets underneath: SSE4.2 is gated on having SSE4.2 and POPCNT instruction sets. AVX2 is gated on having AVX2, FMA, F16C, BMI1, and BMI2 instruction sets. ARMV8A is a basic Armv8-A CPU target ARMV8A_HALFFP is ARMV8A plus the following extensions: fullfp16, dotprod, crypto, crc, rdm, lse. In practice, this means Cortex A75/A55 and later cores. ARMV9A is ARMV8A_HALFFP plus SVE2 support. In practice, this means Cortex X2/A710/A510 and later cores. Important: this target is currently experimental. Dynamic dispatch based on runtime CPU features For all x86/x64 CPU desktop platforms, as well as for 64-bit Arm on Android, Burst takes into account the CPU features available at runtime to dispatch jobs to different versions it compiles. For x86 and x64 CPUs, Burst supports SSE2 and SSE4 instruction sets at runtime only. For example, with dynamic CPU dispatch, if your CPU supports SSE3 and below, Burst selects SSE2 automatically."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstcompile-assembly.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstcompile-assembly.html",
    "title": "Assembly level BurstCompile | FSM Unity Framework",
    "keywords": "Assembly level BurstCompile Use the BurstCompile attribute on an assembly to set options for all Burst jobs and function-pointers within the assembly: [assembly: BurstCompile(CompileSynchronously = true)] For example, if an assembly just contains game code which needs to run quickly, you can use: [assembly: BurstCompile(OptimizeFor = OptimizeFor.FastCompilation)] This means that Burst compiles the code as fast as it possibly can, which means that you can iterate on the game code much more quickly. It also means that other assemblies compile as they did before, which gives you more control on how Burst works with your code. Assembly-level BurstCompile attributes iterate with any job or function-pointer attribute, and also with any globally set options from the Burst Editor menu. Burst prioritizes assembly level attributes in the following order: Editor menu settings take precedence. For example, if you enable Native Debug Compilation from the Burst menu, Burst always compiles your code ready for debugging. Burst checks any BurstCompile attribute on a job or function-pointer. If you have CompileSynchronously = true in BurstCompile, then Burst compiles synchronously Otherwise, Burst sources any remaining settings from any assembly level attribute. For example: [assembly: BurstCompile(OptimizeFor = OptimizeFor.FastCompilation)] // This job will be optimized for fast-compilation, because the per-assembly BurstCompile asked for it [BurstCompile] struct AJob : IJob { // ... } // This job will be optimized for size, because the per-job BurstCompile asked for it [BurstCompile(OptimizeFor = OptimizeFor.Size)] struct BJob : IJob { // ... }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstcompile.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstcompile.html",
    "title": "BurstCompile attribute | FSM Unity Framework",
    "keywords": "BurstCompile attribute To improve the performance of Burst, you can change how it behaves when it compiles a job with the [BurstCompile] attribute. Use it do the following: Use a different accuracy for math functions (for example, sin, cos). Relax the order of math computations so that Burst can rearrange the floating point calculations. Force a synchronous compilation of a job (only for just-in-time compilation). For example, you can use the [BurstCompile] attribute to change the floating precision and float mode of Burst like so: [BurstCompile(FloatPrecision.Med, FloatMode.Fast)] FloatPrecision Use the FloatPrecision enumeration to define Burst's floating precision accuracy. Float precision is measured in ulp (unit in the last place or unit of least precision). This is the space between floating-point numbers: the value the least significant digit represents if it's 1. Unity.Burst.FloatPrecision provides the following accuracy: FloatPrecision.Standard: Default value, which is the same as FloatPrecision.Medium. This provides an accuracy of 3.5 ulp. FloatPrecision.High: Provides an accuracy of 1.0 ulp. FloatPrecision.Medium: Provides an accuracy of 3.5 ulp. FloatPrecision.Low: Has an accuracy defined per function, and functions might specify a restricted range of valid inputs. Note: In previous versions of the Burst API, the FloatPrecision enum was named Accuracy. FloatPrecision.Low If you use the FloatPrecision.Low mode, the following functions have a precision of 350.0 ulp. All other functions inherit the ulp from FloatPrecision.Medium. Unity.Mathematics.math.sin(x) Unity.Mathematics.math.cos(x) Unity.Mathematics.math.exp(x) Unity.Mathematics.math.exp2(x) Unity.Mathematics.math.exp10(x) Unity.Mathematics.math.log(x) Unity.Mathematics.math.log2(x) Unity.Mathematics.math.log10(x) Unity.Mathematics.math.pow(x, y) Negative x to the power of a fractional y aren't supported. Unity.Mathematics.math.fmod(x, y) FloatMode Use the FloatMode enumeration to define Burst's floating point math mode. It provides the following modes: FloatMode.Default: Defaults to FloatMode.Strict mode. FloatMode.Strict: Burst doesn't perform any re-arrangement of the calculation and respects special floating point values (such as denormals, NaN). This is the default value. FloatMode.Fast: Burst can perform instruction re-arrangement and use dedicated or less precise hardware SIMD instructions. FloatMode.Deterministic: Unsupported. Deterministic mode is reserved for a future iteration of Burst. For hardware that can support Multiply and Add (e.g mad a * b + c) into a single instruction, you can use FloatMode.Fast to enable this optimization. However, the reordering of these instructions might lead to a lower accuracy. Use FloatMode.Fast for scenarios where the exact order of the calculation and the uniform handling of NaN values aren't required."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstdiscard.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-burstdiscard.html",
    "title": "BurstDiscard attribute | FSM Unity Framework",
    "keywords": "BurstDiscard attribute If you're running C# code not inside Burst-compiled code, you might want to use managed objects, but not compile these portions of code within Burst. To do this, use the [BurstDiscard] attribute on a method: [BurstCompile] public struct MyJob : IJob { public void Execute() { // Only executed when running from a full .NET runtime // this method call will be discard when compiling this job with // [BurstCompile] attribute MethodToDiscard(); } [BurstDiscard] private static void MethodToDiscard(int arg) { Debug.Log($\"This is a test: {arg}\"); } } Note A method with [BurstDiscard] can't have a return value. You can use a ref or out parameter, which indicates whether the code is running on Burst or managed: [BurstDiscard] private static void SetIfManaged(ref bool b) => b = false; private static bool IsBurst() { var b = true; SetIfManaged(ref b); return b; }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-generic-jobs.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-generic-jobs.html",
    "title": "Generic jobs | FSM Unity Framework",
    "keywords": "Generic jobs Burst compiles a job in two ways: In the Editor, it compiles the job when it's scheduled, known as just-in-time (JIT) compilation. In a player build, it compiles the job as part of the built player, known as ahead-of-time (AOT) compilation. For more information, see the documentation on Compilation. If the job is a concrete type (doesn't use generics), Burst compiles it in both modes (AOT and JIT). However, a generic job might behave in an unexpected way. While Burst supports generics, it has limited support for generic jobs or function pointers. If you notice that a job scheduled in the Editor is running at full speed, but not in a built player, it's might be a problem related to generic jobs. The following example defines a generic job: // Direct generic job [BurstCompile] struct MyGenericJob<TData> : IJob where TData : struct { public void Execute() { ... } } You can also nest generic jobs: // Nested generic job public class MyGenericSystem<TData> where TData : struct { [BurstCompile] struct MyGenericJob : IJob { public void Execute() { ... } } public void Run() { var myJob = new MyGenericJob(); // implicitly MyGenericSystem<TData>.MyGenericJob myJob.Schedule(); } } Jobs that aren't Burst compiled look like this: // Direct Generic Job var myJob = new MyGenericJob<int>(); myJob.Schedule(); // Nested Generic Job var myJobSystem = new MyGenericSystem<float>(); myJobSystem.Run(); In both cases, in a player build, the Burst compiler detects that it has to compile MyGenericJob<int> and MyGenericJob<float>. This is because the generic jobs (or the type surrounding it for the nested job) are used with fully resolved generic arguments (int and float). However, if these jobs are used indirectly through a generic parameter, the Burst compiler can't detect the jobs it has to compile at player build time: public static void GenericJobSchedule<TData>() where TData: struct { // Generic argument: Generic Parameter TData // This Job won't be detected by the Burst Compiler at standalone-player build time. var job = new MyGenericJob<TData>(); job.Schedule(); } // The implicit MyGenericJob<int> will run at Editor time in full Burst speed // but won't be detected at standalone-player build time. GenericJobSchedule<int>(); The same restriction applies if you declare the job in the context of generic parameter that comes from a type: // Generic Parameter TData public class SuperJobSystem<TData> { // Generic argument: Generic Parameter TData // This Job won't be detected by the Burst Compiler at standalone-player build time. public MyGenericJob<TData> MyJob; } If you want to use generic jobs, you must use them directly with fully resolved generic arguments (for example, int, MyOtherStruct). You can't use them with a generic parameter indirection (for example, MyGenericJob<TContext>). Important Burst doesn't support scheduling generic Jobs through generic methods. While this works in the Editor, it doesn't work in the standalone player. Function pointers Function pointers are restricted because you can't use a generic delegate through a function pointer with Burst: public delegate void MyGenericDelegate<T>(ref TData data) where TData: struct; var myGenericDelegate = new MyGenericDelegate<int>(MyIntDelegateImpl); // Will fail to compile this function pointer. var myGenericFunctionPointer = BurstCompiler.CompileFunctionPointer<MyGenericDelegate<int>>(myGenericDelegate); This limitation is because of a limitation of the .NET runtime to interop with such delegates. For more information, see the documentation on Function pointers."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-overview.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-overview.html",
    "title": "Compilation overview | FSM Unity Framework",
    "keywords": "Compilation overview Burst compiles your code in different ways depending on its context. In Play mode, Burst compiles your code just-in-time (JIT). When you build and run your application to a player, Burst compiles ahead-of-time (AOT). Just-in-time compilation While your application is in Play mode in the Editor, Burst compiles your code asynchronously at the point that Unity uses it. This means that your code runs under the default Mono compiler until Burst completes its work in the background. To force Unity to compile your code synchronously while in the Editor, see Synchronous compilation Ahead-of-time compilation When you build your project, Burst compiles all the supported code ahead-of-time (AOT) into a native library which Unity ships with your application. To control how Burst compiles AOT, use the Player Settings window. Depending on the platform you want to build for, AOT compilation might require access to linker tools. For more information, see Building your project. Further resources Synchronous compilation BurstCompile attribute"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-synchronous.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-synchronous.html",
    "title": "Synchronous compilation | FSM Unity Framework",
    "keywords": "Synchronous compilation By default, when in Play mode, the Burst compiler compiles jobs asynchronously. To force synchronous compilation, set the CompileSynchronously property to true, which compiles your method on the first schedule. [BurstCompile(CompileSynchronously = true)] public struct MyJob : IJob { // ... } If you don't set this property, on the first call of a job, Burst compiles it asynchronously in the background, and runs a managed C# job in the mean time. This minimizes any frame hitching and keeps the experience responsive. However, when you set CompileSynchronously = true, no asynchronous compilation can happen. This means that it might take longer for Burst to compile. This pause for compilation affects the current running frame, which means that hitches can happen and it might provide an unresponsive experience for users. In general, only use CompileSynchronously = true in the following situations: If you have a long running job that only runs once. The performance of the compiled code might outweigh the downsides doing the compilation synchronously. If you're profiling a Burst job and want to test the code from the Burst compiler. When you do this, perform a warmup to discard any timing measurements from the first call to the job. This is because the profiling data includes the compilation time and skews the result. To aid with debugging the difference between managed and Burst compiled code."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-warnings.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation-warnings.html",
    "title": "Compilation warnings | FSM Unity Framework",
    "keywords": "Compilation warnings This page describes common compilation warnings, and how to fix them. IgnoreWarning attribute The Unity.Burst.CompilerServices.IgnoreWarningAttribute attribute lets you suppress warnings for a specific function that is being compiled from Burst. However, the warnings that the Burst compiler generates are very important to pay attention to, so this attribute should be used sparingly and only when necessary. The sections below describe the specific situations in which you might want to suppress warnings. BC1370 Warning BC1370 produces the message: An exception was thrown from a function without the correct [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")] guard... This warning happens if Unity encounters a throw in code that [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")] doesn't guard. In the Editor, thrown exceptions will be caught and logged to the Console, but in a Player build, a throw becomes an abort, which crashes your application. Burst warns you about these exceptions, and advises you to place them in functions guarded with [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")], because functions guarded with that attribute will not be included in Player builds. However, if you want to purposely throw an exception to crash your application, use the IgnoreWarningAttribute to suppress the warnings that Burst provides on the throw: [IgnoreWarning(1370)] int DoSomethingMaybe(int x) { if (x < 0) throw new Exception(\"Dang - sorry I crashed your game!\"); return x * x; } Note This warning is only produced for exceptions that persist into Player builds. Editor-only or debug-only exception throws that aren't compiled into Player builds will not trigger this warning. BC1371 Warning BC1371 produces the message: A call to the method 'xxx' has been discarded, due to its use as an argument to a discarded method... To understand this warning, consider the following example: [BurstDiscard] static void DoSomeManagedStuff(int x) { ///.. only run when Burst compilation is disabled } // A function that computes some result which we need to pass to managed code int BurstCompiledCode(int x,int y) { return y+2*x; } [BurstCompile] void BurstMethod() { var myValue = BurstCompiledCode(1,3); DoSomeManagedStuff(myValue); } When Unity compiles your C# code in release mode, it optimizes and removes the local variable myValue. This means that Burst receives something like the following code : [BurstCompile] void BurstedMethod() { DoSomeManagedStuff(BurstCompiledCode(1,3)); } This makes Burst generate the warning, because Burst discards DoSomeManagedStuff, along with the BurstCompiledCode argument. This means that the BurstCompiledCode function is no longer executed, which generates the warning. If this isn't what you intended then ensure the variable has multiple uses. For example: void IsUsed(ref int x) { // Dummy function to prevent removal } [BurstCompile] void BurstedMethod() { var myValue = BurstCompiledCode(1,3); DoSomeManagedStuff(myValue); IsUsed(ref myValue); } Alternatively, if you're happy that the code is being discarded correctly, ignore the warning on the BurstedMethod like so: [IgnoreWarning(1371)] [BurstCompile] void BurstMethod() { var myValue = BurstCompiledCode(1,3); DoSomeManagedStuff(myValue); }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/compilation.html",
    "title": "Compilation | FSM Unity Framework",
    "keywords": "Compilation Burst compiles your code in different ways, depending on its context. You can also change the way that the compiler behaves when compiling code. Topic Description Compilation overview Understand the different compilation types. Synchronous compilation Understand synchronous compilation. BurstCompile attribute Customize the BurstCompile attribute to improve performance. BurstDiscard attribute Use the BurstDiscard attribute to select which portions of code to compile. Generic jobs Understand how Burst compiles jobs. Compilation warnings Fix common compilation warnings. Additional resources C# language support"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-common.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-common.html",
    "title": "Burst Intrinsics Common class | FSM Unity Framework",
    "keywords": "Burst Intrinsics Common class The Unity.Burst.Intrinsics.Common intrinsics are for functionality shared across the hardware targets that Burst supports. Pause Unity.Burst.Intrinsics.Common.Pause is an intrinsic that requests that the CPU pause the current thread. It maps to pause on x86, and yield on ARM. Use it to stop spin locks over contending on an atomic access, which reduces contention and power on that section of code. Prefetch The Unity.Burst.Intrinsics.Common.Prefetch is an experimental intrinsic that provides a hint that Burst should prefetch the memory location into the cache. Because the intrinsic is experimental, you must use the UNITY_BURST_EXPERIMENTAL_PREFETCH_INTRINSIC preprocessor define to get access to it. umul128 Use the Unity.Burst.Intrinsics.Common.umul128 intrinsic to access 128-bit unsigned multiplication. These multiplies are useful for hashing functions. It maps 1:1 with hardware instructions on x86 and ARM targets. InterlockedAnd & InterlockedOr The Unity.Burst.Intrinsics.Common.InterlockedAnd and Unity.Burst.Intrinsics.Common.InterlockedOr are experimental intrinsics that provides atomic and/or operations on int, uint, long, and ulong types. Because these intrinsics are experimental, you must use the UNITY_BURST_EXPERIMENTAL_ATOMIC_INTRINSICS preprocessor define to get access to them."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-dllimport.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-dllimport.html",
    "title": "| FSM Unity Framework",
    "keywords": "DllImport and internal calls To call native functions, use [DllImport]: [DllImport(\"MyNativeLibrary\")] public static extern int Foo(int arg); Burst also supports internal calls implemented inside Unity: // In UnityEngine.Mathf [MethodImpl(MethodImplOptions.InternalCall)] public static extern int ClosestPowerOfTwo(int value); DllImport is only supported for native plug-ins, not platform-dependent libraries like kernel32.dll. For all DllImport and internal calls, you can only use the following types as parameter or return types: Type Supported type Built-in and intrinsic types byte / sbyte short / ushort int / uint long / ulong float double System.IntPtr / System.UIntPtr Unity.Burst.Intrinsics.v64 / Unity.Burst.Intrinsics.v128 / Unity.Burst.Intrinsics.v256 Pointers and references sometype* : Pointer to any of the other types in this list ref sometype : Reference to any of the other types in this list Handle structs unsafe struct MyStruct { void* Ptr; } : Struct containing a single pointer field unsafe struct MyStruct { int Value; } : Struct containing a single integer field Note Passing structs by value isn't supported; you need to pass them through a pointer or reference. The only exception is that handle structs are supported. These are structs that contain a single field of pointer or integer type."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-neon.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-neon.html",
    "title": "Burst Arm Neon intrinsics reference | FSM Unity Framework",
    "keywords": "Burst Arm Neon intrinsics reference This page contains an ordered reference for the APIs in Unity.Burst.Intrinsics.Arm.Neon. For information on how to use these, see the documentation on Processor specific SIMD extensions. Intrinsics type creation and conversion Operation Description APIs vcreate Create vector Click here to expand the API list vcreate_f16 vcreate_f32 vcreate_f64 vcreate_s16 vcreate_s32 vcreate_s64 vcreate_s8 vcreate_u16 vcreate_u32 vcreate_u64 vcreate_u8 vdup_n Duplicate (splat) value Click here to expand the API list vdup_n_f32 vdup_n_f64 vdup_n_s16 vdup_n_s32 vdup_n_s64 vdup_n_s8 vdup_n_u16 vdup_n_u32 vdup_n_u64 vdup_n_u8 vdupq_n_f32 vdupq_n_f64 vdupq_n_s16 vdupq_n_s32 vdupq_n_s64 vdupq_n_s8 vdupq_n_u16 vdupq_n_u32 vdupq_n_u64 vdupq_n_u8 vdup_lane Duplicate (splat) vector element Click here to expand the API list vdup_lane_f32 vdup_lane_f64 vdup_lane_s16 vdup_lane_s32 vdup_lane_s64 vdup_lane_s8 vdup_lane_u16 vdup_lane_u32 vdup_lane_u64 vdup_lane_u8 vdup_laneq_f32 vdup_laneq_f64 vdup_laneq_s16 vdup_laneq_s32 vdup_laneq_s64 vdup_laneq_s8 vdup_laneq_u16 vdup_laneq_u32 vdup_laneq_u64 vdup_laneq_u8 vdupq_lane_f32 vdupq_lane_f64 vdupq_lane_s16 vdupq_lane_s32 vdupq_lane_s64 vdupq_lane_s8 vdupq_lane_u16 vdupq_lane_u32 vdupq_lane_u64 vdupq_lane_u8 vdupq_laneq_f32 vdupq_laneq_f64 vdupq_laneq_s16 vdupq_laneq_s32 vdupq_laneq_s64 vdupq_laneq_s8 vdupq_laneq_u16 vdupq_laneq_u32 vdupq_laneq_u64 vdupq_laneq_u8 vdups_lane_f32 vdups_lane_s32 vdups_lane_u32 vdups_laneq_f32 vdups_laneq_s32 vdups_laneq_u32 vdupb_lane_s8 vdupb_lane_u8 vdupb_laneq_s8 vdupb_laneq_u8 vdupd_lane_f64 vdupd_lane_s64 vdupd_lane_u64 vdupd_laneq_f64 vdupd_laneq_s64 vdupd_laneq_u64 vduph_lane_s16 vduph_lane_u16 vduph_laneq_s16 vduph_laneq_u16 vmov_n Duplicate (splat) value Click here to expand the API list vmov_n_f32 vmov_n_f64 vmov_n_s16 vmov_n_s32 vmov_n_s64 vmov_n_s8 vmov_n_u16 vmov_n_u32 vmov_n_u64 vmov_n_u8 vmovq_n_f32 vmovq_n_f64 vmovq_n_s16 vmovq_n_s32 vmovq_n_s64 vmovq_n_s8 vmovq_n_u16 vmovq_n_u32 vmovq_n_u64 vmovq_n_u8 vcopy_lane Insert vector element from another vector element Click here to expand the API list vcopy_lane_f32 vcopy_lane_f64 vcopy_lane_s16 vcopy_lane_s32 vcopy_lane_s64 vcopy_lane_s8 vcopy_lane_u16 vcopy_lane_u32 vcopy_lane_u64 vcopy_lane_u8 vcopy_laneq_f32 vcopy_laneq_f64 vcopy_laneq_s16 vcopy_laneq_s32 vcopy_laneq_s64 vcopy_laneq_s8 vcopy_laneq_u16 vcopy_laneq_u32 vcopy_laneq_u64 vcopy_laneq_u8 vcopyq_lane_f32 vcopyq_lane_f64 vcopyq_lane_s16 vcopyq_lane_s32 vcopyq_lane_s64 vcopyq_lane_s8 vcopyq_lane_u16 vcopyq_lane_u32 vcopyq_lane_u64 vcopyq_lane_u8 vcopyq_laneq_f32 vcopyq_laneq_f64 vcopyq_laneq_s16 vcopyq_laneq_s32 vcopyq_laneq_s64 vcopyq_laneq_s8 vcopyq_laneq_u16 vcopyq_laneq_u32 vcopyq_laneq_u64 vcopyq_laneq_u8 vcombine Join two vectors into a larger vector Click here to expand the API list vcombine_f16 vcombine_f32 vcombine_f64 vcombine_s16 vcombine_s32 vcombine_s64 vcombine_s8 vcombine_u16 vcombine_u32 vcombine_u64 vcombine_u8 vget_high Get the higher half of the vector Click here to expand the API list vget_high_f32 vget_high_f64 vget_high_s16 vget_high_s32 vget_high_s64 vget_high_s8 vget_high_u16 vget_high_u32 vget_high_u64 vget_high_u8 vget_low Get the lower half of the vector Click here to expand the API list vget_low_f32 vget_low_f64 vget_low_s16 vget_low_s32 vget_low_s64 vget_low_s8 vget_low_u16 vget_low_u32 vget_low_u64 vget_low_u8 Arithmetic Operation Description APIs vadd Add Click here to expand the API list vadd_f32 vadd_f64 vadd_s16 vadd_s32 vadd_s64 vadd_s8 vadd_u16 vadd_u32 vadd_u64 vadd_u8 vaddq_f32 vaddq_f64 vaddq_s16 vaddq_s32 vaddq_s64 vaddq_s8 vaddq_u16 vaddq_u32 vaddq_u64 vaddq_u8 vaddd_s64 vaddd_u64 vaddv Add across vector Click here to expand the API list vaddv_f32 vaddv_s16 vaddv_s32 vaddv_s8 vaddv_u16 vaddv_u32 vaddv_u8 vaddvq_f32 vaddvq_f64 vaddvq_s16 vaddvq_s32 vaddvq_s64 vaddvq_s8 vaddvq_u16 vaddvq_u32 vaddvq_u64 vaddvq_u8 vaddl Add long Click here to expand the API list vaddl_s16 vaddl_s32 vaddl_s8 vaddl_u16 vaddl_u32 vaddl_u8 vaddl_high_s16 vaddl_high_s32 vaddl_high_s8 vaddl_high_u16 vaddl_high_u32 vaddl_high_u8 vaddlv Add long across Vector Click here to expand the API list vaddlv_s16 vaddlv_s32 vaddlv_s8 vaddlv_u16 vaddlv_u32 vaddlv_u8 vaddlvq_s16 vaddlvq_s32 vaddlvq_s8 vaddlvq_u16 vaddlvq_u32 vaddlvq_u8 vaddw Add wide Click here to expand the API list vaddw_s16 vaddw_s32 vaddw_s8 vaddw_u16 vaddw_u32 vaddw_u8 vaddw_high_s16 vaddw_high_s32 vaddw_high_s8 vaddw_high_u16 vaddw_high_u32 vaddw_high_u8 vhadd Halving add Click here to expand the API list vhadd_s16 vhadd_s32 vhadd_s8 vhadd_u16 vhadd_u32 vhadd_u8 vhaddq_s16 vhaddq_s32 vhaddq_s8 vhaddq_u16 vhaddq_u32 vhaddq_u8 vrhadd Rounding halving add Click here to expand the API list vrhadd_s16 vrhadd_s32 vrhadd_s8 vrhadd_u16 vrhadd_u32 vrhadd_u8 vrhaddq_s16 vrhaddq_s32 vrhaddq_s8 vrhaddq_u16 vrhaddq_u32 vrhaddq_u8 vqadd Saturating add Click here to expand the API list vqadd_s16 vqadd_s32 vqadd_s64 vqadd_s8 vqadd_u16 vqadd_u32 vqadd_u64 vqadd_u8 vqaddq_s16 vqaddq_s32 vqaddq_s64 vqaddq_s8 vqaddq_u16 vqaddq_u32 vqaddq_u64 vqaddq_u8 vqaddb_s8 vqaddb_u8 vqaddh_s16 vqaddh_u16 vqadds_s32 vqadds_u32 vqaddd_s64 vqaddd_u64 vsqadd Unsigned saturating Accumulate of signed value Click here to expand the API list vsqadd_u16 vsqadd_u32 vsqadd_u64 vsqadd_u8 vsqaddq_u16 vsqaddq_u32 vsqaddq_u64 vsqaddq_u8 vsqaddb_u8 vsqaddh_u16 vsqadds_u32 vsqaddd_u64 vuqadd Signed saturating Accumulate of unsigned value Click here to expand the API list vuqadd_s16 vuqadd_s32 vuqadd_s64 vuqadd_s8 vuqaddq_s16 vuqaddq_s32 vuqaddq_s64 vuqaddq_s8 vuqaddb_s8 vuqaddh_s16 vuqadds_s32 vuqaddd_s64 vaddhn Add returning high narrow Click here to expand the API list vaddhn_s16 vaddhn_s32 vaddhn_s64 vaddhn_u16 vaddhn_u32 vaddhn_u64 vaddhn_high_s16 vaddhn_high_s32 vaddhn_high_s64 vaddhn_high_u16 vaddhn_high_u32 vaddhn_high_u64 vraddhn Rounding add returning high narrow Click here to expand the API list vraddhn_s16 vraddhn_s32 vraddhn_s64 vraddhn_u16 vraddhn_u32 vraddhn_u64 vraddhn_high_s16 vraddhn_high_s32 vraddhn_high_s64 vraddhn_high_u16 vraddhn_high_u32 vraddhn_high_u64 vpadd Add pairwise (vector) Click here to expand the API list vpadd_f32 vpadd_s16 vpadd_s32 vpadd_s8 vpadd_u16 vpadd_u32 vpadd_u8 vpaddq_f32 vpaddq_f64 vpaddq_s16 vpaddq_s32 vpaddq_s64 vpaddq_s8 vpaddq_u16 vpaddq_u32 vpaddq_u64 vpaddq_u8 vpadds_f32 vpaddd_f64 vpaddd_s64 vpaddd_u64 vpaddl Signed add long pairwise Click here to expand the API list vpaddl_s16 vpaddl_s32 vpaddl_s8 vpaddl_u16 vpaddl_u32 vpaddl_u8 vpaddlq_s16 vpaddlq_s32 vpaddlq_s8 vpaddlq_u16 vpaddlq_u32 vpaddlq_u8 vpadal Signed add and accumulate long pairwise Click here to expand the API list vpadal_s16 vpadal_s32 vpadal_s8 vpadal_u16 vpadal_u32 vpadal_u8 vpadalq_s16 vpadalq_s32 vpadalq_s8 vpadalq_u16 vpadalq_u32 vpadalq_u8 vsub Subtract Click here to expand the API list vsub_f32 vsub_f64 vsub_s16 vsub_s32 vsub_s64 vsub_s8 vsub_u16 vsub_u32 vsub_u64 vsub_u8 vsubq_f32 vsubq_f64 vsubq_s16 vsubq_s32 vsubq_s64 vsubq_s8 vsubq_u16 vsubq_u32 vsubq_u64 vsubq_u8 vsubd_s64 vsubd_u64 vsubl Subtract long Click here to expand the API list vsubl_s16 vsubl_s32 vsubl_s8 vsubl_u16 vsubl_u32 vsubl_u8 vsubl_high_s16 vsubl_high_s32 vsubl_high_s8 vsubl_high_u16 vsubl_high_u32 vsubl_high_u8 vsubw Subtract wide Click here to expand the API list vsubw_s16 vsubw_s32 vsubw_s8 vsubw_u16 vsubw_u32 vsubw_u8 vsubw_high_s16 vsubw_high_s32 vsubw_high_s8 vsubw_high_u16 vsubw_high_u32 vsubw_high_u8 vhsub Halving subtract Click here to expand the API list vhsub_s16 vhsub_s32 vhsub_s8 vhsub_u16 vhsub_u32 vhsub_u8 vhsubq_s16 vhsubq_s32 vhsubq_s8 vhsubq_u16 vhsubq_u32 vhsubq_u8 vqsub Saturating subtract Click here to expand the API list vqsub_s16 vqsub_s32 vqsub_s64 vqsub_s8 vqsub_u16 vqsub_u32 vqsub_u64 vqsub_u8 vqsubq_s16 vqsubq_s32 vqsubq_s64 vqsubq_s8 vqsubq_u16 vqsubq_u32 vqsubq_u64 vqsubq_u8 vqsubb_s8 vqsubb_u8 vqsubh_s16 vqsubh_u16 vqsubs_s32 vqsubs_u32 vqsubd_s64 vqsubd_u64 vsubhn Subtract returning high narrow Click here to expand the API list vsubhn_s16 vsubhn_s32 vsubhn_s64 vsubhn_u16 vsubhn_u32 vsubhn_u64 vsubhn_high_s16 vsubhn_high_s32 vsubhn_high_s64 vsubhn_high_u16 vsubhn_high_u32 vsubhn_high_u64 vrsubhn Rounding subtract returning high narrow Click here to expand the API list vrsubhn_s16 vrsubhn_s32 vrsubhn_s64 vrsubhn_u16 vrsubhn_u32 vrsubhn_u64 vrsubhn_high_s16 vrsubhn_high_s32 vrsubhn_high_s64 vrsubhn_high_u16 vrsubhn_high_u32 vrsubhn_high_u64 Multiply Operation Description APIs vmul Multiply (vector) Click here to expand the API list vmul_f32 vmul_f64 vmul_s16 vmul_s32 vmul_s8 vmul_u16 vmul_u32 vmul_u8 vmulq_f32 vmulq_f64 vmulq_s16 vmulq_s32 vmulq_s8 vmulq_u16 vmulq_u32 vmulq_u8 vmul_n Vector multiply by scalar Click here to expand the API list vmul_n_f32 vmul_n_f64 vmul_n_s16 vmul_n_s32 vmul_n_u16 vmul_n_u32 vmulq_n_f32 vmulq_n_f64 vmulq_n_s16 vmulq_n_s32 vmulq_n_u16 vmulq_n_u32 vmul_lane Multiply (vector) Click here to expand the API list vmul_lane_f32 vmul_lane_f64 vmul_lane_s16 vmul_lane_s32 vmul_lane_u16 vmul_lane_u32 vmul_laneq_f32 vmul_laneq_f64 vmul_laneq_s16 vmul_laneq_s32 vmul_laneq_u16 vmul_laneq_u32 vmulq_lane_f32 vmulq_lane_f64 vmulq_lane_s16 vmulq_lane_s32 vmulq_lane_u16 vmulq_lane_u32 vmulq_laneq_f32 vmulq_laneq_f64 vmulq_laneq_s16 vmulq_laneq_s32 vmulq_laneq_u16 vmulq_laneq_u32 vmuls_lane_f32 vmuls_laneq_f32 vmuld_lane_f64 vmuld_laneq_f64 vmull Multiply long (vector) Click here to expand the API list vmull_s16 vmull_s32 vmull_s8 vmull_u16 vmull_u32 vmull_u8 vmull_high_s16 vmull_high_s32 vmull_high_s8 vmull_high_u16 vmull_high_u32 vmull_high_u8 vmull_n Vector long multiply by scalar Click here to expand the API list vmull_n_s16 vmull_n_s32 vmull_n_u16 vmull_n_u32 vmull_high_n_s16 vmull_high_n_s32 vmull_high_n_u16 vmull_high_n_u32 vmull_lane Multiply long (vector) Click here to expand the API list vmull_lane_s16 vmull_lane_s32 vmull_lane_u16 vmull_lane_u32 vmull_laneq_s16 vmull_laneq_s32 vmull_laneq_u16 vmull_laneq_u32 vmull_high_lane_s16 vmull_high_lane_s32 vmull_high_lane_u16 vmull_high_lane_u32 vmull_high_laneq_s16 vmull_high_laneq_s32 vmull_high_laneq_u16 vmull_high_laneq_u32 vmulx Floating-point multiply extended Click here to expand the API list vmulx_f32 vmulx_f64 vmulx_lane_f32 vmulx_lane_f64 vmulx_laneq_f32 vmulx_laneq_f64 vmulxq_f32 vmulxq_f64 vmulxq_lane_f32 vmulxq_lane_f64 vmulxq_laneq_f32 vmulxq_laneq_f64 vmulxs_f32 vmulxs_lane_f32 vmulxs_laneq_f32 vmulxd_f64 vmulxd_lane_f64 vmulxd_laneq_f64 vmla Multiply-add to accumulator (vector) Click here to expand the API list vmla_f32 vmla_f64 vmla_s16 vmla_s32 vmla_s8 vmla_u16 vmla_u32 vmla_u8 vmlaq_f32 vmlaq_f64 vmlaq_s16 vmlaq_s32 vmlaq_s8 vmlaq_u16 vmlaq_u32 vmlaq_u8 vmla_lane Vector multiply accumulate with scalar Click here to expand the API list vmla_lane_f32 vmla_lane_s16 vmla_lane_s32 vmla_lane_u16 vmla_lane_u32 vmla_laneq_f32 vmla_laneq_s16 vmla_laneq_s32 vmla_laneq_u16 vmla_laneq_u32 vmlaq_lane_f32 vmlaq_lane_s16 vmlaq_lane_s32 vmlaq_lane_u16 vmlaq_lane_u32 vmlaq_laneq_f32 vmlaq_laneq_s16 vmlaq_laneq_s32 vmlaq_laneq_u16 vmlaq_laneq_u32 vmla_n Vector multiply accumulate with scalar Click here to expand the API list vmla_n_f32 vmla_n_s16 vmla_n_s32 vmla_n_u16 vmla_n_u32 vmlaq_n_f32 vmlaq_n_s16 vmlaq_n_s32 vmlaq_n_u16 vmlaq_n_u32 vmlal Multiply-accumulate long (vector) Click here to expand the API list vmlal_s16 vmlal_s32 vmlal_s8 vmlal_u16 vmlal_u32 vmlal_u8 vmlal_high_s16 vmlal_high_s32 vmlal_high_s8 vmlal_high_u16 vmlal_high_u32 vmlal_high_u8 vmlal_lane Multiply-accumulate long with scalar Click here to expand the API list vmlal_lane_s16 vmlal_lane_s32 vmlal_lane_u16 vmlal_lane_u32 vmlal_laneq_s16 vmlal_laneq_s32 vmlal_laneq_u16 vmlal_laneq_u32 vmlal_high_lane_s16 vmlal_high_lane_s32 vmlal_high_lane_u16 vmlal_high_lane_u32 vmlal_high_laneq_s16 vmlal_high_laneq_s32 vmlal_high_laneq_u16 vmlal_high_laneq_u32 vmlal_n Multiply-accumulate long with scalar Click here to expand the API list vmlal_n_s16 vmlal_n_s32 vmlal_n_u16 vmlal_n_u32 vmlal_high_n_s16 vmlal_high_n_s32 vmlal_high_n_u16 vmlal_high_n_u32 vmls Multiply-subtract from accumulator (vector) Click here to expand the API list vmls_f32 vmls_f64 vmls_s16 vmls_s32 vmls_s8 vmls_u16 vmls_u32 vmls_u8 vmlsq_f32 vmlsq_f64 vmlsq_s16 vmlsq_s32 vmlsq_s8 vmlsq_u16 vmlsq_u32 vmlsq_u8 vmls_lane Vector multiply subtract with scalar Click here to expand the API list vmls_lane_f32 vmls_lane_s16 vmls_lane_s32 vmls_lane_u16 vmls_lane_u32 vmls_laneq_f32 vmls_laneq_s16 vmls_laneq_s32 vmls_laneq_u16 vmls_laneq_u32 vmlsq_lane_f32 vmlsq_lane_s16 vmlsq_lane_s32 vmlsq_lane_u16 vmlsq_lane_u32 vmlsq_laneq_f32 vmlsq_laneq_s16 vmlsq_laneq_s32 vmlsq_laneq_u16 vmlsq_laneq_u32 vmls_n Vector multiply subtract with scalar Click here to expand the API list vmls_n_f32 vmls_n_s16 vmls_n_s32 vmls_n_u16 vmls_n_u32 vmlsq_n_f32 vmlsq_n_s16 vmlsq_n_s32 vmlsq_n_u16 vmlsq_n_u32 vmlsl Multiply-subtract long (vector) Click here to expand the API list vmlsl_s16 vmlsl_s32 vmlsl_s8 vmlsl_u16 vmlsl_u32 vmlsl_u8 vmlsl_high_s16 vmlsl_high_s32 vmlsl_high_s8 vmlsl_high_u16 vmlsl_high_u32 vmlsl_high_u8 vmlsl_lane Vector multiply-subtract long with scalar Click here to expand the API list vmlsl_lane_s16 vmlsl_lane_s32 vmlsl_lane_u16 vmlsl_lane_u32 vmlsl_laneq_s16 vmlsl_laneq_s32 vmlsl_laneq_u16 vmlsl_laneq_u32 vmlsl_high_lane_s16 vmlsl_high_lane_s32 vmlsl_high_lane_u16 vmlsl_high_lane_u32 vmlsl_high_laneq_s16 vmlsl_high_laneq_s32 vmlsl_high_laneq_u16 vmlsl_high_laneq_u32 vmlsl_n Vector multiply-subtract long with scalar Click here to expand the API list vmlsl_n_s16 vmlsl_n_s32 vmlsl_n_u16 vmlsl_n_u32 vmlsl_high_n_s16 vmlsl_high_n_s32 vmlsl_high_n_u16 vmlsl_high_n_u32 vqdmull Signed saturating doubling multiply long Click here to expand the API list vqdmull_s16 vqdmull_s32 vqdmullh_s16 vqdmulls_s32 vqdmull_high_s16 vqdmull_high_s32 vqdmull_lane Vector saturating doubling multiply long with scalar Click here to expand the API list vqdmull_lane_s16 vqdmull_lane_s32 vqdmull_laneq_s16 vqdmull_laneq_s32 vqdmullh_lane_s16 vqdmullh_laneq_s16 vqdmulls_lane_s32 vqdmulls_laneq_s32 vqdmull_high_lane_s16 vqdmull_high_lane_s32 vqdmull_high_laneq_s16 vqdmull_high_laneq_s32 vqdmull_n Vector saturating doubling multiply long with scalar Click here to expand the API list vqdmull_n_s16 vqdmull_n_s32 vqdmull_high_n_s16 vqdmull_high_n_s32 vqdmulh Saturating doubling multiply returning high half Click here to expand the API list vqdmulh_s16 vqdmulh_s32 vqdmulhq_s16 vqdmulhq_s32 vqdmulhh_s16 vqdmulhs_s32 vqdmulh_lane Vector saturating doubling multiply high by scalar Click here to expand the API list vqdmulh_lane_s16 vqdmulh_lane_s32 vqdmulh_laneq_s16 vqdmulh_laneq_s32 vqdmulhq_lane_s16 vqdmulhq_lane_s32 vqdmulhq_laneq_s16 vqdmulhq_laneq_s32 vqdmulhh_lane_s16 vqdmulhh_laneq_s16 vqdmulhs_lane_s32 vqdmulhs_laneq_s32 vqdmulh_n Vector saturating doubling multiply high by scalar Click here to expand the API list vqdmulh_n_s16 vqdmulh_n_s32 vqdmulhq_n_s16 vqdmulhq_n_s32 vqrdmulh Saturating rounding doubling multiply returning high half Click here to expand the API list vqrdmulh_s16 vqrdmulh_s32 vqrdmulhq_s16 vqrdmulhq_s32 vqrdmulhh_s16 vqrdmulhs_s32 vqrdmulh_lane Vector saturating rounding doubling multiply high with scalar Click here to expand the API list vqrdmulh_lane_s16 vqrdmulh_lane_s32 vqrdmulh_laneq_s16 vqrdmulh_laneq_s32 vqrdmulhq_lane_s16 vqrdmulhq_lane_s32 vqrdmulhq_laneq_s16 vqrdmulhq_laneq_s32 vqrdmulhh_lane_s16 vqrdmulhh_laneq_s16 vqrdmulhs_lane_s32 vqrdmulhs_laneq_s32 vqrdmulh_n Vector saturating rounding doubling multiply high with scalar Click here to expand the API list vqrdmulh_n_s16 vqrdmulh_n_s32 vqrdmulhq_n_s16 vqrdmulhq_n_s32 vqdmlal Saturating doubling multiply-add long Click here to expand the API list vqdmlal_s16 vqdmlal_s32 vqdmlalh_s16 vqdmlals_s32 vqdmlal_high_s16 vqdmlal_high_s32 vqdmlal_lane Vector saturating doubling multiply-accumulate long with scalar Click here to expand the API list vqdmlal_lane_s16 vqdmlal_lane_s32 vqdmlal_laneq_s16 vqdmlal_laneq_s32 vqdmlalh_lane_s16 vqdmlalh_laneq_s16 vqdmlals_lane_s32 vqdmlals_laneq_s32 vqdmlal_high_lane_s16 vqdmlal_high_lane_s32 vqdmlal_high_laneq_s16 vqdmlal_high_laneq_s32 vqdmlal_n Vector saturating doubling multiply-accumulate long with scalar Click here to expand the API list vqdmlal_n_s16 vqdmlal_n_s32 vqdmlal_high_n_s16 vqdmlal_high_n_s32 vqdmlsl Signed saturating doubling multiply-subtract long Click here to expand the API list vqdmlsl_s16 vqdmlsl_s32 vqdmlslh_s16 vqdmlsls_s32 vqdmlsl_high_s16 vqdmlsl_high_s32 vqdmlsl_lane Vector saturating doubling multiply-subtract long with scalar Click here to expand the API list vqdmlsl_lane_s16 vqdmlsl_lane_s32 vqdmlsl_laneq_s16 vqdmlsl_laneq_s32 vqdmlslh_lane_s16 vqdmlslh_laneq_s16 vqdmlsls_lane_s32 vqdmlsls_laneq_s32 vqdmlsl_high_lane_s16 vqdmlsl_high_lane_s32 vqdmlsl_high_laneq_s16 vqdmlsl_high_laneq_s32 vqdmlsl_n Vector saturating doubling multiply-subtract long with scalar Click here to expand the API list vqdmlsl_n_s16 vqdmlsl_n_s32 vqdmlsl_high_n_s16 vqdmlsl_high_n_s32 vqrdmlah Saturating rounding doubling multiply accumulate returning high half (vector) Click here to expand the API list vqrdmlah_s16 vqrdmlah_s32 vqrdmlahq_s16 vqrdmlahq_s32 vqrdmlahh_s16 vqrdmlahs_s32 vqrdmlah_lane Saturating rounding doubling multiply accumulate returning high half (vector) Click here to expand the API list vqrdmlah_lane_s16 vqrdmlah_lane_s32 vqrdmlah_laneq_s16 vqrdmlah_laneq_s32 vqrdmlahq_lane_s16 vqrdmlahq_lane_s32 vqrdmlahq_laneq_s16 vqrdmlahq_laneq_s32 vqrdmlahh_lane_s16 vqrdmlahh_laneq_s16 vqrdmlahs_lane_s32 vqrdmlsh Saturating rounding doubling multiply subtract returning high half (vector) Click here to expand the API list vqrdmlsh_s16 vqrdmlsh_s32 vqrdmlshq_s16 vqrdmlshq_s32 vqrdmlshh_s16 vqrdmlshs_s32 vqrdmlsh_lane Saturating rounding doubling multiply subtract returning high half (vector) Click here to expand the API list vqrdmlsh_lane_s16 vqrdmlsh_lane_s32 vqrdmlsh_laneq_s16 vqrdmlsh_laneq_s32 vqrdmlshq_lane_s16 vqrdmlshq_lane_s32 vqrdmlshq_laneq_s16 vqrdmlshq_laneq_s32 vqrdmlshh_lane_s16 vqrdmlshh_laneq_s16 vqrdmlshs_lane_s32 vfma Floating-point fused multiply-add to accumulator (vector) Click here to expand the API list vfma_f32 vfma_f64 vfmaq_f32 vfmaq_f64 vfma_n Floating-point fused multiply-add to accumulator (vector) Click here to expand the API list vfma_n_f32 vfma_n_f64 vfmaq_n_f32 vfmaq_n_f64 vfma_lane Floating-point fused multiply-add to accumulator (vector) Click here to expand the API list vfma_lane_f32 vfma_lane_f64 vfma_laneq_f32 vfma_laneq_f64 vfmaq_lane_f32 vfmaq_lane_f64 vfmaq_laneq_f32 vfmaq_laneq_f64 vfmas_lane_f32 vfmas_laneq_f32 vfmad_lane_f64 vfmad_laneq_f64 vfms Floating-point fused multiply-subtract from accumulator (vector) Click here to expand the API list vfms_f32 vfms_f64 vfmsq_f32 vfmsq_f64 vfms_n Floating-point fused multiply-subtract from accumulator (vector) Click here to expand the API list vfms_n_f32 vfms_n_f64 vfmsq_n_f32 vfmsq_n_f64 vfms_lane Floating-point fused multiply-subtract from accumulator (vector) Click here to expand the API list vfms_lane_f32 vfms_lane_f64 vfms_laneq_f32 vfms_laneq_f64 vfmsd_lane_f64 vfmsd_laneq_f64 vfmsq_lane_f32 vfmsq_lane_f64 vfmsq_laneq_f32 vfmsq_laneq_f64 vfmss_lane_f32 vfmss_laneq_f32 vdiv Floating-point divide (vector) Click here to expand the API list vdiv_f32 vdiv_f64 vdivq_f32 vdivq_f64 Data processing Operation Description APIs vpmax Maximum pairwise Click here to expand the API list vpmax_f32 vpmax_s16 vpmax_s32 vpmax_s8 vpmax_u16 vpmax_u32 vpmax_u8 vpmaxq_f32 vpmaxq_f64 vpmaxq_s16 vpmaxq_s32 vpmaxq_s8 vpmaxq_u16 vpmaxq_u32 vpmaxq_u8 vpmaxs_f32 vpmaxqd_f64 vpmaxnm Floating-point maximum number pairwise (vector) Click here to expand the API list vpmaxnm_f32 vpmaxnmq_f32 vpmaxnmq_f64 vpmaxnms_f32 vpmaxnmqd_f64 vpmin Minimum pairwise Click here to expand the API list vpmin_f32 vpmin_s16 vpmin_s32 vpmin_s8 vpmin_u16 vpmin_u32 vpmin_u8 vpminq_f32 vpminq_f64 vpminq_s16 vpminq_s32 vpminq_s8 vpminq_u16 vpminq_u32 vpminq_u8 vpmins_f32 vpminqd_f64 vpminnm Floating-point minimum number pairwise (vector) Click here to expand the API list vpminnm_f32 vpminnmq_f32 vpminnmq_f64 vpminnms_f32 vpminnmqd_f64 vabd Absolute difference Click here to expand the API list vabd_f32 vabd_f64 vabd_s16 vabd_s32 vabd_s8 vabd_u16 vabd_u32 vabd_u8 vabdq_f32 vabdq_f64 vabdq_s16 vabdq_s32 vabdq_s8 vabdq_u16 vabdq_u32 vabdq_u8 vabds_f32 vabdd_f64 vabdl Absolute difference long Click here to expand the API list vabdl_s16 vabdl_s32 vabdl_s8 vabdl_u16 vabdl_u32 vabdl_u8 vabdl_high_s16 vabdl_high_s32 vabdl_high_s8 vabdl_high_u16 vabdl_high_u32 vabdl_high_u8 vaba Absolute difference and accumulate Click here to expand the API list vaba_s16 vaba_s32 vaba_s8 vaba_u16 vaba_u32 vaba_u8 vabaq_s16 vabaq_s32 vabaq_s8 vabaq_u16 vabaq_u32 vabaq_u8 vabal Absolute difference and accumulate long Click here to expand the API list vabal_s16 vabal_s32 vabal_s8 vabal_u16 vabal_u32 vabal_u8 vabal_high_s16 vabal_high_s32 vabal_high_s8 vabal_high_u16 vabal_high_u32 vabal_high_u8 vmax Maximum Click here to expand the API list vmax_f32 vmax_f64 vmax_s16 vmax_s32 vmax_s8 vmax_u16 vmax_u32 vmax_u8 vmaxq_f32 vmaxq_f64 vmaxq_s16 vmaxq_s32 vmaxq_s8 vmaxq_u16 vmaxq_u32 vmaxq_u8 vmaxnm Floating-point maximum number Click here to expand the API list vmaxnm_f32 vmaxnm_f64 vmaxnmq_f32 vmaxnmq_f64 vmaxnmv_f32 vmaxnmvq_f32 vmaxnmvq_f64 vmaxv Maximum across vector Click here to expand the API list vmaxv_f32 vmaxv_s16 vmaxv_s32 vmaxv_s8 vmaxv_u16 vmaxv_u32 vmaxv_u8 vmaxvq_f32 vmaxvq_f64 vmaxvq_s16 vmaxvq_s32 vmaxvq_s8 vmaxvq_u16 vmaxvq_u32 vmaxvq_u8 vmin Minimum Click here to expand the API list vmin_f32 vmin_f64 vmin_s16 vmin_s32 vmin_s8 vmin_u16 vmin_u32 vmin_u8 vminq_f32 vminq_f64 vminq_s16 vminq_s32 vminq_s8 vminq_u16 vminq_u32 vminq_u8 vminnm Floating-point minimum number Click here to expand the API list vminnm_f32 vminnm_f64 vminnmq_f32 vminnmq_f64 vminnmv_f32 vminnmvq_f32 vminnmvq_f64 vminv Minimum across vector Click here to expand the API list vminv_f32 vminv_s16 vminv_s32 vminv_s8 vminv_u16 vminv_u32 vminv_u8 vminvq_f32 vminvq_f64 vminvq_s16 vminvq_s32 vminvq_s8 vminvq_u16 vminvq_u32 vminvq_u8 vabs Absolute value Click here to expand the API list vabs_f32 vabs_f64 vabs_s16 vabs_s32 vabs_s64 vabs_s8 vabsq_f32 vabsq_f64 vabsq_s16 vabsq_s32 vabsq_s64 vabsq_s8 vabsd_s64 vqabs Saturating absolute value Click here to expand the API list vqabs_s16 vqabs_s32 vqabs_s64 vqabs_s8 vqabsq_s16 vqabsq_s32 vqabsq_s64 vqabsq_s8 vqabsb_s8 vqabsh_s16 vqabss_s32 vqabsd_s64 vneg Negate Click here to expand the API list vneg_f32 vneg_f64 vneg_s16 vneg_s32 vneg_s64 vneg_s8 vnegd_s64 vnegq_f32 vnegq_f64 vnegq_s16 vnegq_s32 vnegq_s64 vnegq_s8 vqneg Saturating negate Click here to expand the API list vqneg_s16 vqneg_s32 vqneg_s64 vqneg_s8 vqnegq_s16 vqnegq_s32 vqnegq_s64 vqnegq_s8 vqnegb_s8 vqnegh_s16 vqnegs_s32 vqnegd_s64 vcls Count leading sign bits Click here to expand the API list vcls_s16 vcls_s32 vcls_s8 vclsq_s16 vclsq_s32 vclsq_s8 vclz Count leading zero bits Click here to expand the API list vclz_s16 vclz_s32 vclz_s8 vclz_u16 vclz_u32 vclz_u8 vclzq_s16 vclzq_s32 vclzq_s8 vclzq_u16 vclzq_u32 vclzq_u8 vcnt Population count per byte Click here to expand the API list vcnt_s8 vcnt_u8 vcntq_s8 vcntq_u8 vrecpe Reciprocal estimate Click here to expand the API list vrecpe_f32 vrecpe_f64 vrecpe_u32 vrecpeq_f32 vrecpeq_f64 vrecpeq_u32 vrecpes_f32 vrecped_f64 vrecps Reciprocal step Click here to expand the API list vrecps_f32 vrecps_f64 vrecpsq_f32 vrecpsq_f64 vrecpss_f32 vrecpsd_f64 vrecpx Floating-point reciprocal exponent Click here to expand the API list vrecpxd_f64 vrecpxs_f32 vrsqrte Reciprocal square root estimate Click here to expand the API list vrsqrte_f32 vrsqrte_f64 vrsqrte_u32 vrsqrteq_f32 vrsqrteq_f64 vrsqrteq_u32 vrsqrtes_f32 vrsqrted_f64 vrsqrts Reciprocal square root step Click here to expand the API list vrsqrts_f32 vrsqrts_f64 vrsqrtsq_f32 vrsqrtsq_f64 vrsqrtss_f32 vrsqrtsd_f64 vmovn Extract narrow Click here to expand the API list vmovn_s16 vmovn_s32 vmovn_s64 vmovn_u16 vmovn_u32 vmovn_u64 vmovn_high_s16 vmovn_high_s32 vmovn_high_s64 vmovn_high_u16 vmovn_high_u32 vmovn_high_u64 vmovl Extract long Click here to expand the API list vmovl_s16 vmovl_s32 vmovl_s8 vmovl_u16 vmovl_u32 vmovl_u8 vmovl_high_s16 vmovl_high_s32 vmovl_high_s8 vmovl_high_u16 vmovl_high_u32 vmovl_high_u8 vqmovn Saturating extract narrow Click here to expand the API list vqmovn_s16 vqmovn_s32 vqmovn_s64 vqmovn_u16 vqmovn_u32 vqmovn_u64 vqmovn_high_s16 vqmovn_high_s32 vqmovn_high_s64 vqmovn_high_u16 vqmovn_high_u32 vqmovn_high_u64 vqmovnh_s16 vqmovnh_u16 vqmovns_s32 vqmovns_u32 vqmovnd_s64 vqmovnd_u64 vqmovun Signed saturating extract unsigned narrow Click here to expand the API list vqmovun_s16 vqmovun_s32 vqmovun_s64 vqmovun_high_s16 vqmovun_high_s32 vqmovun_high_s64 vqmovunh_s16 vqmovuns_s32 vqmovund_s64 Comparison Operation Description APIs vceq Compare bitwise equal Click here to expand the API list vceq_f32 vceq_f64 vceq_s16 vceq_s32 vceq_s64 vceq_s8 vceq_u16 vceq_u32 vceq_u64 vceq_u8 vceqq_f32 vceqq_f64 vceqq_s16 vceqq_s32 vceqq_s64 vceqq_s8 vceqq_u16 vceqq_u32 vceqq_u64 vceqq_u8 vceqs_f32 vceqd_f64 vceqd_s64 vceqd_u64 vceqz Compare bitwise equal to zero Click here to expand the API list vceqz_f32 vceqz_f64 vceqz_s16 vceqz_s32 vceqz_s64 vceqz_s8 vceqz_u16 vceqz_u32 vceqz_u64 vceqz_u8 vceqzq_f32 vceqzq_f64 vceqzq_s16 vceqzq_s32 vceqzq_s64 vceqzq_s8 vceqzq_u16 vceqzq_u32 vceqzq_u64 vceqzq_u8 vceqzs_f32 vceqzd_f64 vceqzd_s64 vceqzd_u64 vcge Compare greater than or equal Click here to expand the API list vcge_f32 vcge_f64 vcge_s16 vcge_s32 vcge_s64 vcge_s8 vcge_u16 vcge_u32 vcge_u64 vcge_u8 vcgeq_f32 vcgeq_f64 vcgeq_s16 vcgeq_s32 vcgeq_s64 vcgeq_s8 vcgeq_u16 vcgeq_u32 vcgeq_u64 vcgeq_u8 vcges_f32 vcged_f64 vcged_s64 vcged_u64 vcgez Compare greater than or equal to zero Click here to expand the API list vcgez_f32 vcgez_f64 vcgez_s16 vcgez_s32 vcgez_s64 vcgez_s8 vcgezq_f32 vcgezq_f64 vcgezq_s16 vcgezq_s32 vcgezq_s64 vcgezq_s8 vcgezs_f32 vcgezd_f64 vcgezd_s64 vcle Compare less than or equal Click here to expand the API list vcle_f32 vcle_f64 vcle_s16 vcle_s32 vcle_s64 vcle_s8 vcle_u16 vcle_u32 vcle_u64 vcle_u8 vcleq_f32 vcleq_f64 vcleq_s16 vcleq_s32 vcleq_s64 vcleq_s8 vcleq_u16 vcleq_u32 vcleq_u64 vcleq_u8 vcles_f32 vcled_f64 vcled_s64 vcled_u64 vclez Compare less than or equal to zero Click here to expand the API list vclez_f32 vclez_f64 vclez_s16 vclez_s32 vclez_s64 vclez_s8 vclezq_f32 vclezq_f64 vclezq_s16 vclezq_s32 vclezq_s64 vclezq_s8 vclezs_f32 vclezd_f64 vclezd_s64 vcgt Compare greater than Click here to expand the API list vcgt_f32 vcgt_f64 vcgt_s16 vcgt_s32 vcgt_s64 vcgt_s8 vcgt_u16 vcgt_u32 vcgt_u64 vcgt_u8 vcgtq_f32 vcgtq_f64 vcgtq_s16 vcgtq_s32 vcgtq_s64 vcgtq_s8 vcgtq_u16 vcgtq_u32 vcgtq_u64 vcgtq_u8 vcgts_f32 vcgtd_f64 vcgtd_s64 vcgtd_u64 vcgtz Compare greater than zero Click here to expand the API list vcgtz_f32 vcgtz_f64 vcgtz_s16 vcgtz_s32 vcgtz_s64 vcgtz_s8 vcgtzq_f32 vcgtzq_f64 vcgtzq_s16 vcgtzq_s32 vcgtzq_s64 vcgtzq_s8 vcgtzs_f32 vcgtzd_f64 vcgtzd_s64 vclt Compare less than Click here to expand the API list vclt_f32 vclt_f64 vclt_s16 vclt_s32 vclt_s64 vclt_s8 vclt_u16 vclt_u32 vclt_u64 vclt_u8 vcltq_f32 vcltq_f64 vcltq_s16 vcltq_s32 vcltq_s64 vcltq_s8 vcltq_u16 vcltq_u32 vcltq_u64 vcltq_u8 vclts_f32 vcltd_f64 vcltd_s64 vcltd_u64 vcltz Compare less than zero Click here to expand the API list vcltz_f32 vcltz_f64 vcltz_s16 vcltz_s32 vcltz_s64 vcltz_s8 vcltzq_f32 vcltzq_f64 vcltzq_s16 vcltzq_s32 vcltzq_s64 vcltzq_s8 vcltzs_f32 vcltzd_f64 vcltzd_s64 vcage Floating-point absolute compare greater than or equal Click here to expand the API list vcage_f32 vcage_f64 vcageq_f32 vcageq_f64 vcages_f32 vcaged_f64 vcagt Floating-point absolute compare greater than Click here to expand the API list vcagt_f32 vcagt_f64 vcagtq_f32 vcagtq_f64 vcagts_f32 vcagtd_f64 vcale Floating-point absolute compare less than or equal Click here to expand the API list vcale_f32 vcale_f64 vcaleq_f32 vcaleq_f64 vcales_f32 vcaled_f64 vcalt Floating-point absolute compare less than Click here to expand the API list vcalt_f32 vcalt_f64 vcaltq_f32 vcaltq_f64 vcalts_f32 vcaltd_f64 Bitwise Operation Description APIs vtst Test bits nonzero Click here to expand the API list vtst_s16 vtst_s32 vtst_s64 vtst_s8 vtst_u16 vtst_u32 vtst_u64 vtst_u8 vtstd_s64 vtstd_u64 vtstq_s16 vtstq_s32 vtstq_s64 vtstq_s8 vtstq_u16 vtstq_u32 vtstq_u64 vtstq_u8 vmvn Bitwise NOT Click here to expand the API list vmvn_s16 vmvn_s32 vmvn_s8 vmvn_u16 vmvn_u32 vmvn_u8 vmvnq_s16 vmvnq_s32 vmvnq_s8 vmvnq_u16 vmvnq_u32 vmvnq_u8 vand Bitwise AND Click here to expand the API list vand_s16 vand_s32 vand_s64 vand_s8 vand_u16 vand_u32 vand_u64 vand_u8 vandq_s16 vandq_s32 vandq_s64 vandq_s8 vandq_u16 vandq_u32 vandq_u64 vandq_u8 vorr Bitwise OR Click here to expand the API list vorr_s16 vorr_s32 vorr_s64 vorr_s8 vorr_u16 vorr_u32 vorr_u64 vorr_u8 vorrq_s16 vorrq_s32 vorrq_s64 vorrq_s8 vorrq_u16 vorrq_u32 vorrq_u64 vorrq_u8 vorn Bitwise OR NOT Click here to expand the API list vorn_s16 vorn_s32 vorn_s64 vorn_s8 vorn_u16 vorn_u32 vorn_u64 vorn_u8 vornq_s16 vornq_s32 vornq_s64 vornq_s8 vornq_u16 vornq_u32 vornq_u64 vornq_u8 veor Bitwise exclusive OR Click here to expand the API list veor_s16 veor_s32 veor_s64 veor_s8 veor_u16 veor_u32 veor_u64 veor_u8 veorq_s16 veorq_s32 veorq_s64 veorq_s8 veorq_u16 veorq_u32 veorq_u64 veorq_u8 vbic Bitwise bit clear Click here to expand the API list vbic_s16 vbic_s32 vbic_s64 vbic_s8 vbic_u16 vbic_u32 vbic_u64 vbic_u8 vbicq_s16 vbicq_s32 vbicq_s64 vbicq_s8 vbicq_u16 vbicq_u32 vbicq_u64 vbicq_u8 vbsl Bitwise select Click here to expand the API list vbsl_f32 vbsl_f64 vbsl_s16 vbsl_s32 vbsl_s64 vbsl_s8 vbsl_u16 vbsl_u32 vbsl_u64 vbsl_u8 vbslq_f32 vbslq_f64 vbslq_s16 vbslq_s32 vbslq_s64 vbslq_s8 vbslq_u16 vbslq_u32 vbslq_u64 vbslq_u8 Shift Operation Description APIs vshl Shift left (register) Click here to expand the API list vshl_s16 vshl_s32 vshl_s64 vshl_s8 vshl_u16 vshl_u32 vshl_u64 vshl_u8 vshlq_s16 vshlq_s32 vshlq_s64 vshlq_s8 vshlq_u16 vshlq_u32 vshlq_u64 vshlq_u8 vshld_s64 vshld_u64 vqshl Saturating shift left (register) Click here to expand the API list vqshl_s16 vqshl_s32 vqshl_s64 vqshl_s8 vqshl_u16 vqshl_u32 vqshl_u64 vqshl_u8 vqshlq_s16 vqshlq_s32 vqshlq_s64 vqshlq_s8 vqshlq_u16 vqshlq_u32 vqshlq_u64 vqshlq_u8 vqshlb_s8 vqshlb_u8 vqshlh_s16 vqshlh_u16 vqshls_s32 vqshls_u32 vqshld_s64 vqshld_u64 vqshl_n Saturating shift left (immediate) Click here to expand the API list vqshl_n_s16 vqshl_n_s32 vqshl_n_s64 vqshl_n_s8 vqshl_n_u16 vqshl_n_u32 vqshl_n_u64 vqshl_n_u8 vqshlq_n_s16 vqshlq_n_s32 vqshlq_n_s64 vqshlq_n_s8 vqshlq_n_u16 vqshlq_n_u32 vqshlq_n_u64 vqshlq_n_u8 vqshlb_n_s8 vqshlb_n_u8 vqshlh_n_s16 vqshlh_n_u16 vqshls_n_s32 vqshls_n_u32 vqshld_n_s64 vqshld_n_u64 vqshlu_n Saturating shift left unsigned (immediate) Click here to expand the API list vqshlu_n_s16 vqshlu_n_s32 vqshlu_n_s64 vqshlu_n_s8 vqshlub_n_s8 vqshlud_n_s64 vqshluh_n_s16 vqshluq_n_s16 vqshluq_n_s32 vqshluq_n_s64 vqshluq_n_s8 vqshlus_n_s32 vrshl Rounding shift left (register) Click here to expand the API list vrshl_s16 vrshl_s32 vrshl_s64 vrshl_s8 vrshl_u16 vrshl_u32 vrshl_u64 vrshl_u8 vrshlq_s16 vrshlq_s32 vrshlq_s64 vrshlq_s8 vrshlq_u16 vrshlq_u32 vrshlq_u64 vrshlq_u8 vrshld_s64 vrshld_u64 vqrshl Saturating rounding shift left (register) Click here to expand the API list vqrshl_s16 vqrshl_s32 vqrshl_s64 vqrshl_s8 vqrshl_u16 vqrshl_u32 vqrshl_u64 vqrshl_u8 vqrshlq_s16 vqrshlq_s32 vqrshlq_s64 vqrshlq_s8 vqrshlq_u16 vqrshlq_u32 vqrshlq_u64 vqrshlq_u8 vqrshlb_s8 vqrshlb_u8 vqrshlh_s16 vqrshlh_u16 vqrshls_s32 vqrshls_u32 vqrshld_s64 vqrshld_u64 vshl_n Shift left (immediate) Click here to expand the API list vshl_n_s16 vshl_n_s32 vshl_n_s64 vshl_n_s8 vshl_n_u16 vshl_n_u32 vshl_n_u64 vshl_n_u8 vshlq_n_s16 vshlq_n_s32 vshlq_n_s64 vshlq_n_s8 vshlq_n_u16 vshlq_n_u32 vshlq_n_u64 vshlq_n_u8 vshld_n_s64 vshld_n_u64 vshll_n Shift left long (immediate) Click here to expand the API list vshll_n_s16 vshll_n_s32 vshll_n_s8 vshll_n_u16 vshll_n_u32 vshll_n_u8 vshll_high_n_s16 vshll_high_n_s32 vshll_high_n_s8 vshll_high_n_u16 vshll_high_n_u32 vshll_high_n_u8 vshr_n Shift right (immediate) Click here to expand the API list vshr_n_s16 vshr_n_s32 vshr_n_s64 vshr_n_s8 vshr_n_u16 vshr_n_u32 vshr_n_u64 vshr_n_u8 vshrq_n_s16 vshrq_n_s32 vshrq_n_s64 vshrq_n_s8 vshrq_n_u16 vshrq_n_u32 vshrq_n_u64 vshrq_n_u8 vshrd_n_s64 vshrd_n_u64 vrshr_n Rounding right left (register) Click here to expand the API list vrshr_n_s16 vrshr_n_s32 vrshr_n_s64 vrshr_n_s8 vrshr_n_u16 vrshr_n_u32 vrshr_n_u64 vrshr_n_u8 vrshrq_n_s16 vrshrq_n_s32 vrshrq_n_s64 vrshrq_n_s8 vrshrq_n_u16 vrshrq_n_u32 vrshrq_n_u64 vrshrq_n_u8 vrshrd_n_s64 vrshrd_n_u64 vshrn_n Shift right narrow (immediate) Click here to expand the API list vshrn_n_s16 vshrn_n_s32 vshrn_n_s64 vshrn_n_u16 vshrn_n_u32 vshrn_n_u64 vshrn_high_n_s16 vshrn_high_n_s32 vshrn_high_n_s64 vshrn_high_n_u16 vshrn_high_n_u32 vshrn_high_n_u64 vqshrun_n Signed saturating shift right unsigned narrow (immediate) Click here to expand the API list vqshrun_n_s16 vqshrun_n_s32 vqshrun_n_s64 vqshrunh_n_s16 vqshruns_n_s32 vqshrund_n_s64 vqshrun_high_n_s16 vqshrun_high_n_s32 vqshrun_high_n_s64 vqrshrun_n Signed saturating rounded shift right unsigned narrow (immediate) Click here to expand the API list vqrshrun_n_s16 vqrshrun_n_s32 vqrshrun_n_s64 vqrshrunh_n_s16 vqrshruns_n_s32 vqrshrund_n_s64 vqrshrun_high_n_s16 vqrshrun_high_n_s32 vqrshrun_high_n_s64 vqshrn_n Signed saturating shift right narrow (immediate) Click here to expand the API list vqshrn_n_s16 vqshrn_n_s32 vqshrn_n_s64 vqshrn_n_u16 vqshrn_n_u32 vqshrn_n_u64 vqshrnh_n_s16 vqshrnh_n_u16 vqshrns_n_s32 vqshrns_n_u32 vqshrnd_n_s64 vqshrnd_n_u64 vqshrn_high_n_s16 vqshrn_high_n_s32 vqshrn_high_n_s64 vqshrn_high_n_u16 vqshrn_high_n_u32 vqshrn_high_n_u64 vrshrn_n Rounding shift right narrow (immediate) Click here to expand the API list vrshrn_n_s16 vrshrn_n_s32 vrshrn_n_s64 vrshrn_n_u16 vrshrn_n_u32 vrshrn_n_u64 vrshrn_high_n_s16 vrshrn_high_n_s32 vrshrn_high_n_s64 vrshrn_high_n_u16 vrshrn_high_n_u32 vrshrn_high_n_u64 vqrshrn_n Signed saturating rounded shift right narrow (immediate) Click here to expand the API list vqrshrn_n_s16 vqrshrn_n_s32 vqrshrn_n_s64 vqrshrn_n_u16 vqrshrn_n_u32 vqrshrn_n_u64 vqrshrnh_n_s16 vqrshrnh_n_u16 vqrshrns_n_s32 vqrshrns_n_u32 vqrshrnd_n_s64 vqrshrnd_n_u64 vqrshrn_high_n_s16 vqrshrn_high_n_s32 vqrshrn_high_n_s64 vqrshrn_high_n_u16 vqrshrn_high_n_u32 vqrshrn_high_n_u64 vsra_n Signed shift right and accumulate (immediate) Click here to expand the API list vsra_n_s16 vsra_n_s32 vsra_n_s64 vsra_n_s8 vsra_n_u16 vsra_n_u32 vsra_n_u64 vsra_n_u8 vsraq_n_s16 vsraq_n_s32 vsraq_n_s64 vsraq_n_s8 vsraq_n_u16 vsraq_n_u32 vsraq_n_u64 vsraq_n_u8 vsrad_n_s64 vsrad_n_u64 vrsra_n Signed rounding shift right and accumulate (immediate) Click here to expand the API list vrsra_n_s16 vrsra_n_s32 vrsra_n_s64 vrsra_n_s8 vrsra_n_u16 vrsra_n_u32 vrsra_n_u64 vrsra_n_u8 vrsraq_n_s16 vrsraq_n_s32 vrsraq_n_s64 vrsraq_n_s8 vrsraq_n_u16 vrsraq_n_u32 vrsraq_n_u64 vrsraq_n_u8 vrsrad_n_s64 vrsrad_n_u64 vsri_n Shift right and insert (immediate) Click here to expand the API list vsri_n_s16 vsri_n_s32 vsri_n_s64 vsri_n_s8 vsri_n_u16 vsri_n_u32 vsri_n_u64 vsri_n_u8 vsriq_n_s16 vsriq_n_s32 vsriq_n_s64 vsriq_n_s8 vsriq_n_u16 vsriq_n_u32 vsriq_n_u64 vsriq_n_u8 vsrid_n_s64 vsrid_n_u64 vsli_n Shift left and insert (immediate) Click here to expand the API list vsli_n_s16 vsli_n_s32 vsli_n_s64 vsli_n_s8 vsli_n_u16 vsli_n_u32 vsli_n_u64 vsli_n_u8 vsliq_n_s16 vsliq_n_s32 vsliq_n_s64 vsliq_n_s8 vsliq_n_u16 vsliq_n_u32 vsliq_n_u64 vsliq_n_u8 vslid_n_s64 vslid_n_u64 Floating-point Operation Description APIs vcvt Convert to/from another precision or fixed point, rounding towards zero Click here to expand the API list vcvt_f32_f64 vcvt_f32_s32 vcvt_f32_u32 vcvt_f64_f32 vcvt_f64_s64 vcvt_f64_u64 vcvt_s32_f32 vcvt_s64_f64 vcvt_u32_f32 vcvt_u64_f64 vcvtq_f32_s32 vcvtq_f32_u32 vcvtq_f64_s64 vcvtq_f64_u64 vcvtq_s32_f32 vcvtq_s64_f64 vcvtq_u32_f32 vcvtq_u64_f64 vcvts_f32_s32 vcvts_f32_u32 vcvts_s32_f32 vcvts_u32_f32 vcvtd_f64_s64 vcvtd_f64_u64 vcvtd_s64_f64 vcvtd_u64_f64 vcvt_high_f32_f64 vcvt_high_f64_f32 vcvta Convert to integer, rounding to nearest with ties to away Click here to expand the API list vcvta_s32_f32 vcvta_s64_f64 vcvta_u32_f32 vcvta_u64_f64 vcvtad_s64_f64 vcvtad_u64_f64 vcvtaq_s32_f32 vcvtaq_s64_f64 vcvtaq_u32_f32 vcvtaq_u64_f64 vcvtas_s32_f32 vcvtas_u32_f32 vcvtm Convert to integer, rounding towards minus infinity Click here to expand the API list vcvtm_s32_f32 vcvtm_s64_f64 vcvtm_u32_f32 vcvtm_u64_f64 vcvtmq_s32_f32 vcvtmq_s64_f64 vcvtmq_u32_f32 vcvtmq_u64_f64 vcvtms_s32_f32 vcvtms_u32_f32 vcvtmd_s64_f64 vcvtmd_u64_f64 vcvtn Convert to integer, rounding to nearest with ties to even Click here to expand the API list vcvtn_s32_f32 vcvtn_s64_f64 vcvtn_u32_f32 vcvtn_u64_f64 vcvtnq_s32_f32 vcvtnq_s64_f64 vcvtnq_u32_f32 vcvtnq_u64_f64 vcvtns_s32_f32 vcvtns_u32_f32 vcvtnd_s64_f64 vcvtnd_u64_f64 vcvtp Convert to integer, rounding towards plus infinity Click here to expand the API list vcvtp_s32_f32 vcvtp_s64_f64 vcvtp_u32_f32 vcvtp_u64_f64 vcvtpq_s32_f32 vcvtpq_s64_f64 vcvtpq_u32_f32 vcvtpq_u64_f64 vcvtps_s32_f32 vcvtps_u32_f32 vcvtpd_s64_f64 vcvtpd_u64_f64 vcvtx Convert to lower precision, rounding to nearest with ties to odd Click here to expand the API list vcvtx_f32_f64 vcvtx_high_f32_f64 vcvtxd_f32_f64 vcvt_n Convert to/from fixed point, rounding towards zero Click here to expand the API list vcvt_n_f32_s32 vcvt_n_f32_u32 vcvt_n_f64_s64 vcvt_n_f64_u64 vcvt_n_s32_f32 vcvt_n_s64_f64 vcvt_n_u32_f32 vcvt_n_u64_f64 vcvtq_n_f32_s32 vcvtq_n_f32_u32 vcvtq_n_f64_s64 vcvtq_n_f64_u64 vcvtq_n_s32_f32 vcvtq_n_s64_f64 vcvtq_n_u32_f32 vcvtq_n_u64_f64 vcvts_n_f32_s32 vcvts_n_f32_u32 vcvts_n_s32_f32 vcvts_n_u32_f32 vcvtd_n_f64_s64 vcvtd_n_f64_u64 vcvtd_n_s64_f64 vcvtd_n_u64_f64 vrnd Round to Integral, toward zero Click here to expand the API list vrnd_f32 vrnd_f64 vrndq_f32 vrndq_f64 vrnda Round to Integral, with ties to away Click here to expand the API list vrnda_f32 vrnda_f64 vrndaq_f32 vrndaq_f64 vrndi Round to Integral, using current rounding mode Click here to expand the API list vrndi_f32 vrndi_f64 vrndiq_f32 vrndiq_f64 vrndm Round to Integral, towards minus infinity Click here to expand the API list vrndm_f32 vrndm_f64 vrndmq_f32 vrndmq_f64 vrndn Round to Integral, with ties to even Click here to expand the API list vrndn_f32 vrndn_f64 vrndnq_f32 vrndnq_f64 vrndns_f32 vrndp Round to Integral, towards plus infinity Click here to expand the API list vrndp_f32 vrndp_f64 vrndpq_f32 vrndpq_f64 vrndx Round to Integral exact Click here to expand the API list vrndx_f32 vrndx_f64 vrndxq_f32 vrndxq_f64 Load and store Operation Description APIs vld1 Load vector from memory Click here to expand the API list vld1_f32 vld1_f64 vld1_s16 vld1_s32 vld1_s64 vld1_s8 vld1_u16 vld1_u32 vld1_u64 vld1_u8 vld1q_f32 vld1q_f64 vld1q_s16 vld1q_s32 vld1q_s64 vld1q_s8 vld1q_u16 vld1q_u32 vld1q_u64 vld1q_u8 vst1 Store vector to memory Click here to expand the API list vst1_f32 vst1_f64 vst1_s16 vst1_s32 vst1_s64 vst1_s8 vst1_u16 vst1_u32 vst1_u64 vst1_u8 vst1q_f32 vst1q_f64 vst1q_s16 vst1q_s32 vst1q_s64 vst1q_s8 vst1q_u16 vst1q_u32 vst1q_u64 vst1q_u8 vget_lane Get vector element Click here to expand the API list vget_lane_f32 vget_lane_f64 vget_lane_s16 vget_lane_s32 vget_lane_s64 vget_lane_s8 vget_lane_u16 vget_lane_u32 vget_lane_u64 vget_lane_u8 vgetq_lane_f32 vgetq_lane_f64 vgetq_lane_s16 vgetq_lane_s32 vgetq_lane_s64 vgetq_lane_s8 vgetq_lane_u16 vgetq_lane_u32 vgetq_lane_u64 vgetq_lane_u8 vset_lane Set vector element Click here to expand the API list vset_lane_f32 vset_lane_f64 vset_lane_s16 vset_lane_s32 vset_lane_s64 vset_lane_s8 vset_lane_u16 vset_lane_u32 vset_lane_u64 vset_lane_u8 vsetq_lane_f32 vsetq_lane_f64 vsetq_lane_s16 vsetq_lane_s32 vsetq_lane_s64 vsetq_lane_s8 vsetq_lane_u16 vsetq_lane_u32 vsetq_lane_u64 vsetq_lane_u8 Permutation Operation Description APIs vext Extract vector from pair of vectors Click here to expand the API list vext_f32 vext_f64 vext_s16 vext_s32 vext_s64 vext_s8 vext_u16 vext_u32 vext_u64 vext_u8 vextq_f32 vextq_f64 vextq_s16 vextq_s32 vextq_s64 vextq_s8 vextq_u16 vextq_u32 vextq_u64 vextq_u8 vtbl1 Table vector Lookup Click here to expand the API list vtbl1_s8 vtbl1_u8 vtbx1 Table vector lookup extension Click here to expand the API list vtbx1_s8 vtbx1_u8 vqtbl1 Table vector Lookup Click here to expand the API list vqtbl1_s8 vqtbl1_u8 vqtbl1q_s8 vqtbl1q_u8 vqtbx1 Table vector lookup extension Click here to expand the API list vqtbx1_s8 vqtbx1_u8 vqtbx1q_s8 vqtbx1q_u8 vrbit Reverse bit order Click here to expand the API list vrbit_s8 vrbit_u8 vrbitq_s8 vrbitq_u8 vrev16 Reverse elements in 16-bit halfwords Click here to expand the API list vrev16_s8 vrev16_u8 vrev16q_s8 vrev16q_u8 vrev32 Reverse elements in 32-bit words Click here to expand the API list vrev32_s16 vrev32_s8 vrev32_u16 vrev32_u8 vrev32q_s16 vrev32q_s8 vrev32q_u16 vrev32q_u8 vrev64 Reverse elements in 64-bit doublewords Click here to expand the API list vrev64_f32 vrev64_s16 vrev64_s32 vrev64_s8 vrev64_u16 vrev64_u32 vrev64_u8 vrev64q_f32 vrev64q_s16 vrev64q_s32 vrev64q_s8 vrev64q_u16 vrev64q_u32 vrev64q_u8 vtrn1 Transpose vectors (primary) Click here to expand the API list vtrn1_f32 vtrn1_s16 vtrn1_s32 vtrn1_s8 vtrn1_u16 vtrn1_u32 vtrn1_u8 vtrn1q_f32 vtrn1q_f64 vtrn1q_s16 vtrn1q_s32 vtrn1q_s64 vtrn1q_s8 vtrn1q_u16 vtrn1q_u32 vtrn1q_u64 vtrn1q_u8 vtrn2 Transpose vectors (secondary) Click here to expand the API list vtrn2_f32 vtrn2_s16 vtrn2_s32 vtrn2_s8 vtrn2_u16 vtrn2_u32 vtrn2_u8 vtrn2q_f32 vtrn2q_f64 vtrn2q_s16 vtrn2q_s32 vtrn2q_s64 vtrn2q_s8 vtrn2q_u16 vtrn2q_u32 vtrn2q_u64 vtrn2q_u8 vzip1 Zip vectors (primary) Click here to expand the API list vzip1_f32 vzip1_s16 vzip1_s32 vzip1_s8 vzip1_u16 vzip1_u32 vzip1_u8 vzip1q_f32 vzip1q_f64 vzip1q_s16 vzip1q_s32 vzip1q_s64 vzip1q_s8 vzip1q_u16 vzip1q_u32 vzip1q_u64 vzip1q_u8 vzip2 Zip vectors (secondary) Click here to expand the API list vzip2_f32 vzip2_s16 vzip2_s32 vzip2_s8 vzip2_u16 vzip2_u32 vzip2_u8<br/vzip2q_f32 vzip2q_f64 vzip2q_s16 vzip2q_s32 vzip2q_s64 vzip2q_s8 vzip2q_u16 vzip2q_u32 vzip2q_u64 vzip2q_u8 vuzp1 Unzip vectors (primary) Click here to expand the API list vuzp1_f32 vuzp1_s16 vuzp1_s32 vuzp1_s8 vuzp1_u16 vuzp1_u32 vuzp1_u8 vuzp1q_f32 vuzp1q_f64 vuzp1q_s16 vuzp1q_s32 vuzp1q_s64 vuzp1q_s8 vuzp1q_u16 vuzp1q_u32 vuzp1q_u64 vuzp1q_u8 vuzp2 Unzip vectors (secondary) Click here to expand the API list vuzp2_f32 vuzp2_s16 vuzp2_s32 vuzp2_s8 vuzp2_u16 vuzp2_u32 vuzp2_u8 vuzp2q_f32 vuzp2q_f64 vuzp2q_s16 vuzp2q_s32 vuzp2q_s64 vuzp2q_s8 vuzp2q_u16 vuzp2q_u32 vuzp2q_u64 vuzp2q_u8 Cryptographic Operation APIs CRC32 Click here to expand the API list __crc32b __crc32cb __crc32cd __crc32ch __crc32cw __crc32d __crc32h __crc32w SHA1 Click here to expand the API list vsha1cq_u32 vsha1h_u32 vsha1mq_u32 vsha1pq_u32 vsha1su0q_u32 vsha1su1q_u32 SHA256 Click here to expand the API list vsha256h2q_u32 vsha256hq_u32 vsha256su0q_u32 vsha256su1q_u32 AES Click here to expand the API list vaesdq_u8 vaeseq_u8 vaesimcq_u8 vaesmcq_u8 Miscellaneous Operation Description APIs vsqrt Square root Click here to expand the API list vsqrt_f32 vsqrt_f64 vsqrtq_f32 vsqrtq_f64 vdot Dot product Click here to expand the API list vdot_s32 vdot_u32 vdotq_s32 vdotq_u32 vdot_lane Dot product Click here to expand the API list vdot_lane_s32 vdot_lane_u32 vdot_laneq_s32 vdot_laneq_u32 vdotq_lane_s32 vdotq_lane_u32 vdotq_laneq_s32 vdotq_laneq_u32"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-processors.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics-processors.html",
    "title": "Processor specific SIMD extensions | FSM Unity Framework",
    "keywords": "Processor specific SIMD extensions Burst exposes all Intel SIMD intrinsics from SSE up to and including AVX2 in the Unity.Burst.Intrinsics.X86 family of nested classes. The Unity.Burst.Intrinsics.Arm.Neon class provides intrinsics for Arm Neon's Armv7, Armv8, and Armv8.2 (RDMA, crypto, dotprod). Organizing your code You should statically import these intrinsics because they contain plain static functions: using static Unity.Burst.Intrinsics.X86; using static Unity.Burst.Intrinsics.X86.Sse; using static Unity.Burst.Intrinsics.X86.Sse2; using static Unity.Burst.Intrinsics.X86.Sse3; using static Unity.Burst.Intrinsics.X86.Ssse3; using static Unity.Burst.Intrinsics.X86.Sse4_1; using static Unity.Burst.Intrinsics.X86.Sse4_2; using static Unity.Burst.Intrinsics.X86.Popcnt; using static Unity.Burst.Intrinsics.X86.Avx; using static Unity.Burst.Intrinsics.X86.Avx2; using static Unity.Burst.Intrinsics.X86.Fma; using static Unity.Burst.Intrinsics.X86.F16C; using static Unity.Burst.Intrinsics.X86.Bmi1; using static Unity.Burst.Intrinsics.X86.Bmi2; using static Unity.Burst.Intrinsics.Arm.Neon; Burst CPU intrinsics are translated into specific CPU instructions. However, Burst has a special compiler pass which makes sure that your CPU target set in Burst AOT Settings is compatible with the intrinsics used in your code. This ensures you don't try to call unsupported instructions (for example, AArch64 Neon on an Intel CPU or AVX2 instructions on an SSE4 CPU), which causes the process to abort with an \"Invalid instruction\" exception. A compiler error is generated if the check fails. However, if you want to provide several code paths with different CPU targets, or to make sure your intrinsics code is compatible with any target CPU, you can wrap your intrinsics code with the followinf property checks: IsNeonSupported IsNeonArmv82FeaturesSupported IsNeonCryptoSupported IsNeonDotProdSupported IsNeonRDMASupported For example: if (IsAvx2Supported) { // Code path for AVX2 instructions } else if (IsSse42Supported) { // Code path for SSE4.2 instructions } else if (IsNeonArmv82FeaturesSupported) { // Code path for Armv8.2 Neon instructions } else if (IsNeonSupported) { // Code path for Arm Neon instructions } else { // Fallback path for everything else } These branches don't affect performance. Burst evaluates the IsXXXSupported properties at compile-time and eliminates unsupported branches as dead code, while the active branch stays there without the if check. Later feature levels implicitly include the previous ones, so you should organize tests from most recent to oldest. Burst emits compile-time errors if you've used intrinsics that aren't part of the current compilation target. Burst doesn't bracket these with a feature level test, which helps you to narrow in on what to put inside a feature test. If you run your application in .NET, Mono or IL2CPP without Burst enabled, all the IsXXXSupported properties return false. However, if you skip the test you can still run a reference version of most intrinsics in Mono (exceptions listed below), which is helpful if you need to use the managed debugger. Reference implementations are slow and only intended for managed debugging. Important There isn't a reference managed implementation of Arm Neon intrinsics. This means that you can't use the technique mentioned in the previous paragraph to step through the intrinsics in Mono. FMA intrinsics that operate on doubles don't have a software fallback because of the inherit complexity in emulating fused 64-bit floating point math. Intrinsics use the types v64 (Arm only), v128 and v256, which represent a 64-bit, 128-bit or 256-bit vector respectively. For example, given a NativeArray<float> and a Lut lookup table of v128 shuffle masks, a code fragment like this performs lane left packing, demonstrating the use of vector load/store reinterpretation and direct intrinsic calls: v128 a = Input.ReinterpretLoad<v128>(i); v128 mask = cmplt_ps(a, Limit); int m = movemask_ps(a); v128 packed = shuffle_epi8(a, Lut[m]); Output.ReinterpretStore(outputIndex, packed); outputIndex += popcnt_u32((uint)m); Intel intrinsics The Intel intrinsics API mirrors the C/C++ Intel instrinsics API, with a the following differences: All 128-bit vector types (__m128, __m128i and __m128d) are collapsed into v128 All 256-bit vector types (__m256, __m256i and __m256d) are collapsed into v256 All _mm prefixes on instructions and macros are dropped, because C# has namespaces All bitfield constants (for example, rounding mode selection) are replaced with C# bitflag enum values Arm Neon intrinsics The Arm Neon intrinsics API mirrors the Arm C Language Extensions, with the following differences: All vector types are collapsed into v64 and v128, becoming typeless. This means that the vector type must contain expected element types and count when calling an API. The *x2, *x3, *x4 vector types aren't supported. poly* types aren't supported. reinterpret* functions aren't supported (they aren't needed because of the usage of v64 and v128 vector types). Intrinsic usage is only supported on Armv8 (64-bit) hardware. Burst's CPU intrinsics use typeless vectors. Because of this, Burst doesn't perform any type checks. For example, if you call an intrinsic which processes 4 ints on a vector that was initialized with 4 floats, then there's no compiler error. The vector types have fields that represent every element type, in a union-like struct, which gives you flexibility to use these intrinsics in a way that best fits your code. Arm Neon C intrinsics (ACLE) use typed vectors, for example int32x4_t, and has special APIs (for example, reinterpret_\\*) to convert to a vector of another element type. Burst CPU intrinsics vectors are typeless, so these APIs are not needed. The following APIs provide the equivalent functionality: v64 (Arm Neon only) v128 v256 For a categorized index of Arm Neon intrinsics supported in Burst, see the Arm Neon intrinsics reference."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-burst-intrinsics.html",
    "title": "Burst intrinsics overview | FSM Unity Framework",
    "keywords": "Burst intrinsics overview Burst provides low level intrinsics in the Unity.Burst.Intrinsics namespace. This is useful if you know how to write single instruction, multiple data (SIMD) assembly code, and you want to get extra performance from Burst code. For most use cases, you won't need to use these. This section contains the following information Page Description Burst intrinsics Common class Overview of the Burst.Intrinsics.Common class, which provides functionality shared across the hardware targets that Burst supports. DllImport and internal calls Overview of [DllImport], which is for calling native functions. Processor specific SIMD extensions Overview of the Intel and Arm Neon intrinsics. Arm Neon intrinsics reference Reference of the methods in the Burst.Intrinsics.Arm.Neon class."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-calling-burst-code.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-calling-burst-code.html",
    "title": "Calling Burst-compiled code | FSM Unity Framework",
    "keywords": "Calling Burst-compiled code You can call Burst-compiled methods direct from managed code. Calling generic methods or methods whose declaring type is generic isn't supported, otherwise the rules as for function pointers apply. However, you don't need to worry about the extra boiler plate needed for function pointers. The following example shows a Burst-compiled utility class. Because it uses structs, it passes by reference per the function pointer rules. [BurstCompile] public static class MyBurstUtilityClass { [BurstCompile] public static void BurstCompiled_MultiplyAdd(in float4 mula, in float4 mulb, in float4 add, out float4 result) { result = mula * mulb + add; } } Use this method from managed code like so: public class MyMonoBehaviour : MonoBehaviour { void Start() { var mula = new float4(1, 2, 3, 4); var mulb = new float4(-1,1,-1,1); var add = new float4(99,0,0,0); MyBurstUtilityClass.BurstCompiled_MultiplyAdd(mula, mulb, add, out var result); Debug.Log(result); } } If you attach this script to an object and run it, float4(98f, 2f, -3f, 4f) is printed to the log. Code transformation Burst uses IL Post Processing to automatically transform the code into a function pointer and call. For more information, see the documentation on Function pointers. To disable the direct call transformation, addDisableDirectCall = true to the BurstCompile options. This prevents the Post Processor from running on the code: [BurstCompile] public static class MyBurstUtilityClass { [BurstCompile(DisableDirectCall = true)] public static void BurstCompiled_MultiplyAdd(in float4 mula, in float4 mulb, in float4 add, out float4 result) { result = mula * mulb + add; } }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-function-pointers.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-function-pointers.html",
    "title": "Function pointers | FSM Unity Framework",
    "keywords": "Function pointers To work with dynamic functions that process data based on other data states, use FunctionPointer<T>. Because Burst treats delegates as managed objects, you can't use C# delegates to work with dynamic functions. Support details Function pointers don't support generic delegates. Also, avoid wrapping BurstCompiler.CompileFunctionPointer<T> within another open generic method. If you do this, Burst can't apply required attributes to the delegate, perform additional safety analysis, or perform potential optimizations. Argument and return types are subject to the same restrictions as DllImport and internal calls. For more information, see the documentation on DllImport and internal calls. Interoperability with IL2CPP Interoperability of function pointers with IL2CPP requires System.Runtime.InteropServices.UnmanagedFunctionPointerAttribute on the delegate. Set the calling convention to CallingConvention.Cdecl. Burst automatically adds this attribute to delegates that are used with BurstCompiler.CompileFunctionPointer<T>. Using function pointers To use function pointers, identify the static functions that you want Burst to compile and do the following: Add a [BurstCompile] attribute to these functions Add a [BurstCompile] attribute to the containing type. This helps the Burst compiler find the static methods that have [BurstCompile] attribute Declare a delegate to create the \"interface\" of these functions Add a [MonoPInvokeCallbackAttribute] attribute to the functions. You need to add this so that IL2CPP works with these functions. For example: // Instruct Burst to look for static methods with [BurstCompile] attribute [BurstCompile] class EnclosingType { [BurstCompile] [MonoPInvokeCallback(typeof(Process2FloatsDelegate))] public static float MultiplyFloat(float a, float b) => a * b; [BurstCompile] [MonoPInvokeCallback(typeof(Process2FloatsDelegate))] public static float AddFloat(float a, float b) => a + b; // A common interface for both MultiplyFloat and AddFloat methods public delegate float Process2FloatsDelegate(float a, float b); } Compile these function pointers from regular C# code: // Contains a compiled version of MultiplyFloat with Burst FunctionPointer<Process2FloatsDelegate> mulFunctionPointer = BurstCompiler.CompileFunctionPointer<Process2FloatsDelegate>(MultiplyFloat); // Contains a compiled version of AddFloat with Burst FunctionPointer<Process2FloatsDelegate> addFunctionPointer = BurstCompiler. CompileFunctionPointer<Process2FloatsDelegate>(AddFloat); Using function pointers in a job To use the function pointers directly from a job, pass them to the job struct: // Invoke the function pointers from HPC# jobs var resultMul = mulFunctionPointer.Invoke(1.0f, 2.0f); var resultAdd = addFunctionPointer.Invoke(1.0f, 2.0f); Burst compiles function pointers asynchronously for jobs by default. To force a synchronous compilation of function pointers use [BurstCompile(SynchronousCompilation = true)]. Using function pointers in C# code To use these function pointers from regular C# code, cache the FunctionPointer<T>.Invoke property (which is the delegate instance) to a static field to get the best performance: private readonly static Process2FloatsDelegate mulFunctionPointerInvoke = BurstCompiler.CompileFunctionPointer<Process2FloatsDelegate>(MultiplyFloat).Invoke; // Invoke the delegate from C# var resultMul = mulFunctionPointerInvoke(1.0f, 2.0f); Using Burst-compiled function pointers from C# might be slower than their pure C# version counterparts if the function is too small compared to the overhead of P/Invoke interop. Performance considerations Where possible, you use a job over a function pointer to run Burst compiled code, because jobs are more optimal. Burst provides better aliasing calculations for jobs because the job safety system has more optimizations by default. You also can't pass most of the [NativeContainer] structs like NativeArray directly to function pointers and must use a job struct to do so. Native container structs contain managed objects for safety checks that the Burst compiler can work around when compiling jobs, but not for function pointers. The following example shows a bad example of how to use function pointers in Burst. The function pointer computes math.sqrt from an input pointer and stores it to an output pointer. MyJob feeds this function pointer sources from two NativeArrays which isn't optimal: ///Bad function pointer example [BurstCompile] public class MyFunctionPointers { public unsafe delegate void MyFunctionPointerDelegate(float* input, float* output); [BurstCompile] public static unsafe void MyFunctionPointer(float* input, float* output) { *output = math.sqrt(*input); } } [BurstCompile] struct MyJob : IJobParallelFor { public FunctionPointer<MyFunctionPointers.MyFunctionPointerDelegate> FunctionPointer; [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public unsafe void Execute(int index) { var inputPtr = (float*)Input.GetUnsafeReadOnlyPtr(); var outputPtr = (float*)Output.GetUnsafePtr(); FunctionPointer.Invoke(inputPtr + index, outputPtr + index); } } This example isn't optimal for the following reasons: Burst can't vectorize the function pointer because it's being fed a single scalar element. This means that 4-8x performance is lost from a lack of vectorization. The MyJob knows that the Input and Output native arrays can't alias, but this information isn't communicated to the function pointer. There is a non-zero overhead to constantly branching to a function pointer somewhere else in memory. To use a function pointer in an optimal way, always process batches of data in the function pointer, like so: [BurstCompile] public class MyFunctionPointers { public unsafe delegate void MyFunctionPointerDelegate(int count, float* input, float* output); [BurstCompile] public static unsafe void MyFunctionPointer(int count, float* input, float* output) { for (int i = 0; i < count; i++) { output[i] = math.sqrt(input[i]); } } } [BurstCompile] struct MyJob : IJobParallelForBatch { public FunctionPointer<MyFunctionPointers.MyFunctionPointerDelegate> FunctionPointer; [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public unsafe void Execute(int index, int count) { var inputPtr = (float*)Input.GetUnsafeReadOnlyPtr() + index; var outputPtr = (float*)Output.GetUnsafePtr() + index; FunctionPointer.Invoke(count, inputPtr, outputPtr); } } Thee modified MyFunctionPointer takes a count of elements to process, and loops over the input and output pointers to do a lot of calculations. The MyJob becomes an IJobParallelForBatch, and the count is passed directly into the function pointer. This is better for performance because of the following reasons: Burst vectorizes the MyFunctionPointer call. Because Burst processes count items per function pointer, any overhead of calling the function pointer is reduced by count times. For example, if you run a batch of 128, the function pointer overhead is 1/128th per index of what it was previously. Batching results in a 1.53x performance gain over not batching. However, to get the best possible performance, use a job. This gives Burst the most visibility over what you want it to do, and the most opportunities to optimize: [BurstCompile] struct MyJob : IJobParallelFor { [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public unsafe void Execute(int index) { Output[i] = math.sqrt(Input[i]); } } This runs 1.26x faster than the batched function pointer example, and 1.93x faster than the non-batched function pointer examples. Burst has perfect aliasing knowledge and can make the broadest modifications to the above. This code is also a lot simpler than either of the function pointer cases."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-hpc-overview.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-hpc-overview.html",
    "title": "| FSM Unity Framework",
    "keywords": "HPC# overview Burst uses a high performance subset of C# called High Performance C# (HPC#). Supported C# features in HPC# HPC# supports most expressions and statements in C#. It supports the following: Supported feature Notes Extension methods. Instance methods of structs. Unsafe code and pointer manipulation. Loading from static read-only fields. For more information, see the documentation on Static read-only fields and static constructors. Regular C# control flows. if else switch case for while break continue ref and out parameters fixed statements Some IL opcodes. cpblk initblk sizeof DLLImport and internal calls. For more information, see the documentation on DLLImport and internal calls. try and finally keywords. Burst also supports the associated IDisposable patterns, using and foreach. If an exception happens in Burst, the behavior is different from .NET. In .NET, if an exception occurs inside a try block, control flow goes to the finally block. However, in Burst, if an exception happens inside or outside a try block, the exception throws as if any finally blocks do not exist. Invoking foreach calls is supported by Burst, but there is a foreach edge case that burst currently does not support (see \"Foreach and While\" section for more details). Strings and ProfilerMarker. For more information, see the documentation on Support for Unity Profiler markers. throw expressions. Burst only supports simple throw patterns, for example, throw new ArgumentException(\"Invalid argument\"). When you use simple patterns like this, Burst extracts the static string exception message and includes it in the generated code. Strings and Debug.Log. Only partially supported. For more information, see the documentation on String support and Debug.Log. Burst also provides alternatives for some C# constructions not directly accessible to HPC#: Function pointers as an alternative to using delegates within HPC# Shared static to access static mutable data from both C# and HPC# Exception expressions Burst supports throw expressions for exceptions. Exceptions thrown in the editor can be caught by managed code, and are reported in the console window. Exceptions thrown in player builds will always cause the application to abort. Thus with Burst you should only use exceptions for exceptional behavior. To ensure that code doesn't end up relying on exceptions for things like general control flow, Burst produces the following warning on code that tries to throw within a method not attributed with [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")]: Burst warning BC1370: An exception was thrown from a function without the correct [Conditional(\"ENABLE_UNITY_COLLECTIONS_CHECKS\")] guard. Exceptions only work in the editor and so should be protected by this guard Foreach and While Burst supports invoking foreach and while. However, there is an edge case which is currently unsupported - methods that take one or more generic collection parameters T: IEnumerable<U> and invoke foreach or while on at least one of the collections in the method body. To illustrate, the following example methods exemplify this limitation: public static void IterateThroughConcreteCollection(NativeArray<int> list) { foreach (var element in list) { // This works } } public static void IterateThroughGenericCollection<S>(S list) where S : struct, IEnumerable<int> { foreach (var element in list) { // This doesn't work } } Note that the uppermost method IterateThroughConcreteCollection()'s parameter is specified to be a concrete collection type, in this case NativeArray<int>. Because it's concrete iterating through it inside the method will compile in Burst. In the method IterateThroughGenericCollection() below it, however, the parameter is specified to be a generic collection type S. Iterating through S inside the method will therefore not compile in Burst. It will instead throw the following error: Can't call the method (method name) on the generic interface object type (object name). This may be because you are trying to do a foreach over a generic collection of type IEnumerable. Unsupported C# features in HPC# HPC# doesn't support the following C# features: Catching exceptions catch in a try/catch. Storing to static fields except via Shared Static Any methods related to managed objects, for example, string methods."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-language-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-language-support.html",
    "title": "C# language support | FSM Unity Framework",
    "keywords": "C# language support Burst uses a high performance subset of C# called High Performance C# (HPC#) which has a number of limitations and differences between C#. Topic Description HPC# overview Understand how HPC# works with Burst. C#/.NET type support Understand the supported C# features. C#/.NET System namespace support Understand what's supported in the System namespace. Static read-only fields and static constructor support Use static read-only fields and static constructors in Burst code. String support Use strings in Burst code. Calling Burst compiled code Call Burst compiled code from managed code. Function pointers Use function pointers to work with dynamic functions. SharedStatic struct Use SharedStatic to share static mutable data. Additional resources Burst instrinsics overview"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-shared-static.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-shared-static.html",
    "title": "SharedStatic struct | FSM Unity Framework",
    "keywords": "SharedStatic struct Burst has basic support for accessing static readonly data. However, if you want to share static mutable data between C# and HPC#, use the SharedStatic<T> struct. The following example shows accessing an int static field that both C# and HPC# can change: public abstract class MutableStaticTest { public static readonly SharedStatic<int> IntField = SharedStatic<int>.GetOrCreate<MutableStaticTest, IntFieldKey>(); // Define a Key type to identify IntField private class IntFieldKey {} } C# and HPC# can then access this: // Write to a shared static MutableStaticTest.IntField.Data = 5; // Read from a shared static var value = 1 + MutableStaticTest.IntField.Data; When you use SharedStatic<T>, be aware of the following: The T in SharedStatic<T> defines the data type. To identify a static field, provide a context for it. To do this, create a key for both the containing type (for example, MutableStaticTest in the example above), identify the field (for example, IntFieldKey class in the example above) and pass these classes as generic arguments of SharedStatic<int>.GetOrCreate<MutableStaticTest, IntFieldKey>(). Always initialize the shared static field in C# from a static constructor before accessing it from HPC#. If you don't initialize the data before accessing it, it might lead to an undefined initialization state."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-static-read-only-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-static-read-only-support.html",
    "title": "Static read-only fields and static constructor support | FSM Unity Framework",
    "keywords": "Static read-only fields and static constructor support Burst evaluates all static fields and all static constructors at compile time. It evaluates all the static fields and the static constructors for a given struct together. When there is a static field that isn't read-only in a Burst-compiled struct, a compilation error happens. This is because Burst only supports read-only static fields. When Burst fails to evaluate any static field or static constructor, all fields and constructors fail for that struct. When compile-time evaluation fails, Burst falls back to compiling all static initialization code into an initialization function that it calls once at runtime. This means that your code needs to be Burst compatible, or it will fail compilation if it fails compile-time evaluation. An exception to this is that there's limited support for initializing static read-only array fields as long as they're initialized from either an array constructor or from static data: static readonly int[] MyArray0 = { 1, 2, 3, .. }; static readonly int[] MyArray1 = new int[10]; Language support Burst doesn't support calling external functions and function pointers. It supports using the following base language with static read-only fields and constructors: Managed arrays Strings Limited intrinsic support: Unity.Burst.BurstCompiler.IsEnabled Unity.Burst.BurstRuntime.GetHashCode32 Unity.Burst.BurstRuntime.GetHashCode64 Vector type construction Limited intrinsic assertion support: UnityEngine.Debug.Assert NUnit.Framework.Assert.AreEqual NUnit.Framework.Assert.AreNotEqual Simple throw patterns. Any exceptions thrown during evaluation become compiler errors."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-string-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-string-support.html",
    "title": "String support | FSM Unity Framework",
    "keywords": "String support Burst supports string usage in the following scenarios: Debug.Log Assigning a string to the FixedString structs that Unity.Collections provides, for example FixedString128Bytes. The System.Runtime.CompilerServices attributes [CallerLineNumber], [CallerMemberName], and [CallerFilePath] on arguments to Burst functions. However, you can only pass the strings directly to calls to Debug.Log. A string can be either: A string literal. For example: \"This is a string literal\". An interpolated string using $\"This is an integer {value} or using string.Format, where the string to format is also a string literal. For example, Burst supports the following constructions: Logging with a string literal: Debug.Log(\"This a string literal\"); Logging using string interpolation: int value = 256; Debug.Log($\"This is an integer value {value}\"); This is the same as using string.Format directly: int value = 256; Debug.Log(string.Format(\"This is an integer value {0}\", value)); Supported Debug.Log methods Burst supports the following Debug.Log methods: Debug.Log(object) Debug.LogWarning(object) Debug.LogError(object) String interpolation support String interpolation has the following restrictions: The string must be a string literal Burst supports the following string.Format methods: string.Format(string, object) string.Format(string, object, object) string.Format(string, object, object, object) string.Format(string, object[]). Use this for a string interpolation that contains more than three arguments, for example $\"{arg1} {arg2} {arg3} {arg4} {arg5}\". In this case, the object[] array needs to be a constant size and no arguments should involve control flows (for example, $\"This is a {(cond ? arg1 : arg2)}\"). The string must only use value types The string must take only built-in type arguments: char boolean byte / sbyte double float short / ushort int / uint long / ulong Burst supports sll vector types (for example int2, float3), except half vector types. For example: var value = new float3(1.0f, 2.0f, 3.0f); // Logs \"This value float3(1f, 2f, 3f)\" Debug.Log($\"This value `{value}`\"); Burst doesn't support ToString() of structs. It displays the full name of the struct instead. For more information, see the .NET documentation on String interpolation and Standard numeric format strings. Managed strings You can pass a managed string literal or an interpolated string directly to Debug.Log, but you can't pass a string to a user method or to use them as fields in a struct. To pass around or store strings, use one of the FixedString structs in the Unity.Collections package: int value = 256; FixedString128 text = $\"This is an integer value {value} used with FixedString128\"; MyCustomLog(text); // ... // String can be passed as an argument to a method using a FixedString, // but not using directly a managed `string`: public static void MyCustomLog(in FixedString128 log) { Debug.Log(text); } Arguments and specifiers Burst has limited support for string format arguments and specifiers: int value = 256; // Padding left: \"This value ` 256` Debug.Log($\"This value `{value,5}`\"); // Padding right: \"This value `256 ` Debug.Log($\"This value `{value,-5}`\"); // Hexadecimal uppercase: \"This value `00FF` Debug.Log($\"This value `{value:X4}`\"); // Hexadecimal lowercase: \"This value `00ff` Debug.Log($\"This value `{value:x4}`\"); // Decimal with leading-zero: \"This value `0256` Debug.Log($\"This value `{value:D4}`\");"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-system-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-system-support.html",
    "title": "C#/.NET System namespace support | FSM Unity Framework",
    "keywords": "C#/.NET System namespace support Burst provides support for some of the System namespace, transforming these into Burst compatible variants in the Burst compiler. System.Math Burst supports all methods that System.Math declares, with the following exceptions: double IEEERemainder(double x, double y) is only supported when Api Compatibility Level is set to .NET Standard 2.1 in project settings System.IntPtr Burst supports all methods of System.IntPtr/System.UIntPtr, including the static fields IntPtr.Zero and IntPtr.Size System.Threading.Interlocked Burst supports atomic memory intrinsics for all methods provided by System.Threading.Interlocked (for example, Interlocked.Increment). Make sure that the source location of the interlocked methods are naturally aligned. For example, the alignment of the pointer is a multiple of the pointed-to-type: [StructLayout(LayoutKind.Explicit)] struct Foo { [FieldOffset(0)] public long a; [FieldOffset(5)] public long b; public long AtomicReadAndAdd() { return Interlocked.Read(ref a) + Interlocked.Read(ref b); } } If the pointer to the struct Foo has an alignment of 8, which is the natural alignment of a long value, the Interlocked.Read of a would be successful because it lies on a naturally aligned address. However, b would not be successful and undefined behavior happens at the load of b as a result. System.Threading.Thread Burst supports the MemoryBarrier method of System.Threading.Thread. System.Threading.Volatile Burst supports the non-generic variants of Read and Write provided by System.Threading.Volatile."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-type-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/csharp-type-support.html",
    "title": "C#/.NET type support | FSM Unity Framework",
    "keywords": "C#/.NET type support Burst works on a subset of .NET that doesn't let you use any managed objects or reference types in your code (classes in C#). The following sections gives more details about the constructs that Burst supports, and any limitations they have. Built-in types Array types Struct types Generic types Vector types Enum types Pointer types Span types Tuple types Built-in types Supported built-in types Burst supports the following built-in types: bool byte/sbyte double float int/uint long/ulong short/ushort Unsupported built-in types Burst doesn't support the following built-in types: char decimal string because this is a managed type Array types Supported array types Burst supports read-only managed arrays loaded from static read-only fields: [BurstCompile] public struct MyJob : IJob { private static readonly int[] _preComputeTable = new int[] { 1, 2, 3, 4 }; public int Index { get; set; } public void Execute() { int x = _preComputeTable[0]; int z = _preComputeTable[Index]; } } However, accessing a static read-only managed array has the following restrictions: You can only use the read-only static managed array directly and can't pass it around, for example as a method argument. C# code that doesn't use jobs shouldn't modify the read-only static array's elements. This is because the Burst compiler makes a read-only copy of the data at compilation time. Multi-dimensional arrays aren't supported. If you've used an unsupported static constructor, Burst produces the error BC1361. For more information on how Burst initializes arrays, see Static readonly fields and static constructors. Unsupported array types Burst doesn't support managed arrays. Instead, use a native container such as NativeArray . Struct types Supported structs Burst supports the following structs: Regular structs with any field with supported types Structs with fixed array fields Note Structs with an explicit layout might generate non-optimal native code. Supported struct layout Burst supports the following struct layouts: LayoutKind.Sequential LayoutKind.Explicit StructLayoutAttribute.Pack StructLayoutAttribute.Size Burst supports System.IntPtr and System.UIntPtr natively as intrinsic structs that directly represent pointers. Generic types Burst supports generic types used with structs. It supports full instantiation of generic calls for generic types that have interface constraints, for example when a struct with a generic parameter needs to implement an interface. Note There are restrictions if you use generic jobs. Vector types Burst can translate vector types from Unity.Mathematics to native SIMD vector types with the following first class support for optimizations: bool2/bool3/bool4 uint2/uint3/uint4 int2/int3/int4 float2/float3/float4 Tip For performance reasons, use the 4 wide types (bool4, uint4, float4, int4, ) over the other types. Enum types Supported enum types Burst supports all enums including enums that have a specific storage type, for example, public enum MyEnum : short. Unsupported enums Burst doesn't support Enum methods, for example Enum.HasFlag. Pointer types Burst supports any pointer types to any Burst supported types Span types Burst supports Span<T> and ReadOnlySpan<T> types in the Unity Editors that support them. You can only use span types in Burst jobs or function-pointers, but not across the interface to them. This is because in C#'s implementation of the span types it supports taking spans into managed data types (like a managed array). For example, the following code is invalid: [BurstCompile] public static void SomeFunctionPointer(Span<int> span) {} This is because Span is used across the managed and Burst boundary. In Burst, span types respect any safety check setting, and only perform performance-intensive checks when safety checks are enabled. Tuple types Burst supports value tuples ValueTuple<T1,T2> in Burst-compiled jobs or static methods, but not across the interface to them. This is because value tuples are of struct layout LayoutKind.Auto. Burst does not support LayoutKind.Auto (to see a list of struct layouts Burst supports see the section Struct types). However, one can use a regular struct to emulate a tuple like so: [BurstCompile] private struct MyTuple { public int item1; public float item2; }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/debugging-profiling-tools.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/debugging-profiling-tools.html",
    "title": "Debugging and profiling tools | FSM Unity Framework",
    "keywords": "Debugging and profiling tools The following sections describe how to debug and profile your Burst-compiled code in the Editor and in player builds. Tip Before attempting to debug Burst-compiled code, enable script debugging for the Editor, or a player build by following the steps in Debug C# code in Unity. Although you can theoretically debug Burst-compiled code even when the script compilation mode is set to Release, in practice it doesn't work reliably. Breakpoints might be skipped, and variables might not appear in the Locals window, for example. Debugging Burst-compiled code in the Editor To debug Burst-compiled code in the Editor, you can either use a managed debugger, or a native debugger. This section explains both options. Attach a managed debugger You can attach a managed debugger such as Visual Studio, Visual Studio for Mac, or JetBrains Rider. This is the same type of debugger you can use to debug regular managed C# code in your Unity project. The ways of attaching a debugger differ depending on the version of Unity you're using: Unity 2022.2+: When you place a breakpoint inside Burst-compiled code, and you have a managed debugger attached, Unity disables Burst automatically for that code path. This allows you to use a managed debugger to debug the managed version of your code. When you remove all breakpoints from that code path, Unity re-enables Burst for that code path. Unity 2022.1 and older: Disable Burst, either with the global option in the Editor Burst menu (Jobs > Burst > Enable Compilation), or comment out the [BurstCompile] attribute from the specific entry-point that you want to debug. Attach a native debugger You can attach a native debugger such as Visual Studio or Xcode. Before doing so, you need to disable Burst optimizations. You can do this in the following ways: Use the Native Debug Mode Compilation setting in the Editor Burst menu (Jobs > Burst > Native Debug Mode Compilation). Important: This setting disables optimizations across all jobs, which impacts the performance of Burst code. If you want to disable optimizations only for a specific job, use the other option in this list. Add the Debug = true flag to your job, which disables optimizations and enables debugging on that specific job: [BurstCompile(Debug = true)] public struct MyJob : IJob { // ... } Tip Player builds pick up the Debug flag, so you can also use this to debug a player build. To attach a native debugger to the Unity Editor process, see the native debugging section below. Debugging Burst-compiled code in a player build Because of the way that Unity builds the code for a player, you need to tell the debugging tool where to find the symbols. To do this, point the tool to the folder that contains the lib_burst_generated files, which is usually in the Plugins folder. To debug Burst-compiled code in a player build, you need to attach a native debugger (such as Visual Studio or Xcode) to the player process. Before doing so, you need to: Enable symbol generation. You can do this in either of two ways: Enable the Development Build option before you build the player, or Enable the Force Debug Information option in Burst AOT Player Settings Disable Burst optimizations. You can do this in either of two ways: Disable the Enable Optimizations option in Burst AOT Player Settings. Important: This setting disables optimizations across all jobs, which impacts the performance of Burst code. If you want to disable optimizations only for a specific job, use the other option in this list. Add the Debug = true flag to your job, which disables optimizations and enables debugging on that specific job: [BurstCompile(Debug = true)] public struct MyJob : IJob { // ... } To attach a native debugger to the player process, see the native debugging section below. Native debugging Follow the instructions above to setup native debugging correctly for the Editor or a player build. Then, attach a native debugger such as Visual Studio or Xcode. Native debugging limitations Native debuggers can't discover lambda captures on Entity.ForEach, so you can't inspect variables originating from these. Structs that use [StructLayout(LayoutKind=Explicit)] and have overlapping fields are represented by a struct that hides one of the overlaps. Types that are nested, are namespaced in C/C++ style. e.g. namespace Pillow { public struct Spot { public struct SubSpot { public int a; public int b; } public int a; public int b; public SubSpot sub; } You would refer to SubSpot as Pillow::Spot::SubSpot in this case (for instance if you were trying to cast a pointer in a debugger watch window). Code-based breakpoints Burst supports code-based breakpoints through the System.Diagnostics.Debugger.Break method. This method generates a debug trap in your code. You must attach a debugger to your code so that it can intercept the break. Breakpoints trigger whether you've attached a debugger or not. Burst adds information to track local variables, function parameters and breakpoints. If your debugger supports conditional breakpoints, use these over adding breakpoints in your code, because they only fire when you've attached a debugger. Profiling Burst-compiled code Profiling using standalone profiling tools You can use profiling tools (such as Instruments or Superluminal) to profile Burst-compiled code in a player build. Because of the way that Unity builds the code for a player, you need to tell the profiling tool where to find the symbols. To do this, point the tool to the folder that contains the lib_burst_generated files, which is usually in the Plugins folder. Unity Profiler markers To improve the data you get from Unity Profiler (either for Burst-compiled code running in the Editor or in an attached player), you can create Unity Profiler markers from Burst code by calling new ProfilerMarker(\"MarkerName\"): [BurstCompile] private static class ProfilerMarkerWrapper { private static readonly ProfilerMarker StaticMarker = new ProfilerMarker(\"TestStaticBurst\"); [BurstCompile(CompileSynchronously = true)] public static int CreateAndUseProfilerMarker(int start) { using (StaticMarker.Auto()) { var p = new ProfilerMarker(\"TestBurst\"); p.Begin(); var result = 0; for (var i = start; i < start + 100000; i++) { result += i; } p.End(); return result; } } }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-burst-inspector.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-burst-inspector.html",
    "title": "Burst Inspector window reference | FSM Unity Framework",
    "keywords": "Burst Inspector window reference The Burst Inspector window displays all the jobs and other Burst compile targets in the project. To open the Burst Inspector window, go to Jobs > Burst > Open Inspector. The Burst Inspector displays all the Jobs that it can compile. It also displays the generated intermediate and native assembly code. When opening a new target job in the Burst Inspector, it will try to focus the assembly directly related to the chosen bursted job. Furthermore, if branch flow arrows are shown, and they fill more than half of the assembly view, the inspector will scroll horizontally rightwards to focus the code instead of the branches. Burst Inspector with Branch Flow enabled Burst Inspector panes The Compile Targets pane on the left of the window displays an alphabetical list of the jobs in the project that Burst can compile. By default jobs either in the Unity namespace or with \".Generated\" in the name are excluded. This can be changed via the toggles Show Unity Namespace and Show \".Generated\" respectively. Disabled jobs in the list don't have the [BurstCompile] attribute. The right output pane of the Burst Inspector window displays options to view the assembly and intermediate code for the job you've selected in the Compile Targets list. To expand or collapse elements of the code, select the colored boxes (some with ellipses) . By default. the Burst Inspector automatically collapses blocks that it considers non-essential, such as most directives and data. It is possible to select lines of assembly. This will highlight the selected line, by underlining it. If this line contains any registers, the usage of these registers will be highlighted throughout the code; note that implicit registers are ignored for this feature. To select and copy the text in this pane, either click and drag with your mouse, or use Shift + arrow keys to select the text. To copy the text, either right-click and select Copy Selection, or press Ctrl + C (Command + C on macOS). Default behavior for the Burst Inspector's copy is to include underlying color tags. To change this, right-click with the mouse on the right pane, to open up the context menu, and untick Copy Color Tags. At the top of the window, the following display options are available: Display option Function Output dropdown Use the dropdown to select how to output the information in the Burst Inspector window Plain Without Debug Information Displays the raw output. Plain With Debug Information Displays the raw output with debug information. Enhanced with Minimal Debug Information (Only available in Assembly view) Displays the line information interweaved with the assembly to guide you to what line in your code matches what assembly output. If you've enabled Show Branch Flow, the branch flow indicates where jump instruction might branch off to. Enhanced With Full Debug Information (Only available in Assembly view) Displays the same information as Enhanced with Minimal Debug Information but with debug information included. Coloured With Minimal Debug Information (Only available in Assembly view) Displays the same information as Enhanced with Minimal Debug Information, but displays the output in color. Coloured With Full Debug Information (Only available in Assembly view) Displays the same information as Enhanced with Full Debug Information, but displays the output in color. Safety Checks Enable this option to generate code that includes container access safety checks, for example, to check if a job is attempting to write to a read-only native container. Font Size Select the size of the text in the output pane. Architecture dropdown Select the target architecture for your build. Focus on Code (Only available in Enhanced or Coloured output) Collapses the least important blocks of the disassembly. When you select this, Unity hides most of the assembly language directives and non code segments, allowing you to focus on the code itself. Expand all (Only available in Enhanced or Coloured output) Expands all collapsed blocks of disassembly and displays all the hidden assembly language directives and data elements. Show Branch Flow (Only available in Enhanced or Coloured output) Enable this option to display arrows that show branch flow in the code. When enabled, the code moves to the right, to make space to display the arrows. Highlight SIMD Scalar vs Packed (Only available in Enhanced or Coloured output) Enable this option to display SIMD instruction differently depending on their nature (Whether they work on packed or scalar inputs). This can be used to quickly assess the quality of the generated vectorized code (see SIMD smell test by Andreas Fredriksson). Assembly Displays the final optimized native code that Burst generated. .NET IL Displays the original .NET IL extracted from the job method. LLVM IR (Unoptimized) Displays the internal LLVM IR before optimizations. LLVM IR (Optimized) Displays the internal LLVM IR after optimizations. LLVM IR Optimization Diagnostics Displays LLVM diagnostics of the optimizations, such as if they succeeded or failed."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-burst-menu.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-burst-menu.html",
    "title": "Burst menu reference | FSM Unity Framework",
    "keywords": "Burst menu reference In the Editor, use the settings in the Burst menu to control how Burst works. To access this menu, go to Jobs > Burst. The following settings are available: Setting Function Enable Compilation Enable this setting to activate Burst compilation. When you enable this setting, Burst compiles jobs and Burst custom delegates that you tag with the attribute [BurstCompile]. Enable Safety Checks Choose what safety checks Burst should use. For more information see the Enable Safety Checks setting section of this documentation. Off Disable safety checks across all Burst jobs and function-pointers. Only use this setting if you want more realistic profiling results from in-Editor captures. When you reload the Editor, this setting always resets to On. On Enable safety checks on code that uses collection containers (e.g NativeArray<T>). Checks include job data dependency and container indexes out of bounds. This is the default setting. Force On Force safety checks on even for jobs and function-pointers that have DisableSafetyChecks = true. Use this setting to rule out any problems that safety checks might have caught. Synchronous Compilation Enable this setting to compile Burst synchronously. For more information, see Synchronous compilation. Native Debug Mode Compilation Enable this setting to deactivate optimizations on all code that Burst compiles. This makes it easier to debug via a native debugger. For more information, see Native Debugging tools. Show Timings Enable this setting to log the time it takes to JIT compile a job in the Editor and display it in the Console. For more information see the Show Timings setting section of this documentation. Open Inspector Opens the Burst Inspector window. Enable Safety Checks setting To disable Burst's safety check code, use DisableSafetyChecks. This results in faster code generation, however make sure that you use containers in a safe fashion. To disable safety checks on a job or function-pointer set DisableSafetyChecks to true: [BurstCompile(DisableSafetyChecks = true)] public struct MyJob : IJob { // ... } Burst ignores code marked explicitly with DisableSafetyChecks = true when it safety checks your code if you set Enable Safety Checks to On in the Editor. Select Force On to make Burst to safety check all code, including code marked with DisableSafetyChecks = true. Show Timings setting When you enable the Show Timings setting, Unity logs an output in the Console window for each library of entry points that Burst compiles. Burst batches the compilation into units of methods-per-assembly, and groups multiple entry-points together in a single compilation task. This output is useful if you want to report outliers in compilation to the Burst compiler team (via the Burst forum). Unity splits Burst's output into the following major sections: Method discovery (where Burst works out what it needs to compile) Front end (where Burst turns C# IL into an LLVM IR module) Middle end (where Burst specializes, optimizes, and cleans up the module) Back-end (where Burst turns the LLVM IR module into a native DLL) The compile time in the front end and optimizer is linear to the amount operations that it needs to compile. More functions and more instructions means a longer compile time. The more generic functions you have, the higher the front end performance timings, because generic resolutions have non-zero costs. The compile time in the back-end scales with the number of entry-points in the module. This is because each entry point is in its own native object file. If the optimizer takes a significant amount of time, use [BurstCompile(OptimizeFor = OptimizeFor.FastCompilation)] which reduces the optimizations that Burst does, but compiles things much faster. Profile the job before and after to make sure that this tradeoff is right for that entry-point."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-reference-overview.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/editor-reference-overview.html",
    "title": "Editor reference | FSM Unity Framework",
    "keywords": "Editor reference Explore the specific Burst Editor features. Topic Description Burst menu Use the Burst menu to control the Burst settings in your project. Burst Inspector Use the Burst Inspector to see the jobs and Burst compiled targets in your project. Additional resources Building your project"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/getting-started.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/getting-started.html",
    "title": "Getting started | FSM Unity Framework",
    "keywords": "Getting started Burst is primarily designed to work with Unity's job system. To start using the Burst compiler in your code, decorate a Job struct with the [BurstCompile] attribute. Add the [BurstCompile] attribute to the type and the static method you want Burst to compile. using Unity.Burst; using Unity.Collections; using Unity.Jobs; using UnityEngine; public class MyBurst2Behavior : MonoBehaviour { void Start() { var input = new NativeArray<float>(10, Allocator.Persistent); var output = new NativeArray<float>(1, Allocator.Persistent); for (int i = 0; i < input.Length; i++) input[i] = 1.0f * i; var job = new MyJob { Input = input, Output = output }; job.Schedule().Complete(); Debug.Log(\"The result of the sum is: \" + output[0]); input.Dispose(); output.Dispose(); } // Using BurstCompile to compile a Job with Burst [BurstCompile] private struct MyJob : IJob { [ReadOnly] public NativeArray<float> Input; [WriteOnly] public NativeArray<float> Output; public void Execute() { float result = 0.0f; for (int i = 0; i < Input.Length; i++) { result += Input[i]; } Output[0] = result; } } } Limitations Burst supports most C# expressions and statements, with a few exceptions. For more information, see C# language support. Compilation Burst compiles your code just-in-time (JIT) while in Play mode in the Editor, and ahead-of-time (AOT) when your application runs in a Player. For more information on compilation, see Burst compilation Command line options You can pass the following options to the Unity Editor on the command line to control Burst: --burst-disable-compilation disables Burst. --burst-force-sync-compilation force Burst to compile synchronously. For more information, see Burst compilation."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/index.html",
    "title": "About Burst | FSM Unity Framework",
    "keywords": "About Burst Burst is a compiler that you can use with Unity's job system to create code that enhances and improves your application's performance. It translates your code from IL/.NET bytecode to optimized native CPU code that uses the LLVM compiler. Installation To install this package, follow the instructions in the Package Manager documentation. If you change the Burst package version (for example, via Update), you need to close and restart the Editor. Further resources Videos Conference presentations given by the Burst team: Getting started with Burst - Unite Copenhagen 2019 (slides) Supercharging mobile performance with ARM Neon and Unity Burst Compiler Using Burst Compiler to optimize for Android - Unite Now 2020 Intrinsics: Low-level engine development with Burst - Unite Copenhagen 2019 (slides) Behind the Burst compiler: Converting .NET IL to highly optimized native code - DotNext 2018 Deep dive into the Burst compiler - Unite LA 2018 C# to machine code: GDC 2018 Using the native debugger for Burst compiled code Blogs Blog posts written by members of the Burst team : Raising your game with Burst 1.7 Enhancing mobile performance with the Burst compiler Enhanced aliasing with Burst In parameters in Burst"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/modding-support.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/modding-support.html",
    "title": "Modding support | FSM Unity Framework",
    "keywords": "Modding support From Unity 2021.1, you can load additional Burst compiled libraries, which provide a way to create modifications that use Burst compiled code. Burst only provides a method to load additional libraries, and doesn't provide any tooling to create mods. You need a copy of the Unity Editor to compile the additional libraries. This section gives an example approach to modding with Burst and is a proof of concept. Supported uses You can use this function in Play mode (or Standalone Players) only. Make sure you load the libraries as soon as possible, and before the first Burst compiled use of a C# method. Unity unloads any Burst libraries that BurstRuntime.LoadAdditionalLibraries loads when you exit Play mode in the Editor, quit a Standalone Player. Example modding system Note This example is limited in scope. You should have knowledge of assemblies and asmdefs to follow this example. This example declares an interface that the mods abide by: using UnityEngine; public interface PluginModule { void Startup(GameObject gameObject); void Update(GameObject gameObject); } You can use this interface to create new classes which follow these specifications and ship it separate to your application. Passing a single GameObject along limits the state that the plug-ins can affect. Modding manager The following is an example of a modding manager: using System; using System.Collections.Generic; using System.IO; using System.Reflection; using UnityEngine; using Unity.Burst; public class PluginManager : MonoBehaviour { public bool modsEnabled; public GameObject objectForPlugins; List<PluginModule> plugins; void Start() { plugins = new List<PluginModule>(); // If mods are disabled, early out - this allows us to disable mods, enter Play Mode, exit Play Mode //and be sure that the managed assemblies have been unloaded (assuming DomainReload occurs) if (!modsEnabled) return; var folder = Path.GetFullPath(Path.Combine(Application.dataPath, \"..\", \"Mods\")); if (Directory.Exists(folder)) { var mods = Directory.GetDirectories(folder); foreach (var mod in mods) { var modName = Path.GetFileName(mod); var monoAssembly = Path.Combine(mod, $\"{modName}_managed.dll\"); if (File.Exists(monoAssembly)) { var managedPlugin = Assembly.LoadFile(monoAssembly); var pluginModule = managedPlugin.GetType(\"MyPluginModule\"); var plugin = Activator.CreateInstance(pluginModule) as PluginModule; plugins.Add(plugin); } var burstedAssembly = Path.Combine(mod, $\"{modName}_win_x86_64.dll\"); // Burst dll (assuming windows 64bit) if (File.Exists(burstedAssembly)) { BurstRuntime.LoadAdditionalLibrary(burstedAssembly); } } } foreach (var plugin in plugins) { plugin.Startup(objectForPlugins); } } // Update is called once per frame void Update() { foreach (var plugin in plugins) { plugin.Update(objectForPlugins); } } } This code scans the \"Mods\" folder, and for each folder it finds within, it attempts to load both a managed dll and a Burst compiled dll. It does this by adding them to an internal list that it can then iterate on and call the respective interface functions. The names of the files are arbitrary: see Simple Create Mod Menu Button, which is the code that generated those files. Because this code loads the managed assemblies into the current domain, you need a domain reload to unload those before you can overwrite them. Unity automatically unloads the Burst dll files automatically unloaded when you exit Play mode. This is why a Boolean to disable the modding system is included, for testing in the Editor. A mod that uses Burst Create a separate Unity project for this to use the project to produce the mod. The following script attaches to a UI Canvas that contains a text component called Main UI Label, and changes the text when the mod is used. The text is either Plugin Updated : Bursted or Plugin Updated : Not Bursted. You will see the Plugin Updated : Bursted by default, but if you comment out the lines that load the Burst library in the PluginManager above, then the Burst compiled code doesn't load and the message changes appropriately. using Unity.Burst; using Unity.Collections; using Unity.Jobs; using UnityEngine; using UnityEngine.UI; public class MyPluginModule : PluginModule { Text textComponent; public void Startup(GameObject gameObject) { var childTextComponents = gameObject.GetComponentsInChildren<Text>(); textComponent = null; foreach (var child in childTextComponents) { if (child.name == \"Main UI Label\") { textComponent = child; } } if (textComponent==null) { Debug.LogError(\"something went wrong and i couldn't find the UI component i wanted to modify\"); } } public void Update(GameObject gameObject) { if (textComponent != null) { var t = new CheckBurstedJob { flag = new NativeArray<int>(1, Allocator.TempJob, NativeArrayOptions.UninitializedMemory) }; t.Run(); if (t.flag[0] == 0) textComponent.text = \"Plugin Updated : Not Bursted\"; else textComponent.text = \"Plugin Updated : Bursted\"; t.flag.Dispose(); } } [BurstCompile] struct CheckBurstedJob : IJob { public NativeArray<int> flag; [BurstDiscard] void CheckBurst() { flag[0] = 0; } public void Execute() { flag[0] = 1; CheckBurst(); } } } Put the above script in a folder along with an assembly definition file with an assembly name of TestMod_Managed, so that the next script can locate the managed part. Simple Create Mod Menu button The below script adds a menu button. When you use the menu button, it builds a Standalone Player, then copies the C# managed dll and the lib_burst_generated.dll into a chosen Mod folder. This example assumes you are using Windows. using UnityEditor; using System.IO; using UnityEngine; public class ScriptBatch { [MenuItem(\"Modding/Build X64 Mod (Example)\")] public static void BuildGame() { string modName = \"TestMod\"; string projectFolder = Path.Combine(Application.dataPath, \"..\"); string buildFolder = Path.Combine(projectFolder, \"PluginTemp\"); // Get filename. string path = EditorUtility.SaveFolderPanel(\"Choose Final Mod Location\", \"\", \"\"); FileUtil.DeleteFileOrDirectory(buildFolder); Directory.CreateDirectory(buildFolder); // Build player. var report = BuildPipeline.BuildPlayer(new[] { \"Assets/Scenes/SampleScene.unity\" }, Path.Combine(buildFolder, $\"{modName}.exe\"), BuildTarget.StandaloneWindows64, BuildOptions.Development); if (report.summary.result == UnityEditor.Build.Reporting.BuildResult.Succeeded) { // Copy Managed library var managedDest = Path.Combine(path, $\"{modName}_Managed.dll\"); var managedSrc = Path.Combine(buildFolder, $\"{modName}_Data/Managed/{modName}_Managed.dll\"); FileUtil.DeleteFileOrDirectory(managedDest); if (!File.Exists(managedDest)) // Managed side not unloaded FileUtil.CopyFileOrDirectory(managedSrc, managedDest); else Debug.LogWarning($\"Couldn't update manged dll, {managedDest} is it currently in use?\"); // Copy Burst library var burstedDest = Path.Combine(path, $\"{modName}_win_x86_64.dll\"); var burstedSrc = Path.Combine(buildFolder, $\"{modName}_Data/Plugins/x86_64/lib_burst_generated.dll\"); FileUtil.DeleteFileOrDirectory(burstedDest); if (!File.Exists(burstedDest)) FileUtil.CopyFileOrDirectory(burstedSrc, burstedDest); else Debug.LogWarning($\"Couldn't update bursted dll, {burstedDest} is it currently in use?\"); } } }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-assumerange.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-assumerange.html",
    "title": "AssumeRange attribute | FSM Unity Framework",
    "keywords": "AssumeRange attribute Use the AssumeRange attribute to tell Burst that a given scalar-integer lies within a certain constrained range. If Burst has this information, it can improve the performance of your application. The following code is an example of this: [return:AssumeRange(0u, 13u)] static uint WithConstrainedRange([AssumeRange(0, 26)] int x) { return (uint)x / 2u; } This example tells Burst the following: The variable x is in the closed-interval range [0..26], or more plainly that x >= 0 && x <= 26. The return value from WithConstrainedRange is in the closed-interval range [0..13], or more plainly that x >= 0 && x <= 13. Burst uses these assumptions to create better code generation. However, there are some restrictions: You can only place these on scalar-integer (signed or unsigned) types. The type of the range arguments must match the type being attributed. Burst has deductions for the .Length property of NativeArray and NativeSlice which indicates that these always return non-negative integers: static bool IsLengthNegative(NativeArray<float> na) { // Burst always replaces this with the constant false return na.Length < 0; } For example, if you have a container like the following: struct MyContainer { public int Length; // Some other data... } The following example shows how to tell Burst that Length is always a positive integer: struct MyContainer { private int _length; [return: AssumeRange(0, int.MaxValue)] private int LengthGetter() { return _length; } public int Length { get => LengthGetter(); set => _length = value; } // Some other data... }"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-constant.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-constant.html",
    "title": "Constant intrinsic | FSM Unity Framework",
    "keywords": "Constant intrinsic Use the IsConstantExpression intrinsic to check if a given expression is constant at compile-time: using static Unity.Burst.CompilerServices.Constant; var somethingWhichWillBeConstantFolded = math.pow(42.0f, 42.0f); if (IsConstantExpression(somethingWhichWillBeConstantFolded)) { // Burst knows that somethingWhichWillBeConstantFolded is a compile-time constant } This is useful to check if a complex expression is always constant folded. You can use it for optimizations for a known constant value. For example, if you want to implement a pow-like function for integer powers: using static Unity.Burst.CompilerServices.Constant; public static float MyAwesomePow(float f, int i) { if (IsConstantExpression(i) && (2 == i)) { return f * f; } else { return math.pow(f, (float)i); } } The IsConstantExpression check means that Burst always removes the branch if i isn't constant, because the if condition is false. This means that if i is constant and is equal to 2, you can use a more optimal simple multiply instead. The result of IsConstantExpression intentionally depends on the result of the optimizations being run. Therefore the result can change based on whether a function gets inlined or not. For example in the case above: IsConstantExpression(i) is false on its own, because i is a function argument which is obivously not constant. However, if MyAwesomePow gets inlined with a constant value for i, then it will evaluate to true. But if MyAwesomePow ends up not being inlined for whatever reason, then IsConstantExpression(i) will remain false. Note Constant folding only takes place during optimizations. If you've disabled optimizations, the intrinsic returns false."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-hint.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-hint.html",
    "title": "Hint intrinsics | FSM Unity Framework",
    "keywords": "Hint intrinsics Use the Hint intrinsics to add information to your code which helps with Burst optimization. It has the following methods: Unity.Burst.CompilerServices.Hint.Likely: Tells Burst that a Boolean condition is likely to be true. Unity.Burst.CompilerServices.Hint.Unlikely: Tells Burst that a Boolean condition is unlikely to be true. Unity.Burst.CompilerServices.Hint.Assume: Tells Burst that it can assume a Boolean condition is true. Likely intrinsic The Likely intrinsic is most useful to tell Burst which branch condition has a high probability of being true. This means that Burst can focus on the branch in question for optimization purposes: if (Unity.Burst.CompilerServices.Hint.Likely(b)) { // Any code in here will be optimized by Burst with the assumption that we'll probably get here! } else { // Whereas the code in here will be kept out of the way of the optimizer. } Unlikely intrinsic The Unlikely intrinsic tells Burst the opposite of the Likely intrinsic: the condition is unlikely to be true, and it should optimize against it: if (Unity.Burst.CompilerServices.Hint.Unlikely(b)) { // Whereas the code in here will be kept out of the way of the optimizer. } else { // Any code in here will be optimized by Burst with the assumption that we'll probably get here! } The Likely and Unlikely intrinsics make sure that Burst places the code most likely to be hit after the branching condition in the binary. This means that the code has a high probability of being in the instruction cache. Burst can also hoist the code out of the likely branch and spend extra time optimizing it, and not spend as much time looking at the unlikely code. An example of an unlikely branch is to check if result of an allocation is valid. The allocation is valid most of all the time, so you want the code to be fast with that assumption, but you want an error case to fall back to. Assume intrinsic The Assume intrinsic is powerful. Use it with caution because it tells Burst that a condition is always true. Warning When you use Assume, Burst assumes the value is true without checking whether it's true. Unity.Burst.CompilerServices.Hint.Assume(b); if (b) { // Burst has been told that b is always true, so this branch will always be taken. } else { // Any code in here will be removed from the program because b is always true! } Use the Assume intrinsic to arbitrarily tell Burst that something is true. For example, you can use Assume to tell Burst to assume that a loop end is always a multiple of 16, which means that it can provide perfect vectorization without any scalar spilling for that loop. You could also use it to tell Burst that a value isn't NaN, or it's negative."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-loop-vectorization.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-loop-vectorization.html",
    "title": "Loop vectorization | FSM Unity Framework",
    "keywords": "Loop vectorization Burst uses loop vectorization to improve the performance of your code. It uses this technique to loop over multiple values at the same time, rather than looping over single values at a time, which speeds up the performance of your code. For example: [MethodImpl(MethodImplOptions.NoInlining)] private static unsafe void Bar([NoAlias] int* a, [NoAlias] int* b, int count) { for (var i = 0; i < count; i++) { a[i] += b[i]; } } public static unsafe void Foo(int count) { var a = stackalloc int[count]; var b = stackalloc int[count]; Bar(a, b, count); } Burst converts the scalar loop in Bar into a vectorized loop. Then, instead of looping over a single value at a time, it generates code that loops over multiple values at the same time, which produces faster code. This is the x64 assembly Burst generates for AVX2 for the loop in Bar above: .LBB1_4: vmovdqu ymm0, ymmword ptr [rdx + 4*rax] vmovdqu ymm1, ymmword ptr [rdx + 4*rax + 32] vmovdqu ymm2, ymmword ptr [rdx + 4*rax + 64] vmovdqu ymm3, ymmword ptr [rdx + 4*rax + 96] vpaddd ymm0, ymm0, ymmword ptr [rcx + 4*rax] vpaddd ymm1, ymm1, ymmword ptr [rcx + 4*rax + 32] vpaddd ymm2, ymm2, ymmword ptr [rcx + 4*rax + 64] vpaddd ymm3, ymm3, ymmword ptr [rcx + 4*rax + 96] vmovdqu ymmword ptr [rcx + 4*rax], ymm0 vmovdqu ymmword ptr [rcx + 4*rax + 32], ymm1 vmovdqu ymmword ptr [rcx + 4*rax + 64], ymm2 vmovdqu ymmword ptr [rcx + 4*rax + 96], ymm3 add rax, 32 cmp r8, rax jne .LBB1_4 Burst has unrolled and vectorized the loop into four vpaddd instructions, which calculate eight integer additions each, for a total of 32 integer additions per loop iteration. Loop vectorization intrinsics Burst includes experimental intrinsics to express loop vectorization assumptions: Loop.ExpectVectorized and Loop.ExpectNotVectorized. Burst then validates the loop vectorization at compile-time. This is useful in a situation where you might break the auto vectorization. For example, if you introduce a branch to the code: [MethodImpl(MethodImplOptions.NoInlining)] private static unsafe void Bar([NoAlias] int* a, [NoAlias] int* b, int count) { for (var i = 0; i < count; i++) { if (a[i] > b[i]) { break; } a[i] += b[i]; } } This changes the assembly to the following: .LBB1_3: mov r9d, dword ptr [rcx + 4*r10] mov eax, dword ptr [rdx + 4*r10] cmp r9d, eax jg .LBB1_4 add eax, r9d mov dword ptr [rcx + 4*r10], eax inc r10 cmp r8, r10 jne .LBB1_3 This isn't ideal because the loop is scalar and only has 1 integer addition per loop iteration. It can be difficult to spot this happening in your code, so use the experimental intrinsics Loop.ExpectVectorized and Loop.ExpectNotVectorized to express loop vectorization assumptions. Burst then validates the loop vectorization at compile-time. Because the intrinsics are experimental, you need to use the UNITY_BURST_EXPERIMENTAL_LOOP_INTRINSICS preprocessor define to enable them. The following example shows the original Bar example with the Loop.ExpectVectorized intrinsic: [MethodImpl(MethodImplOptions.NoInlining)] private static unsafe void Bar([NoAlias] int* a, [NoAlias] int* b, int count) { for (var i = 0; i < count; i++) { Unity.Burst.CompilerServices.Loop.ExpectVectorized(); a[i] += b[i]; } } Burst then validates at compile-time whether the loop is vectorized. If the loop isn't vectorized, Burst emits a compiler error. The following example produces an error: [MethodImpl(MethodImplOptions.NoInlining)] private static unsafe void Bar([NoAlias] int* a, [NoAlias] int* b, int count) { for (var i = 0; i < count; i++) { Unity.Burst.CompilerServices.Loop.ExpectVectorized(); if (a[i] > b[i]) { break; } a[i] += b[i]; } } Burst emits the following error at compile-time: LoopIntrinsics.cs(6,9): Burst error BC1321: The loop is not vectorized where it was expected that it is vectorized. Important These intrinsics don't work inside if statements. Burst doesn't prevent this from happening, so you won't see a compile-time error for this."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-overview.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-overview.html",
    "title": "Optimization | FSM Unity Framework",
    "keywords": "Optimization Best practices around optimizing Burst-compiled code. Topic Description Debugging and profiling tools Debug and profile your Burst-compiled code in the Editor and in player builds. Loop vectorization optimization Understand how Burst uses loop vectorization to optimize your code. Memory aliasing Use memory aliasing to tell Burst how your code uses data. AssumeRange attribute Use AssumeRange to tell Burst a given scalar-integer lies within a certain constrained range. Hint intrinsic Use the Hint intrinsic to give Burst more information about your data. Constant intrinsic Use IsConstantExpression top check if an expression is constant at run time. SkipLocalsInit attribute Use SkipLocalsInitAttribute to tell Burst that any stack allocations within a method don't have to be initialized to zero. Additional resources Burst intrinsics"
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-skiplocalsinit.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Documentation~/optimization-skiplocalsinit.html",
    "title": "SkipLocalsInit attribute | FSM Unity Framework",
    "keywords": "SkipLocalsInit attribute Use SkipLocalsInitAttribute, to tell Burst that any stack allocations within a method don't have to be initialized to zero. In C# all local variables are initialized to zero by default. This is useful because it means an entire class of bugs surrounding undefined data disappears. But this can impact runtime performance, because initializing this data to zero takes work: static unsafe int DoSomethingWithLUT(int* data); static unsafe int DoSomething(int size) { int* data = stackalloc int[size]; // Initialize every field of data to be an incrementing set of values. for (int i = 0; i < size; i++) { data[i] = i; } // Use the data elsewhere. return DoSomethingWithLUT(data); } The X86 assembly for this is: push rbp .seh_pushreg rbp push rsi .seh_pushreg rsi push rdi .seh_pushreg rdi mov rbp, rsp .seh_setframe rbp, 0 .seh_endprologue mov edi, ecx lea r8d, [4*rdi] lea rax, [r8 + 15] and rax, -16 movabs r11, offset __chkstk call r11 sub rsp, rax mov rsi, rsp sub rsp, 32 movabs rax, offset burst.memset.inline.X64_SSE4.i32@@32 mov rcx, rsi xor edx, edx xor r9d, r9d call rax add rsp, 32 test edi, edi jle .LBB0_7 mov eax, edi cmp edi, 8 jae .LBB0_3 xor ecx, ecx jmp .LBB0_6 .LBB0_3: mov ecx, eax and ecx, -8 movabs rdx, offset __xmm@00000003000000020000000100000000 movdqa xmm0, xmmword ptr [rdx] mov rdx, rsi add rdx, 16 movabs rdi, offset __xmm@00000004000000040000000400000004 movdqa xmm1, xmmword ptr [rdi] movabs rdi, offset __xmm@00000008000000080000000800000008 movdqa xmm2, xmmword ptr [rdi] mov rdi, rcx .p2align 4, 0x90 .LBB0_4: movdqa xmm3, xmm0 paddd xmm3, xmm1 movdqu xmmword ptr [rdx - 16], xmm0 movdqu xmmword ptr [rdx], xmm3 paddd xmm0, xmm2 add rdx, 32 add rdi, -8 jne .LBB0_4 cmp rcx, rax je .LBB0_7 .p2align 4, 0x90 .LBB0_6: mov dword ptr [rsi + 4*rcx], ecx inc rcx cmp rax, rcx jne .LBB0_6 .LBB0_7: sub rsp, 32 movabs rax, offset \"DoSomethingWithLUT\" mov rcx, rsi call rax nop mov rsp, rbp pop rdi pop rsi pop rbp ret In this example, the movabs rax, offset burst.memset.inline.X64_SSE4.i32@@32 line means that you've had to inject a memset to zero out the data. In the above example, you know that the array is entirely initialized in the following loop, but Burst doesn't know that. To fix this problem, use Unity.Burst.CompilerServices.SkipLocalsInitAttribute, which tells Burst that any stack allocations within a method don't have to be initialized to zero. Note Only use this attribute if you're certain that you won't run into undefined behavior bugs. For example: using Unity.Burst.CompilerServices; static unsafe int DoSomethingWithLUT(int* data); [SkipLocalsInit] static unsafe int DoSomething(int size) { int* data = stackalloc int[size]; // Initialize every field of data to be an incrementing set of values. for (int i = 0; i < size; i++) { data[i] = i; } // Use the data elsewhere. return DoSomethingWithLUT(data); } The assembly after adding the [SkipLocalsInit] on the method is: push rbp .seh_pushreg rbp mov rbp, rsp .seh_setframe rbp, 0 .seh_endprologue mov edx, ecx lea eax, [4*rdx] add rax, 15 and rax, -16 movabs r11, offset __chkstk call r11 sub rsp, rax mov rcx, rsp test edx, edx jle .LBB0_7 mov r8d, edx cmp edx, 8 jae .LBB0_3 xor r10d, r10d jmp .LBB0_6 .LBB0_3: mov r10d, r8d and r10d, -8 movabs rax, offset __xmm@00000003000000020000000100000000 movdqa xmm0, xmmword ptr [rax] mov rax, rcx add rax, 16 movabs rdx, offset __xmm@00000004000000040000000400000004 movdqa xmm1, xmmword ptr [rdx] movabs rdx, offset __xmm@00000008000000080000000800000008 movdqa xmm2, xmmword ptr [rdx] mov r9, r10 .p2align 4, 0x90 .LBB0_4: movdqa xmm3, xmm0 paddd xmm3, xmm1 movdqu xmmword ptr [rax - 16], xmm0 movdqu xmmword ptr [rax], xmm3 paddd xmm0, xmm2 add rax, 32 add r9, -8 jne .LBB0_4 cmp r10, r8 je .LBB0_7 .p2align 4, 0x90 .LBB0_6: mov dword ptr [rcx + 4*r10], r10d inc r10 cmp r8, r10 jne .LBB0_6 .LBB0_7: sub rsp, 32 movabs rax, offset \"DoSomethingWithLUT\" call rax nop mov rsp, rbp pop rbp ret The call to memset is now gone, because you've told Burst that any stack allocations within a method don't have to be initialized to zero."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Burst copyright © 2022 Unity Technologies Source code of the package is licensed under the Unity Companion License (see https://unity3d.com/legal/licenses/unity_companion_license); otherwise licensed under the Unity Package Distribution License (see https://unity3d.com/legal/licenses/Unity_Package_Distribution_License ). Unless expressly provided otherwise, the software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.burst@1.8.10/Third Party Notices.html": {
    "href": "Library/PackageCache/com.unity.burst@1.8.10/Third Party Notices.html",
    "title": "| FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the license(s) indicated below: Component Name: LLVM License Type: Apache 2.0 with LLVM Exceptions Copyright: The LLVM project does not collect copyright assignments, which means that the copyright for the code in the project is held by the respective contributors. Because you (or your company) retain ownership of the code you contribute, you know it may only be used under the terms of the open source license you contributed it under: the license for your contributions cannot be changed in the future without your approval. https://github.com/llvm/llvm-project/tree/main/llvm Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. --- LLVM Exceptions to the Apache 2.0 License --- As an exception, if, as a result of your compiling your source code, portions of this Software are embedded into an Object form of such source code, you may redistribute such embedded portions in such Object form without complying with the conditions of Sections 4(a), 4(b) and 4(d) of the License. In addition, if you combine or link compiled forms of this Software with software that is licensed under the GPLv2 (\"Combined Software\") and if a court of competent jurisdiction determines that the patent provision (Section 3), the indemnity provision (Section 9) or other Section of the License conflicts with the conditions of the GPLv2, you may retroactively and prospectively choose to deem waived or otherwise exclude such Section(s) of the License, but only in their entirety and only with respect to the Combined Software. Component Name: Legacy LLVM License License Type: The University of Illinois/NCSA Open Source License (NCSA) Copyright (c) 2007-2019 University of Illinois at Urbana-Champaign. All rights reserved. https://llvm.org/docs/DeveloperPolicy.html#legacy Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution. LLVM Team, University of Illinois at Urbana-Champaign, nor the names of its contributors may be used to endorse or promote products derived from this Software without specific prior written permission. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE. Component Name: Mono.Cecil License Type: MIT Copyright (c) 2008 - 2015 Jb Evain All rights reserved. Copyright (c) 2008 - 2011 Novell, Inc. All rights reserved. https://github.com/jbevain/cecil Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Component Name: Smash License Type: BSD 2-Clause Copyright (c) 2021, Alexandre Mutel All rights reserved. https://github.com/xoofx/smash Redistribution and use in source and binary forms, with or without modification , are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Component Name: xxHash Library License Type: BSD 2-Clause Copyright (c) 2012-2021, Yann Collet All rights reserved. https://github.com/Cyan4973/xxHash Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Component Name: musl musl as a whole is licensed under the following standard MIT license: Copyright © 2005-2019 Rich Felker, et al. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Authors/contributors include: A. Wilcox Alex Dowad Alex Suykov Alexander Monakov Andre McCurdy Andrew Kelley Anthony G. Basile Aric Belsito Arvid Picciani Bartosz Brachaczek Benjamin Peterson Bobby Bingham Boris Brezillon Brent Cook Chris Spiegel Clément Vasseur Daniel Micay Daniel Sabogal Daurnimator David Carlier David Edelsohn Denys Vlasenko Dmitry Ivanov Dmitry V. Levin Drew DeVault Emil Renner Berthing Fangrui Song Felix Fietkau Felix Janda Gianluca Anzolin Hauke Mehrtens He X Hiltjo Posthuma Isaac Dunham Jaydeep Patil Jens Gustedt Jeremy Huntwork Jo-Philipp Wich Joakim Sindholt John Spencer Josiah Worcester Julien Ramseier Justin Cormack Kaarle Ritvanen Khem Raj Kylie McClain Leah Neukirchen Luca Barbato Luka Perkov M Farkas-Dyck (Strake) Mahesh Bodapati Markus Wichmann Masanori Ogino Michael Clark Michael Forney Mikhail Kremnyov Natanael Copa Nicholas J. Kain orc Pascal Cuoq Patrick Oppenlander Petr Hosek Petr Skocik Pierre Carrier Reini Urban Rich Felker Richard Pennington Ryan Fairfax Samuel Holland Segev Finer Shiz sin Solar Designer Stefan Kristiansson Stefan O'Rear Szabolcs Nagy Timo Teräs Trutz Behn Valentin Ochs Will Dietz William Haddon William Pitcock Portions of this software are derived from third-party works licensed under terms compatible with the above MIT license: The TRE regular expression implementation (src/regex/reg* and src/regex/tre*) is Copyright © 2001-2008 Ville Laurikari and licensed under a 2-clause BSD license (license text in the source files). The included version has been heavily modified by Rich Felker in 2012, in the interests of size, simplicity, and namespace cleanliness. Much of the math library code (src/math/* and src/complex/*) is Copyright © 1993,2004 Sun Microsystems or Copyright © 2003-2011 David Schultz or Copyright © 2003-2009 Steven G. Kargl or Copyright © 2003-2009 Bruce D. Evans or Copyright © 2008 Stephen L. Moshier and labelled as such in comments in the individual source files. All have been licensed under extremely permissive terms. The ARM memcpy code (src/string/arm/memcpy_el.S) is Copyright © 2008 The Android Open Source Project and is licensed under a two-clause BSD license. It was taken from Bionic libc, used on Android. The implementation of DES for crypt (src/crypt/crypt_des.c) is Copyright © 1994 David Burren. It is licensed under a BSD license. The implementation of blowfish crypt (src/crypt/crypt_blowfish.c) was originally written by Solar Designer and placed into the public domain. The code also comes with a fallback permissive license for use in jurisdictions that may not recognize the public domain. The smoothsort implementation (src/stdlib/qsort.c) is Copyright © 2011 Valentin Ochs and is licensed under an MIT-style license. The x86_64 port was written by Nicholas J. Kain and is licensed under the standard MIT terms. The mips and microblaze ports were originally written by Richard Pennington for use in the ellcc project. The original code was adapted by Rich Felker for build system and code conventions during upstream integration. It is licensed under the standard MIT terms. The mips64 port was contributed by Imagination Technologies and is licensed under the standard MIT terms. The powerpc port was also originally written by Richard Pennington, and later supplemented and integrated by John Spencer. It is licensed under the standard MIT terms. All other files which have no copyright comments are original works produced specifically for use as part of this library, written either by Rich Felker, the main author of the library, or by one or more contibutors listed above. Details on authorship of individual files can be found in the git version control history of the project. The omission of copyright and license comments in each file is in the interest of source tree size. In addition, permission is hereby granted for all public header files (include/* and arch//bits/) and crt files intended to be linked into applications (crt/, ldso/dlstart.c, and arch//crt_arch.h) to omit the copyright notice and permission notice otherwise required by the license, and to use these files without any requirement of attribution. These files include substantial contributions from: Bobby Bingham John Spencer Nicholas J. Kain Rich Felker Richard Pennington Stefan Kristiansson Szabolcs Nagy all of whom have explicitly granted such permission. This file previously contained text expressing a belief that most of the files covered by the above exception were sufficiently trivial not to be subject to copyright, resulting in confusion over whether it negated the permissions granted in the license. In the spirit of permissive licensing, and of not having licensing issues being an obstacle to adoption, that text has been removed. Component Name: SLEEF Boost Software License - Version 1.0 - August 17th, 2003 Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Component Name: gRPC for .NET Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Component Name: Google.Protobuf Copyright 2008 Google Inc. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Code generated by the Protocol Buffer compiler is owned by the owner of the input file used when generating it. This code is not standalone and requires a support library to be linked with it. This support library is itself covered by the above license. Component Name: mimalloc MIT License Copyright (c) 2018-2021 Microsoft Corporation, Daan Leijen Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [2.9.7] - 2023-05-06 Bugfix: AxisState was not respecting timescale == 0. Bugfix: Very occasional axis drift in SimpleFollow when viewing angle is +-90 degrees. URP: add temporal effects reset on camera cut for URP 14.0.4 and up. Bugfix: MixingCamera calls OnTransitionFromCamera correctly for all its children. Bugfix: Passive vcams with noise were not respecting transform's z rotation during blends. Regression fix: CinemachineSmoothPath.GetLocalOrientation was returning global orientations. [2.9.5] - 2023-01-16 Unity 2022.2 and up: FocusDistance added to lens settings and is pushed to the camera. Optimized path gizmo drawing. Now 3-5 times faster. TargetGroup now ignores members whose gameObjects are inactive. Bugfix: CinemachinePathBase search radius fixed for not looped paths. Regression fix: POV was not handling ReferenceUp correctly Bugfix: priority ordering was wrong when the difference between any priority values were smaller than integer min or bigger than integer max values. Bugfix: Extensions were not respecting execution order on domain reload. [2.9.4] - 2022-11-18 Bugfix: VirtualCameras did not set the focal length property of physical cameras. Bugfix: could not set main camera to physical mode while a vcam with ModeOverride = None was active [2.9.2] - 2022-10-21 Bugfix: StateDrivenCamera/Clearshot: Transition glitch when backing out of a transition in progress Bugfix: Occasional 1-frame glitch when transitioning between some freelooks Bugfix: Transposer with LockToTarget binding sometimes had gimbal lock. Bugfix: Collider damping is more robust with extreme FreeLook configurations Add support for HDRP 14 (Unity 2022.2) Bugfix: InputValueGain mode of axis input was not framerate-independent Bugfix: When recording with an accumulation buffer, camera lens was not always set correctly Bugfix: POV starts up in its centered position, if recentering is enabled Freelook ForcePosition is more precise now. [2.9.1] - 2022-08-24 Bugfix: CinemachineConfiner was not confining correctly when Confine Screen Edges was enabled and the camera was rotated. AimingRig sample is only optionally dependent on UnityEngine.UI. Bugfix: A memory leak no longer occurs with PostProcessing if no PP layer is present on the camera. Bugfix: Confiner2D confines to midpoint when the camera window is bigger than the axis-aligned bounding box of the input confiner. Bugfix: The FadeOut sample scene shader was culling some objects incorrectly. Bugfix: Freelook had the wrong heading at the first frame, which could cause a slight jitter. Bugfix: FramingTransposer and Composer had a slight rounding error in their Bias fields when the Screen X and Y fields were modified. Bugfix: Fixed spurious Z rotations during blend. Regression fix: POV is relative to its parent transform. Bugfix: Blending speed was not set correctly when blending back and forth between the same cameras. Bugfix: AxisState.Recentering.RecenterNow() did not work reliably. Bugfix: SensorSize is not saved when not using physical camera. Bugfix: No redundant RepaintAllViews calls. Clipper library dependency is no longer conflicting with users. Bugfix: Standalone profiler no longer crashes with CM. Bugfix: Cinemachine does not produce compiler error in unity editor versions older than 2020, when Input System package is installed. Bugfix: EmbeddedAssetProperties were not displayed correctly in the editor. Bugfix: SaveDuringPlay works with ILists now. Bugfix: Paste VirtualCamera and FreeLook components onto prefab works for subcomponents Bugfix: CinemachineInputProvider now correctly tracks enabled state of input action Bugfix: POV orientation was incorrect with World Up override Added AutoEnable option to CinemachineInputHandler [2.9.0-pre.6] - 2022-01-12 Bugfix: Negative Near Clip Plane value is kept when camera is orthographic. Regression fix: could not change the projection of the main camera if a CM virtual camera is active. Regression fix: Axis input was ignoring CM's IgnoreTimeScale setting. New feature: CinemachineBrain may control other GameObject instead of the one it is attached to. Bugfix: Cinemachine assigns a default input controller delegate that returns 0 when the legacy input system is disabled. Cinemachine example scenes show informative text when used with Input System instead of throwing error messages. Regression fix: compilation errors when physics module is not present. Regression fix: virtual cameras no longer forget that they are targeting groups on domain reload. Bugfix: 3rdPersonFollow logged console messages when looking straight up or down. BugFix: InputProvider no longer causes a tiny gc alloc every frame. Regression fix: CinemachineCollider smoothing time did not reset correctly, so it was working once only. Bugfix: Confiner2D now displays the calculated confining area when its vcam is selected. Samples no longer throw errors with HDRP and URP. 3rdPersonWithAimMode and Timeline samples no longer have invalid references. Bugfix: 3rdPersonFollow shows a warning message when no follow target is assigned like the rest of the body components. Added ability to directly set the active blend in CinemachineBrain. Bugfix: OnTargetObjectWarped() did not work properly for 3rdPersonFollow. Bugfix: POV did not properly handle overridden up. Regression fix: removed GC allocs in UpdateTargetCache. Bugfix: async scene load/unload could cause jitter. Bugfix: Input system should be read only once per render frame. Bugfix: Blends were sometimes incorrect when src or dst camera is looking along world up axis. Bugfix: Improve accuracy of Group Framing. New feature: Added scene view overlay tools for Cinemachine components. Cinemachine3rdPersonAim exposes AimTarget, which is the position of where the player would hit. [2.8.0] - 2021-07-13 Bugfix: Freelook prefabs won't get corrupted after editing the Prefab via its instances. Bugfix: 3rdPersonFollow works with Aim components now. Bugfix: Blends between vcams, that are rotated so that their up vector is different from World up, are correct now. Bugfix: POV recentering did not always recenter correctly, when an axis range was limited. Bugfix: Collider sometimes bounced a little when the camera radius was large. Bugfix: CinemachineVolumeSettings inspector was making the game view flicker. Bugfix: CinemachineVolumeSettings inspector displayed a misleading warning message with URP when focus tracking was enabled. Bugfix: Rapidly toggling active cameras before the blends were finished did not use the correct blend time. AimingRig sample scene updated with a better reactive crosshair design. Added API accessor for Active Blend in Clearshot and StateDrivenCamera. Bugfix: Virtual Cameras were not updating in Edit mode when Brain's BlendUpdateMode was FixedUpdate. Bugfix: Lens mode override was not working correctly in all cases. Collider2D inspector: added warning when collider is of the wrong type. [2.8.0-pre.1] - 2021-04-21 Switching targets (Follow, LookAt) is smooth by default. For the old behaviour, set PreviousStateIsValid to false after changing the targets. Bugfix: Reversing a blend in progress respects asymmetric blend times. Regression fix: CmPostProcessing and CmVolumeSettings components setting Depth of Field now works correctly with Framing Transposer. Regression fix: 3rdPersonFollow kept player in view when Z damping was high. Regression fix: Physical camera properties were overwritten by vcams when \"override mode: physical\" was not selected. New sample scene: Boss cam demonstrates how to setup a camera that follows the player and looks at the player and the boss. Boss cam also shows examples of custom extensions. Added simplified modes to Impulse Source. Added secondary reaction settings to Impulse Listener. Added Storyboard support for ScreenSpaceOverlay and ScreenSpaceCamera camera render modes. Added DampingIntoCollision and DampingFromCollision properties to Cinemachine3rdPersonFollow to control how gradually the camera moves to correct for occlusions. Added CinemachineCore.OnTargetObjectWarped() to warp all vcams targeting an object. Added ability for vcam to have a negative near clip plane. Added Draggable Game Window Guides toggle in Cinemachine preferences. When disabled, Game Window guides are only for visualization. Added button to virtual camera inspectors to auto-generate the CinemachineInputProvider component if it is missing. Default PostProcessing profile priority is now configurable and defaults to 1000. Cinemachine3rdPersonFollow now operates without the physics module and without collision resolution. Bugfix: 3rdPersonFollow collision resolution failed when the camera radius was large. Bugfix: 3rdPersonFollow damping occured in world space instead of camera space. Bugfix: 3rdPersonFollow stuttered when Z damping was high. Regression fix: CinemachineInputProvider stopped providing input. Bugfix: Lens aspect and sensorSize were updated when lens OverrideMode != None. Bugfix: Changing targets on a live vcam misbehaved. Bugfix: Framing transposer did not handle empty groups. Bugfix: Interrupting a transition with InheritPosition enabled did not work. Bugfix: Cinemachine3rdPersonFollow handled collisions by default, now it is disabled by default. Bugfix: SaveDuringPlay saved some components that did not have the SaveDuringPlay attribute. Regression fix: Entries in the custom blends editor in CM Brain inspector were not selectable. GameView guides are drawn only if appropriate inspector subsection is expanded. FreeLook rigs are now organized in tabs in the inspector. New sample scene: Boss cam sample scene demonstrates a camera setup to follow the player and to look at the player and the boss. The scene provides examples of custom extensions. New Sample scene: 2D zoom, showing how to zoom an orthographic camera with mouse scroll. New Sample scene: 2D fighters, showing how to add/remove targets gradually to/from a TargetGroup based on some conditions (here, it is the y coord of the players). Bugfix: CinemachineCollider's displacement damping was being calculated in world space instead of camera space. Bugfix: TrackedDolly sometimes introduced spurious rotations if Default Up and no Aim behaviour. Bugfix: 3rdPersonFollow's shoulder now changes smoothly with respect to world-up vector changes. [2.7.2] - 2021-02-15 CinemachineConfiner2D now handles cases where camera window is oversized New sample scene (FadeOutNearbyObjects) demonstrating fade out effect for objects between camera and target using shaders. The example includes a cinemachine extension giving convenient control over the shader parameters Bugfix (1293429) - Brain could choose vcam with not the highest priority in some cases Bugfix: SaveDuringPlay also works on prefab instances Bugfix (1272146) - Adding vcam to a prefab asset no longer causes errors in console Bugfix (1290171) - Impulse manager was not cleared at playmode start Nested Scrub Bubble sample removed (filenames too long), available now as embedded package Compilation guards for physics, animation, and imgui. Cinemachine does not hard depend on anything now Bugfix: CM StoryBoard had a 1 pixel border Bugfix: CM StoryBoard lost viewport reference after hot reload Bugfix: FramingTransposer's TargetMovementOnly damping caused a flick. Bugfix: FreeLook small drift when no user input if SimpleFollowWithWorldUp Bugfix: InheritPosition did not work with SimpleFollow binding mode Bugfix: cleanup straggling post processing profiles when no active vcams Bugfix: Checking whether the Input Action passed to CinemachineInputHandler is enabled before using it. Bugfix: 3rdPersonFollow FOV was blended incorrectly when ReferenceLookAt was set to a faraway target Bugfix: Position predictor not properly reset Bugfix: Create via menu doesn't create as child of selected object Bugfix: Post-processing profiles not cleaned up when no active vcams Bugfix: Install CinemachineExamples Asset Package menu item was failing on 2018.4 / macOS New sample scene (2DConfinerComplex) demonstrating new CinemachineConfiner2D extension. Updated CharacterMovement2D script in 2D sample scenes (2DConfinedTargetGroup, 2DConfiner, 2DConfinerUndersized, 2DTargetGroup) to make jumping responsive. Updated 2DConfinedTargetGroup and 2DConfiner scenes to use new CinemachineConfiner2D extension. [2.7.1] - 2020-11-14 New feature: CinemachineConfiner2D - Improved 2D confiner. Added ApplyAfter option to ImpulseListener, to add control over the ordering of extensions UI update - Moved Cinemachine menu to GameObject Create menu and Right Click context menu for Hierarchy. Virtual Camera Lens inspector supports display of Horizontal FOV Virtual Camera Lens can override orthographic and physical camera settings Bugfix (1060230) - lens inspector sometimes displayed ortho vs perspective incorrectly for a brief time Bugfix (1283984) - Error message when loading new scene with DontDestroyOnLoad bugfix (1284701) - Edge-case exception when vcam is deleted Storyboard Global Mute moved from Cinemachine menu to Cinemachine preferences. Bugfix - long-idle vcams when reawakened sometimes had a single frame with a huge deltaTime Bugfix - PostProcessing temporarily stopped being applied after exiting play mode [2.6.3] - 2020-09-16 Regression fix (1274989) - OnTargetObjectWarped broken for OrbitalTransposer Bugfix (1276391) - CM Brain Reset did not reset Custom Blends asset in inspector Bugfix (1276343) - CM Brain inspector custom blends misaligned dropdown arrow Bugfix (1256530) - disallow multiple components where appropriate Bugfix: BlendList camera was incorrectly holding 0-length camera cuts Bugfix (1174993) - CM Brain logo was not added to Hierarchy next to Main Camera after adding vcam for the first time after importing CM. Bugfix (1100131) - Confiner is aware of 2D collider's offset attribute. [2.6.2] - 2020-09-02 Bugfixes Regression fix: OnCameraCut Memory leak when using Cinemachine with PostProcessing package Bugfix (1272146): Checking for null pipeline, before drawing gizmos. Add support for disabling Physics module [2.6.1] - 2020-08-13 Bugfixes Regression Fix: PostProcessing/VolumeSettings FocusTracksTarget was not accounting for lookAt target offset Regression fix: Confiner no longer confines noise and impulse Bugfix: StateDrivenCamera was choosing parent state if only 1 clip in blendstate, even though there was a vcam assigned to that clip Bugfix: vertical group composition was not composing properly Bugfix: CinemachineNewVirtualCamera.AddComponent() now works properly Bugfix: removed compile errors when Physics2D module is disabled Bugfix: brain updates on scene loaded or unloaded Bugfix (1252431): Fixed unnecessary GC Memory allocation every frame when using timeline Bugfix (1260385): check for prefab instances correctly Bugfix (1266191) Clicking on foldout labels in preferences panel toggles their expanded state Bugfix (1266196) Composer target Size label in preferences panel was too big Bugfix: Scrubbing Cache was locking virtual camera transforms beyond the cache range Improved performance of path gizmo drawing Timeline Scrubbing Cache supports nested timelines, with some known limitations to be addressed with a future Timeline package release Added support for deterministic noise in the context of controlled rendering (via CinemachineCore.CurrentTimeOverride) Added Target Offset field to Framing Transposer Added Multi-object edit capabilities to virtual cameras and extensions Added inspector button to clear the Scrubbing Cache [2.6.0] - 2020-06-04 New Features and Bugfixes Added AxisState.IInputProvider API to better support custom input systems Added CinemachineInpiutProvider behaviour to support Unity's new input system Added Timeline Scrubbing cache: when enabled, simulates damping and noise when scrubbing in timeline Added ManualUpdate mode to the Brain, to allow for custom game loop logic VolumeSettings/PostProcessing: added ability to choose custom target for focus tracking Added CinemachineRecomposer for timeline-tweaking of procedural or recorded vcam Aim output Added GroupWeightManipulator for animating group member weights Impulse: Added PropagationSpeed, to allow the impulse to travel outward in a wave Impulse: added support for continuous impulses Added CinemachineIndependentImpulseListener, to give ImpulseListener ability to any game object Added 3rdPersonFollow and 3rdPersonAim for dead-accurate 3rd-person aiming camera Added ForceCameraPosition API of virtual cameras, to manually initialize a camera's position and rotation Added example scenes: Aiming Rig and Dual Target to show different 3rd person cmera styles FramingTransposer does its work after Aim, so it plays better with Aim components. Framing Transposer: add Damped Rotations option. If unchecked, changes to the vcam's rotation will bypass Damping, and only target motion will be damped. Refactored Lookahead - better stability. New behaviour may require some parameter adjustment in existing content Composer and Framing Transposer: improved handling at edge of hard zone (no juddering) Orbital Transposer / FreeLook: improved damping when target is moving CustomBlends editor UX improvements: allow direct editing of vcam names, as well as dropdown Add Convert to TargetGroup option on LookAt and Follow target fields Confiner: improved stability when ConfineScreenEdges is selected and confing shape is too small Extensions now have PrePipelineMutateState callback CinemachineCore.UniformDeltaTimeOverride works in Edit mode Added TargetAttachment property to vcams. Normally 1, this can be used to relax attention to targets - effectively a damping override Bugfix: Blend Update Method handling was incorrect and caused judder in some circumstances Bugfix: VolumeSettings blending was popping when weight was epsilon if volume altered a non-lerpable value Bugfix (1234813) - Check for deleted freelooks Bugfix (1219867) - vcam popping on disable if blending Bugfix (1214301, 1213836) - disallow structural change when editing vcam prefabs Bugfix (1213471, 1213434): add null check in editor Bugfix (1213488): no solo for prefab vcams Bugfix (1213819): repaintGameView on editor change Bugfix (1217306): target group position drifting when empty or when members are descendants of the group Bugfix (1218695): Fully qualify UnityEditor.Menu to avoid compile errors in some circumstances Bugfix (1222740): Binding Modes, that don't have control over axis value range, are not affected by it. Bugfix (1227606): Timeline preview and playmode not the same for composer with hand-animated rotations Bugfix: Confiner's cache is reset, when bounding shape/volume is changed. Bugfix (1232146): Vcam no longer jerks at edge of confiner bound box. Bugfix (1234966): CompositeCollider scale was applied twice. [2.5.0] - 2020-01-15 Support HDRP 7 and URP simultaneously Accommodate simultaneous precesnce of HDRP and URP Regression fix: Axis was always recentered in Edit mode, even if recentering is off [2.4.0] - 2020-01-10 HDRP 7 support and bugfixes Storyboard: added global mute function New vcams are by default created matching the scene view camera Added ApplyBeforeBody option to POV component, to support working with FramingTransposer Added RectenterTarget to POV component Added OnTransitionFromCamera callback to extensions Added Damping to SameAsFollowTarget and HardLockToTarget components URP 7.1.3: added CinemachinePixelPerfect extension Added Speed Mode to AxisState, to support direct axis control without max speed New example scene: OverTheShoulderAim illustrating how to do over-the-shoulder TPS cam, with Normal and Aim modes Impulse Manager: added option to ignore timescale Framing Transposer: added OnTransition handling for camera rotation if InheritPosition Upgrade to support HDRP and Universal RP 7.0.0 API Upgrade to support HDRP and Universal RP 7.1.0 API Removed Resources diretories Sample scenes now available via package manager Added optional \"Display Name\" field to Cinemachine Shot in Timeline Added \"Adopt Current Camera Settings\" item to vcam inspector context menu Composer and FramingTransposer: allow the dead zone to extend to 2, and the Screen x,Y can range from -0.5 to 1.5 HDRP: lens presets include physical settings if physical camera Regression Fix: Framing Transposer: ignore LookAt target. Use Follow exclusively Bugfix: Framing Transposer was not handling dynamic changes to FOV properly Bugfix: PostProcessing extension was not handling standby update correctly when on Manager Vcams Bugfix: PostProcessing extension was leaking a smallamounts of memory when scenes were unloaded Bugfixes: (fogbugz 1193311, 1193307, 1192423, 1192414): disallow presets for vcams Bugfix: In some heading modes, FreeLook was improperly modifying the axes when activated Bugfix: Orbital transposer was improperly filtering the heading in TargetForward heading mode Bugfix: added EmbeddedAssetHelper null check Bugfix: composer screen guides drawn in correct place for physical camera Bugfix: FreeLook was not respecting wait time for X axis recentering Bugfix: FreeLook X axis was not always perfectly synched between rigs Bugfix (fogbugz 1176866): Collider: clean up static RigidBody on exit Bugfix (fogbugz 1174180): framing transposer wrong ortho size calculation Bugfix (fogbugz 1158509): Split brain.UpdateMethod into VcamUpdateMethod and BrainUpdateMethod, to make blending work correctly Bugfix (fogbugz 1162074): Framing transposer and group transposer only reached half maximum ortho size Bugfix (fogbugz 1165599): Transposer: fix gimbal lock issue in LockToTargetWithWorldUp Bugfix: VolumeSettings: handle layermask in HDAdditionalCameraData Bugfix: use vcam's up when drawing gizmos (orbital transposer and free look) [2.3.4] - 2019-05-22 PostProcessing V3 and bugfixes Added support for PostProcessing V3 - now called CinemachineVolumeSttings Added CinemachineCore.GetBlendOverride delegate to allow applications to override any vcam blend when it happens When a blend is cancelled by the opposite blend, reduce the blend time Orthographic cameras allow a Near Clip of 0 Timeline won't auto-create CM brains when something dragged onto it Confiner: Improvement in automatic path invalidation when number of path points path changes Added CinemachineInpuitAxisDriver utility for overriding the default AxisState behaviour CinemachineCameraOffset: added customizable stage for when to apply the offset Added Loop option to BlendList Camera Improved Lookahead: does not automatically recenter Brain no longer applies time scaling to fixed delta Added dependency on Unity.ugui (2019.2 and up) Bugfix: potential endless loop when using Ignore tag in Collider Bugfix: Allow externally-driven FeeLook XAxis to work properly with SimpleFollow Bugfix: vcams with noise would sometimes show one noiseless frame when they were activated and standby update was not Always Bugfix: Generate a cut event if cutting to a blend-in-progess (fogbugz 1150847) Bugfix: reset lens shift if not physical camera Bugfix: Collider must consider actual target position, not lookahead position Bugfix: FreeLook heading RecenterNow was not working Bugfix: lookahead now takes the overridden Up into account Bugfix: screen composer guides drawn in wrong place for picture-in-picture Bugfix: FreeLook now draws only 1 active composer guide at a time (fogbugz 1138263) Bugfix: cameras sometimes snapped when interrupting blends Bugfix: Path handles no longer scale with the path object Bugfix: Framing Transposer Center on Activate was not working properly (fogbugz 1129824) Bugfix: FreeLook inherit position Bugfix: collider was pushing camera too far if there were multiple overlapping obstacles Bugfix: use IsAssignableFrom instead of IsSubclass in several places Bugfix: when interrupting a blend in progress, Cut was not respected Bugfix: collider minimum occlusion time and smoothing time interaction Bugfix: TargetGroup.RemoveMember error (fogbugz 1119028) Bugfix: TargetGroup member lerping jerk when member weight near 0 Bugfix: Transposer angular damping should be 0 only if binding mode not LockToTarget [2.3.3] - 2019-01-08 Temporary patch to get around a Unity bug in conditional dependencies Removed Cinemachine.Timeline namespace, as a workaround for fogbugz 1115321 [2.3.1] - 2019-01-07 Bugfixes Added timeline dependency OnTargetObjectWarped no longer generates garbage [2.3.0] - 2018-12-20 Support for Unity 2019.1 Added dependency on new unity.timeline Added conditional dependence on PostProcessingV2 No copying CM gizmo into assets folder FreeLook: if inherit position from similar FreeLooks, bypass damping Timeline: improve handling when vcam values are tweaked inside shot inspector (fogbugz 1109024) [2.2.8] - 2018-12-10 Bugfixes, optimizations, and some experimental stuff Transposer: added Angular Damping Mode, to support quaternion calculations in gimbal-lock situations Framing Transposer and Group Transposer: group composing bugfixes, respect min/max limits Added ConemachineCameraOffset extension, to offset the camera a fixed distance at the end of the pipeline Dolly Cart: added support for LateUpdate State-driven-camera: added [NoSaveDuringPlay] to Animated Target and Layer Index Added AxisState.Recentering.RecenterNow() API call to skip wait time and start recentering now (if enabled) Added NoLens blend hint, to leave camera Lens settings alone Updated documentation (corrections, and relocation to prevent importing) Upgrade: added support for nested prefabs in Unity 2018.3 (fogbugz 1077395) Optimization: position predictor is more efficient Optimization: Composer caches some calculations Optimization: Fix editor slowdown when Lens Presets asset is missing Experimental: Optional new damping algorithm: attempt to reduce sensitivity to variable framerate Experimental: Optional new extra-efficient versions of vcam and FreeLook (not back-compatible) Timeline: play/pause doesn't kick out the timeline vcam Path editor: make sure game view gets updated when a path waypoint is dragged in the scene view Composer guides are shown even if Camera is attached to a renderTexture Bugfix: allow impulse definition to be a non-public field (property drawer was complaining) Bugfix: added null check for when there is no active virtual camera Bugfix: CollisionImpulseSource typo in detection of 2D collider Bugfix: PasteComponentValues to prefab vcams and FreeLooks were corrupting scene and prefabs Bugfix: Timeline mixer was glitching for single frames at the end of blends Bugfix: Added OnTransitionFromCamera() to POV and OrbitalTransposer, to transition axes intelligently Regression fix: if no active vcam, don't set the Camera's transform [2.2.7] - 2018-07-24 Mostly bugfixes Bugfix: fogbugz case 1053595: Cinemachine Collider leaves hidden collider at origin that interferes with scene objects Bugfix: fogbugz case 1063754: empty target group produces console messages Bugfix: FreeLook Paste Component Values now pastes the CM subcomponents as well Bugfix: added extra null checks to support cases where current vcam is dynamically deleted Bugfix: reset BlendList when enabled Regression fix: FreeLook axis values get transferred when similar vcams transition Bugfix: cutting to BlendList vcam sometimes produced a few bad frames Bugfix: smart update tracks the targets more efficiently and correctly, and supports RigidBody interpolation (2018.2 and up) Enhancement: POV component interprets POV as relative to parent transform if there is one API change: OnCameraLive and CameraActivated events take outgoing vcam also as parameter (may be null) [2.2.0] - 2018-06-18 Impulse Module and More New Cinemachine Impulse module for event-driven camera shakes New Event Helper script CinemachineTriggerAction takes action on Collider and Collider2D enter/exit events, and exposes them as UnityEvents New performance-tuning feature: Standby Update. Controls how often to update the vcam when it's in Standby. New NoiseSettings editor with signal preview Added Focal Length or Named FOV presets for Camera Lens Added support for Physical Camera: focal length and Lens Offset New improved Group framing algorithm: tighter group framing in GroupComposer and FramingTransposer Collider: now returns TargetIsObscured if the target is offscreen (great for cameras with fixed aim) Collider: added Minimum Occlusion Time setting, to ignore fleeting obstructions Collider: added Transparent Layers mask, to specify solid objects that don't obstruct view Collider: damping will no longer take the camera through obstacles Collider: Added separate damping setting for when target is being occluded vs when camera is being returned to its normal position Collider: added Smoothing setting, to reduce camera jumpiness in environements with lots of obstacles NoiseSettings: added checkbox for pure sine-wave instead of Perlin wave If no LookAt target, PostProcessing FocusTracksTarget offset is relative to camera TrackedDolly: Default up mode sets Up to World Up Virtual Camera: New Transitions section in inspector that gives more control over blending: Blend Hint provides some control over how the position and rotation are interpolated Inherit Position checkbox to ensure smooth positional handoff from outgoing camera OnCameraLive event gets fired when the camera activates. Useful for custom handlers. Added ScreenSpaceAimWhenTargetsDiffer as a vcam blend hint. This influences what happens when blending between vcams with different LookAt targets Increased stability of vcams with very small FOVs Framing Transposer no longer requires LookAt to be null LensSettings Aspect, Orthographic, IsPhysicalCamera, SensorSize properties no longer internal Noise Profiles: don't magically create assets. Prompt user for filename and location of new or cloned profiles Refactored interaction between timeline and CM brain, to improve handling of edge cases (fogbugz case 1048497) Bugfix: StateDrivenCamera Editor was not finding states if target was OverrideController Bugfix when dragging orbital transposer transform: take bias into account Bugfix: SaveDuringPlay was not handling asset fields correctly - was sometimes crushing assets Bugfix: SimpleFollow transposers were not initilizing their position correctly at game start Bugfix: Timeline with CM shot was causing jitter in some FixedUpdate situations Bugfix: Multiple brains with heterogeneous update methods were not behaving correctly. CM will now support this, but you must make sure that the brains have different layer masks. Example scenes now include use of CinemachineTriggerAction script. [2.1.13] - 2018-05-09 Removed dependency on nonexistant Timeline package, minor bugfixes Bugfix: Custom Blends \"Any to Any\" was not working (regression) Bugfix: Composer was sometimes getting wrong aspect if multiple brains with different aspect ratios Bugfix: could not drag vcam transforms if multiple inspectors and one is hidden Bugfix: Framing Transposer initializes in the wrong place - noticeable if dead zone [2.1.12] - 2018-02-26 Storyboard, Bugfixes and other enhancements. Also some restructuring for Package Manager Project restructure: Removed Base, Timeline, and PostFX folders from project root. PostProcessing code must now be manually imported from Cinemachine menu. No more dependencies on scripting defines. New Storyboard extension, to display images over the vcams. Comes with a Waveform monitor window for color grading New option to specify vcam position blend style: linear, spherical, or cylindrical, based on LookAt target Added API to support seamless position warping of target objects: OnTargetObjectWarped(). Added support for custom blend curves Lookahead: added Ignore Y Axis Movement option Added support for cascading blends (i.e. blending from mid-blend looks better) POV/Orbital/FreeLook axis: exposed Min, Max, and Wrap in the UI, for customized axis range FreeLook: added Y Axis recentering POV: Added recentering feature to both axes Path: Added Normalized Path units option: 0 is start of path, 1 is end. Path: added length display in inspector Timeline Clip Editor: vcam sections are now collapsible API enhancement: added Finalize to Pipeline stages, called even for manager-style vcams Bugfix: PostProcessing V2 DoF blending works better Bugfix: OrbitalTransposer works better with WorldUp overrides Bugfix: Remove StateDrivenCamera \"not playing a controller\" warning Bugfix: Handle exceptions thrown by assemblies that don't want to be introspected Bugfix: vcams following physics objects incorrectly snapped to origin after exiting play mode Bugfix: predictor now supports time pause Bugfix: Moved StartCoroutine in Brain to OnEnable() Bugfix: Collider was causing problems in Physics on Android platforms Bugfix: dragging a vcam's position updtaes prefabs properly Bugfix: All extension now respect the \"enabled\" checkbox Bugfix: Undo for Extasion add will no longer generate null references [2.1.10] - 2017-11-28 This is the first UPM release of Unity Package Cinemachine. New Aim component: Same As Follow Target simply uses the same orientation as the Follow target Perlin Noise component: added inspector UI to clone or locate existing Noise profiles, and to create new ones Noise Presets were moved outside of the Examples folder Example Assets are now included as embedded package, not imported by default Bugfix: FreeLook with PositionDelta was not properly updating the heading Bugfix: Transitioning between FreeLooks simetimes caused a short camera freeze Bugfix: Added some null checks to FreeLook, to prevent error messages at build time [2.1.9] - 2017-11-17 Initial version. Version 2.1.9 cloned from private development repository, corresponding to package released on the asset store"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine2D.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine2D.html",
    "title": "2D graphics | FSM Unity Framework",
    "keywords": "2D graphics Cinemachine supports orthographic cameras. When you set the Unity camera's projection to Orthographic, Cinemachine adjusts to accommodate it. In Virtual Camera properties for Lens, FOV is replaced by Orthographic Size. Note that settings related to FOV and certain FOV-oriented behaviors such as Follow Zoom have no effect if the camera is orthographic. In orthographic environments, it doesn’t usually make sense to rotate the camera. Accordingly, Cinemachine offers the Framing Transposer to handle framing and composition without rotating the camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine3rdPersonAim.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine3rdPersonAim.html",
    "title": "Cinemachine 3rd Person Aim Extension | FSM Unity Framework",
    "keywords": "Cinemachine 3rd Person Aim Extension This extension is created to be a part of a 3rd-person camera rig. To preserve aiming accuracy, this extension deliberately cancels out all rotational noise, and forces a hard look at the target point. However, it's still possible to use camera noise with this extension, provided that the noise affects the camera position, instead of the rotation. See the AimingRig sample scene for an example of this. Additionally, if Aim Target Reticle is non-null, this extension will project a ray from the Follow target's position and find the first object that collides with that ray. The Aim Target Reticle object will then be placed on that point in the Game View, to indicate what the player would hit if a shot were to be fired. This point may be different from what the camera is looking at, if the found object is close enough to be affected by parallax as a result of the offset between player and camera. Properties: Property: Function: Aim Collision Filter Objects on these layers will be detected. Ignore Tag Objects with this tag are ignored. It's a good idea to set this field to the target's tag. Aim Distance How far to project the object detection ray. Aim Target Reticle This 2D object will be positioned in the Game View over the raycast hit point, if any, or will remain in the center of the screen if no hit point is detected. May be null, in which case no on-screen indicator will appear."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine3rdPersonFollow.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/Cinemachine3rdPersonFollow.html",
    "title": "3rd Person Follow | FSM Unity Framework",
    "keywords": "3rd Person Follow Use Cinemachine Virtual Camera’s 3rd Person Follow to keep the camera at a constant position and distance relative to a Follow target (subject to damping controls), tracking the target’s movement and rotation. The 3rd Person Follow’s mini-rig setup defines the camera position and distance relative to the target. With a suitable shoulder offset, this mini-rig can produce a 3rd-person camera, where the character is offset in the frame and the camera looks over the character’s shoulder. With different settings, it can produce a first-person camera. For example, a rig set up with these values: Property: Value: Damping X= 0.1 Y= 0.5 Z= 0.3 Shoulder Offset X= 0.4 Y= 1 Z= -0.5 Vertical Arm Length 0.27 Camera Side 1 Camera Distance 2 Camera Collision Filter Default Camera Radius 0.2 Produces this virtual rig in the Scene view: Which results in this Game view: The rig and the camera position are defined by three pivot points (the origin, the shoulder, and the hand) as well as by a camera that is positioned behind the hand. The origin (A): The origin is the Follow target's position. When the target pivots horizontally, the rig rotates with it around this point. The shoulder (B): By default, it is offset to one side, to create an over-the-shoulder follow position. Vertical rotations of the Follow target are transferred here, so the rig rotates horizontally about the origin, and vertically about the shoulder. The hand (C): Vertically offset in relation to the shoulder. The arm length affects the follow target's screen position when the camera rotates vertically. By default, it is offset from the shoulder, so that vertical rotations will keep the character nicely positioned on the screen. For 1st-person cameras, this can be set to 0. The camera (D): The camera's rotation will always be parallel to the Follow target's rotation, but positioned at Camera Distance behind the hand. The camera always looks directly at the hand. Note the rotations on the rig; B rotates horizontally around A. Using A as the origin, B's position is calculated from the Shoulder Offset's X, Y, and Z values . C rotates vertically around B. C's position is calculated from the Vertical Arm Length from B. Positive values result with C above B, negative values result with C below B. Controlling the Camera There is no direct input control for the camera. You must have a controller script that moves and rotates the Follow target; the camera will position and orient itself relative to that. When the Follow target is the character itself, the camera’s rotation always matches the character’s rotation. When the Follow target is an invisible GameObject that can rotate independently of the character, the camera will then be able to rotate around the character. For an example, see the AimingRig sample scene. Built-in Collision Resolution The 3rd-person Follow component has a built-in collision resolution system, so when the target moves close to an obstacle, the camera position is adjusted so that it will never be inside an obstacle; the built-in collision resolution means the camera always keeps the target in sight, despite intervening obstacles. When the target moves too close to an obstacle, the rig bends and stretches to keep the camera outside the obstacle but always with the target in view. Shaky Movement, Steady Aim When combined with the Cinemachine3rdPersonAim extension, the result is a powerful rig that can maintain steady aim for a shooter-type game, even when the camera movement is shaky or noisy. Cinemachine3rdPersonAim re-adjusts the camera orientation to maintain a fixed point at the center of the screen, correcting for variations due to hand-held camera noise or shaking target motion. Properties: Property: Function: Damping The responsiveness of the camera in tracking the target. Each axis can have its own setting. The value is the approximate time it takes the camera to catch up to the target's new position. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Shoulder Offset Position of the shoulder pivot relative to the follow target origin. This offset is in target-local space. Vertical Arm Length Vertical offset of the hand in relation to the shoulder. Arm length affects the follow target's screen position when the camera rotates vertically. Camera Side Specifies which shoulder the camera is on (left, right, or somewhere in-between). Camera Distance Specifies the distance from the hand to the camera. Camera Collision Filter Specifies which layers will be included or excluded from collision resolution. Ignore Tag Obstacles with this tag will be ignored by collision resolution. It is recommended to set this field to the target's tag. Camera Radius Specifies how close the camera can get to collidable obstacles without adjusting its position. Damping Into Collision Specifies how gradually the camera moves to correct for an occlusion. Higher numbers move the camera more gradually. Damping From Collision Specifies how gradually the camera returns to its normal position after having been corrected by the built-in collision resolution system. Higher numbers move the camera more gradually back to normal."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimComposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimComposer.html",
    "title": "Composer | FSM Unity Framework",
    "keywords": "Composer This Virtual Camera Aim algorithm rotates the camera to face the Look At target. It also applies offsets, damping, and composition rules. Examples of targets for aiming: the upper spine or head bone of a character, vehicles, or dummy objects which are controlled or animated programmatically. Properties: Property: Function: Center On Activate Forces the camera to the center of the screen when the camera becomes live. Tracked Object Offset Offset from the center of the Look At target, in target-local space. Fine-tune the tracking target position when the desired area is not the tracked object’s center. You can also use Scene Handles to modify this property. Lookahead Time Adjust the offset based on the motion of the Look At target. The algorithm estimates the point that the target will be this many seconds into the future. This feature is sensitive to noisy animation. It can amplify the noise, resulting in undesirable camera jitter. If the camera jitters unacceptably when the target is in motion, turn down this property or animate the target more smoothly. Lookahead Smoothing Controls the smoothness of the lookahead algorithm. Larger values smooth out jittery predictions and increase prediction lag. Lookahead Ignore Y Toggle to ignore movement along the Y axis for lookahead calculations. Horizontal Damping How responsively the camera follows the target in the screen-horizontal direction. Use small numbers for more responsive, rapid rotation of the camera to keep the target in the dead zone. Use larger numbers for a more heavy, slowly-responding camera. Vertical Damping How responsively the camera follows the target in the screen-vertical direction. Use different vertical and horizontal settings to give a wide range of camera behaviors. Screen X Horizontal screen position for the center of the dead zone. The camera rotates so that the target appears here. Screen Y Vertical screen position for target. The camera rotates so that the target appears here. Dead Zone Width The width of the screen region within which the camera ignores any movement of the target. If the target is positioned anywhere within this region, the Virtual Camera doesn't update its rotation. This is useful for ignoring minor target movement. Dead Zone Height The height of the screen region within which the camera ignores any movement of the target. If the target is positioned anywhere within this region, the Virtual Camera doesn't update its rotation. This is useful for ignoring minor target movement. Soft Zone Width The width of the soft zone. If the target appears in this region of the screen, the camera will rotate to push it back out to the dead zone, in the time specified by the Horizontal Damping setting. Soft Zone Height The height of the soft zone. If the target appears in this region of the screen, the camera will rotate to push it back out to the dead zone, in the time specified by the Vertical Damping setting. Bias X Positions the soft zone horizontally, relative to the dead zone. Bias Y Positions the soft zone vertically, relative to the dead zone."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimDoNothing.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimDoNothing.html",
    "title": "Do Nothing | FSM Unity Framework",
    "keywords": "Do Nothing This Virtual Camera Aim algorithm does not aim the Virtual Camera. Choose this algorithm for static shots or for animating the rotation directly with custom scripts."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimGroupComposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimGroupComposer.html",
    "title": "Group Composer | FSM Unity Framework",
    "keywords": "Group Composer This Virtual Camera Aim algorithm aims the camera at multiple GameObjects. Otherwise, it behaves identically to the Composer and has the same settings. If the Look At target is a Cinemachine Target Group, the algorithm adjusts the FOV and the camera distance to ensure that the group of targets is framed properly."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimHardLook.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimHardLook.html",
    "title": "Hard Look At | FSM Unity Framework",
    "keywords": "Hard Look At This Virtual Camera Aim algorithm rotates the Virtual Camera to keep the Look At target in the center of the camera frame."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimPOV.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimPOV.html",
    "title": "POV | FSM Unity Framework",
    "keywords": "POV This Virtual Camera Aim algorithm aims the camera in response to the user’s input. Properties: Property: Function: Vertical Axis Controls the vertical orientation of the Virtual Camera’s aim. Value The current value of the axis to aim the camera at, in degrees. Accepted values are -90 to 90. Value Range The minimum and maximum values for the vertical axis of the Virtual Camera. Wrap If checked, the axis wraps around the Value Range values, forming a loop. Max Speed The maximum speed of this axis in degrees/second, or the multipler for the input value if Speed Mode is set to InputValueGain. Speed Mode How the axis responds to input. MaxSpeed (the default) clamps the maximum speed at which the axis can change, regardless of the input. Input Value Gain multiplies the input value by MaxSpeed. Accel Time The amount of time in seconds it takes to accelerate to Max Speed with the supplied axis at its maximum value. Decel Time The amount of time in seconds it takes to decelerate the axis to zero if the supplied axis is in a neutral position. Input Axis Name The name of this axis as specified in Unity Input Manager. To disable the automatic updating of this axis, set this property to an empty string. Input Axis Value The value of the input axis. A value of 0 means no input. You can drive this directly from a custom input system, or you can set the Input Axis Name and have the value driven by the Unity Input Manager. Invert Check to invert the raw value of the input axis before it is used. Vertical Recentering Controls automatic vertical recentering when the player gives no input. Enabled Check to enable automatic vertical recentering. Wait Time If no user input has been detected on the vertical axis, the camera waits this long in seconds before recentering. Recentering Time Maximum angular speed of recentering. Accelerates into and decelerates out of this. Horizontal Axis Controls the horizontal orientation. Value The current value of the axis, in degrees. Accepted values are -180 to 180. Value Range The minimum and maximum values for the axis. Wrap If checked, the axis wraps around the Value Range values, forming a loop. Max Speed The maximum speed of this axis in degrees/second, or the multipler for the input value if Speed Mode is set to InputValueGain. Speed Mode How the axis responds to input. MaxSpeed (the default) clamps the maximum speed at which the axis can change, regardless of the input. Input Value Gain multiplies the input value by MaxSpeed. Accel Time The amount of time in seconds it takes to accelerate to Max Speed with the supplied Axis at its maximum value. Decel Time The amount of time in seconds it takes to decelerate the axis to zero if the supplied axis is in a neutral position. Input Axis Name The name of this axis as specified in the Unity Input Manager. Set this property to an empty string to disable automatic update of this axis. Input Axis Value The value of the input axis. A value of 0 means no input. You can drive this directly from a custom input system, or you can set the Input Axis Name and have the value driven by the Unity Input Manager. Invert Check to invert the raw value of the input axis before it is used. Horizontal Recentering Controls automatic vertical recentering when the player gives no input. Enabled Check to enable automatic vertical recentering. Wait Time If no user input has been detected on the vertical axis, the camera waits this long in seconds before recentering. Recentering Time Maximum angular speed of recentering. Accelerates into and decelerates out of this."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimSameAsFollow.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAimSameAsFollow.html",
    "title": "Same As Follow Target | FSM Unity Framework",
    "keywords": "Same As Follow Target This Virtual Camera Aim algorithm matches the orientation of the Follow target. When used with the Hard Lock to Target algorithm in the Body properties, this algorithm makes the Virtual Camera match the path and rotation of a control GameObject."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAlternativeInput.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineAlternativeInput.html",
    "title": "Alternative Input Systems | FSM Unity Framework",
    "keywords": "Alternative Input Systems Some Cinemachine components (e.g. FreeLook, POV, OrbitalTransposer) require user input to position or orient the camera. By default, Cinemachine gets user input by querying the standard Unity Input.GetAxis(name). When alternative input systems are used in a project, this default behaviour must be overridden so that input is obtained from the appropriate source. Cinemachine has defined an interface: Cinemachine.AxisState.IInputAxisProvider. If a MonoBehaviour implementing this interface is added to a Virtual Camera or FreeLook, then it will be queried for input instead of the standard input system. Cinemachine ships with an example of such a behaviour that uses the new UnityEngine.Input package: CinemachineInputProvider. It has dual purpose: Provide a simple out-of-the-box adapter for the new UnityEngine.Input package, to cover common usecases To serve as a source code example of how to integrate Cinemachine with custom or 3rd-party input systems CinemachineInputProvider This is a behaviour that is intended to be added to a virtual camera or FreeLook. It is an input source override, which causes the virtual camera to obtain input using the new UnityEngine.Input package instead of the standard Unity Input system. This behaviour is available when the UnityEngine.Input package is installed in the project. Properties: Property: Function: Player Index Which player's input controls to query. Leave this at the default value of -1 for single-player games. Otherwise this should be the index of the player in the UnityEngine.Inout.InputUser.all list XY Axis A Vector2 input action that will supply values for the X and Y axes. Can be null if X and Y axes are not used Z Axis A float input action that will supply values for the Z axis. Can be null if Z axis is not used"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBindingModes.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBindingModes.html",
    "title": "Binding Modes | FSM Unity Framework",
    "keywords": "Binding Modes The binding mode defines the coordinate space Unity uses to interpret the camera offset from the target and the damping. Lock To Target Makes the virtual camera use the local frame of the Follow target. When the target rotates, the camera rotates with it to maintain the offset and to maintain the same view of the target. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees Lock To Target No Roll Makes the virtual camera use the local frame of the Follow target, with roll set to 0. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees Lock To Target On Assign Makes the orientation of the virtual camera match the local frame of the Follow target, at the moment when the virtual camera is activated or when the target is assigned. This offset remains constant in world space. The camera does not rotate along with the target. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees Lock To Target With World Up Makes the virtual camera use the local frame of the Follow target with tilt and roll set to 0. This binding mode ignores all target rotations except yaw. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees World Space The offset is interpreted in world space relative to the origin of the Follow target. The camera will not change position when the target rotates. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees Simple Follow With World Up Simple follow with world up interprets the offset and damping values in camera-local space. This mode emulates the action a human camera operator would take when instructed to follow a target. The camera attempts to move as little as possible to maintain the same distance from the target; the direction of the camera with regard to the target does not matter. Regardless of the orientation of the target, the camera tries to preserve the same distance and height from it. Start Pitch, 45 degrees Yaw, 45 degrees Roll, 45 degrees"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBlendListCamera.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBlendListCamera.html",
    "title": "Cinemachine Blend List Camera | FSM Unity Framework",
    "keywords": "Cinemachine Blend List Camera The Cinemachine Blend List Camera component executes a sequence of blends or cuts among its child Virtual Cameras. When the Blend List camera is activated, it executes its list of instructions, activating the first child Virtual Camera in the list, holding for a designated time, then cutting or blending to the next child, and so on. The Blend List camera holds the last Virtual Camera until Cinemachine Brain or Timeline deactivates the Blend List camera. Tip: Use a Blend List Camera instead of Timeline for simpler, automatic sequences. Properties: Property: Function: Solo Toggles whether or not the Blend List camera is temporarily live. Use this property to get immediate visual feedback in the Game view to adjust the Virtual Camera. Game Window Guides Toggles the visibility of compositional guides in the Game view. This property applies to all Virtual Cameras. Save During Play Check to apply the changes while in Play mode. Use this feature to fine-tune a Virtual Camera without having to remember which properties to copy and paste. This property applies to all Virtual Cameras. Priority The importance of this Blend List camera for choosing the next shot. A higher value indicates a higher priority. Cinemachine Brain chooses the next live Virtual Camera from all Virtual Cameras that are activated and have the same or higher priority as the current live Virtual Camera. This property has no effect when using a Virtual Camera with Timeline. Standby Update Controls how often the virtual camera is updated when the virtual camera is not live. Loop When enabled, the child virtual cameras will cycle indefintely instead of stopping on the last virtual camera in the list. Look At The default target GameObject that the children Virtual Camera move with. The Blend List camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Follow The default target GameObject to aim the Unity camera at. The Blend List camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Show Debug Text Check to display a textual summary of the live Virtual Camera and blend in the view. Enable All Child Cameras Check to activate all child cameras. This is useful if animating them in Timeline, but consumes extra resources. Instructions The set of instructions for enabling child cameras."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBlending.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBlending.html",
    "title": "Blending between Virtual Cameras | FSM Unity Framework",
    "keywords": "Blending between Virtual Cameras Use blending properties to specify how the Cinemachine Brain component performs a blend between virtual cameras. A Cinemachine blend is not a fade, wipe, or dissolve. Rather, Cinemachine Brain performs a smooth animation of the position, rotation, and other settings of the Unity camera from one Virtual Camera to the next. For blends between specific Virtual Cameras, use the Custom Blends list in the Cinemachine Brain component. Use the Default Blend property in Cinemachine Brain to specify blends between Virtual Cameras that do not have custom blends. The From and To settings are name-based, not references. This means that Cinemachine finds cameras by matching their names to the settings. They are not linked to specific GameObjects. The built-in dropdowns can be used to select a virtual camera from the current scene, or the name can be typed directly into the text boxes. If a name does not match any virtual camera in the current scene, the field will be highlighted in yellow. Use the reserved name **ANY CAMERA** to blend from or to any Virtual Camera. When Cinemachine begins a transition from one virtual camera to another, it will look in this asset for an entry that matches the upcoming transition, and apply that blend definition. If none is found, then the CinemachineBrain's DefaultBlend setting will apply. If multiple entries in the Custom Blends asset match the upcoming transition, Cinemachine will choose the one with the strongest specificity. For example, if blending from vcam1 to vcam2, and the custom blends asset contains an entry for vcam1-to-AnyCamera, and another entry for vcam1-to-vcam2, then the vcam1-to-vcam2 entry will apply. If multiple entries in the Custom Blends asset match the upcoming transition with equally-strong specificity, then the first one found will apply. Properties: Property: Function: From The name of the Virtual Camera to blend from. Use the name **ANY CAMERA** to blend from any Virtual Camera. This property is available only for custom blends. To The name of the Virtual Camera to blend to. Use the name **ANY CAMERA** to blend to any Virtual Camera. This property is available only for custom blends. Style Default Blend Shape of the blend curve. Cut Zero-length blend. Ease In Out S-shaped curve, giving a gentle and smooth transition. Ease In Linear out of the outgoing Virtual Camera, and ease into the incoming Virtual Camera. Ease Out Ease out of the outgoing Virtual Camera, and blend linearly into the incoming Virtual Camera. Hard In Ease out of the outgoing Virtual Camera, and accelerate into the incoming Virtual Camera. Hard Out Accelerate out of the outgoing Virtual Camera, and ease into the incoming Virtual Camera. Linear Linear blend. mechanical-looking. Custom Custom blend curve. Allows you to draw a custom blend curve. Time Duration (in seconds) of the blend."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBody3rdPersonFollow.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBody3rdPersonFollow.html",
    "title": "3rd Person Follow | FSM Unity Framework",
    "keywords": "3rd Person Follow This Virtual Camera Body algorithm is intended for use to implement a 3rd-person or 1st person camera. The algorithm places the camera on a mini-rig with 3 pivot points: Pivot point 1 is the origin, which is the Follow target's position. When the target rotates horizontally, the rig rotates with it around this point. Pivot point 2 is the shoulder, and by default is offset to one side of Pivot Point 1, to create an over-the-shoulder follow position. To make a 1st-person camera, set this offset to 0, or to whatever will give an appropriate 1st-person effect given your Follow target's position. Vertical rotations of the Follow target are transferred here, so the rig rotates horizontally about the origin, and vertically about the shoulder. Pivot point 3 is the hand. This is by default offset from the shoulder, so that vertical rotations will keep the character nicely positioned on the screen. For 1st-person cameras, this can be set to 0. Finally, the camera is positioned behind the hand, at a specifiable distance from it. The camera's rotation will always be parallel to the Follow target's rotation, but positioned behind the hand. The camera's position and rotation are controlled by moving and rotating the Follow target, not by independent camera controls. The 3rd-person Follow module has a built-in collision resolution system, so that if the target moves close to an obstacle, the camera position will be adjusted so that it will never be inside an obstacle. Properties: Property: Function: Damping How responsively the camera tracks the target. Each axis (camera-local) can have its own setting. Value is the approximate time it takes the camera to catch up to the target's new position. Smaller values give a more rigid effect, larger values give a squishier one Shoulder Offset Position of the shoulder pivot relative to the Follow target origin. This offset is in target-local space. Vertical Arm Length Vertical offset of the hand in relation to the shoulder. Arm length will affect the follow target's screen position when the camera rotates vertically. Camera Side Specifies which shoulder (left, right, or in-between) the camera is on. Camera Distance How far behind the hand the camera will be placed. Camera Collision Filter Camera will avoid obstacles on these layers. Ignore Tag Obstacles with this tag will be ignored. It is a good idea to set this field to the target's tag. Camera Radius Specifies how close the camera can get to obstacles."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyDoNothing.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyDoNothing.html",
    "title": "Do Nothing | FSM Unity Framework",
    "keywords": "Do Nothing This Virtual Camera Body algorithm does not move the Virtual Camera; it does not modify its position. Choose this algorithm for static shots or for animating the camera position directly with your custom scripts."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyFramingTransposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyFramingTransposer.html",
    "title": "Framing Transposer | FSM Unity Framework",
    "keywords": "Framing Transposer This Virtual Camera Body algorithm moves the camera in a fixed screen-space relationship to the Follow target. You can also specify offsets, damping, and composition rules. Framing Transposer only changes the camera’s position in space. It does not re-orient or otherwise aim the camera. Framing Transposer is designed for 2D and orthographic cameras. But it works also with perspective cameras and 3D environments. This algorithm first moves the camera along the camera Z axis until the Follow target is at the desired distance from the camera’s X-Y plane. It then moves the camera in its X-Y plane until the Follow target is at the desired point on the camera’s screen. Note: Framing Transposer ignores the LookAt target - only the Follow target is used. If the Follow target is a Target Group, then additional properties are available to frame the entire group. Properties Property: Function: Lookahead Time Adjusts the offset of the Virtual Camera from the Follow target based on the motion of the target. Cinemachine estimates the point where the target will be this many seconds into the future. This feature is sensitive to noisy animation and can amplify the noise, resulting in undesirable camera jitter. If the camera jitters unacceptably when the target is in motion, turn down this property, or animate the target more smoothly. Lookahead Smoothing The smoothness of the lookahead algorithm. Larger values smooth out jittery predictions and increase prediction lag. Lookahead Ignore Y If checked, ignore movement along the Y axis for lookahead calculations. X Damping How responsively the camera tries to maintain the offset in the x-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Using different settings per axis can yield a wide range of camera behaviors. Y Damping How responsively the camera tries to maintain the offset in the y-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Z Damping How responsively the camera tries to maintain the offset in the z-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Target Movement Only If this is enabled, then damping only applies to the motion of the target. Camera rotation changes will bypass damping. Screen X Horizontal screen position for target. The camera moves to position the tracked object here. Screen Y Vertical screen position for target, The camera moves to position the tracked object here. Camera Distance The distance to maintain along the camera axis from the Follow target. Dead Zone Width Do not move the camera horizontally when the target is within this range of the position. Dead Zone Height Do not move the camera vertically if the target is within this range of the position. Dead Zone Depth Do not move the camera along its z-axis if the Follow target is within this distance of the specified camera distance. Unlimited Soft Zone If checked, then the soft zone is unlimited in size. Soft Zone Width When the target is in this range, move the camera horizontally to frame the target in the dead zone. The Damping properties affect the rate of the camera movement. Soft Zone Height When the target is in this range, move the camera vertically to frame the target in the dead zone. The Damping properties affect the rate of the camera movement. Bias X Moves the target position horizontally away from the center of the soft zone. Bias Y Moves the target position vertically away from the center of the soft zone. Group Framing Mode Available when Follow specifies a Target Group. Specifies the screen dimensions to consider when framing. Horizontal Consider only the horizontal dimension. Ignore vertical framing. Vertical Consider only the vertical dimension. Ignore horizontal framing. Horizontal And Vertical Use the larger of the horizontal and vertical dimensions to get the best fit. None Don’t do any framing adjustment. Adjustment Mode How to adjust the camera to get the desired framing. You can zoom, dolly in or out, or do both. Available when Follow specifies a Target Group. Zoom Only Don’t move the camera, only adjust the FOV. Dolly Only Move the camera, don’t change the FOV. Dolly Then Zoom Move the camera as much as permitted by the ranges, then adjust the FOV if necessary to make the shot. Group Framing Size The bounding box that the targets should occupy. Use 1 to fill the whole screen, 0.5 to fill half the screen, and so on. Available when Follow specifies a Target Group. Max Dolly In The maximum distance toward the target to move the camera. Available when Follow specifies a Target Group. Max Dolly Out The maximum distance away from the target to move the camera. Available when Follow specifies a Target Group. Minimum Distance Set this to limit how close to the target the camera can get. Available when Follow specifies a Target Group. Maximum Distance Set this to limit how far from the target the camera can get. Available when Follow specifies a Target Group. Minimum FOV If adjusting FOV, do not set the FOV lower than this. Available when Follow specifies a Target Group. Maximum FOV If adjusting FOV, do not set the FOV higher than this. Available when Follow specifies a Target Group. Minimum Ortho Size If adjusting Orthographic Size, do not set it lower than this. Available when Follow specifies a Target Group. Maximum Ortho Size If adjusting Orthographic Size, do not set it higher than this. Available when Follow specifies a Target Group."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyHardLockTarget.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyHardLockTarget.html",
    "title": "Hard Lock to Target | FSM Unity Framework",
    "keywords": "Hard Lock to Target This Virtual Camera Body algorithm uses the same position at the Follow target. In other words, the target acts as a mounting point for the Virtual Camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyOrbitalTransposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyOrbitalTransposer.html",
    "title": "Orbital Transposer | FSM Unity Framework",
    "keywords": "Orbital Transposer This Virtual Camera Body algorithm moves the Unity camera in a variable relationship to the Virtual Camera’s Follow target. It optionally accepts player input, which allows the player to dynamically control the position of the camera relative to the target. The Orbital Transposer introduces the concept of heading, which is the direction in which the target is moving or facing. Orbital Transposer attempts to position the camera so that it points in the same direction as the heading direction. By default, this position is directly behind the target. Control this relationship by adjusting the Heading Bias property. If you attach an input controller to the Orbital Transposer, then the player can also control the camera. This allows the player to move the camera to any spot on an orbit around the target. Configure the Orbital Transposer to take its input from any axis that you set up in the Input Manager. Or control the value directly using a custom input system. Orbital Transposer optionally re-centers the camera automatically. When Recenter To Target Heading is checked, Orbital Transposer automatically moves the camera back to the target heading. You can specify the length of time to wait after it has detected no user input and the speed of the recentering. Properties: Property: Function: Binding Mode The coordinate space to use to interpret the offset from the target. Lock To Target On Assign Makes the orientation of the virtual camera match the local frame of the Follow target, at the moment when the virtual camera is activated or when the target is assigned. This offset remains constant in world space. The camera doesn't rotate along with the target. Lock To Target With World Up Makes the virtual camera use the local frame of the Follow target with tilt and roll set to 0. This binding mode ignores all target rotations except yaw. Lock To Target No Roll Makes the virtual camera use the local frame of the Follow target, with roll set to 0. Lock To Target Makes the virtual camera use the local frame of the Follow target. When the target rotates, the camera moves with it to maintain the offset and to maintain the same view of the target. World Space The offset is interpreted in world space relative to the origin of the Follow target. The camera will not change position when the target rotates. Simple Follow With World Up Simple follow with world up interprets the offset and damping values in camera-local space. This mode emulates the action a human camera operator would take when instructed to follow a target. The camera attempts to move as little as possible to maintain the same distance from the target; the direction of the camera with regard to the target does not matter. Regardless of the orientation of the target, the camera tries to preserve the same distance and height from it. Follow Offset The position offset to attempt to maintain from the Follow target. You can also use Scene Handles to modify this property. X Damping How responsively the camera tries to maintain the offset in the x-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Using different settings per axis can yield a wide range of camera behaviors. Not available when Binding Mode is Simple Follow With World Up. Y Damping How responsively the camera tries to maintain the offset in the y-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Z Damping How responsively the camera tries to maintain the offset in the z-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Yaw Damping How responsively the camera tries to track the target rotation’s y angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Available when Binding Mode is Lock to Target With World Up, Lock to Target No Roll, or Lock to Target. Pitch Damping How responsively the camera tries to track the target rotation’s x angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Available when Binding Mode is Lock to Target No Roll or Lock to Target. Roll Damping How responsively the camera tries to track the target rotation’s z angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Available when Binding Mode is Lock to Target. Heading Specifies how to calculate the heading of the Follow target. Definition Choose Position Delta to calculate heading based on the difference in the position of the target from the last update and the current frame. Choose Velocity to use the velocity of the Rigidbody of the target. If the target has no Rigidbody component, reverts to Position Delta. Choose Target Forward to use the target's local Forward axis as the heading. Choose World Forward to use a constant world-space Forward as heading. Velocity Filter Strength Controls the smoothing of the velocity when using Position Delta or Velocity in Definition. Bias Angular offset in the orbit to place the camera, relative to the heading. Measured in degrees. An axis value of 0 will put the camera here. Recenter To Target Heading Controls automatic recentering when the player gives no input. Enabled Check to enable automatic recentering. Wait Time If no user input has been detected on the axis, the camera waits this long in seconds before recentering. Recentering Time Maximum angular speed of recentering. Accelerates into and decelerates out of this. X Axis Heading Control. The settings here control the behaviour of the camera in response to the player’s input. Value The current value of the axis, in degrees. Min Value The minimum value for the axis. Max Value The maximum value for the axis. Wrap If checked, then the axis wraps around at the Min and Max values, forming a loop. Max Speed The maximum speed of this axis in degrees/second, or the multipler for the input value if Speed Mode is set to InputValueGain. Speed Mode How the axis responds to input. MaxSpeed (the default) clamps the maximum speed at which the axis can change, regardless of the input. Input Value Gain multiplies the input value by MaxSpeed. Accel Time The amount of time in seconds to accelerate to MaxSpeed with the supplied axis at its maximum value. Decel Time The amount of time in seconds o decelerate the axis to zero if the supplied axis is in a neutral position. Input Axis Name The name of this axis as specified in the Unity Input manager. Set to an empty string to disable the automatic updating of this axis. Input Axis Value The value of the input axis. A value of 0 means no input. Drive this directly from a custom input system, or set the Input Axis Name to drive the value by the Unity Input Manager. Invert Check to invert the raw value of the input axis before it is used."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyTrackedDolly.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyTrackedDolly.html",
    "title": "Tracked Dolly | FSM Unity Framework",
    "keywords": "Tracked Dolly This Virtual Camera Body algorithm restricts the Virtual Camera to move along a predefined path. Use the Path Position property to specify where to put the Virtual Camera on the path. Use Auto-Dolly mode to move the Virtual Camera to a position on the path that is closest to the Follow target. When enabled, Auto-Dolly automatically animates the position of the Virtual Camera to the position on the path that’s closest to the target. Tip: Choose your path shapes with care when using Auto-Dolly mode. This becomes problematic on paths that form an arc around some point. As an extreme example, consider a perfectly circular path with the Follow target at the center. The closest point on the path becomes unstable because all points on the circular path are equally close to the target. In this situation, moving the Follow target small distances can cause the camera to move large distances on the track. Properties: Property: Function: Path The path that the camera moves along. This property must refer to a Cinemachine Path or Cinemachine Smooth Path. Path Position The position along the path to place the camera. Animate this property directly or enable Auto-Dolly. The value is in the units specified by Position Units. Position Units The unit of measure for Path Position. Path Units Use waypoints along the path. The value 0 represents the first waypoint on the path, 1 is the second waypoint, and so on. Distance Use distance along the path. The path is sampled according to the Resolution property of the path. Cinemachine creates a distance lookup table, which it stores in an internal cache. Normalized Use the beginning and end of the path. The value 0 represents the beginning of the path, 1 is the end of the path. Path Offset The position of the camera relative to the path. X is perpendicular to the path, Y is up, and Z is parallel to the path. Use this property to offset the camera from the path itself. X Damping How responsively the camera tries to maintain its position in a direction perpendicular to the path. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Using different settings per axis can yield a wide range of camera behaviors. Y Damping How responsively the camera tries to maintain its position in the path-local up direction. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Z Damping How responsively the camera tries to maintain its position in a direction parallel to the path. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Camera Up How to set the up vector for the Virtual Camera. This affects the screen composition because the camera Aim algorithms try to respect the up direction. Default Do not modify the up direction of the Virtual Camera. Instead, use the World Up Override property in Cinemachine Brain. Path Use the path’s up vector at the current point. Path No Roll Use the path’s up vector at the current point, but with the roll set to zero. Follow Target Use the up vector from the Follow target’s transform. Follow Target No Roll Use the up vector from the Follow target’s transform, but with the roll zeroed out. Pitch Damping How responsively the camera tracks the target rotation’s x angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Yaw Damping How responsively the camera tracks the target rotation’s y angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Roll Damping How responsively the camera tracks the target rotation’s z angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Auto Dolly Controls how automatic dollying occurs. A Follow target is necessary to use this feature. Enabled Check to enable the automatic dolly. Note: this can have some performance impact, depending on the search resolution. Position Offset Offset, in position units, from the closest point on the path to the follow target. Search Radius The number of segments on either side of the current segment. Use 0 for the entire path. Use a lower number when the path’s shape relative to the target position causes the closest point on the path to become unstable. Search Resolution Cinemachine searches a segment by dividing it into many straight pieces. The higher the number, the more accurate the result. However, performance is proportionally slower for higher numbers."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyTransposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBodyTransposer.html",
    "title": "Transposer | FSM Unity Framework",
    "keywords": "Transposer This Virtual Camera Body algorithm moves the Virtual Camera in a fixed offset to the Follow target. It also applies damping. The fixed offset can be interpreted in various ways, depending on the Binding Mode. Properties: Property: Function: Binding Mode The coordinate space to use to interpret the offset from the target. Lock To Target On Assign Makes the orientation of the virtual camera match the local frame of the Follow target, at the moment when the virtual camera is activated or when the target is assigned. This offset remains constant in world space. The camera does not rotate along with the target. Lock To Target With World Up Makes the virtual camera use the local frame of the Follow target with tilt and roll set to 0. This binding mode ignores all target rotations except yaw. Lock To Target No Roll Makes the virtual camera use the local frame of the Follow target, with roll set to 0. Lock To Target Makes the virtual camera use the local frame of the Follow target. When the target rotates, the camera moves with it to maintain the offset and to maintain the same view of the target. World Space The offset is interpreted in world space relative to the origin of the Follow target. The camera will not change position when the target rotates. Simple Follow With World Up Simple follow with world up interprets the offset and damping values in camera-local space. This mode emulates the action a human camera operator would take when instructed to follow a target. The camera attempts to move as little as possible to maintain the same distance from the target; the direction of the camera with regard to the target does not matter. Regardless of the orientation of the target, the camera tries to preserve the same distance and height from it. Follow Offset The distance to maintain the Virtual Camera relative to the Follow target. Set X, Y, and Z to 0 to place the camera at the centre of the target. The default is 0, 0, -10, respectively, which places the camera behind the target. X Damping How responsively the camera tries to maintain the offset in the x-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Y Damping How responsively the camera tries to maintain the offset in the y-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Z Damping How responsively the camera tries to maintain the offset in the z-axis. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Pitch Damping How responsively the camera tracks the target rotation’s x angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Yaw Damping How responsively the camera tracks the target rotation’s y angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Roll Damping How responsively the camera tracks the target rotation’s z angle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBrainProperties.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineBrainProperties.html",
    "title": "Setting Cinemachine Brain properties | FSM Unity Framework",
    "keywords": "Setting Cinemachine Brain properties The Cinemachine Brain is a component in the Unity camera itself. Cinemachine Brain monitors all active Virtual Cameras in the Scene. It chooses the next Virtual Camera to control the Unity camera. It also controls the cut or blend from the current Virtual Camera to the next. To add a Cinemachine Brain component to a Unity camera, do one of the following: Add a Virtual Camera, or other Cinemachine object, to your Scene. Unity adds a Cinemachine Brain component to the Unity camera for you if there isn’t one already. Add a Cinemachine Brain component to the Unity camera yourself. Tip: You can also control Virtual Cameras from Timeline. Timeline overrides the decisions that Cinemachine Brain makes. Cinemachine Brain holds the following key properties: Blend Settings: A list that defines how to blend from one Virtual Camera to another. For example, add an item to the list for a 4 second blend from vcam1 to vcam2 then add another item for a 1 second blend from vcam2 back to vcam1. If a blend between two cameras isn’t defined, Cinemachine Brain uses its default blend. Layer Filter: Cinemachine Brain uses only those Virtual Cameras that pass the culling mask of the Unity camera. You can set up split-screen environments by using the culling mask to filter layers. Event Dispatching: Cinemachine Brain fires events when it changes shot. It fires an event when a Virtual Camera goes live. It also fires an event when it cuts from one Virtual Camera to another. Use the latter event to reset temporal post effects. Properties: Property: Function: Show Debug Text Check to display a textual summary of the live Virtual Camera and blend in the view. Show Camera Frustum Check to display the frustum of the camera in the Scene view. Ignore Time Scale Check to make the Virtual Cameras respond in real time to user input and damping, even if the game is running in slow motion. World Up Override The Y axis of the specified GameObject defines the worldspace up vector for Virtual Cameras. Use this property in top-down game environments. Set to None to use the worldspace Y axis. Setting this appropriately is important to avoid gimbal-lock in extreme up/down conditions. Update Method When to update the position and rotation of the Virtual Cameras. Fixed Update Synchronize Virtual Camera update with the Physics module, in FixedUpdate. Late Update In MonoBehaviour LateUpdate. Smart Update Update each virtual camera according to how its target is updated. This is the recommended setting. Manual Update Virtual Cameras do not update automatically. You must explicitly call brain.ManualUpdate() at an appropriate time in your game loop. This should be after any camera LookAt or Follow targets have moved. This is an advanced feature. Blend Update Method When to resolve the blends and update the main camera. Late Update In MonoBehaviour LateUpdate. This is the recommended setting. Fixed Update Use this setting only if your Update Method is FixedUpdate and you see judder when blending. Default Blend The blend to use when you haven’t explicitly defined a blend between two Virtual Cameras. Cut Zero-length blend. Ease In Out S-shaped curve, giving a gentle and smooth transition. Ease In Linear out of the outgoing shot, and easy into the incoming. Ease Out Easy out of the outgoing shot, and linear into the incoming. Hard In Easy out of the outgoing, and hard into the incoming. Hard Out Hard out of the outgoing, and easy into the incoming. Linear Linear blend. Mechanical-looking. Custom Custom blend curve. Draw the curve you want. Custom Blends The asset that contains custom settings for blends between specific Virtual Cameras in your Scene. Create Asset Create an asset containing a list of custom blends between Virtual Cameras. Camera Cut Event This event fires when a Virtual Camera goes live and there is no blend. Camera Activated Event This event fires when a Virtual Camera goes live. If a blend is involved, then the event fires on the first frame of the blend."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineClearShot.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineClearShot.html",
    "title": "Cinemachine Clear Shot Camera | FSM Unity Framework",
    "keywords": "Cinemachine Clear Shot Camera The Cinemachine ClearShot Camera component chooses among its children Virtual Cameras for the best quality shot of the target. Use Clear Shot to set up complex multi-camera coverage of a Scene to guarantee a clear view of the target. This can be a very powerful tool. Virtual Camera children with Cinemachine Collider extensions analyze the Scene for target obstructions, optimal target distance, and so on. Clear Shot uses this information to choose the best child to activate. Tip: To use a single Cinemachine Collider for all Virtual Camera children, add a Cinemachine Collider extension to the ClearShot GameObject instead of each of its Virtual Camera children. This Cinemachine Collider extension applies to all of the children, as if each of them had that Collider as its own extension. If multiple child cameras have the same shot quality, the Clear Shot camera chooses the one with the highest priority. You can also define custom blends between the ClearShot children. Properties: Property: Function: Game Window Guides Enables the displays of overlays in the Game window. Adjust the color and opacity in Cinemachine Preferences. This is a global setting, shared by all virtial cameras. Save During Play When enabled, virtual camera setting changes made during Play mode are propagated back to the scene when Play mode is exited. This is a global setting, shared by all objects that support Save During Play. Look At The default target GameObject that the children Virtual Camera move with. The Clear Shot camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Follow The target GameObject to aim the Unity camera at. The Clear Shot camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Show Debug Text Check to display a textual summary of the live Virtual Camera and blend in the Game view. Activate After Wait this many seconds before activating a new child camera. Min Duration An active camera must be active for at least this many seconds, unless a higher-priority camera becomes active. Randomize Choice Check to choose a random camera if multiple cameras have equal shot quality. Uncheck to use the order of the child Virtual Cameras and their priorities. Default Blend The blend to use when you haven’t explicitly defined a blend between two Virtual Cameras. Priority Determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineCollider.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineCollider.html",
    "title": "Cinemachine Collider | FSM Unity Framework",
    "keywords": "Cinemachine Collider Cinemachine Collider is an extension for the Cinemachine Virtual Camera. It post-processes the final position of the Virtual Camera to attempt to preserve the line of sight with the Look At target of the Virtual Camera. It does this by moving away from the GameObjects that obstruct the view. Add a Cinemachine Collider extension to a Cinemachine Virtual Camera to do any of the following tasks: Push the camera away from obstructing obstacles in the Scene. Place the camera in front of obstacles that come between the Virtual Camera and its Look At target. Evaluate shot quality. Shot quality is a measure of the distance of the Virtual Camera from its ideal position, the distance of the Virtual Camera to its target, and the obstacles that block the view of the target. Other modules use shot quality, including Clear Shot. The Collider uses a Physics Raycaster. Therefore, Cinemachine Collider requires that potential obstacles have collider volumes. There is a performance cost for this requirement. If this cost is prohibitive in your game, consider implementing this functionality in a different way. Properties: Property: Function: Collide Against Cinemachine Collider considers GameObjects in these layers to be potential obstacles. It ignores GameObjects that are not in the selected layers. Minimum Distance From Target Ignore obstacles that are less than this distance from the target's pivot point. Avoid Obstacles Check to allow the Collider to move the camera in the Scene when the target is obscured by an obstacle. Use the Distance Limit, Camera Radius, and Strategy properties to adjust how to avoid obstacles. If left unchecked, the Cinemachine Collider will report shot quality based on obstacles, but will not attempt to move the camera to improve the shot. Distance Limit The maximum raycast distance when checking if the line of sight to this camera’s target is clear. Enter 0 to use the current actual distance to the target. Available when Avoid Obstacles is checked. Camera Radius Distance to maintain from any obstacle. Try to keep this value small for best results. Increase it if you are seeing inside obstacles due to a large FOV on the camera. Available when Avoid Obstacles is checked. Strategy The way in which the Collider attempts to preserve sight of the target. Available when Avoid Obstacles is checked. Pull Camera Forward Move the camera forward along its Z axis until it is in front of the obstacle that is nearest to the target. Preserve Camera Height Move the camera to an alternate point of view while attempting to keep the camera at its original height. Preserve Camera Distance Move the camera to an alternate point of view while attempting to keep the camera at its original distance from the target. Smoothing Time Minimum number of seconds to hold the camera at the nearest point to the target. Can be used to reduce excess camera movement in environments with lots of obstacles. Available when Avoid Obstacles is checked. Damping How quickly to return the camera to its normal position after an occlusion has gone away. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Available when Avoid Obstacles is checked. Damping When Occluded How quickly to move the camera to avoid an obstacle. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Available when Avoid Obstacles is checked. Optimal Target Distance If greater than zero, give a higher score to shots when the target is closer to this distance. Set this property to 0 to disable this feature. Transparent Layers Objects on these layers will never obstruct the view of the target. Minimum Occlusion Time Do not take action action unless the occulsion has lasted at least this long. Maximum Effort Upper limit on how many obstacle hits to process. Higher numbers may impact performance. In most environments four (4) hits is enough. Ignore Tag Obstacles with this tag will be ignored. It is recommended to set this field to the target's tag."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineColliderConfiner.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineColliderConfiner.html",
    "title": "Avoiding collisions and evaluating shots | FSM Unity Framework",
    "keywords": "Avoiding collisions and evaluating shots As characters and objects move around in a complex Scene, obstacles in the Scene sometimes come between a camera and its target. Similarly, you might need to move a camera to a position in the Scene that another GameObject already occupies. Cinemachine provides extensions to handle these situations: Cinemachine Collider Cinemachine Confiner"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineCollisionImpulseSource.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineCollisionImpulseSource.html",
    "title": "| FSM Unity Framework",
    "keywords": "#Cinemachine Collision Impulse Source For collision-based impulses, attach a CinemachineCollisionImpulseSource component to a GameObject that has a Collider or Collider2D component. CinemachineCollisionImpulseSource generates an impulse when something collides with the GameObject or enters a trigger zone. To add a Collision Impulse Source component: Select the GameObject that you want to trigger impulses, navigate to its Inspector, and click the Add Component button. Go to Scripts > Cinemachine and select Cinemachine Collision Impulse Source. By default, an Impulse Source affects every Impulse Listener in range, but you can apply channel filtering to make Impulse Sources affect some Impulse Listeners and not others. Properties: The properties in the Cinemachine Collision Impulse Source Inspector window are divided into the following sections. Impulse Channel Impulse Type Impulse Shape Trigger Object Filter How to Generate The Impulse Impulse Channel Impulse Listeners filter impulses based on channels to control which Impulse Sources they react to. Channels work like Camera Layers, but are distinct from them. These properties control the channels that the Collision Impulse Source broadcasts impulse signals on. For details, see documentation on Filtering. Property: Function: Impulse Channel Choose one or more channels from the drop-down. Click Edit to modify existing channels or add new ones. Impulse Type You can choose the level of complexity, depending on your needs. Changing the Impulse Type brings up range, dissipation, and propagation speed controls, as appropriate. Property: Function: Impulse Type You can choose from the following Impulse types: Uniform: The impulse travels with infinite speed, and will be heard at the same time and in the same way by all listeners, no matter where they are in space. Dissipating: The strength of the impulse decreases as the distance from the source increases. Listeners that are farther away will feel a weaker signal than listeners that are closer. Propagating: In addition to being dissipating, the impulse signal travels with finite speed outward from the source. Listeners that are farther away will feel the impulse at a later time than listeners that are close. Legacy: This mode exists to support projects made with earlier versions of Impulse, and has a more complex way of defining the impulse signal. We recommend using one of the other settings. Dissipation Distance This setting defines the distance over which the impulse dissipates. Beyond this distance, the impulse will not be felt. Propagation Speed This defines, in m/s, how quickly the impulse signal propagates outwards through space from its origin. The default value, 343, is the speed of sound. Dissipation Rate This defines how quickly the dissipation occurs over the dissipation distance. As shown in the image below, expanding the curve shows a graph illustrating the signal strength across the dissipation radius. The origin is in the center of the X axis. Moving the slider adjusts the blue picture. Impulse Shape This defines the curve that specifies the shape of the signal, and the time over which the curve is emitted. Property: Function: Predefined Impulse Shape You can choose from one of the following predefined Shapes: Recoil, Bump, Explosion, or Rumble. s (seconds) field: Sets the duration of the impulse. Opening up the property shows you a picture of the impulse: Custom Impulse Shape You can draw your own custom impulse shape (animation curve). Select Custom from the drop-down menu and click on the green icon to pop up an editor as shown below. Default Velocity Specifies the direction in space that the impulse will have by default. Test with Force Allows you to invoke the default impulse from the inspector (when playing) with the specified force multiplier, to see what it looks like. Trigger Object Filter These properties filter the GameObjects that trigger an impulse when they collide or enter the trigger zone. GameObjects in layers specified by the Layer Mask trigger impulses unless you tag them with the Ignore Tag value. For details, see documentation on Filtering. Property: Function: Layer Mask Rigidbody components in these layers that collide with an Impulse Source or enter the trigger zone cause the Source to broadcast its signal. Ignore Tag GameObjects with this tag do not generate an impulse, even if they are in a layer specified in Layer Mask. How To Generate The Impulse An impulse is triggered at a location by an impact mass moving at an impact velocity. Unity dynamically calculates the mass and velocity of the Rigidbody or Rigidbody 2D component that triggers the impulse. The How To Generate The Impulse properties control how the mass and velocity of the Rigidbody affect the strength and direction of the signal. Property: Function: Use Impact Direction Enable this setting to rotate the impulse signal to correspond to the direction of the impact velocity. For example, if the raw signal vibrates vertically but the impact direction is horizontal, you could check this property to make the resulting impulse signal vibrate horizontally. Disable to use the direction of the raw signal irrespective of the impact direction. Scale Impact With Mass Enable this setting to increase or decrease the amplitude of the impulse signal based on the mass of the colliding GameObjects. The masses are specified in the Rigidbody or RigidBody2D component of the GameObject that contains the Cinemachine Impulse Source, and of the colliding GameObject. Disable this setting to use a constant mass of 1. Scale Impact With Speed Enable this setting to scale the amplitude of the signal based on the speed of the impact. Faster moving GameObjects have a greater momentum, and therefore produce a stronger signal. Disable this setting to ignore the speed of the colliding GameObject."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineConfiner.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineConfiner.html",
    "title": "Cinemachine Confiner | FSM Unity Framework",
    "keywords": "Cinemachine Confiner Use the Cinemachine Confiner extension to limit the camera’s position to a volume or area. Confiner operates in 2D or 3D mode. The mode influences the kind of bounding shape it accepts. In 3D mode, the camera’s position in 3D is confined to a volume. This also works for 2D games, but you need to take the depth into account. In 2D mode, you don’t have to worry about depth. For orthographic cameras, there is an additional option to confine the screen edges, not just the camera point. This ensures that the entire screen area stays within the bounding area. Property: Function: Confine Mode Operate using a 2D bounding area or a 3D bounding volume. Confine 2D Use a Collider2D bounding area. Confine 3D Use a 3D Collider bounding volume. Bounding Volume The 3D volume to contain the camera in. This property is available when Confine Mode is set to Confine 3D. Bounding Shape 2D The 2D area to contain the camera in. This property is available when Confine Mode is set to Confine 2D. Confine Screen Edges Check to confine screen edges to the area when the camera is orthographic. When unchecked, confine only the camera center. Has no effect if camera is in perspective mode. Damping How gradually to return the camera to the bounding volume or area if it goes beyond the borders. Higher numbers are more gradual."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineConfiner2D.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineConfiner2D.html",
    "title": "Cinemachine Confiner 2D | FSM Unity Framework",
    "keywords": "Cinemachine Confiner 2D Use the Cinemachine Confiner 2D extension to confine the camera’s position so that the screen edges stay within a shape defined by a 2D polygon. This works for orthographic or perspective cameras, provided that the camera's forward vector remains parallel to the bounding shape’s normal (that is, that the camera is looking straight at the polygon, and not obliquely at it). When confining the camera, the Cinemachine Confiner 2D considers the camera’s view size at the polygon plane, and its aspect ratio. Based on this information and the input polygon, it computes a second (smaller) polygon, and constrains the camera’s transform to it. Computation of this secondary polygon is resource-intensive, so you should only do this when absolutely necessary. Necessary use cases in which you need to recompute the cached secondary polygon include: when the input polygon’s points change, when the input polygon is non-uniformly scaled. In these cases, for efficiency reasons, Cinemachine does not automatically regenerate the inner polygon. The client needs to call the InvalidateCache() method to trigger the recalculation. You can do this from; the script by calling InvalidateCache, or the component inspector; to do so, press the Invalidate Cache button. If the input polygon scales uniformly or translates or rotates, the cache remains valid. Oversize Windows If sections of the confining polygon are too small to fully contain the camera window, Cinemachine calculates a polygon skeleton for those regions. This is a shape with no area, that serves as a place to put the camera when it is confined to this region of the shape. Skeleton computation is the most resource-heavy part of the cache calculation, so it is a good idea to tune this with some care: To optimize the skeleton calculation, set the Max Window Size property to the largest size you expect the camera window to have. Cinemachine does not spend time calculating the skeleton for window sizes larger than that. Properties: Property: Function: Bounding Shape 2D Set the 2D shape you want to confine the camera viewport to. Damping Damping Is applied around corners to avoid jumps. Higher numbers are more gradual. Max Window Size To optimize computation and memory performance, set this to the largest view size that the camera is expected to have. The Confiner 2D does not compute a polygon cache for frustum sizes larger than this. This refers to the size in world units of the frustum at the confiner plane (for orthographic cameras, this is just the orthographic size). If set to 0, then Cinemachine ignores this parameter and calculates a polygon cache for all potential window sizes."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineDolly.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineDolly.html",
    "title": "Using dolly paths | FSM Unity Framework",
    "keywords": "Using dolly paths A dolly path is an array of waypoints in a Scene. Use a dolly path to specify a fixed course to position or animate a Virtual Camera. Use the Tracked Dolly algorithm to use a dolly path with your Virtual Camera. Cinemachine provides two types of dolly paths: Smooth Path: Each waypoint has a position and roll. Cinemachine uses Bezier interpolation between the waypoints. Smooth Path gives you less control over the position of the path but animation along the path is always smooth and continuous. This is the recommended path type. Path: Each waypoint has a position, tangent, and roll. Use Path for finer control of the track’s position between waypoints. Animating a VIrtual Camera along a Path might cause jerky motion over waypoints when tangents don’t allow a smooth transition to and from the waypoint. To create a Virtual Camera with a dolly path: In the Unity menu, choose GameObject > Cinemachine > Dolly Camera with Track. A new Virtual Camera and dolly track appears in the Hierarchy. By default, the dolly track GameObject is a Smooth Path. In the Hierarchy window, select the new dolly track GameObject. In the Inspector, add and adjust waypoints. To create a path GameObject: In the Unity menu, choose GameObject > Create Empty. Add the path component, either CinemachinePath or CinemachineSmoothPath To edit a waypoint in a path: In the Hierarchy window, select the path GameObject. Select a waypoint by either clicking its index number in the Scene view or clicking its number in the Path Details section in the Inspector. Edit the waypoint by manipulating its Gizmo in the Scene view or edit its properties in the Inspector. Note: When editing a waypoint for a regular path in the Scene view, the waypoint has two Gizmos: for the position and for the tangent."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineDollyCart.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineDollyCart.html",
    "title": "Cinemachine Dolly Cart | FSM Unity Framework",
    "keywords": "Cinemachine Dolly Cart Cinemachine Dolly Cart is a component that constrains the transform of its GameObject to a Cinemachine Path or Cinemachine Smooth Path. Use it to animate a GameObject along a path, or as a Follow target for Virtual Cameras. Properties: Property: Function: Path The path to follow. Update Method When to move the cart when velocity is non-zero. Use Update for normal MonoBehaviour updating, Fixed Update for updates in sync with the Physics module, FixedUpdate(). Position Units The unit of measure for Position. Path Units Use waypoints along the path. The value 0 represents the first waypoint on the path, 1 is the second waypoint, and so on. Distance Use distance along the path. The path is sampled according to the Resolution property of the path. Cinemachine creates a distance lookup table, which it stores in an internal cache. Normalized Use the beginning and end of the path. The value 0 represents the beginning of the path, 1 is the end of the path. Speed Move the cart with this speed. The value is interpreted according to Position Units. Position The position along the path to place the cart. This can be animated directly or, if the speed is non-zero, will be updated automatically. The value is interpreted according to Position Units."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineExternalCamera.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineExternalCamera.html",
    "title": "Cinemachine External Camera - (deprecated) | FSM Unity Framework",
    "keywords": "Cinemachine External Camera - (deprecated) Note: This component is deprecated in favour of using a normal CinemachineVirtualCamera with Do Nothing in both Aim and Body. This component will expose a non-cinemachine camera to the cinemachine system, allowing it to participate in blends. Just add it as a component alongside an existing Unity Camera component. You will need to take steps (e.g. disabling the Camera component) to ensure that the Camera doesn't fight with the main Cinemachine Camera. Properties: Property: Function: Look At The object that the camera is looking at, if defined. This can be empty, but setting this may improve the quality of the blends to and from this camera. Blend Hint Hint for blending positions to and from this camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineFollowZoom.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineFollowZoom.html",
    "title": "Cinemachine Follow Zoom | FSM Unity Framework",
    "keywords": "Cinemachine Follow Zoom This extension adjusts the FOV of the lens to keep the target object at a constant size on the screen, regardless of camera and target position. Properties: Property: Function: Width The shot width to maintain, in world units, at target distance. The FOV will be adjusted so that an object of this size, at target distance, will fill the screen. Damping Increase this value to soften the responsiveness of the follow-zoom. Small numbers make the camera more responsive. Larger numbers make the camera respond more slowly. Min FOV Lower limit for the FOV that this behavior generates. Max FOV Upper limit for the FOV that this behavior generates."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineFreeLook.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineFreeLook.html",
    "title": "Cinemachine Free Look Camera | FSM Unity Framework",
    "keywords": "Cinemachine Free Look Camera The Cinemachine Free Look Camera component provides a third-person camera experience. This Cinemachine Virtual Camera orbits around its subject along a position specified by three separate camera rigs: Top, Middle, and Bottom. Each rig defines a ring around the target, with its own radius, height offset, composer, lens properties, and Noise settings. These are the same as the properties for a regular Virtual Camera. Cinemachine Free Look Camera also defines a spline that connects the rigs. The spline defines the camera’s position when blending between the rigs. Free Look uses player input along the x and y axes. The x axis controls the orbital position along the Top, Middle, or Bottom horizontal orbits, like the Orbital Transposer. The y-axis controls the vertical position, using the spline to determine the position between rigs. Properties: Property: Function: Solo Toggles whether or not the Virtual Camera is temporarily live. Use this property to get immediate visual feedback in the Game view to adjust the Virtual Camera. Game Window Guides Toggles the visibility of compositional guides in the Game view. These guides are available when Look At specifies a GameObject and the Aim section uses Composer or Group Composer, or when Follow specifies a target and the Body section uses Framing Composer. This property applies to all Virtual Cameras. Save During Play Check to apply the changes while in Play mode. Use this feature to fine-tune a Virtual Camera without having to remember which properties to copy and paste. This property applies to all Virtual Cameras. Priority The importance of this Virtual Camera for choosing the next shot. A higher value indicates a higher priority. Cinemachine Brain chooses the next live Virtual Camera from all Virtual Cameras that are activated and have the same or higher priority as the current live Virtual Camera. This property has no effect when using a Virtual Camera with Timeline. Follow The target GameObject to move with. The Body properties use this target to update the position of the Unity camera. Look At The target GameObject to aim at. The Aim properties use this target to update the rotation of the Unity camera. This property is normally the same as the Follow target. Common Lens Check to apply a common lens setting to all three child rigs. Uncheck to use separate lens settings for each child rig. Lens If Common Lens is checked, these lens settings apply to all three rigs. These properties mirror their counterparts in the property settings for the Unity camera. Vertical FOV This is the camera view in vertical degrees. For example, to specify the equivalent of a 50mm lens on a Super 35 sensor, enter a Field of View of 19.6 degrees. This property is available when the Unity camera with the Cinemachine Brain component uses a Projection of Perspective. Orthographic Size When using an orthographic camera, this defines the half-height, in world coordinates, of the camera view. This property is available when the Unity camera with the Cinemachine Brain component uses a Projection of Orthographic. Near Clip Plane The closest point relative to the camera where drawing occurs. Far Clip Plane The furthest point relative to the camera where drawing occurs. Dutch Dutch angle. Tilts the Unity camera on the Z axis, in degrees. This property is unique to the Virtual Camera; there is no counterpart property in the Unity camera. Transitions Blend Hint How to blend to and from the Free Look camera. Use None for default linear interpolation of position and aim. Use Spherical Position to blend spherically about the Look At position, linearly blending between Look At targets. Use Cylindrical Position to blend in a cylindrical path around the Look At target and interpolate linearly on the vertical axis. Use Screen Space Aim When Targets Differ for a standard linear position blend, with screen-space angular blend between differing Look At targets. Inherit Position When this virtual camera goes Live, attempt to force the position to be the same as the current position of the Unity Camera, allowing the virtual camera to apply its damping to accomplish the blend Camera Activated This event is invoked when the Virtual Camera becomes live. Attach custom handlers here if you need them. Y Axis, X Axis The vertical and horizontal axes for blending between rigs. For Y Axis the Value range is 0 to 1 and represents the blend position between the Top, Middle, and Bottom rigs. The value 0.5 represents the middle rig. For X Axis, the Value is the angular deviation (in degrees) of the camera from directly behind the target. Max Speed The maximum speed of this axis in degrees/second, or the multipler for the input value if Speed Mode is set to InputValueGain. Speed Mode How the axis responds to input. MaxSpeed (the default) clamps the maximum speed at which the axis can change, regardless of the input. Input Value Gain multiplies the input value by MaxSpeed. Accel Time The amount of time in seconds to accelerate to Max Speed. Decel Time The amount of time in seconds to decelerate to zero. Input Axis Name The name of this axis as specified in Unity Input manager. Setting to an empty string disables the automatic updating of the axis. Input Axis Value The value of the input axis. A value of 0 means no input. To drive this directly, use a custom input system. Or you set the Axis Name to control the value with the Unity Input Manager. Invert Check to invert the raw value of the input axis. The X Axis and Y Axis have opposite defaults. Checking this box causes the view to move opposite the input for the X Axis. Checking the box for the Y Axis causes the view to move in the same direction as the input. Y Axis Recentering Controls the automatic recentering on the y axis. Enabled Check to enable automatic recentering. Wait Time When no user input has been detected on the axis, the camera waits this long in seconds before recentering. Recentering Time Maximum angular speed of recentering. Accelerates into and decelerates out of the centered position. Orbits Properties for defining the Top, Middle, and Bottom rigs. Binding Mode The coordinate space to use to interpret the offset from the target. Lock To Target On Assign Makes the orientation of the virtual camera match the local frame of the Follow target, at the moment when the virtual camera is activated or when the target is assigned. This offset remains constant in world space. The camera does not rotate along with the target. Lock To Target With World Up Makes the virtual camera use the local frame of the Follow target with tilt and roll set to 0. This binding mode ignores all target rotations except yaw. Lock To Target No Roll Makes the virtual camera use the local frame of the Follow target, with roll set to 0. Lock To Target Makes the virtual camera use the local frame of the Follow target. When the target rotates, the camera moves with it to maintain the offset and to maintain the same view of the target. World Space The offset is interpreted in world space relative to the origin of the Follow target. The camera will not change position when the target rotates. Simple Follow With World Up Simple follow with world up interprets the offset and damping values in camera-local space. This mode emulates the action a human camera operator would take when instructed to follow a target. The camera attempts to move as little as possible to maintain the same distance from the target; the direction of the camera with regard to the target does not matter. Regardless of the orientation of the target, the camera tries to preserve the same distance and height from it. Spline Curvature The tautness of the line that connects the rigs’ orbits. This line determines the final placement on the y axis. Height, Radius The radius and height of the Top, Middle, and Bottom rigs relative to the Follow target."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulse.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulse.html",
    "title": "Impulse | FSM Unity Framework",
    "keywords": "Impulse Cinemachine Impulse generates and manages camera shake in response to game events. For example, you can use Impulse to make a Cinemachine Virtual Camera shake when one GameObject collides with another, or when something in your Scene explodes. Impulse has two parts: 1. Impulse Source: a component that emits a signal that originates at a point in space and propagates outwards, much like a sound wave or a shock wave. This emission is triggered by events in the game. The signal consists of a direction, and a curve specifying the strength of the signal as a function of time. Together, these effectively define a shake along a specified axis, lasting a specified amount of time. This shake travels outward from the point of origin, and when it reaches the location of an Impulse Listener, that listener can respond to it. 2. Impulse Listener: a Cinemachine extension that allows a Virtual Camera to “hear” an impulse, and react to it by shaking. It’s useful to think about this in terms of individual “impulses.” An impulse is a single occurrence of an Impulse Source emitting a signal. Collisions and events in your Scenes trigger impulses, Impulse Sources generate impulses, and Impulse Listeners react to impulses. Getting started with Impulse To set up and use Impulse in a Scene, do the following: Add Cinemachine Impulse Source or Cinemachine Collision Impulse Source components to one or more GameObjects that you want to trigger camera shake. Add a Cinemachine Impulse Listener extension to one or more Cinemachine virtual cameras so they can detect and react to impulses."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseFiltering.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseFiltering.html",
    "title": "Filtering impulses | FSM Unity Framework",
    "keywords": "Filtering impulses Filtering lets you fine-tune how and when an Impulse Source generates impulses. Cinemachine Impulse allows two types of filtering: Use channel filtering to set things up so that an Impulse Listener reacts to certain Impulse Sources and ignores others. See Filtering with channels below for details. Use Trigger Object Filtering with Collision Impulse Sources to set things up so that only certain GameObjects trigger an impulse. See Filtering with layers and tags for details. Filtering with channels By default, every Impulse Listener reacts to every Impulse Source within range. Channels allow you to more precisely control which Impulse Sources an Impulse Listener reacts to. To set up channel filtering, you need to do three things: Set up your channels Set your Impulse Sources to broadcast on one or more channels Set your Impulse Listeners to listen to one or more channels When an Impulse Listener is listening to specific channels, it only reacts to Impulse Sources that broadcast on those channels. Adding channels The CinemachineImpulseChannels script creates channels in your Scene. It has one channel by default, and you can add as many new channels as you need, up to a maximum of 31. To add new channels: Inspect the CinemachineImpulseChannels (Impulse Channel > Edit) script by doing one of the following: In the Cinemachine Impulse Listener inspector, navigate to the Channel Mask drop-down and click the Edit button next to it. In the Cinemachine Impulse Source or Cinemachine Collision Impulse Source inspector, navigate to the Impulse Channel drop-down and click the Edit button next to it. Expand the Impulse Channels property group and set the Size property to the number of channels you want. A new entry appears for each channel. Rename your new channels. Channels are available from the channel drop-down in the Inspector as soon as you add them. Setting listen / broadcast channels After setting up your channels, you need to define how your Impulse Listeners and Impulse Sources use them. Inspect each Impulse Listener, and choose the channels you want it to listen to from the Channel Mask drop-down. Inspect each Impulse Source or Collision Impulse Source, and choose the channels you want it to broadcast on from the Impulse Channel drop-down. You can select multiple filters from the drop down. You can also choose Everything to use all filters, or Nothing to use none of them. Filtering with layers and tags You can use Unity’s Layers and Tags to specify which GameObjects trigger an impulse when they collide with a Collision Impulse Source, or enter a trigger zone. This is called Trigger Object Filtering. The Cinemachine Collision Impulse Source component has two Trigger Object Filter properties: The Layer Mask drop-down lists all of the Scene’s layers. When you select one or more layers, GameObjects in those layers trigger impulses when they collide with the Impulse Source. The Impulse Source ignores collisions with GameObjects on other layers. The Ignore Tag drop-down lists all of the Scene’s tags. When you select a tag, GameObjects with that tag do not trigger impulses when they collide with the Impulse Source, even if they are in a layer specified in Layer Mask. For example, in a Scene where a large animal is lumbering through a forest, you might want the camera to shake when it collides with large trees, but not small saplings. One way to set that up would be to make the animal a Collision Impulse Source, put all of the large trees on their own layer, and select that as the Layer Mask. If all of the trees, large ones and saplings alike, are already on the same layer, you could assign a special tag to the saplings, and use the Ignore Tag property to filter them out."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseListener.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseListener.html",
    "title": "Cinemachine Impulse Listener | FSM Unity Framework",
    "keywords": "Cinemachine Impulse Listener Impulse signals and sources don’t do anything on their own. An Impulse Listener is a Cinemachine extension that allows a virtual camera to “hear” impulse vibration signals and react to them. Default implementations shipped with Cinemachine respond by applying the received signal directly to the listener’s transform position, causing it to shake along with the signal. Additionally, it’s possible to specify a secondary response - usually a randomized vibration along all positions and rotation axes - to give character to the listener’s movement. It’s as if the listener were supported on springs and then kicked by the impulse signal. In addition to the thrust given by the kick itself, there will be randomized shaking due to the springs. When you add an Impulse Listener extension to a virtual camera, it makes the camera shake in response to the signals emitted from Impulse Sources. In the simplest case, the Impulse Listener applies the signal verbatim to the camera’s Transform, causing it to shake. In the image below, the figure’s feet are Impulse Sources. When they collide with the floor (A) they generate impulses. The camera is an Impulse Listener and reacts to the impulses by shaking (B), which shakes the resulting image in the Game view (C). To add an Impulse Listener to a Cinemachine virtual camera: Select the virtual camera, navigate to the Inspector window and expand the Cinemachine Virtual Camera script. Go to Extensions > Add Extension, and select CinemachineImpulseListener. In the real world, some cameras are mounted less rigidly than others, and tend to shake more as a result. The Impulse Listener’s Gain property emulates this behavior by amplifying or attenuating impulse vibration signals. Higher values cause the camera to shake more. TIP: You can create your own Impulse Listener to interpret vibration signals any way you like. By default, an Impulse Listener reacts to every Impulse Source in range, but you can apply channel filtering to make a Listener respond to some Sources and ignore others. ##Properties: Property: Function: Apply After Obstacles with this tag will be ignored. It is recommended to set this field to the target's tag. Channel Mask Specifies the Impulse channels to react to. For details, see Filtering with channels. Gain This is how much the received impulse signal will be magnified by for the purposes of reacting. It’s a simple multiplier applied to the incoming signal. The default value is 1. Use 2D Distance Enable this setting to ignore the z axis when calculating camera distance from the Impulse Source. Use this property for 2D games. Use Camera Space Interprets the impulse signal in camera space as opposed to world space. So if the impulse Y axis is vibrating, then the listener will move up and down on its local Y axis. Reaction Settings Lets you set a secondary noise that gets triggered by the impulse signal. Choose the noise setting and tune it with the amplitude and frequency gain. Duration sets the fade-out time for the secondary noise. Time is approximate. This will scale automatically with stronger impulses. The listener combines the original impulse signal and the reaction and applies it to the object it’s on. This could be a camera, a vcam, or any other object. Custom listeners can easily be authored to apply the signal in nonstandard ways (for example, convert Z motion to FOV)."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseSource.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseSource.html",
    "title": "Cinemachine Impulse Source | FSM Unity Framework",
    "keywords": "Cinemachine Impulse Source Use the Cinemachine Impulse Source component to generate impulses on events that are not collisions or Collider triggers. This is a generic Impulse Source that exposes a family of GenerateImpulse() API methods. These methods generate impulses at the specified locations and with the specified velocities and strengths. Call these methods directly from your game logic, or use them with UnityEvents. TIP: You can use the script for this component as an example to reference when creating your own custom impulse-generating classes. To add a Cinemachine Impulse Source to your Scene: Select the GameObject that you want to trigger camera shake, navigate to its Inspector, and click the Add Component button. Go to Scripts > Cinemachine, and select Cinemachine Impulse Source. By default, an Impulse Source affects every Impulse Listener in range, but you can apply channel filtering to make Impulse Sources affect some Impulse Listeners and not others. ##Properties: The properties in the Cinemachine Impulse Source Inspector window are divided into the following sections. Impulse Channel Impulse Type Impulse Shape Impulse Channel Impulse Listeners filter impulses based on channels to control which Impulse Sources they react to. Channels work like Camera Layers, but are distinct from them. These properties control the channels that the Impulse Source broadcasts impulse signals on. For details, see documentation on Filtering. Property: Function: Impulse Channel Choose one or more channels from the drop-down. Click Edit to modify existing channels or add new ones. Impulse Type You can choose the level of complexity, depending on your needs. Changing the Impulse Type brings up range, dissipation, and propagation speed controls, as appropriate. Property: Function: Impulse Type You can choose from the following Impulse types: Uniform: The impulse travels with infinite speed, and will be heard at the same time and in the same way by all listeners, no matter where they are in space. Dissipating: The strength of the impulse decreases as the distance from the source increases. Listeners that are farther away will feel a weaker signal than listeners that are closer. Propagating: In addition to being dissipating, the impulse signal travels with finite speed outward from the source. Listeners that are farther away will feel the impulse at a later time than listeners that are close. Legacy: This mode exists to support projects made with earlier versions of Impulse, and has a more complex way of defining the impulse signal. We recommend using one of the other settings. Dissipation Distance This setting defines the distance over which the impulse dissipates. Beyond this distance, the impulse will not be felt. Propagation Speed This defines, in m/s, how quickly the impulse signal propagates outwards through space from its origin. The default value, 343, is the speed of sound. Dissipation Rate This defines how quickly the dissipation occurs over the dissipation distance. As shown in the image below, expanding the curve shows a graph illustrating the signal strength across the dissipation radius. The origin is in the center of the X axis. Moving the slider adjusts the blue picture. Impulse Shape This defines the curve that specifies the shape of the signal, and the time over which the curve is emitted. Property: Function: Predefined Impulse Shape You can choose from one of the following predefined Shapes: Recoil, Bump, Explosion, or Rumble. s (seconds) field: Sets the duration of the impulse. Opening up the property shows you a picture of the impulse: Custom Impulse Shape You can draw your own custom impulse shape (animation curve). Select Custom from the drop-down menu and click on the green icon to pop up an editor as shown below. Default Velocity Specifies the direction in space that the impulse will have by default. Test with Force Allows you to invoke the default impulse from the inspector (when playing) with the specified force multiplier, to see what it looks like."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseSourceOverview.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineImpulseSourceOverview.html",
    "title": "Cinemachine Impulse Sources | FSM Unity Framework",
    "keywords": "Cinemachine Impulse Sources An Impulse Source is a component that emits a vibration signal from a point in Scene space. Game events can cause an Impulse Source to emit a signal from the place where the event occurs. The event triggers impulses, and the source generates impulses. Virtual cameras with an Impulse Listener extension react to impulses by shaking. In the image below, the figure's feet are Impulse Sources. When they collide with the floor (A) they generate impulses. The camera is an Impulse Listener and reacts to the impulses by shaking (B), which shakes the resulting image in the game view (C). Cinemachine ships with two types of Impulse Source components. Cinemachine Collision Impulse Source generates impulses in reaction to collisions and trigger zones. Cinemachine Impulse Source generates impulses in reaction to events other than collisions. Your Scene can have as many Impulse Sources as you want. Here are a few examples of where you might use Impulse Source components in a Scene: On each of a giant’s feet, so that the ground shakes when the giant walks. On a projectile that explodes when it hits a target. On the surface of a gelatin planet that wobbles when something touches it. By default, an Impulse Source affects every Impulse Listener in range, but you can apply channel filtering to make Sources affect some Listeners and not others. Key Impulse Source properties While the vibration signal defines the basic “shape” of the camera shake, the Impulse Source controls several other important properties that define the impulses it generates. For descriptions of all Impulse Source properties, as well as instructions for adding Impulse Sources to your scene, see documentation on the Impulse Source and Collision Impulse Source components."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineManagerCameras.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineManagerCameras.html",
    "title": "Managing and grouping Virtual Cameras | FSM Unity Framework",
    "keywords": "Managing and grouping Virtual Cameras A manager camera oversees many Virtual Cameras but acts as a single Virtual Camera from the point of view of Cinemachine Brain and Timeline. Cinemachine includes these manager cameras: Free Look Camera: an enhanced Orbital Transposer. It manages three horizontal orbits, arranged vertically to surround an avatar. Mixing Camera: uses the weighted average of up to eight child Virtual Cameras. Blend List Camera: executes a sequence of blends or cuts of its child Virtual Cameras. Clear Shot Camera: picks the child Virtual Camera with the best view of the target. State-Driven Camera: picks a child Virtual Camera in reaction to changes in animation state. Because manager cameras act like normal Virtual Cameras, you can nest them. In other words, create arbitrarily complex camera rigs that combine regular Virtual Cameras and manager cameras."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineMixingCamera.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineMixingCamera.html",
    "title": "Cinemachine Mixing Camera | FSM Unity Framework",
    "keywords": "Cinemachine Mixing Camera The Cinemachine Mixing Camera component uses the weighted average of its child Virtual Cameras to compute the position and other properties of the Unity camera. Mixing Camera manages up to eight child Virtual Cameras. In the Mixing Camera component, these Virtual Cameras are fixed slots, not a dynamic array. Mixing Camera uses this implementation to support weight animation in Timeline. Timeline cannot animate array elements. To create a Mixing Camera: In the Unity menu, choose GameObject > Cinemachine > Mixing Camera. A new Mixing Camera appears in the Hierarchy window. By default, Unity also adds two Virtual Cameras as children of the Mixing Camera. Adjust the children Virtual Cameras. Add up to six more child cameras. Select the Mixing Camera in the Hierarchy window then adjust the Child Camera Weights in the Inspector window. Properties: Property: Function: Solo Toggles whether or not the Mixing Camera is temporarily live. Use this property to get immediate visual feedback in the Game view to adjust the Virtual Camera. Game Window Guides Toggles the visibility of compositional guides in the Game view. This property applies to all Virtual Cameras. Save During Play Check to apply the changes while in Play mode. Use this feature to fine-tune a Virtual Camera without having to remember which properties to copy and paste. This property applies to all Virtual Cameras. Priority The importance of this Mixing Camera for choosing the next shot. A higher value indicates a higher priority. Cinemachine Brain chooses the next live Virtual Camera from all Virtual Cameras that are activated and have the same or higher priority as the current live Virtual Camera. This property has no effect when using a Virtual Camera with Timeline. Standby Updates Controls how often the virtual camera is updated when the virtual camera is not live. Child Camera Weights The weight of the Virtual Camera. Each child Virtual Camera has a corresponding Weight property. Note that setting one camera's weight to 1 does not put the other weights to zero. The contribution of any individual camera is its weight divided by the sum of all the child weights. Mix Result A graphical representation of the weights of the child Virtual Cameras. The light part of the bar of each child camera represents the proportion of its contribution to the final position of the Mixing Camera. When the bar is completely dark, the camera makes no contribution to the position of the Mixing Camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineMultipleCameras.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineMultipleCameras.html",
    "title": "Multiple Unity cameras | FSM Unity Framework",
    "keywords": "Multiple Unity cameras Split-screen and picture-in-picture effects require the use of more than one Unity camera. Each Unity camera presents its own view on the player’s screen. To use a multi-camera split-screen for two players: For each player, create a layer. For example, for two players, create layers named P1 and P2. Add two Unity cameras to your Scene, set up their viewports, and give each one its own Cinemachine Brain component. For each Unity camera, set the Culling Mask to the appropriate layer while excluding the other layer. For example, set the first Unity camera to include layer P1 while excluding P2. Add 2 Virtual Cameras, one to follow each player to follow the players. Assign each Virtual Camera to a player layer."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineNoiseProfiles.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineNoiseProfiles.html",
    "title": "Working with noise profiles | FSM Unity Framework",
    "keywords": "Working with noise profiles A noise profile is an asset that defines a procedural curve for camera noise. The Basic Multi Channel Perlin component applies a noise profile to the movement of the camera. Cinemachine applies noise movement after computing the position of the camera. This way, camera noise does not affect the computation of camera movement in future updates. Cinemachine includes some predefined profile assets. Choose a predefined noise profile in the Noise component. Create your own noise profile asset by choosing Create > Cinemachine > NoiseSettings in the Project window. The properties in the Inspector show graphs that give a visual representation of the noise profile. There are properties for the x, y, and z axes for position and rotation. Each axis may have more than one layer. For realistic procedural noise, choose frequencies and amplitudes with care to ensure an interesting noise quality that is not obviously repetitive. The most convincing camera shakes use Rotation noise because that’s where the camera is aiming. Handheld camera operators tend to shake more rotationally than they do positionally. After specifying Rotation noise, add Position noise. Convincing noise profiles typically mix low, medium, and high frequencies together. When creating a new noise profile, start with these three layers of noise for each axis. For amplitude, use larger values for wider lenses to shake the camera noticeably. For telephoto lenses, use smaller amplitude values because the narrower FOV amplifies the effect. For frequency, a typical low range is 0.1-0.5 Hz, the mid range 0.8-1.5, and the high 3-4. The highest useful frequency depends on the frame rate of your game. A game typically runs at 30 or 60Hz. Noise frequencies higher than the frame rate of your game fall between the cracks of the Nyquist rate. In other words, they will not be directly tracked. For example, if your game runs at 60 frames/second and you set a frequency to 100, you will get choppy camera noise. This is because your game can’t render something that moves faster than the frame rate. Properties: Property: Function: Preview Time The number of seconds to display in the graphs in the Inspector. This property is for editing in the Inspector; it does not affect the content of the noise profile asset that you are editing. Preview Height The vertical height of the graphs of the noise profile in the Inspector. This property is for editing noise profiles; it does not affect the noise profile asset. Animated Check to show a moving representation of an example of the noise profile in the graph. This property is for editing noise profiles; it does not affect the noise profile asset. Position Noise A graphical representation of all noise layers for all axes for camera movement. Position X, Position Y, Position Z The layers of noise for each axis to apply to camera movement. Each axis has a graphical representation of its layers. Each layer has properties for Frequency, Amplitude, and optional Perlin noise. Click + or - to add and remove layers, respectively. Frequency The frequency of the wave in the noise layer, in Hz. Amplitude The amplitude (height) of the wave in the noise layer, in distance units. Non-random wave if checked Check to remove the Perlin noise from the noise layer. Without Perlin noise, Cinemachine uses a regular sine wave. Uncheck to apply Perlin noise to the layer, randomizing both the frequency and the amplitude while remaining in the neighbourhood of the selected values. Rotation Noise A graphical representation of all noise layers for all axes for camera rotation. Rotation X, Rotation Y, Rotation Z The layers of noise for each axis to apply to camera rotation. Each layer has properties for Frequency, Amplitude, and optional Perlin Noise. Click + or - to add and remove layers, respectively. Frequency The frequency of the wave in the noise layer, in Hz. Amplitude The amplitude (height) of the wave in the noise layer, in degrees. Non-random wave if checked Check to remove the Perlin noise from the noise layer. Without Perlin noise, Cinemachine uses a regular sine wave. Uncheck to include random Perlin noise variation, randomizing both the frequency and the amplitude while remaining in the neighbourhood of the selected values."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePath.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePath.html",
    "title": "Cinemachine Path | FSM Unity Framework",
    "keywords": "Cinemachine Path Cinemachine Path is a component that defines a world-space path, consisting of an array of waypoints. Each waypoint specifies position, tangent, and roll settings. Bezier interpolation is performed between the waypoints, to get a smooth and continuous path. Tip: While the path position will always be smooth and continuous, it is still possible to get jarring movement when animating along the path. This happens when tangents aren’t set to ensure continuity of both the first and second order derivatives. It is not easy to get this right. To avoid this potentially jarring movement, use Cinemachine Smooth Path. CinemachineSmoothPath sets the tangents automatically to ensure complete smoothness. Properties: Property: Function: Resolution Path samples per waypoint. Cinemachine uses this value to limit the granularity when calculating path distances. The crosshatches on the path gizmo in the scene view reflect this value Appearance The settings that control how the path appears in the Scene view. Path Color The color of the path when it is selected. Inactive Path Color The color of the path when it is not selected. Width The width of the railroad tracks that represent the path. Looped Check to join the ends of the path to form a continuous loop. Path Length The length of the path in distance units (read-only). Selected Waypoint Properties for the waypoint you selected in the Scene view or in the Waypoints list. Prefer Tangent Drag Check to use the Gizmo for the tangent of a waypoint when the Gizmos for the tangent and position coincide in the Scene view. Path Details The list of waypoints that define the path. Position Position in path-local space (i.e. relative to the transform of the path object itself) Tangent Offset from the position, which defines the tangent of the curve at the waypoint. The length of the tangent encodes the strength of the bezier handle. The same handle is used symmetrically on both sides of the waypoint, to ensure smoothness. Roll The roll of the path at this waypoint. The other orientation axes are inferred from the tangent and world up."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePixelPerfect.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePixelPerfect.html",
    "title": "Using the Cinemachine Pixel Perfect extension | FSM Unity Framework",
    "keywords": "Using the Cinemachine Pixel Perfect extension Both the Pixel Perfect Camera and Cinemachine modify the Camera’s orthographic size. Using these two systems together in a single Scene would cause them to fight for control over the Camera and produce unwanted results. The Cinemachine Pixel Perfect extension solves this incompatibility. Cinemachine Pixel Perfect is an extension for the Cinemachine Virtual Camera that alters the orthographic size of the virtual camera. The extension detects the presence of the Pixel Perfect Camera component, and uses the component settings to calculate for the correct orthographic size of the virtual camera that best retains the Sprites in a pixel-perfect resolution. To add this extension to your virtual cameras, use the Add Extension dropdown menu on the Cinemachine Virtual Camera Inspector window. Add this extension to each virtual camera in your Project. For each virtual camera attached with this extension, the Pixel Perfect Camera component then calculates a pixel-perfect orthographic size that best matches the original size of the virtual camera during __Play Mode __ or when Run In Edit Mode is enabled. This is done to match the original framing of each virtual camera as close as possible when the pixel-perfect calculations are implemented. When the Cinemachine Brain component blends between multiple virtual cameras, the rendered image is temporarily not pixel-perfect during the transition between cameras. The image becomes pixel-perfect once the view fully transitions to a single virtual camera. The following are the current limitations of the extension: When a virtual camera with the Pixel Perfect extension is set to follow a Target Group, there may be visible choppiness when the virtual camera is positioned with the Framing Transposer component. If the Upscale Render Texture option is enabled on the Pixel Perfect Camera, there are less possible pixel-perfect resolutions that match the original orthographic size of the virtual cameras. This may cause the framing of the virtual cameras to be off by quite a large margin after the pixel-perfect calculations."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePostProcessing.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachinePostProcessing.html",
    "title": "Post Processing Extension | FSM Unity Framework",
    "keywords": "Post Processing Extension Use the Cinemachine Post Processing extension to attach a Postprocessing V2 profile to a Virtual Camera. Note 1: Unity recommends using Postprocessing V2 instead of Postprocessing V1. Note 2: With HDRP and URP 7 and up, The PostProcessing package is deprecated, and is implemented natively by HDRP and URP. In that case, please see the CinemachineVolumeSettings extension. The Cinemachine Post Processing extension holds a Post-Processing Profile asset to apply to a Virtual Camera when it is activated. If the camera is blending with another Virtual Camera, then the blend weight is applied to the Post Process effects also. Before attaching post processing profiles to Virtual Cameras, you first need to set up your project to use post processing. To set up project to use Post Processing V2 with Cinemachine: Install the Postprocessing V2 package. Select your Unity camera with Cinemachine Brain in the Scene view. Add the component named Post-Process Layer. This will enable Post Process profiles to affect the Camera. To add a Post Process profile to a Virtual Camera Select your Virtual Camera in the Scene view or Hierarchy window. In the Inspector, choose Add Extension > CinemachinePostProcessing, then configre the Profile asset to have the effects you want when this virtual camera is live. Properties: Property: Function: Profile The Post-Processing profile to activate when this Virtual Camera is live. Focus Tracks Target This is obsolete, please use Focus Tracking. Focus Tracking If the profile has the appropriate overrides, will set the base focus distance to be the distance from the selected target to the camera. The Focus Offset field will then modify that distance. None No focus tracking Look At Target Focus offset is relative to the LookAt target Follow Target Focus offset is relative to the Follow target Custom Target Focus offset is relative to the Custom target Camera Focus offset is relative to the camera Focus Target The target to use if Focus Tracks Target is set to Custom Target Focus Offset Used when Focus Tracking is not None. Offsets the sharpest point away from the location of the focus target."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineRecomposer.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineRecomposer.html",
    "title": "Cinemachine Recomposer Extension | FSM Unity Framework",
    "keywords": "Cinemachine Recomposer Extension Use the Cinemachine Recomposer extension is an add-on module for Cinemachine Virtual Camera that adds a final tweak to the camera composition. It is intended for use in a Timeline context, where you want to hand-adjust the output of procedural or recorded camera aiming. All of these properties can be animated within the Timeline. Properties: Property: Function: Apply After The Volume Settings profile to activate when this Virtual Camera is live. Body Camera has been positioned but not yet rotated Aim Camera has been rotated and positioned, but no noise or collision resolution applied Noise Camera has been positioned, rotated, and noise and other corrections applied Finalize Default setting. Applied after all standard virtual camera processing has occurred Tilt Add a vertical rotation to the camera's current rotation Pan Add a horizontal rotation to the camera's current rotation Dutch Add a tilt (local Z rotation) to the current camera's rotation Zoom Scale Scale the current zoom Follow Attachment When this is less than 1, damping on the Follow target is increased. When the value is zero, damping is infinite - effectively \"letting go\" of the target Look At Attachment When this is less than 1, damping on the Look At target is increased. When the value is zero, damping is infinite - effectively \"letting go\" of the target"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSavingDuringPlay.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSavingDuringPlay.html",
    "title": "Saving in Play Mode | FSM Unity Framework",
    "keywords": "Saving in Play Mode It’s often most convenient to adjust camera settings while the game is playing. But normally, Unity does not save your changes to the Scene when you exit Play Mode. Cinemachine has a special feature to preserve the tweaks you make during Play Mode. It doesn’t save structural changes, like adding or removing a behavior. With the exception of certain properties, Cinemachine preserves most of the settings in your Virtual Cameras when you exit Play Mode. When you exit Play Mode, Cinemachine scans the Scene to collect any changed properties in the Virtual Cameras. Cinemachine saves these changes a second or so after exiting. Use the Edit > Undo command to revert these changes. Check Save During Play on any Virtual Camera in the Inspector to enable this feature. This is a global property, not per-camera, so you only need to check or uncheck it once. Cinemachine components have the special attribute [SaveDuringPlay] to enable this functionality. Feel free to use it on classes within your own custom scripts too if you need it. To exclude a field in a class with the [SaveDuringPlay] attribute, add the [NoSaveDuringPlay] attribute to the field."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSetUpVCam.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSetUpVCam.html",
    "title": "Setting up Virtual Cameras | FSM Unity Framework",
    "keywords": "Setting up Virtual Cameras In your project, organize your Scene Hierarchy to have a single Unity camera with a CinemachineBrain component and many Virtual Cameras. To add a Virtual Camera to a Scene: In the Unity menu, choose GameObject > Cinemachine > Virtual Camera. Unity adds a new GameObject with a Cinemachine Virtual Camera component. If necessary, Unity also adds a Cinemachine Brain component to the Unity camera GameObject for you. Use the Follow property to specify a GameObject to follow. The Virtual Camera automatically positions the Unity camera relative to this GameObject at all times, even as you move it in the Scene. Use the Look At property to specify the GameObject that the Virtual Camera should aim at. The Virtual Camera automatically rotates the Unity camera to face this GameObject at all times, even as you move it in the Scene. Customize the Virtual Camera as needed. Choose the algorithm for following and looking at, and adjust settings such as the follow offset, the follow damping, the screen composition, and the damping used when re-aiming the camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSmoothPath.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineSmoothPath.html",
    "title": "Cinemachine Smooth Path | FSM Unity Framework",
    "keywords": "Cinemachine Smooth Path A Cinemachine Smooth Path is a component that defines a world-space path, consisting of an array of waypoints. Each waypoint has position and roll settings. Cinemachine uses Bezier interpolation to calculate positions between the waypoints to get a smooth and continuous path. The path passes through all waypoints. Unlike Cinemachine Path, first and second order continuity is guaranteed, which means that not only the positions but also the angular velocities of objects animated along the path will be smooth and continuous. Properties: Property: Function: Resolution Path samples per waypoint. Cinemachine uses this value to calculate path distances. Looped If checked, then the path ends are joined to form a continuous loop. Resolution Path samples per waypoint. This is used for calculating path distances. Appearance The settings that control how the path appears in the Scene view. Path Color The color of the path when it is selected. Inactive Path Color The color of the path when it is not selected. Width The width of the railroad tracks that represent the path. Looped Check to join the ends of the path to form a continuous loop. Path Length The length of the path in distance units (read-only). Selected Waypoint Properties for the waypoint you selected in the Scene view or in the Waypoints list. Waypoints The list of waypoints that define the path. They are interpolated using a bezier curve. Position Position in path-local space. Roll Defines the roll of the path at this waypoint. The other orientation axes are inferred from the tangent and world up."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineStateDrivenCamera.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineStateDrivenCamera.html",
    "title": "Cinemachine State-Driven Camera | FSM Unity Framework",
    "keywords": "Cinemachine State-Driven Camera The Cinemachine State-Driven Camera component activates a child Virtual Camera when an animation target changes states. For example, consider your avatar’s local-motion system and orbit camera. Your game feels more alive to the player when the camera shakes more as your avatar runs. When the avatar walks, blend for example to a Virtual Camera with more damping. The animation target for a State-Driven Camera is a GameObject with an Animator component controlled by an Animator Controller. Assign normal Look At and Follow targets to each child Virtual Camera. If a child Virtual Camera has no Look At or Follow target, State-Driven camera uses its own, respective targets. State-Driven Camera has a list that assigns child Virtual Cameras to animation states. You can define default and custom blends between the State-Driven children. In the Inspector, the State-Driven camera lists its Virtual Camera children. Use this list to add and delete child Virtual Cameras, and assign priorities. To create a State-Driven camera: Set up the animation target GameObject to control it with an Animator Controller. In the Unity menu, choose GameObject > Cinemachine > State-Driven Camera. A new State-Driven camera appears in the hierarchy with a new child Virtual Camera. In the Inspector, assign the animation target you created in step 1 to the Animated Target property. If needed, add more child VIrtual Cameras either by clicking + in Virtual Camera Children or dragging and dropping existing Virtual Cameras in the Hierarchy window. Use the State to assign child Virtual Cameras to the animation states. Properties: Property: Function: Solo Toggles whether or not the Virtual Camera is temporarily live. Use this property to get immediate visual feedback in the Game view to adjust the Virtual Camera. Standby Update Controls how often the virtual camera is updated when the virtual camera is not live. Game Window Guides Toggles the visibility of compositional guides in the Game view. These guides are available when Look At specifies a GameObject and the Aim section uses Composer or Group Composer, or when Follow specifies a target and the Body section uses Framing Composer. This property applies to all Virtual Cameras. Save During Play Check to apply the changes while in Play mode. Use this feature to fine-tune a Virtual Camera without having to remember which properties to copy and paste. This property applies to all Virtual Cameras. Priority The importance of this State-Driven camera for choosing the next shot. A higher value indicates a higher priority. This property has no effect when using a State-Driven camera with Timeline. Look At The default target GameObject that the children Virtual Camera move with. The State-Driven camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Follow The default target GameObject to aim the Unity camera at. The State-Driven camera uses this target when the child does not specify this target. May be empty if all of the children define targets of their own. Animated Target The GameObject that contains the Animator Controller. The State-Drive camera reacts to the animation state changes from this GameObject. Layer The animation layer to observe in the Animated Target. Show Debug Text Check to display a textual summary of the live Virtual Camera and blend in the game view. Default Blend The blend which is used if you don’t explicitly define a blend between two Virtual Cameras. Custom Blends The asset which contains custom settings for specific child blends. State The state to which the virtual camera will be mapped. Camera The virtual camera to activate for this state. Wait The delay to activate the virtual camera (in seconds) once this state is entered. For example, the animation target moves from an active Walk state, to a Jog state, to a Run state. If the Jog Wait time is set to four seconds, the Walk virtual camera remains active for those four seconds even though the Jog state is now active. If the Jog state duration is less than the Wait time before the animation target passes into another state, the Jog virtual camera will not activate and is bypassed. Min The minimum length of time (in seconds) the virtual camera must remain active once it is activated. For example, the animation target enters the Run state from the Jog state. It has spent five seconds in the Jog state before moving to the Run state. The Jog Min is set to 12 seconds. This means that the Jog virtual camera remains the active camera for an additional seven seconds even though the animation target is in the Run state. Virtual Camera Children The list of Virtual Cameras that are children of the State-Driven camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineStoryboard.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineStoryboard.html",
    "title": "Storyboard | FSM Unity Framework",
    "keywords": "Storyboard Use the Cinemachine Storyboard extension to let artists, producers, and directors contribute to your game development. Cinemachine Storyboard places a still image in screen space over the output of the Unity camera. Storyboard simplifies animatics for your team. Start with still images to pre-visualize terrain, layout, movement, lighting, timing, and so on. Following the intentions of the Storyboard image, developers incrementally add the assets, GameObjects, and settings that implement the Scene. Use the properties in the Storyboard component to hide and show the image to compare it to the actual rendering of the Unity camera. Storyboards can be muted at a global level - completely disabling them. To toggle this, go to the Cinemachine Storyboard component and enable Storyboard Global Mute. Properties: Property: Function: Storyboard Global Mute When enabled, all storyboards are globally muted. Show Image Toggle the visibility of the storyboard image. Image The image to display as an overlay over the output of the Virtual Camera. Aspect How to handle differences between image aspect and screen aspect. Best Fit Resize the image as large as possible on the screen without cropping. Preserve the vertical and horizontal proportions. Crop Image To Fit Resize the image to fill the screen, cropping if necessary. Preserve the vertical and horizontal proportions. Stretch To Fit Resize the image to fill the screen, adjusting the vertical or horizontal width if necessary. Alpha The opacity of the image. Use 0 for transparent, 1 for opaque. Center The screen-space position of the image. Use 0 for center. Rotation The screen-space rotation of the image. Scale The screen-space scaling of the image. Sync Scale Check to synchronize the scale of the x and y axes. Mute Camera Check to prevent the Virtual Camera from updating the position, rotation, or scale of the Unity camera. Use this feature to prevent Timeline from blending the camera to an unintended position in the Scene. Split View Wipe the image on and off horizontally. Waveform Monitor Opens the Waveform monitor window. This is very performance-intensive, use with care."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTargetGroup.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTargetGroup.html",
    "title": "Cinemachine Target Group | FSM Unity Framework",
    "keywords": "Cinemachine Target Group Use Cinemachine Target Group to treat multiple GameObjects as a single Look At target. Use a Target Group with the Group Composer algorithm. To create a Virtual Camera with a Target Group: In the Unity menu, choose GameObject > Cinemachine > Target Group Camera. Unity adds a new Virtual Camera and Target Group to the Scene. The Follow and Look At targets in the Virtual Camera refer to the new Target Group. In the Hierarchy, select the new Target Group object. In the Inspector, click the + sign to add a new item to the group. In the new item, assign a GameObject (you can drag and drop from the Hierarchy), and edit the Weight and Radius properties. To add more GameObjects to the Target Group, repeat steps 3-4. Properties: Property: Function: Position Mode How to calculate the position of the Target Group. Group Center Use the center of the axis-aligned bounding box that contains all items of the Target Group. Group Average Use the weighted average of the positions of the items in the Target Group. Rotation Mode How to calculate the rotation of the Target Group. Manual Use the values specified in the Rotation properties of the Target Group’s transform. This is the recommended setting. Group Average Weighted average of the orientation of the items in the Target Group. Update Method When to update the transform of the Target Group. Update Update in the normal MonoBehaviour Update() method. Fixed Update Updated in sync with the Physics module, in FixedUpdate(). Late Update Updated in MonoBehaviour LateUpdate(). Targets The list of target GameObjects. Weight How much weight to give the item when averaging. Cannot be negative. Radius The radius of the item, used for calculating the bounding box. Cannot be negative."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTimeline.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTimeline.html",
    "title": "Cinemachine and Timeline | FSM Unity Framework",
    "keywords": "Cinemachine and Timeline Use Timeline to activate, deactivate, and blend between Virtual Cameras. In Timeline, combine Cinemachine with other GameObjects and assets to interactively implement and tune rich cutscenes, even interactive ones. Tip: For simple shot sequences, use a Cinemachine Blend List Camera instead of Timeline. Timeline overrides the priority-based decisions made by Cinemachine Brain. When the timeline finishes, control returns to the Cinemachine Brain, which chooses the Virtual Camera with the highest Priority setting. You control Virtual Cameras in Timeline with a Cinemachine Shot Clip. Each shot clip points to a Virtual Camera to activate then deactivate. Use a sequence of shot clips to specify the order and duration of each shot. To cut between two Virtual Cameras, place the clips next to each other. To blend between two Virtual Cameras, overlap the clips. To create a Timeline for Cinemachine: Create an empty GameObject in your Scene by choosing the __GameObject > Create Empty __menu item. Give the empty GameObject a descriptive name. For example, IntroTimeline. In your Scene, select your empty Timeline object as the focus to create a Timeline Asset and instance. Click the padlock button to lock the TImeline window to make it easier to add and adjust tracks. Drag a Unity camera with a CinemachineBrain component onto the Timeline Editor, then choose Create Cinemachine Track from the drop-down menu. Add other tracks to the Timeline for controlling the subjects of your Scene. For example, add an Animation track to animate your main character. Tip: Delete the default track that refers to your Timeline object. This track isn’t necessary for Timeline. For example, in the Timeline editor, right-click the track for IntroTimeline and choose Delete. To add Cinemachine Shot Clips to a Cinemachine Track: In the Cinemachine Track, right-click and choose Add Cinemachine Shot Clip. Do one of the following: To add an existing Virtual Camera to the shot clip, drag and drop it onto the Virtual Camera property in the Cinemachine Shot component. To create a new Virtual Camera and add it to the shot clip, click Create in the Cinemachine Shot component. In the Timeline editor, adjust the order, duration, cutting, and blending of the shot clip. Adjust the properties of the Virtual Camera to place it in the Scene and specify what to aim at or follow. To animate properties of the Virtual Camera, create an Animation Track for it and animate as you would any other GameObject. Organize your Timeline tracks to fine-tune your Scene."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTopDown.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineTopDown.html",
    "title": "Top-down games | FSM Unity Framework",
    "keywords": "Top-down games Cinemachine Virtual Cameras are modelled after human camera operators and how they operate real-life cameras. As such, they have a sensitivity to the up/down axis, and always try to avoid introducing roll into the camera framing. Because of this sensitivity, the Virtual Camera avoids looking straight up or down for extended periods. They may do it in passing, but if the Look At target is straight up or down for extended periods, they will not always give the desired result. Tip: You can deliberately roll by animating properties like Dutch in a Virtual Camera. If you are building a top-down game where the cameras look straight down, the best practice is to redefine the up direction, for the purposes of the camera. You do this by setting the World Up Override in the Cinemachine Brain to a GameObject whose local up points in the direction that you want the Virtual Camera’s up to normally be. This is applied to all Virtual Cameras controlled by that Cinemachine Brain."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineUsing.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineUsing.html",
    "title": "Using Cinemachine | FSM Unity Framework",
    "keywords": "Using Cinemachine Using Cinemachine requires a new way of thinking about working with cameras. For example, you might have invested heavily in carefully scripted camera behaviors. However, Cinemachine can give the same results, if not better, in less time. Virtual Cameras Cinemachine does not create new cameras. Instead, it directs a single Unity camera for multiple shots. You compose these shots with Virtual Cameras. Virtual Cameras move and rotate the Unity camera and control its settings. The Virtual Cameras are separate GameObjects from the Unity Camera, and behave independently. They are not nested within each other. For example, a Scene might look like this: The main tasks that the Virtual Camera does for you: Positions the Unity camera in the Scene. Aims the Unity camera at something. Adds procedural noise to the Unity camera. Noise simulates things like handheld effects or vehicle shakes. Cinemachine encourages you to create many Virtual Cameras. The Virtual Camera is designed to consume little processing power. If your Scene is performance-sensitive, deactivate all but the essential Virtual Cameras at any given moment for best performance. It is recommended that you use a single Virtual Camera for a single shot. Take advantage of this to create dramatic or subtle cuts or blends. Examples: For a cutscene where two characters exchange dialog, use three Virtual Cameras: one camera for a mid-shot of both characters, and separate Virtual Cameras for a close-up of each character. Use Timeline to synchronize audio with the Virtual Cameras. Duplicate an existing Virtual Camera so that both Virtual Cameras are in the same position in the Scene. For the second Virtual Camera, change the FOV or composition. When a player enters a trigger volume, Cinemachine blends from the first to the second Virtual Camera to emphasize a change in action. One Virtual Camera has control of the Unity camera at any point in time. This is the live Virtual Camera. The exception to this rule is when a blend occurs from one Virtual Camera to the next. During the blend, both Virtual Cameras are live. Cinemachine Brain The Cinemachine Brain is a component in the Unity Camera itself. The Cinemachine Brain monitors all active Virtual Cameras in the Scene. To specify the next live Virtual Camera, you activate or deactivate the desired Virtual Camera's game object. Cinemachine Brain then chooses the most recently activated Virtual Camera with the same or higher priority as the live Virtual Camera. It performs a cut or blend between the previous and new Virtual Cameras. Tip: Use Cinemachine Brain to respond to dynamic game events in real time. It allows your game logic to control the camera by manipulating priorities. This is particularly useful for live gameplay, where action isn’t always predictable. Use Timeline to choreograph cameras in predictable situations, like cutscenes. Timeline overrides the Cinemachine Brain priority system to give you precise, to-the-frame camera control. Moving and aiming Use the Body properties in a Virtual Camera to specify how to move it in the Scene. Use the Aim properties to specify how to rotate it. A Virtual Camera has two targets: The Follow target specifies a GameObject for the Virtual Camera to move with. The Look At target specifies the GameObject to aim at. Cinemachine includes a variety of procedural algorithms to control moving and aiming. Each algorithm solves a specific problem, and has properties to customize the algorithm for your specific needs. Cinemachine implements these algorithms as CinemachineComponent objects. Use the CinemachineComponent class to implement a custom moving or aiming behavior. The Body properties offer the following procedural algorithms for moving the Virtual Camera in a Scene: Transposer: Move in a fixed relationship to the Follow target, with optional damping. Do Nothing: Do not move the Virtual Camera. Framing Transposer: Move in a fixed screen-space relationship to the Follow target, with optional damping. Orbital Transposer: Move in a variable relationship to the Follow target, optionally accepting player input. Tracked Dolly: Move along a predefined path. Hard Lock to Target: Use the same position at the Follow target. The Aim properties offer the following procedural algorithms for rotating a Virtual Camera to face the Look At target: Composer: Keep the Look At target in the camera frame, with compositional constraints. Group Composer: Keep multiple Look At targets in the camera frame. Do Nothing: Do not rotate the Virtual Camera. POV: Rotate the Virtual Camera based on the user’s input. Same As Follow Target: Set the camera’s rotation to the rotation of the Follow target. Hard Look At: Keep the Look At target in the center of the camera frame. Composing a shot The Framing Transposer, Composer, and Group Composer algorithms define areas in the camera frame for you to compose a shot: Dead zone: The area of the frame that Cinemachine keeps the target in. Soft zone: If the target enters this region of the frame, the camera will re-orient to put it back in the dead zone. It will do this slowly or quickly, according to the time specified in the Damping settings. Screen: The screen position of the center of the dead zone. 0.5 is the center of the screen. Damping: Simulates the lag that a real camera operator introduces while operating a heavy physical camera. Damping specifies quickly or slowly the camera reacts when the target enters the soft zone while the camera tracks the target. Use small numbers to simulate a more responsive camera, rapidly moving or aiming the camera to keep the target in the dead zone. Larger numbers simulate heavier cameras, The larger the value, the more Cinemachine allows the target to enter the soft zone. The Game Window Guides gives an interactive, visual indication of these areas. The guides appear as tinted areas in the Game view. The clear area indicates the dead zone. The blue-tinted area indicates the soft zone. The position of the soft and dead zones indicates the screen position. The red-tinted area indicates the no pass area, which the target never enters. The yellow square indicates the target. Adjust these areas to get a wide range of camera behaviors. To do this, drag their edges in the Game view or edit their properties in the Inspector window. For example, use larger damping values to simulate a larger, heavier camera, or enlarge the soft zone and dead zone to create an area in the middle of the camera frame that is immune to target motion. Use this feature for things like animation cycles, where you don’t want the camera to track the target if it moves just a little. Using noise to simulate camera shake Real-world physical cameras are often heavy and cumbersome. They are hand-held by the camera operator or mounted on unstable objects like moving vehicles. Use Noise properties to simulate these real-world qualities for cinematic effect. For example, you could add a camera shake when following a running character to immerse the player in the action. At each frame update, Cinemachine adds noise separately from the movement of the camera to follow a target. Noise does not influence the camera’s position in future frames. This separation ensures that properties like damping behave as expected."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCamera.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCamera.html",
    "title": "Setting Virtual Camera properties | FSM Unity Framework",
    "keywords": "Setting Virtual Camera properties The Cinemacine Virtual Camera is a component that you add to an empty GameObject. It represents a Virtual Camera in the Unity Scene. Use the Aim, Body, and Noise properties to specify how the Virtual Camera animates position, rotation, and other properties. The Virtual Camera applies these settings to the Unity Camera when Cinemachine Brain or Timeline transfers control of the Unity camera to the Virtual Camera. You can also use Scene Handles to modify some common properties. At any time, each Virtual Camera may be in one of these states: Live: The Virtual Camera actively controls a Unity camera that has a Cinemachine Brain. When a Cinemachine Brain blends from one Virtual Camera to the next, both Virtual Cameras are live. When the blend is complete, there is only one live Virtual Camera. Standby: The Virtual Camera doesn’t control the Unity camera. However, it still follows and aims at its targets, and updates at every frame. A Virtual Camera in this state is activated and has a priority that is the same as or lower than the live Virtual Camera. Disabled: The Virtual Camera doesn’t control the Unity camera and doesn’t actively follow or aim at its targets. A Virtual Camera in this state doesn’t consume processing power. To disable a Virtual Camera, deactivate its game object. The Virtual Camera is present but disabled in the Scene. However, even though the game object is deactivated, the virtual camera can still control the Unity camera if the virtual camera is participating in a blend, or if it is invoked by Timeline. Properties Property: Function: Solo Toggles whether or not the Virtual Camera is temporarily live. Use this property to get immediate visual feedback in the Game view to adjust the Virtual Camera. Game Window Guides Toggles the visibility of compositional guides in the Game view. These guides are available when Look At specifies a GameObject and the Aim section uses Composer or Group Composer, or when Follow specifies a target and the Body section uses Framing Composer. This property applies to all Virtual Cameras. Save During Play Check to apply the changes while in Play mode. Use this feature to fine-tune a Virtual Camera without having to remember which properties to copy and paste. This property applies to all Virtual Cameras. Priority The importance of this Virtual Camera for choosing the next shot. A higher value indicates a higher priority. Cinemachine Brain chooses the next live Virtual Camera from all Virtual Cameras that are activated and have the same or higher priority as the current live Virtual Camera. This property has no effect when using a Virtual Camera with Timeline. Follow The target GameObject that the Virtual Camera moves with. The Body properties use this target to update the position of the Unity camera. Keep this property empty to make the Unity camera use the position of the Virtual Camera’ transform. For example, you might choose to animate the Virtual Camera in Timeline. Look At The target GameObject to aim the Unity camera at. The Aim properties use this target to update the rotation of the Unity camera. Keep this property empty to make the Unity camera use the orientation of the Virtual Camera. Standby Update Controls how often the virtual camera is updated when the virtual camera is not live. Position Blending Style for blending positions to and from this Virtual Camera. Linear Standard linear position blend. Spherical Spherical blend about the Look At position, if there is a Look At target. Cylindrical Cylindrical blend about the Look At position, if there is a Look At target. Vertical coordinate is linearly interpolated. Lens These properties mirror their counterparts in the properties for the Unity camera. Field Of View The camera view in vertical degrees. For example, to specify the equivalent of a 50mm lens on a Super 35 sensor, enter a Field of View of 19.6 degrees. This property is available when the Unity camera with the Cinemachine Brain component uses a Projection of Perspective. You can also use Scene Handles to modify this property. Presets A drop-down menu of settings for commonly-used lenses. Choose Edit Presets to add or edit the asset that contains a default list of lenses. Orthographic Size When using an orthographic camera, defines the half-height of the camera view, in world coordinates. Available when the Unity camera with the Cinemachine Brain component uses a Projection of Orthographic. Near Clip Plane The closest point relative to the camera where drawing occurs. You can also use Scene Handles to modify this property. Far Clip Plane The furthest point relative to the camera where drawing occurs. You can also use Scene Handles to modify this property. Dutch Dutch angle. Tilts the Unity camera on the z-axis, in degrees. This property is unique to the Virtual Camera; there is no counterpart property in the Unity camera. Mode Override Allows you to select a different camera mode to apply to the Unity camera component when Cinemachine activates this Virtual Camera. Important: All the changes applied to the Camera component through this setting will remain after the Virtual Camera deactivation. If you set a mode override in any Virtual Camera, you should set one in all Virtual Cameras. None Leaves the Projection and Physical Camera properties unchanged in the Camera. Orthographic Sets the Projection property to Orthographic. Perspective Sets the Projection property to Perspective and disables the Physical Camera feature and properties. Physical Sets the Projection property to Perspective and enables the Physical Camera feature and properties. Blend Hint Provides hints for blending positions to and from the virtual camera. Inherit Position When enabled, whenever this virtual camera goes live, forces the initial position to be the same as the current position of the Unity Camera, if possible. Extensions Components that add extra behaviors to the Virtual Camera. Add Extension Choose a new extension to add to the Virtual Camera."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraAim.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraAim.html",
    "title": "Aim properties | FSM Unity Framework",
    "keywords": "Aim properties Use the Aim properties to specify how to rotate the Virtual Camera. To change the camera’s position, use the Body properties. Do Nothing: Do not procedurally rotate the Virtual Camera. Composer: Keep the Look At target in the camera frame. Group Composer: Keep multiple Look At targets in the camera frame. Hard Look At: Keep the Look At target in the center of the camera frame. POV: Rotate the Virtual Camera based on the user’s input. Same As Follow Target: Set the camera’s rotation to the rotation of the Follow target."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraBody.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraBody.html",
    "title": "Body properties | FSM Unity Framework",
    "keywords": "Body properties Use the Body properties to specify the algorithm that moves the Virtual Camera in the Scene. To rotate the camera, set the Aim properties. Cinemachine includes these algorithms for moving a Virtual Camera: Do Nothing: does not move the Virtual Camera. 3rd Person follow: Pivots the camera horizontally and vertically around the player, to the Follow target. Framing Transposer: moves in a fixed screen-space relationship to the Follow target. Hard Lock to Target: uses the same position at the Follow target. Orbital Transposer: moves in a variable relationship to the Follow target, optionally accepting player input. Tracked Dolly: moves along a predefined path. Transposer: moves in a fixed relationship to the Follow target."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraExtensions.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraExtensions.html",
    "title": "Extensions | FSM Unity Framework",
    "keywords": "Extensions Extensions are components that augment the behavior of a Virtual Camera. For example, the Collider extension moves a camera out of the way of GameObjects that obstruct the camera’s view of its target. Cinemachine includes a variety of extensions. Create your own custom extensions by deriving from the CinemachineExtension class. To add an extension to a Virtual Camera: Select your Virtual Camera in the Scene view or Hierarchy window. In the Inspector, use the Add Extension drop-down menu to choose the extension."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraNoise.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVirtualCameraNoise.html",
    "title": "Noise properties | FSM Unity Framework",
    "keywords": "Noise properties Use Noise properties in a Virtual Camera to simulate camera shake. Cinemachine includes a Basic Multi Channel Perlin component, which adds Perlin noise to the movement of the Virtual Camera. Perlin noise is a technique to compute random movement with a natural behavior. The Basic Multi Channel Perlin component applies a noise profile. A noise profile is an Asset that defines the behavior of noise over time. Cinemachine includes a few noise profile assets. You can edit these and create your own. To apply noise: Select your Virtual Camera in the Scene view or Hierarchy window. In the Inspector, use the Noise drop-down menu to choose Basic Multi Channel Perlin. In Noise Profile, choose an existing profile asset or create your own profile. Use Amplitude Gain and Frequency Gain to fine-tune the noise. Properties Property: Function: Noise Profile The noise profile asset to use. Amplitude Gain Gain to apply to the amplitudes defined in the noise profile. Use 1 to use the amplitudes defined in the noise profile. Setting this to 0 mutes the noise. Tip: Animate this property to ramp the noise effect up and down. Frequency Gain Factor to apply to the frequencies defined in the noise profile. Use 1 to use the frequencies defined in the noise profile. Use larger values to shake the camera more rapidly. Tip: Animate this property to ramp the noise effect up and down. Pivot Offset When rotating the camera, offset the camera's pivot by the indicated x, y, and z distance when applying rotational noise. This generates some positional variation that corresponds to the rotation noise."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVolumeSettings.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/CinemachineVolumeSettings.html",
    "title": "Cinemachine Volume Settings Extension | FSM Unity Framework",
    "keywords": "Cinemachine Volume Settings Extension Use the Cinemachine Volume Settings extension to attach an HDRP/URP VolumeSettings profile to a Virtual Camera. The Cinemachine Volume Settings extension holds a Volume Settings Profile asset to apply to a Virtual Camera when it is activated. If the camera is blending with another Virtual Camera, then the blend weight is applied to the Volume Settings effects also. To add a Volume Settings profile to a Virtual Camera Select your Virtual Camera in the Scene view or Hierarchy window. In the Inspector, choose Add Extension > CinemachineVolumeSettings, then configure the Profile asset to have the effects you want when this virtual camera is live. Properties: Property: Function: Profile The Volume Settings profile to activate when this Virtual Camera is live. Focus Tracks Target This is obsolete, please use Focus Tracking. Focus Tracking If the profile has the appropriate overrides, will set the base focus distance to be the distance from the selected target to the camera. The Focus Offset field will then modify that distance. None No focus tracking Look At Target Focus offset is relative to the LookAt target Follow Target Focus offset is relative to the Follow target Custom Target Focus offset is relative to the Custom target Camera Focus offset is relative to the camera Focus Target The target to use if Focus Tracks Target is set to Custom Target Focus Offset Used when Focus Tracking is not None. Offsets the sharpest point away from the location of the focus target."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Cinemachine Using Cinemachine Using Virtual Cameras Setting Virtual Camera properties Scene Handles Body properties Do Nothing 3rd Person Follow Framing Transposer Hard Lock to Target Orbital Transposer Tracked Dolly Transposer Binding Modes Aim properties Composer Group Composer Do Nothing POV Same As Follow Target Hard Look At Noise properties Working with noise profiles Blending between Virtual Cameras Managing and grouping Virtual Cameras Cinemachine Free Look Cinemachine Mixing Camera Cinemachine Blend List Camera Cinemachine Clear Shot Cinemachine State-Driven camera Virtual Camera extensions Extensions for avoiding collisions and evaluating shots Cinemachine Collider Cinemachine Confiner Cinemachine Confiner2D Follow Zoom extension Pixel Perfect extension Post Processing extension Volume Settings extension Storyboard extension 3rd Person Aim extension Recomposer extension Saving in Play Mode Using dolly paths Cinemachine Path Cinemachine Smooth Path Cinemachine Dolly Cart Multiple Unity cameras Cinemachine External Camera Cinemachine Target Group Cinemachine Brain Cinemachine and Timeline Cinemachine and 2D graphics Cinemachine and top-down games Alternative Input Systems Cinemachine Impulse Cinemachine Impulse Sources Cinemachine Collision Impulse Source Cinemachine Impulse Source Cinemachine Impulse Listener Filtering impulses"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/cinemachine.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/cinemachine.html",
    "title": "Cinemachine | FSM Unity Framework",
    "keywords": "Cinemachine Cinemachine is a suite of modules for operating the Unity camera. Cinemachine solves the complex mathematics and logic of tracking targets, composing, blending, and cutting between shots. It's designed to significantly reduce the number of time-consuming manual manipulations and script revisions that take place during development. The procedural nature of these modules makes Cinemachine bug-resistant. When you make adjustments—for example, change an animation, vehicle speed, terrain, or other GameObjects in your Scene—Cinemachine dynamically adjusts its behavior to make the best shot. There is no need, for example, to re-write camera scripts just because a character turns left instead of right. Cinemachine works in real time across all genres including FPS, third person, 2D, side-scroller, top down, and RTS. It supports as many shots in your Scene as you need. Its modular system lets you compose sophisticated behaviors. Cinemachine works well with other Unity tools, acting as a powerful complement to Timeline, animation, and post-processing assets. Create your own extensions or integrate it with your custom camera scripts. Requirements Cinemachine has no external dependencies. Just install it and start using it. This version of Cinemachine is compatible with the following versions of the Unity Editor: 2018.4.17f1 and later"
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/cinemachineAPI.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/cinemachineAPI.html",
    "title": "| FSM Unity Framework",
    "keywords": "AxisState Type: struct Namespace: Cinemachine Axis state for defining how to react to player input. The settings here control the responsiveness of the axis to player input. Fields Name Type Description Value Single The current value of the axis. m_MaxSpeed Single The maximum speed of this axis in units/second. m_AccelTime Single The amount of time in seconds it takes to accelerate to MaxSpeed with the supplied Axis at its maximum value. m_DecelTime Single The amount of time in seconds it takes to decelerate the axis to zero if the supplied axis is in a neutral position. m_InputAxisName String The name of this axis as specified in Unity Input manager. Setting to an empty string will disable the automatic updating of this axis. m_InputAxisValue Single The value of the input axis. A value of 0 means no input. You can drive this directly from a custom input system, or you can set the Axis Name and have the value driven by the internal Input Manager. m_InvertInput Boolean If checked, then the raw value of the input axis will be inverted before it is used. m_MinValue Single The minimum value for the axis. m_MaxValue Single The maximum value for the axis. m_Wrap Boolean If checked, then the axis will wrap around at the min/max values, forming a loop. Methods Void Validate() Call from OnValidate: Make sure the fields are sensible. Boolean Update(Single deltaTime) Updates the state of this axis based on the axis defined by AxisState.m_AxisName. Param Type Description deltaTime Single Delta time in seconds. Returns: Returns true if this axis' input was non-zero this Update, false otherwise. AxisState.Recentering Type: struct Namespace: Cinemachine Helper for automatic axis recentering. Fields Name Type Description m_enabled Boolean If checked, will enable automatic recentering of the axis. If unchecked, recenting is disabled. m_WaitTime Single If no user input has been detected on the axis, the axis will wait this long in seconds before recentering. m_RecenteringTime Single Maximum angular speed of recentering. Will accelerate into and decelerate out of this. Methods Void Validate() Call this from OnValidate(). Void CancelRecentering() Cancel any recentering in progress. Void DoRecentering(AxisState& axis, Single deltaTime, Single recenterTarget) Bring the axis back to the centered state. Param Type Description axis AxisState& deltaTime Single recenterTarget Single CinemachineBasicMultiChannelPerlin Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase As a part of the Cinemachine Pipeline implementing the Noise stage, this component adds Perlin Noise to the Camera state, in the Correction channel of the CameraState. The noise is created by using a predefined noise profile asset. This defines the shape of the noise over time. You can scale this in amplitude or in time, to produce a large family of different noises using the same profile. Properties Name Type Description IsValid Boolean [Get] True if the component is valid, i.e. it has a noise definition and is enabled. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Noise stage. Possible Values: - Body - Aim - Noise - Finalize Fields Name Type Description m_NoiseProfile NoiseSettings The asset containing the Noise Profile. Define the frequencies and amplitudes there to make a characteristic noise profile. Make your own or just use one of the many presets. m_AmplitudeGain Single Gain to apply to the amplitudes defined in the NoiseSettings asset. 1 is normal. Setting this to 0 completely mutes the noise. m_FrequencyGain Single Scale factor to apply to the frequencies defined in the NoiseSettings asset. 1 is normal. Larger magnitudes will make the noise shake more rapidly. Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies noise to the Correction channel of the CameraState if the delta time is greater than 0. Otherwise, does nothing. Param Type Description curState CameraState& The current camera state. deltaTime Single How much to advance the perlin noise generator. Noise is only applied if this value is greater than or equal to 0. CinemachineBlendDefinition Type: struct Namespace: Cinemachine Definition of a Camera blend. This struct holds the information necessary to generate a suitable AnimationCurve for a Cinemachine Blend. Properties Name Type Description BlendCurve AnimationCurve [Get] A normalized AnimationCurve specifying the interpolation curve for this camera blend. Y-axis values must be in range [0,1] (internally clamped within Blender) and time must be in range of [0, 1]. Fields Name Type Description m_Style Style Shape of the blend curve. Possible Values: - Cut: Zero-length blend. - EaseInOut: S-shaped curve, giving a gentle and smooth transition. - EaseIn: Linear out of the outgoing shot, and easy into the incoming. - EaseOut: Easy out of the outgoing shot, and linear into the incoming. - HardIn: Easy out of the outgoing, and hard into the incoming. - HardOut: Hard out of the outgoing, and easy into the incoming. - Linear: Linear blend. Mechanical-looking. - Custom: Custom blend curve. m_Time Single Duration of the blend, in seconds. m_CustomCurve AnimationCurve A user-defined AnimationCurve, used only if style is Custom. Curve MUST be normalized, i.e. time range [0...1], value range [0...1]. CinemachineBlenderSettings Type: class Namespace: Cinemachine Inherits: ScriptableObject Asset that defines the rules for blending between Virtual Cameras. Fields Name Type Description m_CustomBlends CustomBlend[] The array containing explicitly defined blends between two Virtual Cameras. Methods AnimationCurve GetBlendCurveForVirtualCameras(String fromCameraName, String toCameraName, AnimationCurve defaultCurve) Attempts to find a blend curve which matches the to and from cameras as specified. If no match is found, the function returns either the default blend for this Blender or NULL depending on the state of returnDefaultOnNoMatch. Param Type Description fromCameraName String The game object name of the from camera. toCameraName String The game object name of the to camera. defaultCurve AnimationCurve Curve to return if no curve found. Can be NULL. CinemachineBlenderSettings.CustomBlend Type: struct Namespace: Cinemachine Container specifying how two specific Cinemachine Virtual Cameras blend together. Fields Name Type Description m_From String When blending from this camera. m_To String When blending to this camera. m_Blend CinemachineBlendDefinition Blend curve definition. CinemachineBlendListCamera Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera This is a virtual camera \"manager\" that owns and manages a collection of child Virtual Cameras. When the camera goes live, these child vcams are enabled, one after another, holding each camera for a designated time. Blends between cameras are specified. The last camera is held indefinitely. Properties Name Type Description Description String [Get] Gets a brief debug description of this virtual camera, for use when displaying debug info. LiveChild ICinemachineCamera [Get,Set] Get the current \"best\" child virtual camera, that would be chosen if the State Driven Camera were active. LiveChildOrSelf ICinemachineCamera [Get] Return the live child. State CameraState [Get] The State of the current live child. LookAt Transform [Get,Set] Get the current LookAt target. Returns parent's LookAt if parent is non-null and no specific LookAt defined for this camera. Follow Transform [Get,Set] Get the current Follow target. Returns parent's Follow if parent is non-null and no specific Follow defined for this camera. ChildCameras CinemachineVirtualCameraBase[] [Get] The list of child cameras. These are just the immediate children in the hierarchy. IsBlending Boolean [Get] Is there a blend in progress? Fields Name Type Description m_LookAt Transform Default object for the camera children to look at (the aim target), if not specified in a child camera. May be empty if all of the children define targets of their own. m_Follow Transform Default object for the camera children wants to move with (the body target), if not specified in a child camera. May be empty if all of the children define targets of their own. m_ShowDebugText Boolean When enabled, the current child camera and blend will be indicated in the game window, for debugging. m_EnableAllChildCameras Boolean Force all child cameras to be enabled. This is useful if animating them in Timeline, but consumes extra resources. m_ChildCameras CinemachineVirtualCameraBase[] Internal API for the editor. Do not use this field. m_Instructions Instruction[] The set of instructions for enabling child cameras. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods virtual Boolean IsLiveChild(ICinemachineCamera vcam) Check whether the vcam a live child of this camera. Param Type Description vcam ICinemachineCamera The Virtual Camera to check. Returns: True if the vcam is currently actively influencing the state of this vcam. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void OnTransitionFromCamera(ICinemachineCamera fromCam, Vector3 worldUp, Single deltaTime) Notification that this virtual camera is going live. Param Type Description fromCam ICinemachineCamera The camera being deactivated. May be null. worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than or equal to 0). virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Called by CinemachineCore at designated update time so the vcam can position itself and track its targets. This implementation updates all the children, chooses the best one, and implements any required blending. Param Type Description worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than or equal to 0). protected virtual Void OnEnable() Makes sure the internal child cache is up to date. Void OnTransformChildrenChanged() Makes sure the internal child cache is up to date. protected virtual Void OnGUI() Displays the current active camera on the game screen, if requested. Void ValidateInstructions() Internal API for the inspector editor. CinemachineBrain Type: class Namespace: Cinemachine Inherits: MonoBehaviour CinemachineBrain is the link between the Unity Camera and the Cinemachine Virtual Cameras in the scene. It monitors the priority stack to choose the current Virtual Camera, and blend with another if necessary. Finally and most importantly, it applies the Virtual Camera state to the attached Unity Camera. The CinemachineBrain is also the place where rules for blending between virtual cameras are defined. Camera blending is an interpolation over time of one virtual camera position and state to another. If you think of virtual cameras as cameramen, then blending is a little like one cameraman smoothly passing the camera to another cameraman. You can specify the time over which to blend, as well as the blend curve shape. Note that a camera cut is just a zero-time blend. Properties Name Type Description OutputCamera Camera [Get] Get the Unity Camera that is attached to this GameObject. This is the camera that will be controlled by the brain. PostProcessingComponent Component [Get,Set] Internal support for opaque post-processing module. SoloCamera ICinemachineCamera (static) [Get,Set] API for the Unity Editor. Show this camera no matter what. This is static, and so affects all Cinemachine brains. DefaultWorldUp Vector3 [Get] Get the default world up for the virtual cameras. IsBlending Boolean [Get] Is there a blend in progress? ActiveBlend CinemachineBlend [Get] Get the current blend in progress. Returns null if none. ActiveVirtualCamera ICinemachineCamera [Get] Get the current active virtual camera. CurrentCameraState CameraState [Get] The current state applied to the unity camera (may be the result of a blend). Fields Name Type Description m_ShowDebugText Boolean When enabled, the current camera and blend will be indicated in the game window, for debugging. m_ShowCameraFrustum Boolean When enabled, the camera's frustum will be shown at all times in the scene view. m_IgnoreTimeScale Boolean When enabled, the cameras will always respond in real-time to user input and damping, even if the game is running in slow motion. m_WorldUpOverride Transform If set, this object's Y axis will define the worldspace Up vector for all the virtual cameras. This is useful for instance in top-down game environments. If not set, Up is worldspace Y. Setting this appropriately is important, because Virtual Cameras don't like looking straight up or straight down. m_UpdateMethod UpdateMethod Use FixedUpdate if all your targets are animated during FixedUpdate (e.g. RigidBodies), LateUpdate if all your targets are animated during the normal Update loop, and SmartUpdate if you want Cinemachine to do the appropriate thing on a per-target basis. SmartUpdate is the recommended setting. Possible Values: - FixedUpdate: Virtual cameras are updated in sync with the Physics module, in FixedUpdate. - LateUpdate: Virtual cameras are updated in MonoBehaviour LateUpdate. - SmartUpdate: Virtual cameras are updated according to how the target is updated. m_DefaultBlend CinemachineBlendDefinition The blend that is used in cases where you haven't explicitly defined a blend between two Virtual Cameras. m_CustomBlends CinemachineBlenderSettings This is the asset that contains custom settings for blends between specific virtual cameras in your scene. m_CameraCutEvent BrainEvent This event will fire whenever a virtual camera goes live and there is no blend. m_CameraActivatedEvent VcamEvent This event will fire whenever a virtual camera goes live. If a blend is involved, then the event will fire on the first frame of the blend. Methods static Color GetSoloGUIColor() API for the Unity Editor. Returns: Color used to indicate that a camera is in Solo mode. Boolean IsLive(ICinemachineCamera vcam) True if the ICinemachineCamera the current active camera, or part of a current blend, either directly or indirectly because its parents are live. Param Type Description vcam ICinemachineCamera The camera to test whether it is live. Returns: True if the camera is live (directly or indirectly) or part of a blend in progress. CinemachineClearShot Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera Cinemachine ClearShot is a \"manager camera\" that owns and manages a set of Virtual Camera gameObject children. When Live, the ClearShot will check the children, and choose the one with the best quality shot and make it Live. This can be a very powerful tool. If the child cameras have CinemachineCollider extensions, they will analyze the scene for target obstructions, optimal target distance, and other items, and report their assessment of shot quality back to the ClearShot parent, who will then choose the best one. You can use this to set up complex multi-camera coverage of a scene, and be assured that a clear shot of the target will always be available. If multiple child cameras have the same shot quality, the one with the highest priority will be chosen. You can also define custom blends between the ClearShot children. Properties Name Type Description Description String [Get] Gets a brief debug description of this virtual camera, for use when displaying debug info. LiveChild ICinemachineCamera [Get,Set] Get the current \"best\" child virtual camera, that would be chosen if the ClearShot camera were active. State CameraState [Get] The CameraState of the currently live child. LiveChildOrSelf ICinemachineCamera [Get] Return the live child. LookAt Transform [Get,Set] Get the current LookAt target. Returns parent's LookAt if parent is non-null and no specific LookAt defined for this camera. Follow Transform [Get,Set] Get the current Follow target. Returns parent's Follow if parent is non-null and no specific Follow defined for this camera. IsBlending Boolean [Get] Is there a blend in progress? ChildCameras CinemachineVirtualCameraBase[] [Get] The list of child cameras. These are just the immediate children in the hierarchy. Fields Name Type Description m_LookAt Transform Default object for the camera children to look at (the aim target), if not specified in a child camera. May be empty if all children specify targets of their own. m_Follow Transform Default object for the camera children wants to move with (the body target), if not specified in a child camera. May be empty if all children specify targets of their own. m_ShowDebugText Boolean When enabled, the current child camera and blend will be indicated in the game window, for debugging. m_ChildCameras CinemachineVirtualCameraBase[] Internal API for the editor. Do not use this filed. m_ActivateAfter Single Wait this many seconds before activating a new child camera. m_MinDuration Single An active camera must be active for at least this many seconds. m_RandomizeChoice Boolean If checked, camera choice will be randomized if multiple cameras are equally desirable. Otherwise, child list order and child camera priority will be used. m_DefaultBlend CinemachineBlendDefinition The blend which is used if you don't explicitly define a blend between two Virtual Cameras. m_CustomBlends CinemachineBlenderSettings This is the asset which contains custom settings for specific blends. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods virtual Boolean IsLiveChild(ICinemachineCamera vcam) Check whether the vcam a live child of this camera. Param Type Description vcam ICinemachineCamera The Virtual Camera to check. Returns: True if the vcam is currently actively influencing the state of this vcam. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Called by CinemachineCore at designated update time so the vcam can position itself and track its targets. This implementation updates all the children, chooses the best one, and implements any required blending. Param Type Description worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than 0). protected virtual Void OnEnable() Makes sure the internal child cache is up to date. Void OnTransformChildrenChanged() Makes sure the internal child cache is up to date. protected virtual Void OnGUI() Displays the current active camera on the game screen, if requested. Void ResetRandomization() If RandomizeChoice is enabled, call this to re-randomize the children next frame. This is useful if you want to freshen up the shot. virtual Void OnTransitionFromCamera(ICinemachineCamera fromCam, Vector3 worldUp, Single deltaTime) Notification that this virtual camera is going live. This implementation resets the child randomization. Param Type Description fromCam ICinemachineCamera The camera being deactivated. May be null. worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than or equal to 0). CinemachineCollider Type: class Namespace: Cinemachine Inherits: CinemachineExtension An add-on module for Cinemachine Virtual Camera that post-processes the final position of the virtual camera. Based on the supplied settings, the Collider will attempt to preserve the line of sight with the LookAt target of the virtual camera by moving away from objects that will obstruct the view. Additionally, the Collider can be used to assess the shot quality and report this as a field in the camera State. Properties Name Type Description DebugPaths List`1 [Get] Inspector API for debugging collision resolution path. Fields Name Type Description m_CollideAgainst LayerMask The Unity layer mask against which the collider will raycast. m_IgnoreTag String Obstacles with this tag will be ignored. It is a good idea to set this field to the target's tag. m_MinimumDistanceFromTarget Single Obstacles closer to the target than this will be ignored. m_AvoidObstacles Boolean When enabled, will attempt to resolve situations where the line of sight to the target is blocked by an obstacle. m_DistanceLimit Single The maximum raycast distance when checking if the line of sight to this camera's target is clear. If the setting is 0 or less, the current actual distance to target will be used. m_CameraRadius Single Camera will try to maintain this distance from any obstacle. Try to keep this value small. Increase it if you are seeing inside obstacles due to a large FOV on the camera. m_Strategy ResolutionStrategy The way in which the Collider will attempt to preserve sight of the target. Possible Values: - PullCameraForward - PreserveCameraHeight - PreserveCameraDistance m_MaximumEffort Int32 Upper limit on how many obstacle hits to process. Higher numbers may impact performance. In most environments, 4 is enough. m_Damping Single The gradualness of collision resolution. Higher numbers will move the camera more gradually away from obstructions. m_OptimalTargetDistance Single If greater than zero, a higher score will be given to shots when the target is closer to this distance. Set this to zero to disable this feature. Methods Boolean IsTargetObscured(ICinemachineCamera vcam) See whether an object is blocking the camera's view of the target. Param Type Description vcam ICinemachineCamera The virtual camera in question. This might be different from the virtual camera that owns the collider, in the event that the camera has children. Returns: True if something is blocking the view. Boolean CameraWasDisplaced(CinemachineVirtualCameraBase vcam) See whether the virtual camera has been moved by the collider. Param Type Description vcam CinemachineVirtualCameraBase The virtual camera in question. This might be different from the virtual camera that owns the collider, in the event that the camera has children. Returns: True if the virtual camera has been displaced due to collision or target obstruction. protected virtual Void OnDestroy() Cleanup. protected virtual Void PostPipelineStageCallback(CinemachineVirtualCameraBase vcam, Stage stage, CameraState& state, Single deltaTime) Callback to to the collision resolution and shot evaluation. Param Type Description vcam CinemachineVirtualCameraBase stage Stage state CameraState& deltaTime Single CinemachineComposer Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to aim the camera at the vcam's LookAt target object, with configurable offsets, damping, and composition rules. The composer does not change the camera's position. It will only pan and tilt the camera where it is, in order to get the desired framing. To move the camera, you have to use the virtual camera's Body section. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a LookAt defined. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Aim stage. Possible Values: - Body - Aim - Noise - Finalize TrackedPoint Vector3 [Get] Internal API for inspector. SoftGuideRect Rect [Get,Set] Internal API for the inspector editor. HardGuideRect Rect [Get,Set] Internal API for the inspector editor. Fields Name Type Description OnGUICallback Action Used by the Inspector Editor to display on-screen guides. m_TrackedObjectOffset Vector3 Target offset from the target object's center in target-local space. Use this to fine-tune the tracking target position when the desired area is not the tracked object's center. m_LookaheadTime Single This setting will instruct the composer to adjust its target offset based on the motion of the target. The composer will look at a point where it estimates the target will be this many seconds into the future. Note that this setting is sensitive to noisy animation, and can amplify the noise, resulting in undesirable camera jitter. If the camera jitters unacceptably when the target is in motion, turn down this setting, or animate the target more smoothly. m_LookaheadSmoothing Single Controls the smoothness of the lookahead algorithm. Larger values smooth out jittery predictions and also increase prediction lag. m_LookaheadIgnoreY Boolean If checked, movement along the Y axis will be ignored for lookahead calculations. m_HorizontalDamping Single How aggressively the camera tries to follow the target in the screen-horizontal direction. Small numbers are more responsive, rapidly orienting the camera to keep the target in the dead zone. Larger numbers give a more heavy slowly responding camera. Using different vertical and horizontal settings can yield a wide range of camera behaviors. m_VerticalDamping Single How aggressively the camera tries to follow the target in the screen-vertical direction. Small numbers are more responsive, rapidly orienting the camera to keep the target in the dead zone. Larger numbers give a more heavy slowly responding camera. Using different vertical and horizontal settings can yield a wide range of camera behaviors. m_ScreenX Single Horizontal screen position for target. The camera will rotate to position the tracked object here. m_ScreenY Single Vertical screen position for target, The camera will rotate to position the tracked object here. m_DeadZoneWidth Single Camera will not rotate horizontally if the target is within this range of the position. m_DeadZoneHeight Single Camera will not rotate vertically if the target is within this range of the position. m_SoftZoneWidth Single When target is within this region, camera will gradually rotate horizontally to re-align towards the desired position, depending on the damping speed. m_SoftZoneHeight Single When target is within this region, camera will gradually rotate vertically to re-align towards the desired position, depending on the damping speed. m_BiasX Single A non-zero bias will move the target position horizontally away from the center of the soft zone. m_BiasY Single A non-zero bias will move the target position vertically away from the center of the soft zone. Methods protected virtual Vector3 GetLookAtPointAndSetTrackedPoint(Vector3 lookAt) Apply the target offsets to the target location. Also set the TrackedPoint property, taking lookahead into account. Param Type Description lookAt Vector3 The unoffset LookAt point. Returns: The LookAt point with the offset applied. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the us that a target got warped, so that we can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void PrePipelineMutateCameraState(CameraState& curState) Param Type Description curState CameraState& virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies the composer rules and orients the camera accordingly. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for calculating damping. If less than zero, then target will snap to the center of the dead zone. CinemachineConfiner Type: class Namespace: Cinemachine Inherits: CinemachineExtension An add-on module for Cinemachine Virtual Camera that post-processes the final position of the virtual camera. It will confine the virtual camera's position to the volume specified in the Bounding Volume field. Properties Name Type Description IsValid Boolean [Get] Check if the bounding volume is defined. Fields Name Type Description m_ConfineMode Mode The confiner can operate using a 2D bounding shape or a 3D bounding volume. Possible Values: - Confine2D - Confine3D m_BoundingVolume Collider The volume within which the camera is to be contained. m_BoundingShape2D Collider2D The 2D shape within which the camera is to be contained. m_ConfineScreenEdges Boolean If camera is orthographic, screen edges will be confined to the volume. If not checked, then only the camera center will be confined. m_Damping Single How gradually to return the camera to the bounding volume if it goes beyond the borders. Higher numbers are more gradual. Methods Boolean CameraWasDisplaced(CinemachineVirtualCameraBase vcam) See whether the virtual camera has been moved by the confiner. Param Type Description vcam CinemachineVirtualCameraBase The virtual camera in question. This might be different from the virtual camera that owns the confiner, in the event that the camera has children. Returns: True if the virtual camera has been repositioned. protected virtual Void PostPipelineStageCallback(CinemachineVirtualCameraBase vcam, Stage stage, CameraState& state, Single deltaTime) Callback to to the camera confining. Param Type Description vcam CinemachineVirtualCameraBase stage Stage state CameraState& deltaTime Single Void InvalidatePathCache() Call this if the bounding shape's points change at runtime. CinemachineDollyCart Type: class Namespace: Cinemachine Inherits: MonoBehaviour This is a very simple behaviour that constrains its transform to a CinemachinePath. It can be used to animate any objects along a path, or as a Follow target for Cinemachine Virtual Cameras. Fields Name Type Description m_Path CinemachinePathBase The path to follow. m_UpdateMethod UpdateMethod When to move the cart, if Velocity is non-zero. Possible Values: - Update - FixedUpdate m_PositionUnits PositionUnits How to interpret the Path Position. If set to Path Units, values are as follows: 0 represents the first waypoint on the path, 1 is the second, and so on. Values in-between are points on the path in between the waypoints. If set to Distance, then Path Position represents distance along the path. Possible Values: - PathUnits - Distance - Normalized m_Speed Single Move the cart with this speed along the path. The value is interpreted according to the Position Units setting. m_Position Single The position along the path at which the cart will be placed. This can be animated directly or, if the velocity is non-zero, will be updated automatically. The value is interpreted according to the Position Units setting. CinemachineExternalCamera Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera This component will expose a non-cinemachine camera to the cinemachine system, allowing it to participate in blends. Just add it as a component alongside an existing Unity Camera component. Properties Name Type Description State CameraState [Get] Get the CameraState, as we are able to construct one from the Unity Camera. LookAt Transform [Get,Set] The object that the camera is looking at. Follow Transform [Get,Set] This vcam defines no targets. Fields Name Type Description m_LookAt Transform The object that the camera is looking at. Setting this will improve the quality of the blends to and from this camera. m_PositionBlending PositionBlendMethod Hint for blending positions to and from this virtual camera. Possible Values: - Linear - Spherical - Cylindrical CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Do not call this method. Param Type Description worldUp Vector3 deltaTime Single CinemachineFollowZoom Type: class Namespace: Cinemachine Inherits: CinemachineExtension An add-on module for Cinemachine Virtual Camera that adjusts the FOV of the lens to keep the target object at a constant size on the screen, regardless of camera and target position. Fields Name Type Description m_Width Single The shot width to maintain, in world units, at target distance. m_Damping Single Increase this value to soften the aggressiveness of the follow-zoom. Small numbers are more responsive, larger numbers give a more heavy slowly responding camera. m_MinFOV Single Lower limit for the FOV that this behaviour will generate. m_MaxFOV Single Upper limit for the FOV that this behaviour will generate. Methods protected virtual Void PostPipelineStageCallback(CinemachineVirtualCameraBase vcam, Stage stage, CameraState& state, Single deltaTime) Callback to preform the zoom adjustment. Param Type Description vcam CinemachineVirtualCameraBase stage Stage state CameraState& deltaTime Single CinemachineFramingTransposer Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a Cinemachine Component in the Body section of the component pipeline. Its job is to position the camera in a fixed screen-space relationship to the vcam's Follow target object, with offsets and damping. The camera will be first moved along the camera Z axis until the Follow target is at the desired distance from the camera's X-Y plane. The camera will then be moved in its XY plane until the Follow target is at the desired point on the camera's screen. The FramingTansposer will only change the camera's position in space. It will not re-orient or otherwise aim the camera. For this component to work properly, the vcam's LookAt target must be null. The Follow target will define what the camera is looking at. If the Follow target is a CinemachineTargetGroup, then additional controls will be available to dynamically adjust the camera's view in order to frame the entire group. Although this component was designed for orthographic cameras, it works equally well with perspective cameras and can be used in 3D environments. Properties Name Type Description SoftGuideRect Rect [Get,Set] Internal API for the inspector editor. HardGuideRect Rect [Get,Set] Internal API for the inspector editor. IsValid Boolean [Get] True if component is enabled and has a valid Follow target. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Body stage. Possible Values: - Body - Aim - Noise - Finalize TrackedPoint Vector3 [Get] Internal API for inspector. m_LastBounds Bounds [Get] For editor visulaization of the calculated bounding box of the group. m_lastBoundsMatrix Matrix4x4 [Get] For editor visualization of the calculated bounding box of the group. TargetGroup CinemachineTargetGroup [Get] Get Follow target as CinemachineTargetGroup, or null if target is not a group. Fields Name Type Description OnGUICallback Action Used by the Inspector Editor to display on-screen guides. m_LookaheadTime Single This setting will instruct the composer to adjust its target offset based on the motion of the target. The composer will look at a point where it estimates the target will be this many seconds into the future. Note that this setting is sensitive to noisy animation, and can amplify the noise, resulting in undesirable camera jitter. If the camera jitters unacceptably when the target is in motion, turn down this setting, or animate the target more smoothly. m_LookaheadSmoothing Single Controls the smoothness of the lookahead algorithm. Larger values smooth out jittery predictions and also increase prediction lag. m_LookaheadIgnoreY Boolean If checked, movement along the Y axis will be ignored for lookahead calculations. m_XDamping Single How aggressively the camera tries to maintain the offset in the X-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's x-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_YDamping Single How aggressively the camera tries to maintain the offset in the Y-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's y-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_ZDamping Single How aggressively the camera tries to maintain the offset in the Z-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's z-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_ScreenX Single Horizontal screen position for target. The camera will move to position the tracked object here. m_ScreenY Single Vertical screen position for target, The camera will move to position the tracked object here. m_CameraDistance Single The distance along the camera axis that will be maintained from the Follow target. m_DeadZoneWidth Single Camera will not move horizontally if the target is within this range of the position. m_DeadZoneHeight Single Camera will not move vertically if the target is within this range of the position. m_DeadZoneDepth Single The camera will not move along its z-axis if the Follow target is within this distance of the specified camera distance. m_UnlimitedSoftZone Boolean If checked, then soft zone will be unlimited in size. m_SoftZoneWidth Single When target is within this region, camera will gradually move horizontally to re-align towards the desired position, depending on the damping speed. m_SoftZoneHeight Single When target is within this region, camera will gradually move vertically to re-align towards the desired position, depending on the damping speed. m_BiasX Single A non-zero bias will move the target position horizontally away from the center of the soft zone. m_BiasY Single A non-zero bias will move the target position vertically away from the center of the soft zone. m_GroupFramingMode FramingMode What screen dimensions to consider when framing. Can be Horizontal, Vertical, or both. Possible Values: - Horizontal: Consider only the horizontal dimension. Vertical framing is ignored. - Vertical: Consider only the vertical dimension. Horizontal framing is ignored. - HorizontalAndVertical: The larger of the horizontal and vertical dimensions will dominate, to get the best fit. - None: Don't do any framing adjustment. m_AdjustmentMode AdjustmentMode How to adjust the camera to get the desired framing. You can zoom, dolly in/out, or do both. Possible Values: - ZoomOnly - DollyOnly - DollyThenZoom m_GroupFramingSize Single The bounding box of the targets should occupy this amount of the screen space. 1 means fill the whole screen. 0.5 means fill half the screen, etc. m_MaxDollyIn Single The maximum distance toward the target that this behaviour is allowed to move the camera. m_MaxDollyOut Single The maximum distance away the target that this behaviour is allowed to move the camera. m_MinimumDistance Single Set this to limit how close to the target the camera can get. m_MaximumDistance Single Set this to limit how far from the target the camera can get. m_MinimumFOV Single If adjusting FOV, will not set the FOV lower than this. m_MaximumFOV Single If adjusting FOV, will not set the FOV higher than this. m_MinimumOrthoSize Single If adjusting Orthographic Size, will not set it lower than this. m_MaximumOrthoSize Single If adjusting Orthographic Size, will not set it higher than this. Methods virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the us that a target got warped, so that we can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Positions the virtual camera according to the transposer rules. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for damping. If less than 0, no damping is done. CinemachineFreeLook Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera A Cinemachine Camera geared towards a 3rd person camera experience. The camera orbits around its subject with three separate camera rigs defining rings around the target. Each rig has its own radius, height offset, composer, and lens settings. Depending on the camera's position along the spline connecting these three rigs, these settings are interpolated to give the final camera position and state. Properties Name Type Description RigNames String[] (static) [Get] Names of the 3 child rigs. State CameraState [Get] The camera state, which will be a blend of the child rig states. LookAt Transform [Get,Set] Get the current LookAt target. Returns parent's LookAt if parent is non-null and no specific LookAt defined for this camera. Follow Transform [Get,Set] Get the current Follow target. Returns parent's Follow if parent is non-null and no specific Follow defined for this camera. LiveChildOrSelf ICinemachineCamera [Get] Returns the rig with the greatest weight. Fields Name Type Description m_LookAt Transform Object for the camera children to look at (the aim target). m_Follow Transform Object for the camera children wants to move with (the body target). m_PositionBlending PositionBlendMethod Hint for blending positions to and from this virtual camera. Possible Values: - Linear - Spherical - Cylindrical m_CommonLens Boolean If enabled, this lens setting will apply to all three child rigs, otherwise the child rig lens settings will be used. m_Lens LensSettings Specifies the lens properties of this Virtual Camera. This generally mirrors the Unity Camera's lens settings, and will be used to drive the Unity camera when the vcam is active. m_YAxis AxisState The Vertical axis. Value is 0..1. Chooses how to blend the child rigs. m_YAxisRecentering Recentering Controls how automatic recentering of the Y axis is accomplished. m_XAxis AxisState The Horizontal axis. Value is -180...180. This is passed on to the rigs' OrbitalTransposer component. m_Heading Heading The definition of Forward. Camera will follow behind. m_RecenterToTargetHeading Recentering Controls how automatic recentering of the X axis is accomplished. m_BindingMode BindingMode The coordinate space to use when interpreting the offset from the target. This is also used to set the camera's Up vector, which will be maintained when aiming the camera. Possible Values: - LockToTargetOnAssign: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame at the moment when the virtual camera was enabled, or when the target was assigned. - LockToTargetWithWorldUp: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the tilt and roll zeroed out. - LockToTargetNoRoll: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the roll zeroed out. - LockToTarget: Camera will be bound to the Follow target using the target's local frame. - WorldSpace: Camera will be bound to the Follow target using a world space offset. - SimpleFollowWithWorldUp: Offsets will be calculated relative to the target, using Camera-local axes. m_SplineCurvature Single Controls how taut is the line that connects the rigs' orbits, which determines final placement on the Y axis. m_Orbits Orbit[] The radius and height of the three orbiting rigs. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods protected virtual Void OnValidate() Enforce bounds for fields, when changed in inspector. CinemachineVirtualCamera GetRig(Int32 i) Get a child rig. Param Type Description i Int32 Rig index. Can be 0, 1, or 2. Returns: The rig, or null if index is bad. protected virtual Void OnEnable() Updates the child rig cache. protected virtual Void OnDestroy() Makes sure that the child rigs get destroyed in an undo-firndly manner. Invalidates the rig cache. virtual Boolean IsLiveChild(ICinemachineCamera vcam) Check whether the vcam a live child of this camera. Returns true if the child is currently contributing actively to the camera state. Param Type Description vcam ICinemachineCamera The Virtual Camera to check. Returns: True if the vcam is currently actively influencing the state of this vcam. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Called by CinemachineCore at designated update time so the vcam can position itself and track its targets. All 3 child rigs are updated, and a blend calculated, depending on the value of the Y axis. Param Type Description worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than 0). virtual Void OnTransitionFromCamera(ICinemachineCamera fromCam, Vector3 worldUp, Single deltaTime) If we are transitioning from another FreeLook, grab the axis values from it. Param Type Description fromCam ICinemachineCamera The camera being deactivated. May be null. worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than or equal to 0). Vector3 GetLocalPositionForCameraFromInput(Single t) Returns the local position of the camera along the spline used to connect the three camera rigs. Does not take into account the current heading of the camera (or its target). Param Type Description t Single The t-value for the camera on its spline. Internally clamped to the value [0,1]. Returns: The local offset (back + up) of the camera WRT its target based on the supplied t-value. CinemachineGroupComposer Type: class Namespace: Cinemachine Inherits: CinemachineComposer This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to aim the camera at a target object, with configurable offsets, damping, and composition rules. In addition, if the target is a CinemachineTargetGroup, the behaviour will adjust the FOV and the camera distance to ensure that the entire group of targets is framed properly. Properties Name Type Description TargetGroup CinemachineTargetGroup [Get] Get LookAt target as CinemachineTargetGroup, or null if target is not a group. m_LastBounds Bounds [Get] For editor visulaization of the calculated bounding box of the group. m_lastBoundsMatrix Matrix4x4 [Get] For editor visualization of the calculated bounding box of the group. Fields Name Type Description m_GroupFramingSize Single The bounding box of the targets should occupy this amount of the screen space. 1 means fill the whole screen. 0.5 means fill half the screen, etc. m_FramingMode FramingMode What screen dimensions to consider when framing. Can be Horizontal, Vertical, or both. Possible Values: - Horizontal: Consider only the horizontal dimension. Vertical framing is ignored. - Vertical: Consider only the vertical dimension. Horizontal framing is ignored. - HorizontalAndVertical: The larger of the horizontal and vertical dimensions will dominate, to get the best fit. m_FrameDamping Single How aggressively the camera tries to frame the group. Small numbers are more responsive, rapidly adjusting the camera to keep the group in the frame. Larger numbers give a more heavy slowly responding camera. m_AdjustmentMode AdjustmentMode How to adjust the camera to get the desired framing. You can zoom, dolly in/out, or do both. Possible Values: - ZoomOnly - DollyOnly - DollyThenZoom m_MaxDollyIn Single The maximum distance toward the target that this behaviour is allowed to move the camera. m_MaxDollyOut Single The maximum distance away the target that this behaviour is allowed to move the camera. m_MinimumDistance Single Set this to limit how close to the target the camera can get. m_MaximumDistance Single Set this to limit how far from the target the camera can get. m_MinimumFOV Single If adjusting FOV, will not set the FOV lower than this. m_MaximumFOV Single If adjusting FOV, will not set the FOV higher than this. m_MinimumOrthoSize Single If adjusting Orthographic Size, will not set it lower than this. m_MaximumOrthoSize Single If adjusting Orthographic Size, will not set it higher than this. OnGUICallback Action Used by the Inspector Editor to display on-screen guides. m_TrackedObjectOffset Vector3 Target offset from the target object's center in target-local space. Use this to fine-tune the tracking target position when the desired area is not the tracked object's center. m_LookaheadTime Single This setting will instruct the composer to adjust its target offset based on the motion of the target. The composer will look at a point where it estimates the target will be this many seconds into the future. Note that this setting is sensitive to noisy animation, and can amplify the noise, resulting in undesirable camera jitter. If the camera jitters unacceptably when the target is in motion, turn down this setting, or animate the target more smoothly. m_LookaheadSmoothing Single Controls the smoothness of the lookahead algorithm. Larger values smooth out jittery predictions and also increase prediction lag. m_LookaheadIgnoreY Boolean If checked, movement along the Y axis will be ignored for lookahead calculations. m_HorizontalDamping Single How aggressively the camera tries to follow the target in the screen-horizontal direction. Small numbers are more responsive, rapidly orienting the camera to keep the target in the dead zone. Larger numbers give a more heavy slowly responding camera. Using different vertical and horizontal settings can yield a wide range of camera behaviors. m_VerticalDamping Single How aggressively the camera tries to follow the target in the screen-vertical direction. Small numbers are more responsive, rapidly orienting the camera to keep the target in the dead zone. Larger numbers give a more heavy slowly responding camera. Using different vertical and horizontal settings can yield a wide range of camera behaviors. m_ScreenX Single Horizontal screen position for target. The camera will rotate to position the tracked object here. m_ScreenY Single Vertical screen position for target, The camera will rotate to position the tracked object here. m_DeadZoneWidth Single Camera will not rotate horizontally if the target is within this range of the position. m_DeadZoneHeight Single Camera will not rotate vertically if the target is within this range of the position. m_SoftZoneWidth Single When target is within this region, camera will gradually rotate horizontally to re-align towards the desired position, depending on the damping speed. m_SoftZoneHeight Single When target is within this region, camera will gradually rotate vertically to re-align towards the desired position, depending on the damping speed. m_BiasX Single A non-zero bias will move the target position horizontally away from the center of the soft zone. m_BiasY Single A non-zero bias will move the target position vertically away from the center of the soft zone. Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies the composer rules and orients the camera accordingly. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for calculating damping. If less than zero, then target will snap to the center of the dead zone. CinemachineHardLockToTarget Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to place the camera on the Follow Target. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a LookAt defined. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Aim stage. Possible Values: - Body - Aim - Noise - Finalize Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies the composer rules and orients the camera accordingly. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for calculating damping. If less than zero, then target will snap to the center of the dead zone. CinemachineHardLookAt Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to aim the camera hard at the LookAt target. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a LookAt defined. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Aim stage. Possible Values: - Body - Aim - Noise - Finalize Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies the composer rules and orients the camera accordingly. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for calculating damping. If less than zero, then target will snap to the center of the dead zone. CinemachineMixingCamera Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera CinemachineMixingCamera is a \"manager camera\" that takes on the state of the weighted average of the states of its child virtual cameras. A fixed number of slots are made available for cameras, rather than a dynamic array. We do it this way in order to support weight animation from the Timeline. Timeline cannot animate array elements. Properties Name Type Description State CameraState [Get] The blended CameraState. LookAt Transform [Get,Set] Not used. Follow Transform [Get,Set] Not used. LiveChildOrSelf ICinemachineCamera [Get] Return the live child. ChildCameras CinemachineVirtualCameraBase[] [Get] Get the cached list of child cameras. These are just the immediate children in the hierarchy. Note: only the first entries of this list participate in the final blend, up to MaxCameras. Fields Name Type Description m_Weight0 Single The weight of the first tracked camera. m_Weight1 Single The weight of the second tracked camera. m_Weight2 Single The weight of the third tracked camera. m_Weight3 Single The weight of the fourth tracked camera. m_Weight4 Single The weight of the fifth tracked camera. m_Weight5 Single The weight of the sixth tracked camera. m_Weight6 Single The weight of the seventh tracked camera. m_Weight7 Single The weight of the eighth tracked camera. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods Single GetWeight(Int32 index) Get the weight of the child at an index. Param Type Description index Int32 The child index. Only immediate CinemachineVirtualCameraBase children are counted. Returns: The weight of the camera. Valid only if camera is active and enabled. Void SetWeight(Int32 index, Single w) Set the weight of the child at an index. Param Type Description index Int32 The child index. Only immediate CinemachineVirtualCameraBase children are counted. w Single The weight to set. Can be any non-negative number. Single GetWeight(CinemachineVirtualCameraBase vcam) Get the weight of the child CinemachineVirtualCameraBase. Param Type Description vcam CinemachineVirtualCameraBase The child camera. Returns: The weight of the camera. Valid only if camera is active and enabled. Void SetWeight(CinemachineVirtualCameraBase vcam, Single w) Set the weight of the child CinemachineVirtualCameraBase. Param Type Description vcam CinemachineVirtualCameraBase The child camera. w Single The weight to set. Can be any non-negative number. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. protected virtual Void OnEnable() Makes sure the internal child cache is up to date. Void OnTransformChildrenChanged() Makes sure the internal child cache is up to date. protected virtual Void OnValidate() Makes sure the weights are non-negative. virtual Boolean IsLiveChild(ICinemachineCamera vcam) Check whether the vcam a live child of this camera. Param Type Description vcam ICinemachineCamera The Virtual Camera to check. Returns: True if the vcam is currently actively influencing the state of this vcam. protected Void InvalidateListOfChildren() Invalidate the cached list of child cameras. protected Void ValidateListOfChildren() Rebuild the cached list of child cameras. virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Do not call this method. Called by CinemachineCore at designated update time so the vcam can position itself and track its targets. This implementation computes and caches the weighted blend of the tracked cameras. Param Type Description worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than 0). CinemachineOrbitalTransposer Type: class Namespace: Cinemachine Inherits: CinemachineTransposer This is a CinemachineComponent in the Body section of the component pipeline. Its job is to position the camera in a variable relationship to a the vcam's Follow target object, with offsets and damping. This component is typically used to implement a camera that follows its target. It can accept player input from an input device, which allows the player to dynamically control the relationship between the camera and the target, for example with a joystick. The OrbitalTransposer introduces the concept of Heading, which is the direction in which the target is moving, and the OrbitalTransposer will attempt to position the camera in relationship to the heading, which is by default directly behind the target. You can control the default relationship by adjusting the Heading Bias setting. If you attach an input controller to the OrbitalTransposer, then the player can also control the way the camera positions itself in relation to the target heading. This allows the camera to move to any spot on an orbit around the target. Fields Name Type Description m_Heading Heading The definition of Forward. Camera will follow behind. m_RecenterToTargetHeading Recentering Automatic heading recentering. The settings here defines how the camera will reposition itself in the absence of player input. m_XAxis AxisState Heading Control. The settings here control the behaviour of the camera in response to the player's input. m_HeadingIsSlave Boolean Drive the x-axis setting programmatically. Automatic heading updating will be disabled. m_BindingMode BindingMode The coordinate space to use when interpreting the offset from the target. This is also used to set the camera's Up vector, which will be maintained when aiming the camera. Possible Values: - LockToTargetOnAssign: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame at the moment when the virtual camera was enabled, or when the target was assigned. - LockToTargetWithWorldUp: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the tilt and roll zeroed out. - LockToTargetNoRoll: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the roll zeroed out. - LockToTarget: Camera will be bound to the Follow target using the target's local frame. - WorldSpace: Camera will be bound to the Follow target using a world space offset. - SimpleFollowWithWorldUp: Offsets will be calculated relative to the target, using Camera-local axes. m_FollowOffset Vector3 The distance vector that the transposer will attempt to maintain from the Follow target. m_XDamping Single How aggressively the camera tries to maintain the offset in the X-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's x-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_YDamping Single How aggressively the camera tries to maintain the offset in the Y-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's y-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_ZDamping Single How aggressively the camera tries to maintain the offset in the Z-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's z-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_PitchDamping Single How aggressively the camera tries to track the target rotation's X angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_YawDamping Single How aggressively the camera tries to track the target rotation's Y angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_RollDamping Single How aggressively the camera tries to track the target rotation's Z angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. Methods protected virtual Void OnValidate() Single UpdateHeading(Single deltaTime, Vector3 up, AxisState& axis) Update the X axis and calculate the heading. This can be called by a delegate with a custom axis. Used for damping. If less than 0, no damping is done.World Up, set by the CinemachineBrainAxis value. Param Type Description deltaTime Single Used for damping. If less than 0, no damping is done. up Vector3 World Up, set by the CinemachineBrain. axis AxisState& Returns: Axis value. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the us that a target got warped, so that we can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Positions the virtual camera according to the transposer rules. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for damping. If less than 0, no damping is done. CinemachineOrbitalTransposer.Heading Type: struct Namespace: Cinemachine How the \"forward\" direction is defined. Orbital offset is in relation to the forward direction. Fields Name Type Description m_Definition HeadingDefinition How 'forward' is defined. The camera will be placed by default behind the target. PositionDelta will consider 'forward' to be the direction in which the target is moving. Possible Values: - PositionDelta: Target heading calculated from the difference between its position on the last update and current frame. - Velocity: Target heading calculated from its Rigidbody's velocity. If no Rigidbody exists, it will fall back to HeadingDerivationMode.PositionDelta. - TargetForward: Target heading calculated from the Target Transform's euler Y angle. - WorldForward: Default heading is a constant world space heading. m_VelocityFilterStrength Int32 Size of the velocity sampling window for target heading filter. This filters out irregularities in the target's movement. Used only if deriving heading from target's movement (PositionDelta or Velocity). m_Bias Single Where the camera is placed when the X-axis value is zero. This is a rotation in degrees around the Y axis. When this value is 0, the camera will be placed behind the target. Nonzero offsets will rotate the zero position around the target. CinemachinePath Type: class Namespace: Cinemachine Inherits: CinemachinePathBase Defines a world-space path, consisting of an array of waypoints, each of which has position, tangent, and roll settings. Bezier interpolation is performed between the waypoints, to get a smooth and continuous path. Properties Name Type Description MinPos Single [Get] The minimum value for the path position. MaxPos Single [Get] The maximum value for the path position. Looped Boolean [Get] True if the path ends are joined to form a continuous loop. DistanceCacheSampleStepsPerSegment Int32 [Get] When calculating the distance cache, sample the path this many times between points. Fields Name Type Description m_Looped Boolean If checked, then the path ends are joined to form a continuous loop. m_Waypoints Waypoint[] The waypoints that define the path. They will be interpolated using a bezier curve. m_Resolution Int32 Path samples per waypoint. This is used for calculating path distances. m_Appearance Appearance The settings that control how the path will appear in the editor scene view. Methods virtual Vector3 EvaluatePosition(Single pos) Get a worldspace position of a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space position of the point along at path at pos. virtual Vector3 EvaluateTangent(Single pos) Get the tangent of the curve at a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space direction of the path tangent. Length of the vector represents the tangent strength. virtual Quaternion EvaluateOrientation(Single pos) Get the orientation the curve at a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space orientation of the path, as defined by tangent, up, and roll. CinemachinePath.Waypoint Type: struct Namespace: Cinemachine A waypoint along the path. Fields Name Type Description position Vector3 Position in path-local space. tangent Vector3 Offset from the position, which defines the tangent of the curve at the waypoint. The length of the tangent encodes the strength of the bezier handle. The same handle is used symmetrically on both sides of the waypoint, to ensure smoothness. roll Single Defines the role of the path at this waypoint. The other orientation axes are inferred from the tangent and world up. CinemachinePathBase.Appearance Type: class Namespace: Cinemachine This class holds the settings that control how the path will appear in the editor scene view. The path is not visible in the game view. Fields Name Type Description pathColor Color The color of the path itself when it is active in the editor. inactivePathColor Color The color of the path itself when it is inactive in the editor. width Single The width of the railroad-tracks that are drawn to represent the path. CinemachinePOV Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to aim the camera in response to the user's mouse or joystick input. The composer does not change the camera's position. It will only pan and tilt the camera where it is, in order to get the desired framing. To move the camera, you have to use the virtual camera's Body section. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a LookAt defined. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Aim stage. Possible Values: - Body - Aim - Noise - Finalize Fields Name Type Description m_VerticalAxis AxisState The Vertical axis. Value is -90..90. Controls the vertical orientation. m_VerticalRecentering Recentering Controls how automatic recentering of the Vertical axis is accomplished. m_HorizontalAxis AxisState The Horizontal axis. Value is -180..180. Controls the horizontal orientation. m_HorizontalRecentering Recentering Controls how automatic recentering of the Horizontal axis is accomplished. Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Applies the axis values and orients the camera accordingly. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for calculating damping. Not used. CinemachineSameAsFollowTarget Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Aim section of the component pipeline. Its job is to match the orientation of the Follow target. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a Follow target defined. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Aim stage. Possible Values: - Body - Aim - Noise - Finalize Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Orients the camera to match the Follow target's orientation. Param Type Description curState CameraState& The current camera state. deltaTime Single Not used. CinemachineSmoothPath Type: class Namespace: Cinemachine Inherits: CinemachinePathBase Defines a world-space path, consisting of an array of waypoints, each of which has position and roll settings. Bezier interpolation is performed between the waypoints, to get a smooth and continuous path. The path will pass through all waypoints, and (unlike CinemachinePath) first and second order continuity is guaranteed. Properties Name Type Description MinPos Single [Get] The minimum value for the path position. MaxPos Single [Get] The maximum value for the path position. Looped Boolean [Get] True if the path ends are joined to form a continuous loop. DistanceCacheSampleStepsPerSegment Int32 [Get] When calculating the distance cache, sample the path this many times between points. Fields Name Type Description m_Looped Boolean If checked, then the path ends are joined to form a continuous loop. m_Waypoints Waypoint[] The waypoints that define the path. They will be interpolated using a bezier curve. m_Resolution Int32 Path samples per waypoint. This is used for calculating path distances. m_Appearance Appearance The settings that control how the path will appear in the editor scene view. Methods virtual Void InvalidateDistanceCache() Call this if the path changes in such a way as to affect distances or other cached path elements. virtual Vector3 EvaluatePosition(Single pos) Get a worldspace position of a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space position of the point along at path at pos. virtual Vector3 EvaluateTangent(Single pos) Get the tangent of the curve at a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space direction of the path tangent. Length of the vector represents the tangent strength. virtual Quaternion EvaluateOrientation(Single pos) Get the orientation the curve at a point along the path. Param Type Description pos Single Postion along the path. Need not be normalized. Returns: World-space orientation of the path, as defined by tangent, up, and roll. CinemachineSmoothPath.Waypoint Type: struct Namespace: Cinemachine A waypoint along the path. Fields Name Type Description position Vector3 Position in path-local space. roll Single Defines the roll of the path at this waypoint. The other orientation axes are inferred from the tangent and world up. CinemachineStateDrivenCamera Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera This is a virtual camera \"manager\" that owns and manages a collection of child Virtual Cameras. These child vcams are mapped to individual states in an animation state machine, allowing you to associate specific vcams to specific animation states. When that state is active in the state machine, then the associated camera will be activated. You can define custom blends and transitions between child cameras. In order to use this behaviour, you must have an animated target (i.e. an object animated with a state machine) to drive the behaviour. Properties Name Type Description Description String [Get] Gets a brief debug description of this virtual camera, for use when displaying debug info. LiveChild ICinemachineCamera [Get,Set] Get the current \"best\" child virtual camera, that would be chosen if the State Driven Camera were active. LiveChildOrSelf ICinemachineCamera [Get] Return the live child. State CameraState [Get] The State of the current live child. LookAt Transform [Get,Set] Get the current LookAt target. Returns parent's LookAt if parent is non-null and no specific LookAt defined for this camera. Follow Transform [Get,Set] Get the current Follow target. Returns parent's Follow if parent is non-null and no specific Follow defined for this camera. ChildCameras CinemachineVirtualCameraBase[] [Get] The list of child cameras. These are just the immediate children in the hierarchy. IsBlending Boolean [Get] Is there a blend in progress? Fields Name Type Description m_LookAt Transform Default object for the camera children to look at (the aim target), if not specified in a child camera. May be empty if all of the children define targets of their own. m_Follow Transform Default object for the camera children wants to move with (the body target), if not specified in a child camera. May be empty if all of the children define targets of their own. m_AnimatedTarget Animator The state machine whose state changes will drive this camera's choice of active child. m_LayerIndex Int32 Which layer in the target state machine to observe. m_ShowDebugText Boolean When enabled, the current child camera and blend will be indicated in the game window, for debugging. m_EnableAllChildCameras Boolean Force all child cameras to be enabled. This is useful if animating them in Timeline, but consumes extra resources. m_ChildCameras CinemachineVirtualCameraBase[] Internal API for the editor. Do not use this field. m_Instructions Instruction[] The set of instructions associating virtual cameras with states. These instructions are used to choose the live child at any given moment. m_DefaultBlend CinemachineBlendDefinition The blend which is used if you don't explicitly define a blend between two Virtual Camera children. m_CustomBlends CinemachineBlenderSettings This is the asset which contains custom settings for specific child blends. m_ParentHash ParentHash[] Internal API for the Inspector editor. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods virtual Boolean IsLiveChild(ICinemachineCamera vcam) Check whether the vcam a live child of this camera. Param Type Description vcam ICinemachineCamera The Virtual Camera to check. Returns: True if the vcam is currently actively influencing the state of this vcam. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Do not call this method. Called by CinemachineCore at designated update time so the vcam can position itself and track its targets. This implementation updates all the children, chooses the best one, and implements any required blending. Param Type Description worldUp Vector3 Default world Up, set by the CinemachineBrain. deltaTime Single Delta time for time-based effects (ignore if less than or equal to 0). protected virtual Void OnEnable() Makes sure the internal child cache is up to date. Void OnTransformChildrenChanged() Makes sure the internal child cache is up to date. protected virtual Void OnGUI() Displays the current active camera on the game screen, if requested. static String CreateFakeHashName(Int32 parentHash, String stateName) API for the inspector editor. Animation module does not have hashes for state parents, so we have to invent them in order to implement nested state handling. Param Type Description parentHash Int32 stateName String Void ValidateInstructions() Internal API for the inspector editor. CinemachineStoryboard Type: class Namespace: Cinemachine Inherits: CinemachineExtension An add-on module for Cinemachine Virtual Camera that places an image in screen space over the camera's output. Fields Name Type Description m_ShowImage Boolean If checked, the specified image will be displayed as an overlay over the virtual camera's output. m_Image Texture The image to display. m_Aspect FillStrategy How to handle differences between image aspect and screen aspect. Possible Values: - BestFit - CropImageToFit - StretchToFit m_Alpha Single The opacity of the image. 0 is transparent, 1 is opaque. m_Center Vector2 The screen-space position at which to display the image. Zero is center. m_Rotation Vector3 The screen-space rotation to apply to the image. m_Scale Vector2 The screen-space scaling to apply to the image. m_SyncScale Boolean If checked, X and Y scale are synchronized. m_MuteCamera Boolean If checked, Camera transform will not be controlled by this virtual camera. m_SplitView Single Wipe the image on and off horizontally. Methods protected virtual Void PostPipelineStageCallback(CinemachineVirtualCameraBase vcam, Stage stage, CameraState& state, Single deltaTime) Standard CinemachineExtension callback. Param Type Description vcam CinemachineVirtualCameraBase stage Stage state CameraState& deltaTime Single protected virtual Void OnDestroy() protected virtual Void ConnectToVcam(Boolean connect) Param Type Description connect Boolean CinemachineTargetGroup Type: class Namespace: Cinemachine Inherits: MonoBehaviour Defines a group of target objects, each with a radius and a weight. The weight is used when calculating the average position of the target group. Higher-weighted members of the group will count more. The bounding box is calculated by taking the member positions, weight, and radii into account. Properties Name Type Description BoundingBox Bounds [Get] The axis-aligned bounding box of the group, computed using the targets positions and radii. IsEmpty Boolean [Get] Return true if there are no members with weight > 0. Fields Name Type Description m_PositionMode PositionMode How the group's position is calculated. Select GroupCenter for the center of the bounding box, and GroupAverage for a weighted average of the positions of the members. Possible Values: - GroupCenter: Group position will be the center of the group's axis-aligned bounding box. - GroupAverage: Group position will be the weighted average of the positions of the members. m_RotationMode RotationMode How the group's rotation is calculated. Select Manual to use the value in the group's transform, and GroupAverage for a weighted average of the orientations of the members. Possible Values: - Manual: Manually set in the group's transform. - GroupAverage: Weighted average of the orientation of its members. m_UpdateMethod UpdateMethod When to update the group's transform based on the position of the group members. Possible Values: - Update - FixedUpdate - LateUpdate m_Targets Target[] The target objects, together with their weights and radii, that will contribute to the group's average position, orientation, and size. Methods Bounds GetViewSpaceBoundingBox(Matrix4x4 mView) The axis-aligned bounding box of the group, in a specific reference frame. Param Type Description mView Matrix4x4 The frame of reference in which to compute the bounding box. Returns: The axis-aligned bounding box of the group, in the desired frame of reference. CinemachineTargetGroup.Target Type: struct Namespace: Cinemachine Holds the information that represents a member of the group. Fields Name Type Description target Transform The target objects. This object's position and orientation will contribute to the group's average position and orientation, in accordance with its weight. weight Single How much weight to give the target when averaging. Cannot be negative. radius Single The radius of the target, used for calculating the bounding box. Cannot be negative. CinemachineTrackedDolly Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase A Cinemachine Virtual Camera Body component that constrains camera motion to a CinemachinePath. The camera can move along the path. This behaviour can operate in two modes: manual positioning, and Auto-Dolly positioning. In Manual mode, the camera's position is specified by animating the Path Position field. In Auto-Dolly mode, the Path Position field is animated automatically every frame by finding the position on the path that's closest to the virtual camera's Follow target. Properties Name Type Description IsValid Boolean [Get] True if component is enabled and has a path. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Body stage. Possible Values: - Body - Aim - Noise - Finalize Fields Name Type Description m_Path CinemachinePathBase The path to which the camera will be constrained. This must be non-null. m_PathPosition Single The position along the path at which the camera will be placed. This can be animated directly, or set automatically by the Auto-Dolly feature to get as close as possible to the Follow target. The value is interpreted according to the Position Units setting. m_PositionUnits PositionUnits How to interpret Path Position. If set to Path Units, values are as follows: 0 represents the first waypoint on the path, 1 is the second, and so on. Values in-between are points on the path in between the waypoints. If set to Distance, then Path Position represents distance along the path. Possible Values: - PathUnits - Distance - Normalized m_PathOffset Vector3 Where to put the camera relative to the path position. X is perpendicular to the path, Y is up, and Z is parallel to the path. This allows the camera to be offset from the path itself (as if on a tripod, for example). m_XDamping Single How aggressively the camera tries to maintain its position in a direction perpendicular to the path. Small numbers are more responsive, rapidly translating the camera to keep the target's x-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_YDamping Single How aggressively the camera tries to maintain its position in the path-local up direction. Small numbers are more responsive, rapidly translating the camera to keep the target's y-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_ZDamping Single How aggressively the camera tries to maintain its position in a direction parallel to the path. Small numbers are more responsive, rapidly translating the camera to keep the target's z-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_CameraUp CameraUpMode How to set the virtual camera's Up vector. This will affect the screen composition, because the camera Aim behaviours will always try to respect the Up direction. Possible Values: - Default: Leave the camera's up vector alone. It will be set according to the Brain's WorldUp. - Path: Take the up vector from the path's up vector at the current point. - PathNoRoll: Take the up vector from the path's up vector at the current point, but with the roll zeroed out. - FollowTarget: Take the up vector from the Follow target's up vector. - FollowTargetNoRoll: Take the up vector from the Follow target's up vector, but with the roll zeroed out. m_PitchDamping Single How aggressively the camera tries to track the target rotation's X angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_YawDamping Single How aggressively the camera tries to track the target rotation's Y angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_RollDamping Single How aggressively the camera tries to track the target rotation's Z angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_AutoDolly AutoDolly Controls how automatic dollying occurs. A Follow target is necessary to use this feature. Methods virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Positions the virtual camera according to the transposer rules. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for damping. If less that 0, no damping is done. CinemachineTrackedDolly.AutoDolly Type: struct Namespace: Cinemachine Controls how automatic dollying occurs. Fields Name Type Description m_Enabled Boolean If checked, will enable automatic dolly, which chooses a path position that is as close as possible to the Follow target. Note: this can have significant performance impact. m_PositionOffset Single Offset, in current position units, from the closest point on the path to the follow target. m_SearchRadius Int32 Search up to how many waypoints on either side of the current position. Use 0 for Entire path. m_SearchResolution Int32 We search between waypoints by dividing the segment into this many straight pieces. The higher the number, the more accurate the result, but performance is proportionally slower for higher numbers. CinemachineTransposer Type: class Namespace: Cinemachine Inherits: CinemachineComponentBase This is a CinemachineComponent in the Body section of the component pipeline. Its job is to position the camera in a fixed relationship to the vcam's Follow target object, with offsets and damping. The Transposer will only change the camera's position in space. It will not re-orient or otherwise aim the camera. To that, you need to instruct the vcam in the Aim section of its pipeline. Properties Name Type Description EffectiveOffset Vector3 [Get] Get the target offset, with sanitization. IsValid Boolean [Get] True if component is enabled and has a valid Follow target. Stage Stage [Get] Get the Cinemachine Pipeline stage that this component implements. Always returns the Body stage. Possible Values: - Body - Aim - Noise - Finalize Fields Name Type Description m_BindingMode BindingMode The coordinate space to use when interpreting the offset from the target. This is also used to set the camera's Up vector, which will be maintained when aiming the camera. Possible Values: - LockToTargetOnAssign: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame at the moment when the virtual camera was enabled, or when the target was assigned. - LockToTargetWithWorldUp: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the tilt and roll zeroed out. - LockToTargetNoRoll: Camera will be bound to the Follow target using a frame of reference consisting of the target's local frame, with the roll zeroed out. - LockToTarget: Camera will be bound to the Follow target using the target's local frame. - WorldSpace: Camera will be bound to the Follow target using a world space offset. - SimpleFollowWithWorldUp: Offsets will be calculated relative to the target, using Camera-local axes. m_FollowOffset Vector3 The distance vector that the transposer will attempt to maintain from the Follow target. m_XDamping Single How aggressively the camera tries to maintain the offset in the X-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's x-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_YDamping Single How aggressively the camera tries to maintain the offset in the Y-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's y-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_ZDamping Single How aggressively the camera tries to maintain the offset in the Z-axis. Small numbers are more responsive, rapidly translating the camera to keep the target's z-axis offset. Larger numbers give a more heavy slowly responding camera. Using different settings per axis can yield a wide range of camera behaviors. m_PitchDamping Single How aggressively the camera tries to track the target rotation's X angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_YawDamping Single How aggressively the camera tries to track the target rotation's Y angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. m_RollDamping Single How aggressively the camera tries to track the target rotation's Z angle. Small numbers are more responsive. Larger numbers give a more heavy slowly responding camera. Methods protected virtual Void OnValidate() Derived classes should call this from their OnValidate() implementation. virtual Void MutateCameraState(CameraState& curState, Single deltaTime) Positions the virtual camera according to the transposer rules. Param Type Description curState CameraState& The current camera state. deltaTime Single Used for damping. If less than 0, no damping is done. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the us that a target got warped, so that we can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. protected Void InitPrevFrameStateInfo(CameraState& curState, Single deltaTime) Initializes the state for previous frame if appropriate. Param Type Description curState CameraState& deltaTime Single protected Void TrackTarget(Single deltaTime, Vector3 up, Vector3 desiredCameraOffset, Vector3& outTargetPosition, Quaternion& outTargetOrient) Positions the virtual camera according to the transposer rules. Param Type Description deltaTime Single Used for damping. If less than 0, no damping is done. up Vector3 Current camera up. desiredCameraOffset Vector3 Where we want to put the camera relative to the follow target. outTargetPosition Vector3& Resulting camera position. outTargetOrient Quaternion& Damped target orientation. Vector3 GeTargetCameraPosition(Vector3 worldUp) Internal API for the Inspector Editor, so it can draw a marker at the target. Param Type Description worldUp Vector3 Quaternion GetReferenceOrientation(Vector3 worldUp) Internal API for the Inspector Editor, so it can draw a marker at the target. Param Type Description worldUp Vector3 CinemachineVirtualCamera Type: class Namespace: Cinemachine Inherits: CinemachineVirtualCameraBase Implements: ICinemachineCamera This behaviour is intended to be attached to an empty Transform GameObject, and it represents a Virtual Camera within the Unity scene. The Virtual Camera will animate its Transform according to the rules contained in its CinemachineComponent pipeline (Aim, Body, and Noise). When the virtual camera is Live, the Unity camera will assume the position and orientation of the virtual camera. A virtual camera is not a camera. Instead, it can be thought of as a camera controller, not unlike a cameraman. It can drive the Unity Camera and control its position, orientation, lens settings, and PostProcessing effects. Each Virtual Camera owns its own Cinemachine Component Pipeline, through which you provide the instructions for dynamically tracking specific game objects. A virtual camera is very lightweight, and does no rendering of its own. It merely tracks interesting GameObjects, and positions itself accordingly. A typical game can have dozens of virtual cameras, each set up to follow a particular character or capture a particular event. A Virtual Camera can be in any of three states: Live: The virtual camera is actively controlling the Unity Camera. The virtual camera is tracking its targets and being updated every frame. Standby: The virtual camera is tracking its targets and being updated every frame, but no Unity Camera is actively being controlled by it. This is the state of a virtual camera that is enabled in the scene but perhaps at a lower priority than the Live virtual camera. Disabled: The virtual camera is present but disabled in the scene. It is not actively tracking its targets and so consumes no processing power. However, the virtual camera can be made live from the Timeline. The Unity Camera can be driven by any virtual camera in the scene. The game logic can choose the virtual camera to make live by manipulating the virtual cameras' enabled flags and their priorities, based on game logic. In order to be driven by a virtual camera, the Unity Camera must have a CinemachineBrain behaviour, which will select the most eligible virtual camera based on its priority or on other criteria, and will manage blending. Properties Name Type Description State CameraState [Get] The CameraState object holds all of the information necessary to position the Unity camera. It is the output of this class. LookAt Transform [Get,Set] Get the LookAt target for the Aim component in the CinemachinePipeline. If this vcam is a part of a meta-camera collection, then the owner's target will be used if the local target is null. Follow Transform [Get,Set] Get the Follow target for the Body component in the CinemachinePipeline. If this vcam is a part of a meta-camera collection, then the owner's target will be used if the local target is null. UserIsDragging Boolean [Get,Set] API for the editor, to make the dragging of position handles behave better. Fields Name Type Description m_LookAt Transform The object that the camera wants to look at (the Aim target). If this is null, then the vcam's Transform orientation will define the camera's orientation. m_Follow Transform The object that the camera wants to move with (the Body target). If this is null, then the vcam's Transform position will define the camera's position. m_PositionBlending PositionBlendMethod Hint for blending positions to and from this virtual camera. Possible Values: - Linear - Spherical - Cylindrical m_Lens LensSettings Specifies the lens properties of this Virtual Camera. This generally mirrors the Unity Camera's lens settings, and will be used to drive the Unity camera when the vcam is active. CinemachineGUIDebuggerCallback Action This is deprecated. It is here to support the soon-to-be-removed Cinemachine Debugger in the Editor. m_ExcludedPropertiesInInspector String[] Inspector control - Use for hiding sections of the Inspector UI. m_LockStageInInspector Stage[] Inspector control - Use for enabling sections of the Inspector UI. m_Priority Int32 The priority will determine which camera becomes active based on the state of other cameras and this camera. Higher numbers have greater priority. Methods virtual Void InternalUpdateCameraState(Vector3 worldUp, Single deltaTime) Internal use only. Do not call this method. Called by CinemachineCore at the appropriate Update time so the vcam can position itself and track its targets. This class will invoke its pipeline and generate a CameraState for this frame. Param Type Description worldUp Vector3 deltaTime Single protected virtual Void OnEnable() Make sure that the pipeline cache is up-to-date. protected virtual Void OnDestroy() Calls the DestroyPipelineDelegate for destroying the hidden child object, to support undo. protected virtual Void OnValidate() Enforce bounds for fields, when changed in inspector. Void InvalidateComponentPipeline() Editor API: Call this when changing the pipeline from the editor. Will force a rebuild of the pipeline cache. Transform GetComponentOwner() Get the hidden CinemachinePipeline child object. CinemachineComponentBase[] GetComponentPipeline() Get the component pipeline owned by the hidden child pipeline container. For most purposes, it is preferable to use the GetCinemachineComponent method. CinemachineComponentBase GetCinemachineComponent(Stage stage) Get the component set for a specific stage. Param Type Description stage Stage The stage for which we want the component. Possible Values: - Body - Aim - Noise - Finalize Returns: The Cinemachine component for that stage, or null if not defined. T GetCinemachineComponent[T]() Get an existing component of a specific type from the cinemachine pipeline. T AddCinemachineComponent[T]() Add a component to the cinemachine pipeline. Void DestroyCinemachineComponent[T]() Remove a component from the cinemachine pipeline. virtual Void OnTargetObjectWarped(Transform target, Vector3 positionDelta) This is called to notify the vcam that a target got warped, so that the vcam can update its internal state to make the camera also warp seamlessly. Param Type Description target Transform The object that was warped. positionDelta Vector3 The amount the target's position changed. LensSettings Type: struct Namespace: Cinemachine Describes the FOV and clip planes for a camera. This generally mirrors the Unity Camera's lens settings, and will be used to drive the Unity camera when the vcam is active. Fields Name Type Description FieldOfView Single This is the camera view in vertical degrees. For cinematic people, a 50mm lens on a super-35mm sensor would equal a 19.6 degree FOV. OrthographicSize Single When using an orthographic camera, this defines the half-height, in world coordinates, of the camera view. NearClipPlane Single This defines the near region in the renderable range of the camera frustum. Raising this value will stop the game from drawing things near the camera, which can sometimes come in handy. Larger values will also increase your shadow resolution. FarClipPlane Single This defines the far region of the renderable range of the camera frustum. Typically you want to set this value as low as possible without cutting off desired distant objects. Dutch Single Camera Z roll, or tilt, in degrees. Methods static LensSettings FromCamera(Camera fromCamera) Creates a new LensSettings, copying the values from the supplied Camera. Param Type Description fromCamera Camera The Camera from which the FoV, near and far clip planes will be copied. static LensSettings Lerp(LensSettings lensA, LensSettings lensB, Single t) Linearly blends the fields of two LensSettings and returns the result. Param Type Description lensA LensSettings The LensSettings to blend from. lensB LensSettings The LensSettings to blend to. t Single The interpolation value. Internally clamped to the range [0,1]. Returns: Interpolated settings. Void Validate() Make sure lens settings are sane. Call this from OnValidate(). NoiseSettings Type: class Namespace: Cinemachine Inherits: ScriptableObject This is an asset that defines a noise profile. A noise profile is the shape of the noise as a function of time. You can build arbitrarily complex shapes by combining different base perlin noise frequencies at different amplitudes. The frequencies and amplitudes should be chosen with care, to ensure an interesting noise quality that is not obviously repetitive. As a mathematical side-note, any arbitrary periodic curve can be broken down into a series of fixed-amplitude sine-waves added together. This is called fourier decomposition, and is the basis of much signal processing. It doesn't really have much to do with this asset, but it's super interesting! Properties Name Type Description PositionNoise TransformNoiseParams[] [Get] Gets the array of positional noise channels for this NoiseSettings. OrientationNoise TransformNoiseParams[] [Get] Gets the array of orientation noise channels for this NoiseSettings. Fields Name Type Description m_Position TransformNoiseParams[] These are the noise channels for the virtual camera's position. Convincing noise setups typically mix low, medium and high frequencies together, so start with a size of 3. m_Orientation TransformNoiseParams[] These are the noise channels for the virtual camera's orientation. Convincing noise setups typically mix low, medium and high frequencies together, so start with a size of 3. Methods Void CopyFrom(NoiseSettings other) Clones the contents of the other asset into this one. Param Type Description other NoiseSettings static Vector3 GetCombinedFilterResults(TransformNoiseParams[] noiseParams, Single time, Vector3 timeOffsets) Get the noise signal value at a specific time. Param Type Description noiseParams TransformNoiseParams[] The parameters that define the noise function. time Single The time at which to sample the noise function. timeOffsets Vector3 Start time offset for each channel. Returns: The 3-channel noise signal value at the specified time. NoiseSettings.NoiseParams Type: struct Namespace: Cinemachine Describes the behaviour for a channel of noise. Fields Name Type Description Amplitude Single The amplitude of the noise for this channel. Larger numbers vibrate higher. Frequency Single The frequency of noise for this channel. Higher magnitudes vibrate faster. NoiseSettings.TransformNoiseParams Type: struct Namespace: Cinemachine Contains the behaviour of noise for the noise module for all 3 cardinal axes of the camera. Fields Name Type Description X NoiseParams Noise definition for X-axis. Y NoiseParams Noise definition for Y-axis. Z NoiseParams Noise definition for Z-axis."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/handles.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/handles.html",
    "title": "Cinemachine Scene Handles | FSM Unity Framework",
    "keywords": "Cinemachine Scene Handles Cinemachine Scene Handles are a group of 3D controls that allow you to manipulate virtual camera parameters visually in the Scene view. You can use the handle tools to interactively adjust the selected object's parameters quickly and efficiently rather than controlling them via the inspector. Cinemachine Scene Handles automatically appear in the Scene view toolbar overlay when you select a type of virtual camera with an associated handle. Types of Scene Handles The following four Cinemachine Handle tools are available in the toolbar: 1. Field of View (FOV) The FOV tool can adjust Vertical FOV, Horizontal FOV, Orthographic Size, or Focal Length depending on what's selected by the user. It can control: Vertical or Horizontal FOV (depending on the selection in the Main Camera) when the camera is in Perspective mode. Orthographic Size when the camera is in Orthographic mode. Focal length when the camera is in Physical mode. For more information on the Field of View (FOV) property, see Setting Virtual Camera properties. 2. Far/Near clip planes You can drag the points to increase the far clip plane and near clip plane. For more information on the Far and Near clip plane properties, see Setting Virtual Camera properties. 3. Follow offset The Follow offset is an offset from the Follow Target. You can drag the points to increase or decrease the Follow offset position. For more information on the Follow offset property, see Orbital Transposer properties. 4. Tracked object offset This starts from where the camera is placed. You can drag the points to increase or decrease the tracking target position when the desired area isn't the tracked object’s center. For more information on the Tracked object offset property, see Composer properties. Cinemachine tool settings Cinemachine tool settings are automatically displayed when a Free Look camera is selected. These settings allow you to adjust the position of the three separate camera rigs: Top, Middle, and Bottom. For more information, see Cinemachine Free Look Camera. To deactivate the Cinemachine tool settings for a Free Look camera: Right-click on the Scene tab in the Scene view. Select Overlays and then Cinemachine tools from the pop-up menu."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Documentation~/index.html",
    "title": "About Cinemachine | FSM Unity Framework",
    "keywords": "About Cinemachine Cinemachine is a suite of modules for operating the Unity camera. Cinemachine solves the complex mathematics and logic of tracking targets, composing, blending, and cutting between shots. It is designed to significantly reduce the number of time-consuming manual manipulations and script revisions that take place during development. The procedural nature of these modules makes Cinemachine bug-resistant. When you make adjustments—for example, change an animation, vehicle speed, terrain, or other GameObjects in your Scene—Cinemachine dynamically adjusts its behavior to make the best shot. There is no need, for example, to re-write camera scripts just because a character turns left instead of right. Cinemachine works in real time across all genres including FPS, third person, 2D, side-scroller, top down, and RTS. It supports as many shots in your Scene as you need. Its modular system lets you compose sophisticated behaviors. Cinemachine works well with other Unity tools, acting as a powerful complement to Timeline, animation, and post-processing assets. Create your own extensions or integrate it with your custom camera scripts. Installing Cinemachine Cinemachine is a free package, available for any project. You install Cinemachine like any other package. After you install Cinemachine, a new Cinemachine folder appears in the Gizmos folder of your Project window, and a new GameObject > Cinemachine menu is available. You can also access this menu when you right click inside the Hierarchy view. Requirements Cinemachine has no external dependencies. Just install it and start using it. If you are also using the Post Processing Stack (version 2), then adapter modules are provided - protected by ifdef directives which auto-define if the presence of the Post Processing Stack is detected. There are similar ifdef-protected behaviours for other packages, such as HDRP Volumes, Timeline, and Pixel-Perfect. This version of Cinemachine is supported by the following versions of the Unity Editor: 2019.4+ and later Upgrading from the Cinemachine Asset Package If you already installed Cinemachine from the Unity Asset Store, you can upgrade to the Cinemachine Package. To upgrade to the Cinemachine Package: In Unity Editor, 2019.4+ or later, open your project. Save the current Scene you are working on. Create a new, empty Scene. In the Project window, delete the Cinemachine Asset and any CinemachinePostProcessing adaptor assets you may have installed. Install the Cinemachine package."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Cinemachine copyright © 2021 Unity Technologies Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.cinemachine@2.9.7/Third Party Notices.html": {
    "href": "Library/PackageCache/com.unity.cinemachine@2.9.7/Third Party Notices.html",
    "title": "Clipper | FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the license(s) indicated below: Clipper Copyright © 2010-2014 Angus Johnson Boost Software License - Version 1.0 - August 17th, 2003 Permission is hereby granted, free of charge, to any person or organization obtaining a copy of the software and accompanying documentation covered by this license (the \"Software\") to use, reproduce, display, distribute, execute, and transmit the Software, and to prepare derivative works of the Software, and to permit third-parties to whom the Software is furnished to do so, all subject to the following: The copyright notices in the Software and this entire statement, including the above license grant, this restriction and the following disclaimer, must be included in all copies of the Software, in whole or in part, and all derivative works of the Software, unless such copies or derivative works are solely in the form of machine-executable object code generated by a source language processor. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. [2.2.0] - 2023-10-06 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added button for organization owner/admins to upgrade to DevOps subscription. Included new decorators for retained & locked files. Changed Updated description in the package.json, including an updated link to get started. Fixed Fixed failed operations when the workspace is already locked. [2.1.0] - 2023-09-01 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added command to support Hub creating a new project, and connecting a project to Unity Version Control. Show a message with a link to invite users to the organization after the first checkin. Changed Moved the button to invite users to the organization from the submenu to the toolbar. Removed Don't write cloudProjectId in ProjectSettings.asset anymore since it should only be managed by Services. Fixed Fixed Add to ignored/hidden changes list from the Project window creating a negative rule. Fixed Switch to changeset not working on Gluon partial workspace. [2.0.7] - 2023-07-25 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Fixed Fixed the Unity Version Control icon disappearing from the Editor Toolbar on domain reload. Fixed the popup stating \"An existing checkout operation has locked the workspace\" when trying to check in a scene with unsaved changes. [2.0.5] - 2023-05-31 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Fixed Fixed remaining references to 'Plastic SCM' in localized labels. [2.0.4] - 2023-04-14 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Fixed Fixed 'Texture2D' does not contain a definition for 'ignoreMipmapLimit' error when installing Unity Version Control on previous Unity Editor Versions Fixed broken sign in dialog style when waiting for user to complete sign in Fixed NullReferenceException when opening a new project and the user doesn't have a Unity Version Control organization linked to a Unity ID [2.0.3] - 2023-03-29 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Changed Changed the icons for Unity Version Control rebranding Changed onboarding workflow Fixed Fixed blurry icons in the Unity Version Control window and toolbar button Fixed Pending Changes tab not always opening its selected item's location in Project window Fixed \"Checked-out (changed)\" status icon not showing up on Pending Changes tab Fixed issue that prevented new packages from being installed unless user enters play mode [2.0.1] - 2023-02-17 Unity Version Control is now available as part of the Version Control Package! You can enable Unity Version Control via Window > Unity Version Control to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Changed Updated branding from \"Plastic SCM\" to \"Unity Version Control\" Improved offline experience by disabling the plugin when there is no internet connection [2.0.0] - 2023-01-11 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Changed Removed Collab from the package [1.17.7] - 2022-10-28 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added offline mode toggle for smoother offline experience Fixed Fixed performance issue with FindWorkspaceForPath method called multiple times every frame Fixed performance issue with UI.CooldownWindowDelayer.OnUpdate running on project without Plastic SCM workspace [1.17.6] - 2022-10-06 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Changed Changed the \"Go back to changeset\" option in Changesets tab to \"Revert to changeset\" Improved notification banner appearance Fixed Fixed editor refresh triggering when a workspace update is in progress Fixed pending changes show global ignored as private Removed encryption checkbox from create organization dialog [1.17.2] - 2022-07-06 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added notification banner on the status bar for live updates Changed Renamed \"Invite members to workspace\" option to \"Invite members to organization\" Fixed Fixed not being able to view changesets in a Gluon workspace Fixed not being able to insert carriage return in checkin dialog [1.17.1] - 2022-06-21 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Fixed Fixed missing references in synced prefabs [1.17.0] - 2022-06-13 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added option to enable changelists and display them in pending changes tab Added changelist related options to pending changes context menu Fixed Fixed editor hangs when there is no network available Fixed existing checkout has locked the workspace error Fixed checkin fails over unstable connection [1.15.18] - 2022-05-18 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Fixed Fixed editor hang when entering Play Mode [1.15.17] - 2022-04-27 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added checkin comment column to Incoming Changes view Changed Updated Go Back confirmation message to be consistent with feature Updated Create Child Branch dialog to focus on branch name field when opened Improved messaging of Subtractive Merge after using Go Back feature Fixed Fixed assets not added correctly when Plastic SCM window is not open Fixed wrong position of overlay icons on Pending Changes view Disallowed Go Back feature to a changeset from another branch [1.15.16] - 2022-03-28 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added \"Switch to changeset\" menu option in changesets view Added \"Go back to changeset\" menu option in changesets view Changed Removed category icons from views Removed \"com.unity.services.core\" package dependency Fixed Fixed light theme icons used in dark theme after pulling incoming changes Fixed \"Input string was not in a correct format\" error [1.15.15] - 2022-03-09 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added checkout option in scene prefab view Changed Updated file overlay icon size to adapt to project window zoom level Updated the styling of number of items in a category in Gluon incoming changes view Fixed Fixed Plastic X not opening from plugin menu Fixed error when trying to invite members to proect Fixed editor unhandled errors being hijacked by the plugin Fixed toolbar icon not displaying incoming changes notification when Plastic window is closed Fixed VCCache::instance != NULL error when opening a project with Plastic window opened [1.15.13] - 2022-02-14 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added branch name column in changeset view Changed Updated checkin comment box to keep the last comment after checkin error Fixed Fixed performance regression in large projects due to FindObjectsOfTypeAll calls [1.15.12] - 2022-01-27 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added option to \"Save Revision as\" to the context menu in the changesets view Added incoming changes overview bar for Gluon workspace Changed Updated the styling for number of items in category for pending changes view Updated the styling for number of items in category for changesets view Updated the styling for tabs close button Updated the color in different sections of the plugin Reduced dialog padding for the \"Create Branch\" dialog Updated the display overlay icons to show even if PlasticSCM window is closed Updated styling of number of items in incoming changes category Improved plugin initialization process and let the plugin functions without needing the Plastic window opened Disabled the invite button when user does not have invite permission or not on a cloud repo Fixed Fixed size info in incoming changes view does not match actual changes size Fixed checkin and checkout options not respecting inspector locked status Fixed buttons in inspector view displayed even when Plastic window is closed Fixed icon incorrect sizes Fixed errors on create branch dialog Fixed Newtonsoft.Json.dll conflicts with other external packages Fixed editor objects count increasing when hovering over Plastic window or toolbar button Fixed ArgumentOutOfRange exception when creating a branch Fixed scene reloading not happening after creating a new branch [1.15.7] - 2021-12-02 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added option to \"Save Revision as\" to the context menu in the changesets view Added incoming changes overview bar for Gluon workspace Changed Moved Plastic Package settings to the Unity Project Settings window Refined styling for Updating Workplace success state Updated texts for empty state and overview bar Removed Incoming Changes notification from empty state Updated the text for Forced Checkout option Refined the status overlay icons Updated the refresh icon on the toolbar Updated the texts for empty checkin message dialog Fixed Fixed capitalization of Pending Changes and File History tab names Fixed the amount of spacing after the Item column title in the Pending Changes tab Removed pin striping from line items in File History tab Fixed project view context menu and icons missing after Collaborate project migration Fixed migrated projects not downloading correctly from Unity Hub [1.15.4] - 2021-11-10 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Add option to \"Add to ignore file\" in context menu in the project view Added empty state message for Pending Changes tab Added success state message for Pending Changes tab Added metrics for Branches tab functionalities Changed Removed pinstriping in the Gluon Incoming Changes window Removed the “Nothing to download” bar from the Incoming Changes window when there are no items to download Changed the default metadata columns shown in the Incoming Changes screen Updated the alignment of sorting arrows to the right of the column Fixed Fixed UI overlays in Project view missing on changed assets when force checkout is disabled Fixed console error when selecting object in Scene view hierarchy or creating a new asset Fixed NullReferenceException after closing the Plastic SCM window [1.15.1] - 2021-10-21 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added visual overview bar to the incoming changes tab Added progress dialog for the migration process Added Branches tab that shows a list of all branches in the repository Added option and dialog to create a child branch from selected branch. Added option to switch to another branch Added option and dialog to rename a branch Added option to delete a branch Added a preference to save if the window should open the Branches tab by default Added metrics for Plastic SCM installation window usage Changed Updated texts for workspace modes selection and checkin comment box Updated status bar notification icons Fixed Fixed inverted text for the force checkout option Fixed typing capital O in checkin comment would open the selected item Fixed loading indicator not centered on Plastic SCM installation window Fixed installing Plastic SCM would sign out user from the plugin Removed extra refresh button on Gluon's Incoming Changes tab Fixed loading indicator not centered on Plastic SCM installation window Fixed missing Plastic SCM window option when user is not signed in on Unity Hub Removed meta file warning message for the deleted Beta folder Fixed Plastic SCM menu missing from Project view context menu [1.13.5] - 2021-09-27 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added workspace migration from Collab to Plastic which can be done with or without Plastic installed Added notification status icons Added light and dark mode versions of avatar icon Changed Updated texts for migration Improved usage analytics around Editor and Plugin version Workspace Migration Adjustments Fixed Renamed the CoreServices namespace so it doesn't conflict with other packages Devex integration to properly depend on Core Fixed some situations where the history window would be blank Fixed missing Enterprise login link Fixed low resolution icons in light theme [1.11.2] - 2021-08-27 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added horizontal scroll bar to Changesets list for easier viewing Added auto-login for SSO credentials handler Added metrics for changeset tab usage Added metrics for checkin actions Added new Undo icon Added missing API documentation Added ability to modify assets without checkout Added ability to allow empty checkin messages Added empty checking message localization Added Plastic toolbar button to Unity editor Added notification icon for incoming changes to Plastic toolbar button Changed Removed the unneeded refresh button from History Tab Moved search bar to the top right global icon section in all tabs Updated capitalization of options in the Settings context menu Updated tab button styling to be consistent with Unity Editor conventions Status bar visible across all tabs Moved refresh button to the toolbar at the top right corner of the window Moved changesets time period selector to the right corner of the window Removed \"Changes of changeset\" header on the Changesets tab Moved number of selected items next to \"Item\" metadata title on the Pending Changes tab Improved refresh icon resolution Changed changesets detail to appear in vertical column Reduced default number of columns in changesets tab The number of changesets is no longer displayed in changesets tab Changed Launch branch explorer into an icon with tooltip Removed the hide changes button in changesets tab Moved incoming change prompt and button into a status bar Changed \"Launch Plastic\" to \"Launch Plastic SCM\" in options menu Wording change for plastic installation Updated file status icons Fixed Fixed a bug where the Texture2D error would pop up after downloading a project Fixed a bug when context menu would sometimes disappear Fixed small textbox on checkin dialog when launched from context menu Fixed a workspace NullReferenceException bug Fixed notification icon not showing on Plastic window Fixed auto login errors not showing up for users Fixed unexpected error message after user switched workspace to a label [1.9.0] - 2021-07-13 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added Checkin and Update confirmation notification Added auto sign in when logged into Unity account Changed Simplified UI: decluttered UI Improved load time performance Fixed Fixed view not switching to workspace after creating an Enterprise Gluon workspace Fixed contextual menu not showing up in project view Fixed SSO renew token after password change Fixed some namespace collisions with Antlr3 [1.7.1] - 2021-06-25 Plastic SCM for Unity is now available as part of the Version Control Package! You can enable Plastic SCM via Window > Plastic SCM to get started! If you have previously used the Unity Asset Store Plastic SCM plug-in, you can now simply use this package. Make sure you delete the plug-in from your project. Removing a previously added Plastic SCM Asset Store Plug-In: Select the PlasticSCM folder in the Assets\\Plugins folder on the Project tab, then click Edit > Delete Close the Unity Editor and open your project again. You will find the Plastic SCM menu item in the Window menu. Added Added support for inviting other members. This option is available from the gear / settings icon. Added support for signing in with Cloud Edition. This is available during the onboarding screen if you have never signed in. Added support for turning off Plastic in their project. This option removes the Plastic metadata from your directory. This option is available under Assets > Plastic SCM > Turn off Plastic SCM Added notification on the Plastic SCM tab title to indicate incoming changes. Users will no longer need to have the Plastic SCM window visible to know there are incoming changes. Auto configuration of SSO Added date column in incoming changes Changed Updating license to better conform with expected customer usage. Updated documentation file to meet standards. Updated third-party usage. No longer requires downloading of the full Plastic client. Basic features will work without additional installation. Features that require the full Plastic client will allow download and install as needed. Usability improvements around checking in code Improved update workspace tab UX Plastic SCM context menu is now available even if the Plastic SCM window is closed Fixed Stability and performance improvements [1.5.7] - 2021-04-07 Unreleased The Version Control package will be expanding to include both Collaborate and Plastic SCM version control interfaces. This release is preparing for that move and contains no new functionality or bug fixes for Collaborate. Changed Collaborate Package renamed to Version Control with changes to package display name and description. Fixed Fixed NPE when updating the version of the Collab package. [1.3.9] - 2020-07-13 Fixed Unnecessary use of texture compression in icons that slowed down platform switching Update publish button state when selected changes update Use colorized icons when changes are available. [1.3.8] - 2020-06-08 Fixed Fix incorrect priority of error messages Fix Collab button being stuck in inprogress state Fix error when partially publishing without the window open [1.3.7] - 2020-01-30 Changed Bulk revert is now supported. Collab is blocked in play mode. Fixed Fixed services window's links to open Collab. [1.3.6] - 2020-01-21 Fixed Fixed compile errors when removing the NUnit package by removing unnecessary references. [1.3.5] - 2020-01-08 Fixed Fix \"accept mine\" / \"accept remote\" icon swap in conflicts view. [1.3.4] - 2019-12-16 Changed Window state is no longer restored after the window is closed and opened. Fixed History tab failing to load on startup if it is left open in the previous session. Progress bar percentage not matching the bar. History list correctly updates after a new revision is published. UI instabilities when restoring or going back to a revision with a different package manifest. Improve handling of changes to the project id. [1.3.3] - 2019-12-10 Changed Disable UI test cases that can be unstable. [1.3.2] - 2019-12-05 Changed Update UX to UIElements. Increased minimum supported version to 2020.1. Update Documentation to required standards. [1.2.16] - 2019-02-11 Fixed Update stylesheet to pass USS validation [1.2.15] - 2018-11-16 Changed Added support for non-experimental UIElements. [1.2.11] - 2018-09-04 Fixed Made some performance improvements to reduce impact on ReloadAssemblies. [1.2.9] - 2018-08-13 Fixed Test issues for the Collab History Window are now fixed. [1.2.7] - 2018-08-07 Fixed Toolbar drop-down will no longer show up when package is uninstalled. [1.2.6] - 2018-06-15 Fixed Fixed an issue where Collab's History window wouldn't load properly. [1.2.5] - 2018-05-21 This is the first release of Unity Package CollabProxy. Added Collab history and toolbar windows Collab view and presenter classes Collab Editor tests for view and presenter"
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/AccessRemoteProjects.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/AccessRemoteProjects.html",
    "title": "Access remote projects | FSM Unity Framework",
    "keywords": "Access remote projects In the Unity Hub v3, click Open > Open Remote Project to see the list of your version control repositories that contain a Unity project. Select the project and click Next. Select the Editor version and platform and click the change version button. Your local version control workspace will be created for you. The latest version of the project will be downloaded and the Editor will open with the latest version of your Unity project."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/AddMembers.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/AddMembers.html",
    "title": "Add team members | FSM Unity Framework",
    "keywords": "Add team members To invite team members to contribute to your project: Click the settings menu (gear icon) and click Invite Members to Workspace. In your DevOps version control dashboard, click Add new user. You can also send invitations and add different permission types for each user."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/CreateProjects.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/CreateProjects.html",
    "title": "Create projects | FSM Unity Framework",
    "keywords": "Create projects To create projects: In Unity, open the version control window and click on Create Workspace. It will suggest names for your repository (shared files and history) and workspace (your local copy). If you wish to use an existing version control repository, click the three dots next to the repository name, and select a repository from the list. Select the type of workspace that fits your needs. local workspace With this workspace, you can work with branching and merging. Gluon workspace This workspace tailored for artists allows you to pick the files you want to work on and check them back in without updating your whole workspace. Add asset files associated with your project. version control will display the project files from the asset folder in the Pending changes tab. You can choose specific files to include or add all to the repository by selecting the files and clicking Checkin changes. version control will automatically perform a check in for appropriate folders and files – such as package files and project settings – when it’s set up from the Unity Editor. You can view these in the Changesets tab. Once your initial asset check in is complete, you’re set up with version control for Unity and ready to create."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/ExistingRepo.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/ExistingRepo.html",
    "title": "Getting started with an existing Unity version control repository | FSM Unity Framework",
    "keywords": "Getting started with an existing Unity version control repository Suppose you want to start working on a Unity project in an existing Unity version control repository and already have a Unity version control account linked to your Unity ID. In that case, you will be able to open the project straight from the Unity Hub. A workspace will automatically be created for your project on your machine. In the Unity Hub v3 Beta, click Open > Open remote project to see the list of your Unity version control repositories that contain a Unity project. Click the project and click Next. Click the Editor version and platform and click the change version button. In the Editor pop-up, click the Migrate button to migrate your local workspace to a Unity version control workspace Once the migration is completed, click the Open Unity version control button. Accessing the Unity version control Window You can access the Unity version control window in the Unity Editor by clicking Window > Unity version control."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/Get started.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/Get started.html",
    "title": "Getting started with Unity version control | FSM Unity Framework",
    "keywords": "Getting started with Unity version control You can use Unity version control directly in Unity and is available via the Version Control package in the Unity Package Manager. Learn more about Unity version control Cloud Edition. To start with a new version control repository for your project, see Getting started with a new repository. To start from an existing Unity version control repository, see Getting started with an existing repository."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/GitUsers.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/GitUsers.html",
    "title": "Unity version control for Git users | FSM Unity Framework",
    "keywords": "Unity version control for Git users GIT Unity VCS Explanation To Commit To Check in To Check in is to submit changes to the repo. Commit Changeset Each new change on the history of the repo, grouping several individual file and directory changes. Master Main When you create a repo in Unity VCS, there's always an \"empty\" branch. Unity VCS calls it Main. To checkout To update Downloading content to the workspace (working copy). This is called \"update\" because in Unity VCS, \"checkout\" has a different meaning. Checkout When you checkout a file in Unity VCS, you're saying you are going to modify the file. Exclusive checkout or lock This is locking a file so nobody can touch it. It’s only useful for non-mergeable files, like binaries, images, or art in a video game. Rebase Unity VCS handles branching differently than Git. In Unity VCS, a rebase is just a merge operation. Repository Repository Where the entire history of the project is stored. Working copy Workspace In Git, you have the working copy and the repository in the exact location. You have a working copy and a .git hidden dir with the repository. In Unity VCS, this is slightly different since repositories and workspaces are separated. You can have several workspaces working with the same local repository."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/Glossary.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/Glossary.html",
    "title": "Glossary | FSM Unity Framework",
    "keywords": "Glossary General terms Ignore file A special file used in many Version Control Systems which specifies files to be excluded from version control. In Unity projects, several files can be excluded from version control. Using an Ignore File is the best way to achieve this. See Using external version control systems with Unity. Project In Unity, you use a project to design and develop a game. A project stores all of the files related to a game, such as the asset and Scene files. See 2D or 3D projects. Version Control A system for managing file changes. You can use Unity in conjunction with most version control tools, including Perforce , Git, Mercurial, and perforce. See Version Control. Unity version control terms Checkin Checkin is the act of submitting changes to the repo. You must enter a comment in the text box before you can check in your changes. Developer Workflow Developers have access to the branch explorer directly from inside Unity and easily switch branches. Gluon Workflow Artists can take advantage of the Gluon visualized interface and workflow from inside Unity. Organization The organization handles different sets of repositories in the Cloud. Inside the organization, you can create as many repositories as you need. Workspace Your workspace interacts with the version control, where you download the files and make the required changes for each checkin."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/MainFeatures.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/MainFeatures.html",
    "title": "Overview of features | FSM Unity Framework",
    "keywords": "Overview of features Pending Changes The Pending Changes tab allows you to view all pending changes in your workspace. These changes are not checked into the repository. In this tab, you can select which files you want to check in, add a comment, and check in the changes. Note : You can check in a specific file using the version control contextual menu in the project view or the Checkin button in the Inspector window. In the example below, the user adds a GameScene. They can check in the scene using the Pending Changes tab or the Checkin option in the contextual menu. Incoming Changes The Incoming Changes tab allows you to view all incoming changes and conflicts and update your local project. Any changes made to your project prompts an \"Incoming changes\" notification at the top right of the version control window. Tip : Check the Incoming Changes tab frequently to avoid facing future change conflicts in your team. Project History Use the Changesets tab to view all changes made to your project as they occur chronologically, along with who made the changes and when. You can sort by columns and alter the chronological view of the story. Double-click any file in a changeset to go to the File History tab, and display every changeset. In the File History view, right-click on a change and click Save the revision as… to restore the file's former state. This is useful if you had previously deleted some logic that you now need. You can also view the changes made to a specific file in the Project view through a contextual menu, then revert to an earlier revision of the file."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/MoreHelp.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/MoreHelp.html",
    "title": "More help | FSM Unity Framework",
    "keywords": "More help To find more information on working with the Unity version control plug-in, see Getting started with Unity Version control. You can also post and find questions related to Unity version control in the Unity forum."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/NewRepo.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/NewRepo.html",
    "title": "Getting started with a new version control repository | FSM Unity Framework",
    "keywords": "Getting started with a new version control repository Note: To start from an existing version control repository, see Getting started with an existing version control repository. You can walk through a straightforward onboarding wizard when creating a repository for your Unity project. This new wizard will help you: Set up your account and configure your repository for your Unity project, enabling you to sync to a version control Cloud Edition repository. Generate a standard ignore file that prevents unnecessary components of your Unity project from being checked in. Automatically do the first check-in so that your repository is in sync with your local changes. Open your Unity project. To access the version control window in the Unity Editor, click Window > version control: In the version control onboarding window, complete the steps to continue: Unity connects your project to your version control Cloud repository; version control automatically creates an ignore file in the workspace for Unity projects so it doesn't track files that shouldn't be part of the repository. It also creates a standard automatic checkin during the initial setup. So now you're all set to start using version control! Note: Basic version control actions, such as viewing pending changes, checking in changes, and viewing changesets, don’t require a version control Client install. However, if you want to use more advanced features, such as branching and diffing changeset, you will be prompted to download the version control client (if you have not already done so):"
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/QuickStartGuide.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/QuickStartGuide.html",
    "title": "Quick start guide | FSM Unity Framework",
    "keywords": "Quick start guide The Version Control package will allow you to use Unity Version Control (Unity VCS) for your projects in the Unity Editor. Unity VCS integrates version control in Unity that will abstract version control complexity. It will also enable you to work collaboratively on more complex projects by providing additional VCS features such as branching, locking, merging, and a standalone GUI. The Version Control package follows the Unity support schedule. Currently, the minimum supported version of the Unity Editor is 2021.3 LTS. Getting started with Unity version control"
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/ReconnectCB.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/ReconnectCB.html",
    "title": "Connect Unity Cloud Build | FSM Unity Framework",
    "keywords": "Connect Unity Cloud Build Unity Cloud Build is a continuous integration that automatically creates multiplatform builds in the Cloud in minutes. You can point Cloud Build toward your version control system to: Automate new builds Build faster Catch problems earlier Iterate on your builds more efficiently with agility. To get started, see Pay as you go with Cloud Build."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About version control Quick start guide Create projects Access remote projects Add team members Connect Cloud Build Get started with Unity version control Get started with a new version control repository Get started with an existing version control repository Main features Pending Changes Incoming Changes Project History Unity version control for Git users Glossary General terms version control terms More help"
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Documentation~/index.html",
    "title": "About Version Control | FSM Unity Framework",
    "keywords": "About Version Control The Version Control package provides an in-editor interface for teams to work with Unity Version Control (Unity VCS). Unity Version Control It is a free Unity plug-in that gives you the ability to use Unity VCS, a leading version control solution, directly in Unity. Get started with Unity VCS."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Version Control copyright © 2023 Unity Technologies Licensed under the Unity Package Distribution License (see https://unity3d.com/legal/licenses/Unity_Package_Distribution_License). Unless expressly provided otherwise, the software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/README.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/README.html",
    "title": "Unity Version Control Package | FSM Unity Framework",
    "keywords": "Unity Version Control Package This package provides an in-editor interface for teams to work with Unity Version Control (Unity VCS), our leading version control solution, directly in Unity. Note this project is the natural evolution of the old Collaborate package, hence its name. Documentation - Changelog - Yamato Compatibility The minimum supported version of the Unity Editor is 2021.3 LTS. Windows and macOS are officially supported. The solution is exclusively targeting .NetStandard 2.0, and will not work with the legacy Mono runtime. Maintenance This project is currently maintained by the VCS Ecosystem team (@vcs-ecosystem-team), part of UGS DevOps. All suggestions and issues are very welcome in the Slack channel #devs-unity-version-control. Development For developers Option 1: clone this repository out into the packages/ directory in a project. Option 2: clone elsewhere and link with the packages/manifest.json file in the project: \"com.unity.collab-proxy\": \"file:/some/path/to/package\" To add testing support also add the testibles section to the manifest. Your manifest should look like this: { \"dependencies\": { \"com.unity.collab-proxy\": \"file:/some/path/to/package\", ... }, \"testables\": [ \"com.unity.collab-proxy\", ... ] } For internal testers Simply add the git url into the packages/manifest.json file: \"com.unity.collab-proxy\": \"git://git@github.cds.internal.unity3d.com:unity/com.unity.cloud.collaborate.git\" If you need a specific revisision: \"com.unity.collab-proxy\": \"git://git@github.cds.internal.unity3d.com:unity/com.unity.cloud.collaborate.git#<rev>\" If you need more information, read the Documentation for package dependencies from git. Code style is as dictated in Unity Meta."
  },
  "Library/PackageCache/com.unity.collab-proxy@2.2.0/Third Party Notices.html": {
    "href": "Library/PackageCache/com.unity.collab-proxy@2.2.0/Third Party Notices.html",
    "title": "| FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the license(s) indicated below: Component Name: File System Watcher https://docs.microsoft.com/en-us/dotnet/api/system.io.filesystemwatcher?view=net-5.0 License Type: MIT Copyright (c) Microsoft Corporation Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Component Name: Zlib64 https://zlib.net/ License Type: zlib/libpng License (Zlib) version 1.2.11, January 15th, 2017 Copyright (C) 1995-2017 Jean-loup Gailly and Mark Adler This software is provided 'as-is', without any express or implied warranty. In no event will the authors be held liable for any damages arising from the use of this software. Permission is granted to anyone to use this software for any purpose, including commercial applications, and to alter it and redistribute it freely, subject to the following restrictions: The origin of this software must not be misrepresented; you must not claim that you wrote the original software. If you use this software in a product, an acknowledgment in the product documentation would be appreciated but is not required. Altered source versions must be plainly marked as such, and must not be misrepresented as being the original software. This notice may not be removed or altered from any source distribution. Jean-loup Gailly Mark Adler jloup@gzip.org madler@alumni.caltech.edu Component Name: LZ4 Library/LZ4x64 https://github.com/lz4 License Type: BSD [The BSD License] Copyright (c) 2011-2020, Yann Collet All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Component Name: Antlr3/Antlr https://www.antlr3.org/ License Type: BSD [The BSD License] Copyright (c) 2010 Terence Parr All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the author nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Component Name: Log4Net https://logging.apache.org/log4net/ License Type: Apache 2.0 Copyright (c) 2004-2017 The Apache Software Foundation Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
  },
  "Library/PackageCache/com.unity.editorcoroutines@1.0.0/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.editorcoroutines@1.0.0/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [1.0.0] - 2020-04-01 Changed Package has been verified for 2020.2 [0.1.0-preview.2] - 2020-03-17 Changed Lowered data size for the EditorWaitForSeconds class by half. [0.1.0-preview.1] - 2020-01-08 Added Added support for AsyncOperation subclasses. Changed Fixed unstable test. [0.0.2-preview.1] - 2019-01-25 Changed Fixed a compilation issue caused by using the 'default' literal. [0.0.1-preview.5] - 2019-01-14 Changed Updated Readme.md. Added unified yield statement processor. Added stack based processing of nested yield statements. Updated tests. Lowered memory footprint of editor coroutine instances. Removed Removed recursive handling of nested yield statements. Removed specialized yield statement processors. [0.0.1-preview.4] - 2018-12-7 Added API documentation. Changed Fixed line endings for the EditorCourtineTests.cs source file. [0.0.1-preview.3] - 2018-10-11 Changed Updated LICENSE.md. Updated manifest to reflect correct minimum supported version. [0.0.1-preview.2] - 2018-10-11 Added Added stub documentation via com.unity.editorcoroutines.md. [0.0.1-preview.1] - 2018-10-10 Added Added nesting support for editor coroutines. Added abitrary enumerator support for editor coroutines. Created specialized EditorWaitForSeconds class with access to it's wait time ( same behavior as WaitForSeconds). This is the first release of Unity Package Editor Coroutines. Source code release of the Editor Coroutines package, with no added documentation or stripping of default Package Creation Kit files."
  },
  "Library/PackageCache/com.unity.editorcoroutines@1.0.0/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.editorcoroutines@1.0.0/Documentation~/TableOfContents.html",
    "title": "Unity Editor Coroutines | FSM Unity Framework",
    "keywords": "Unity Editor Coroutines Editor Coroutines overview"
  },
  "Library/PackageCache/com.unity.editorcoroutines@1.0.0/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.editorcoroutines@1.0.0/Documentation~/index.html",
    "title": "About Editor Coroutines | FSM Unity Framework",
    "keywords": "About Editor Coroutines The Editor Coroutines package allows the user to start the execution of iterator methods within the Editor similar to how we handle Coroutines inside MonoBehaviour scripts during runtime. Installing Editor Coroutines To install this package, follow the instructions in the Package Manager documentation. Note: While this package is in preview, the Package Manager needs to be configured to show Preview Packages. (Under the Advanced drop-down menu, enable Show preview packages.) Then search for the Editor Coroutines package. Using Editor Coroutines To learn how to use the Editor Coroutines package in your project, please refer to the Scripting API section of the documentation. Technical details Requirements This version of Editor Coroutines is compatible with the following versions of the Unity Editor: 2018.1 and later (recommended) Note: If you install the Memory Profiler package it will automatically install the Editor Coroutines package as a dependency. Known limitations Editor Coroutines version 0.0.1-preview.2 includes the following known limitation(s): The iterator functions passed to Editor Coroutines do not support yielding any of the instruction classes present inside the Unity Scripting API (e.g., WaitForSeconds, WaitForEndOfFrame), except for the CustomYieldInstruction derived classes with the MoveNext method implemented. Tip: yield return null is a way to skip a frame within the Editor. Package contents The following table indicates the root folders in the package where you can find useful resources: Location Description Documentation~ Contains the documentation for the package. Tests Contains the unit tests for the package. Document revision history Date Reason June 20, 2019 Removed deprecated manual link. Dec 7, 2018 Api documentation added. Matches package version 0.0.1-preview.4. Oct 11, 2018 Document created. Matches package version 0.0.1-preview.2."
  },
  "Library/PackageCache/com.unity.editorcoroutines@1.0.0/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.editorcoroutines@1.0.0/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.core.editorcoroutines copyright © 2018 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.editorcoroutines@1.0.0/README.html": {
    "href": "Library/PackageCache/com.unity.editorcoroutines@1.0.0/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "Editor Coroutines"
  },
  "Library/PackageCache/com.unity.ext.nunit@1.0.6/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ext.nunit@1.0.6/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.0.6] - 2020-11-30 isExplicitlyReferenced set to 0 (case 1296162) [1.0.5] - 2020-11-04 Removed pdb files [1.0.4] - 2020-11-03 Added the portable-pdb (DSTR-37) [1.0.3] - 2020-10-30 Fixed being able to load mdb or portable-pdb symbolsbug (DSTR-37) Minimum unity version updated (case 1279253) [1.0.2] - 2019-12-04 Added missed metafiles [0.0.1] - 2019-02-21 This is the first release of Unity Package com.unity.ext.nunit. Migrated the custom version of nunit from inside of unity."
  },
  "Library/PackageCache/com.unity.ext.nunit@1.0.6/Documentation~/ext.nunit.html": {
    "href": "Library/PackageCache/com.unity.ext.nunit@1.0.6/Documentation~/ext.nunit.html",
    "title": "Custom Nunit build to work with Unity | FSM Unity Framework",
    "keywords": "Custom Nunit build to work with Unity This version of nunit works with all platforms, il2cpp and Mono AOT. For Nunit Documentation: https://github.com/nunit/docs/wiki/NUnit-Documentation"
  },
  "Library/PackageCache/com.unity.ext.nunit@1.0.6/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ext.nunit@1.0.6/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Copyright (c) 2018 Charlie Poole, Rob Prouse Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.ext.nunit@1.0.6/README.html": {
    "href": "Library/PackageCache/com.unity.ext.nunit@1.0.6/README.html",
    "title": "Custom Nunit build to work with Unity | FSM Unity Framework",
    "keywords": "Custom Nunit build to work with Unity This version of nunit works with all platforms, il2cpp and Mono AOT. For Nunit Documentation: https://github.com/nunit/docs/wiki/NUnit-Documentation"
  },
  "Library/PackageCache/com.unity.feature.development@1.0.1/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.feature.development@1.0.1/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.0.1] - 2021-06-15 Changes Added the Code Coverage package [1.0.0] - 2021-04-23 This is the first release of Engineering feature set."
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/CHANGELOG.html",
    "title": "Code Editor Package for Rider | FSM Unity Framework",
    "keywords": "Code Editor Package for Rider [3.0.26] - 2023-10-04 https://github.com/JetBrains/resharper-unity/issues/2421 https://github.com/JetBrains/resharper-unity/issues/2422 [3.0.25] - 2023-08-18 unification of functionality to search JetBrains installations and open solution and file in Rider [3.0.22] - 2023-05-2 RIDER-82999 Unity's plugin SyncAll does not regenerate project files, and instead does basically nothing. #2401 Compilation issue with Unity 2021.3.0f1 [3.0.21] - 2023-04-18 RIDER-92424 JetBrains Rider Editor 3.0.20 package Update for Unity, Cause's Rider to Slows to a Crawl after updating RIDER-92419 JetBrains Rider Editor 3.0.20 for Unity has duplicate assemblies loaded into runtime [3.0.20] - 2023-04-05 fix loading Rider integration EditorPlugin on first switch of External Editor to Rider, see RIDER-91185 Keep the the PackageManager in sync with the Rider changes made to the manifest.json, it should help with RIDER-77343 Support CompilerOptions.RoslynAdditionalFilePaths and CompilerOptions.AnalyzerConfigPath [3.0.18] - 2023-01-09 RIDER-74818 Unity doesn't get to play mode if Editor is not running and user starts debug or profiling Improve performance of project generation - avoid using Directory.Exists avoid doing ProjectGeneration twice on the first start-up [3.0.17] - 2022-12-01 Avoid adding asset project parts to both editor and player projects, fixes the following issues: RIDER-75500 Local package references completions shows duplicate entries if player projects are generated RIDER-73795 Conversion to guid is not offered for assemblies with generated player projects RIDER-71238 No usages can be found for the assembly if player projects are generated [3.0.16] - 2022-09-09 Update the changelog Add folders to the generated csproj files Avoid extra RequestScriptReload call on the first start Fix shader support for folders in packages, but outside asmdef [3.0.15] - 2022-05-24 Cleanup cache after project generation to reduce memory consumption Performance optimization RIDER-76126 Rider package should generate an empty csproj for empty Unity project RIDER-77206 Unity 2020.1.3 'PlayerSettings' does not contain a definition for 'suppressCommonWarnings [3.0.14] - 2022-04-21 Move Rider package persisted state to Library, to avoid vcs collisions or adding it specifically to gitignore [3.0.13] - 2022-03-24 fix RIDER-69927 \"Test not run\" status is shown for the test suite when running unit tests for Unity project fix RIDER-74676 Unity plugin \"JetBrainseRider Editor\" completely breaks <= 2019.1.9 fix RIDER-71503 Unity Hang on \"Domain Unload\", caused by dispose of FileSystemWatcher [3.0.12] - 2022-01-28 Fix bug, which was introduced in 3.0.10: New script was not added to the csproj, because cached list of assemblies was used. [3.0.10] - 2021-12-09 Fix presentation of the TargetFramework in the csproj Fix: Auto-generated solution doesn't compile when code overrides virtual functions in other assemblies Fix RIDER-72234 Avoid full project generation, when only content of assembly was changed Fix RIDER-71985 Building large Unity projects randomly fails Fix RIDER-72174 Looking for Rider installed by dotUltimate installer [3.0.9] - 2021-11-09 Fix path for Roslyn analyser supplied with a package Minimal requirement for roslyn analyzer scope is Unity 2020.3.6f1 and above [3.0.8] - 2021-11-08 Technical release [3.0.7] - 2021-05-07 RIDER-60815 Simplify extensions lists for Rider package Fix csc.rsp -nullable+ / -nullable- parsing https://github.com/van800/com.unity.ide.rider/issues/7 Support -warnaserror/-warnaserror-:/-warnaserror+: in csc.rsp [3.0.6] - 2021-04-06 Fix bug: For Unity 2021.1+ Switching external editor from VS => Rider won't create the connection between Unity and Rider. When PlayerSettings.suppressCommonWarnings is true, it is reflected in the generated csproj with NoWarn \"0169\", \"0649\" By default include T4 templates in the generated solution (RIDER-37159) RIDER-60554 Unity crash in case of project without Unity Test Framework Package. RIDER-60445 Fix presentation of Rider external editor, when it is installed in a custom location. Improve project files generation performance RIDER-60508 Project Generation for projects without any cs files - add reference to UnityEditor/UnityEngine, so that Rider would detect Unity path and version and provide rich features for shader file. [3.0.5] - 2021-02-25 More stable in case of possible Rider product code change, improve test. Allows using \"Rider for Unreal\" with Unity projects (https://youtrack.jetbrains.com/issue/RIDER-51203) Remove implicit dependency to Test-Framework package Fix \"Unreachable code detected\" warning (https://youtrack.jetbrains.com/issue/RIDER-57930) [3.0.4] - 2021-01-26 Use LangVersion provided by Unity for generated csproj Improve documentation Support nullable provided in csc,rsp Avoid doing work in Unity secondary processes in UNITY_2021_1_OR_NEWER with UnityEditor.MPE.ProcessLevel.Secondary [3.0.3] - 2020-11-18 Update License Avoid connecting Rider from secondary UnityEditor instances Fix RIDER-53082 - Generate csproj without cs files, when there are any assets inside [3.0.2] - 2020-10-27 Speedup ProjectGeneration Fix RIDER-51958. Callbacks OnGeneratedCSProjectFiles would not work, but show a Warning instead. Remove release configuration Call RequestScriptReload, when External Editor is changed in Unity. [3.0.1] - 2020-10-02 RIDER-46658 Rider does not run PlayMode tests when ValueSource is combined with parameterized TestFixture RIDER-49947 Invoking PlayerSettings.SetScriptingDefineSymbolsForGroup() does not update definitions in Rider. Add static entrypoint Packages.Rider.Editor.RiderScriptEditor.SyncSolution to allow generating solution from commandline. [2.0.7] - 2020-08-18 Improve performance Add support for asmdef Root Namespace in .csproj generation ProjectGeneration for custom roslyn analysers https://docs.unity3d.com/2020.2/Documentation/Manual/roslyn-analyzers.html Switch target platform in Unity would regenerate csproj files (https://github.com/JetBrains/resharper-unity/issues/1740) [2.0.6] - 2020-08-10 Improve performance Add support for asmdef Root Namespace in .csproj generation ProjectGeneration for custom roslyn analysers https://docs.unity3d.com/2020.2/Documentation/Manual/roslyn-analyzers.html Switch target platform in Unity would regenerate csproj files (https://github.com/JetBrains/resharper-unity/issues/1740) [2.0.5] - 2020-05-27 Fix Regression in 2.0.3: In Unity 2019.2.9 on Mac, changing csproj and calling AssetDatabase.Refresh is not regenerating csproj. Regenerate projects on changes in manifest.json and Project Settings (EditorOnlyScriptingUserSettings.json) (#51) Fix: Assembly references to package assemblies break IDE projects. Fix: Reporting test duration. [2.0.2] - 2020-03-18 fix bug in searching Rider path on MacOS [2.0.1] - 2020-03-05 Speed improvements, ProjectTypeGuids for unity-generated project Improve UI for Project Generation settings Changes in csc.rsp would cause project-generation Remove NoWarn 0169 from generated csproj Support custom JetBrains Toolbox installation location [1.2.1] - 2019-12-09 Load optimised EditorPlugin version compiled to net 461, with fallback to previous version. On ExternalEditor settings page: reorder Generate all ... after Extensions handled Better presentation for Rider of some version in ExternalEditors list Initial support for Code Coverage with dotCover plugin in Rider Added support for Player Project generation [1.1.4] - 2019-11-21 Fix warning - unreachable code [1.1.3] - 2019-10-17 Update External Editor, when new toolbox build was installed Add xaml to default list of extensions to include in csproj Avoid initializing Rider package in secondary Unity process, which does Asset processing Reflect multiple csc.rsp arguments to generated csproj files: https://github.com/JetBrains/resharper-unity/issues/1337 Setting, which allowed to override LangVersion removed in favor of langversion in csc.rsp Environment.NewLine is used in generated project files instead of Windows line separator. [1.1.2] - 2019-09-18 performance optimizations: avoid multiple evaluations avoid reflection in DisableSyncSolutionOnceCallBack project generation optimization fixes: avoid compilation error with incompatible Test Framework package [1.1.1] - 2019-08-26 parse nowarn in csc.rsp warning, when Unity was started from Rider, but external editor was different improved unit test support workaround to avoid Unity internal project-generation (fix #28) [1.1.0] - 2019-07-02 new setting to manage list of extensions to be opened with Rider avoid breaking everything on any unhandled exception in RiderScriptEditor cctor hide Rider settings, when different Editor is selected dynamically load only newer rider plugins path detection (work on unix symlinks) speed up for project generation lots of bug fixing [1.0.8] - 2019-05-20 Fix NullReferenceException when External editor was pointing to non-existing Rider everything was broken by null-ref. [1.0.7] - 2019-05-16 Initial migration steps from rider plugin to package. Fix OSX check and opening of files. [1.0.6] - 2019-04-30 Ensure asset database is refreshed when generating csproj and solution files. [1.0.5] - 2019-04-27 Add support for generating all csproj files. [1.0.4] - 2019-04-18 Fix relative package paths. Fix opening editor on mac. [1.0.3] - 2019-04-12 Fixing null reference issue for callbacks to Asset pipeline. [1.0.2] - 2019-01-01 This is the first release of Unity Package rider_editor. Using the newly created api to integrate Rider with Unity."
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/CONTRIBUTING.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/CONTRIBUTING.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing All contributions are subject to the Unity Contribution Agreement(UCA) By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions. Once you have a change ready following these ground rules. Simply make a pull request"
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/README.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/README.html",
    "title": "Code Editor Package for Rider | FSM Unity Framework",
    "keywords": "Code Editor Package for Rider This package is not intended to be modified by users. Nor does it provide any api intended to be included in user projects."
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About JetBrains Rider Editor Using the JetBrains Rider Editor package"
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/index.html",
    "title": "About JetBrains Rider Editor | FSM Unity Framework",
    "keywords": "About JetBrains Rider Editor The JetBrains Rider editor package integrates support for the JetBrains Rider .NET Integrated Development Environment (IDE), into the Unity Editor. This package provides an end-point for Rider to call different Unity APIs and to generate .csproj and .sln files, which Rider uses to implement support for Unity in its plug-in. This package ensures that IDE features like autocomplete suggestions and flagging dependency conflicts work in Rider. It uses .cproj and .sln files which store information about your project such as: Versioning information Build files Platform requirements Web server or database settings Not all code in Unity is directly visible to code editors, particularly when using packages. This is because packages don’t provide their own .csproj files, and Unity doesn’t create them for installed packages by default. This means that IDE features like autocomplete suggestions and flagging dependency conflicts do not work with code in these packages. The purpose of this package is to produce the .csproj files that make these features possible by default when you use Rider. Installation As of Unity version 2019.2, this package comes as a part of the default Unity installation. If you are updating your project from an older version of Unity, you might need to install this package via the Package Manager. Requirements This version of the JetBrains Rider editor package is compatible with the following versions of the Unity Editor: 2019.2.6 or later To use this package, you must have the following third-party products installed: JetBrains Rider version 2019.3 or newer For more information about the Rider IDE, see the JetBrains Rider documentation. Submitting issues This package is maintained by JetBrains and Unity. Submit issues to the JetBrains/resharper-unity/issues GitHub page. Unity intends for this package to become accessible to the public on GitHub in the future."
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/using-the-jetbrains-rider-editor-package.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/Documentation~/using-the-jetbrains-rider-editor-package.html",
    "title": "Using the JetBrains Rider Editor package | FSM Unity Framework",
    "keywords": "Using the JetBrains Rider Editor package To use the package, go to Edit > Preferences > External Tools, click on the External Script Editor dropdown menu and select your version of Rider. When you select this option, the window reloads. After the window reloads, new settings that control production of .csproj files become available. External Tools tab in the Preferences window Commandline endpoints Q: Generate sln/csproj files for CI? A: Unity -batchmode -quit -projectPath ProjectPath -executeMethod Packages.Rider.Editor.RiderScriptEditor.SyncSolution Q: Generate sln/csproj and open External Editor? A: Unity -batchmode -quit -projectPath ProjectPath -executeMethod Packages.Rider.Editor.RiderScriptEditor.SyncSolutionAndOpenExternalEditor Package preferences Property: Description: Extensions handled This field lists the file extensions that open in JetBrains Rider. This field contains a variety of extensions by default. Generate .csproj files for: Each setting in this list enables or disables production of .csproj files for a different type of package. The Regenerate project files button updates existing .csproj files and creates the necessary new ones based on the settings you choose. These settings control whether to generate .csproj files for any installed packages. For more information on how to install packages, see the Adding and removing packages documentation. __ Embedded packages__ Any package that appears under your project’s Packages folder is an embedded package. An embedded package is not necessarily built-in; you can create your own packages and embed them inside your project. This setting is enabled by default. For more information on embedded packages, see the Embedded dependencies documentation. __ Local packages__ Any package that you install from a local repository stored on your machine, but from outside of your Unity project. This setting is enabled by default. __ Registry packages__ Any package that you install from either the official Unity registry or a custom registry. Packages in the Unity registry are available to install directly from the Package Manager. For more information about the Unity package registry, see the Package Registry section of the Unity Package Manager documentation. For information on creating and using custom registries in addition to the Unity registry, see the Scoped package registries documentation. __ Git packages__ Any package you install directly from a Git repository using a URL. __ Built-in packages__ Any package that is already installed as part of the default Unity installation. __ Tarball packages__ Any package you install from a GZip tarball archive on the local machine, outside of your Unity project. __ Unknown packages__ Any package which Unity cannot determine an origin for. This could be because the package doesn’t list its origin, or that Unity doesn’t recognize the origin listed. Player projects For each player project, generate an additional .csproj file named 'originalProjectName.Player.csproj'. This allows different project types to have their code included in Rider’s systems, such as assembly definitions or testing suites. This package also adds a second tab under Preferences named Rider, pictured below. Rider tab in the Preferences window Note The Logging Level menu does not control the level of Unity's logging, only the level of log messages that Rider package logs in its own log file. For more information on controlling Unity's logging level, see the Stack Trace Logging section of the Console Window documentation. Property: Description: Pass Console to Rider If Pass Console to Rider is enabled, Rider can access data that Unity sends to the Unity Console and display it within its own environment instead. Log file The Log file field contains an Open log button. Select this button to open the log file inside the Rider IDE. This button is unavailable when Logging Level is set to OFF. Logging Level The Logging Level menu controls how detailed are the Rider package logs. Those logs may be used for troubleshooting communication between Rider and Unity. Rider package logs all messages of the type you select as well as any messages of a more severe type. For example, if you choose WARN, then Rider logs all ERROR and FATAL messages as well as WARN messages. The message types are listed below in order of severity, with FATAL as the most severe type of message and TRACE as the least severe. OFF Rider does not produce any logs. **FATAL Logs information relating to serious problems that cause the application to crash. This setting produces the smallest logs. ERROR Logs information about errors that prevent some functionality from working, but don’t cause the application to fail (for example, a failed database connection). WARN Logs information about possible problems, or any unusual behaviour. Warnings don’t indicate that something has gone wrong, but that Unity detects something that might potentially cause an issue if not investigated. INFO Logs information about normal operation of the application, such as a successful database connection attempt. VERBOSE Logs detailed but not exhaustive information about your code. This setting is helpful for checking how your code executes or providing diagnostic information for other developers. TRACE Logs as much information about the application as possible. This can create a very large and detailed log, so it’s good practice to only use it when attempting to find the cause of a specific issue with your code."
  },
  "Library/PackageCache/com.unity.ide.rider@3.0.26/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ide.rider@3.0.26/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "MIT License Copyright (c) 2019 Unity Technologies Copyright (c) 2019 JetBrains s.r.o. All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/CHANGELOG.html",
    "title": "Code Editor Package for Visual Studio | FSM Unity Framework",
    "keywords": "Code Editor Package for Visual Studio [2.0.22] - 2023-10-03 Integration: Add support for XDG_DATA_DIRS and .desktop files on Linux for VS Code discovery. Use compile-time platform-specifics instead of using runtime conditions. Project generation: Suppress USG0001 warnings. Mark referenced assemblies as private (to not copy extra files to output directory when building). Add Unity capability to SDK-Style projects. Prevent circular dependency errors with SDK-Style projects. [2.0.21] - 2023-09-05 Integration: Only disable the legacy com.unity.ide.vscode package going forward. Fix json parsing issues with specific non-UTF code pages. Project generation: Target netstandard2.1 instead of netstandard2.0. Set defaultSolution in settings.json. Remove files.exclude entries for root csproj and sln files in settings.json when needed. Add vstuc launch configuration to launch.json when needed. Add visualstudiotoolsforunity.vstuc entry to extensions.json when needed. You can prevent the package from patching those configuration files by creating a .vscode/.vstupatchdisable file. [2.0.20] - 2023-06-27 Integration: Internal API refactoring. Add support for Visual Studio Code. Project generation: Add support for Sdk Style project generation. Fix an issue related to missing properties with 2021.3. [2.0.18] - 2023-03-17 Integration: Performance improvements with EditorApplication.update callbacks. Project generation: Add extra compiler options for analyzers and source generators. [2.0.17] - 2022-12-06 Integration: Fix rare deadlocks while discovering or launching Visual Studio on Windows. Improve launching Visual Studio on macOs. Project generation: Include analyzers from response files. Update supported C# versions. Performance improvements. [2.0.16] - 2022-06-08 Integration: Prevent ADB Refresh while being in safe-mode with a URP project Fixed an issue keeping the progress bar visible even after opening a script with Visual Studio. [2.0.15] - 2022-03-21 Integration: Improved project generation performance. Added support for keeping file/folder structure when working with external packages. Fixed project generation not being refreshed when selecting Visual Studio as the preferred external editor. [2.0.14] - 2022-01-14 Integration: Remove package version checking. [2.0.13] - 2022-01-12 Integration: Fixed wrong path to analyzers in generated projects when using external packages. Fixed selective project generation not creating Analyzer/LangVersion nodes. Fixed asmdef references with Player projects. Documentation: Added new documentation including ToC, overview, how to use and images. [2.0.12] - 2021-10-20 Integration: Do not block asset opening when only a VS instance without a loaded solution is found. Only check package version once per Unity session. Improved support for Visual Studio For Mac 2022. [2.0.11] - 2021-07-01 Integration: Added support for Visual Studio and Visual Studio For Mac 2022. Fixed an issue when the package was enabled for background processes. Project generation: Use absolute paths for Analyzers and rulesets. [2.0.10] - 2021-06-10 Project generation: Improved project generation performance when a file is moved, deleted or modified. Integration: Improved Inner-loop performance by avoiding to call the package manager when looking up vswhere utility. Fixed a network issue preventing the communication between Visual Studio and Unity on Windows. [2.0.9] - 2021-05-04 Project generation: Added support for CLI. Integration: Improved performance when discovering Visual Studio installations. Warn when legacy assemblies are present in the project. Warn when the package version is not up-to-date. [2.0.8] - 2021-04-09 Project generation: Improved generation performance (especially with DOTS enabled projects). Improved stability. Updated Analyzers lookup strategy. Fixed .vsconfig file not generated when using \"regenerate all\". Integration: Improved automation plugins. Documentation: Open sourced automation plugins. [2.0.7] - 2021-02-02 Integration: Remove com.unity.nuget.newtonsoft-json dependency in favor of the built-in JsonUtility for the VS Test Runner. [2.0.6] - 2021-01-20 Project generation: Improved language version detection. Integration: Added support for the VS Test Runner. Added initial support for displaying asset usage. Fixed remaining issues with special characters in file/path. [2.0.5] - 2020-10-30 Integration: Disable legacy pdb symbol checking for Unity packages. [2.0.4] - 2020-10-15 Project generation: Added support for embedded Roslyn analyzer DLLs and ruleset files. Warn the user when the opened script is not part of the generation scope. Warn the user when the selected Visual Studio installation is not found. Generate a .vsconfig file to ensure Visual Studio installation is compatible. Integration: Fix automation issues on MacOS, where a new Visual Studio instance is opened every time. [2.0.3] - 2020-09-09 Project generation: Added C#8 language support. Added UnityProjectGeneratorVersion property. Local and Embedded packages are now selected by default for generation. Added support for asmdef root namespace. Integration: When the user disabled auto-refresh in Unity, do not try to force refresh the Asset database. Fix Visual Studio detection issues with languages using special characters. [2.0.2] - 2020-05-27 Added support for solution folders. Only bind the messenger when the VS editor is selected. Warn when unable to create the messenger. Fixed an initialization issue triggering legacy code generation. Allow package source in assembly to be generated when referenced from asmref. [2.0.1] - 2020-03-19 When Visual Studio installation is compatible with C# 8.0, setup the language version to not prompt the user with unsupported constructs. (So far Unity only supports C# 7.3). Use Unity's TypeCache to improve project generation speed. Properly check for a managed assembly before displaying a warning regarding legacy PDB usage. Add support for selective project generation (embedded, local, registry, git, builtin, player). [2.0.0] - 2019-11-06 Improved Visual Studio and Visual Studio for Mac automatic discovery. Added support for the VSTU messaging system (start/stop features from Visual Studio). Added support for solution roundtrip (preserves references to external projects and solution properties). Added support for VSTU Analyzers (requires Visual Studio 2019 16.3, Visual Studio for Mac 8.3). Added a warning when using legacy pdb symbol files. Fixed issues while Opening Visual Studio on Windows. Fixed issues while Opening Visual Studio on Mac. [1.1.1] - 2019-05-29 Fix Bridge assembly loading with non VS2017 editors. [1.1.0] - 2019-05-27 Move internal extension handling to package. [1.0.11] - 2019-05-21 Fix detection of visual studio for mac installation. [1.0.10] - 2019-05-04 Fix ignored comintegration executable. [1.0.9] - 2019-03-05 Updated MonoDevelop support, to pass correct arguments, and not import VSTU plugin. Use release build of COMIntegration for Visual Studio. [1.0.7] - 2019-04-30 Ensure asset database is refreshed when generating csproj and solution files. [1.0.6] - 2019-04-27 Add support for generating all csproj files. [1.0.5] - 2019-04-18 Fix relative package paths. Fix opening editor on mac. [1.0.4] - 2019-04-12 Fixing null reference issue for callbacks to AssetPostProcessor. Ensure Path.GetFullPath does not get an empty string. [1.0.3] - 2019-01-01 This is the first release of Unity Package visualstudio_editor. Using the newly created api to integrate Visual Studio with Unity."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/CONTRIBUTING.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/CONTRIBUTING.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing All contributions are subject to the Unity Contribution Agreement(UCA) and Microsoft Contributor License Agreement (CLA) By making a pull request, you are confirming agreement to the terms and conditions of the UCA and CLA, including that your contributions are your original creation and that you have complete right and authority to make your contributions. Once you have a change ready following these ground rules. Simply make a pull request"
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/README.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/README.html",
    "title": "Code Editor Package for Visual Studio | FSM Unity Framework",
    "keywords": "Code Editor Package for Visual Studio This package is not intended to be modified by users. Nor does it provide any api intended to be included in user projects."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Visual Studio Editor Using the Visual Studio Editor package"
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/index.html",
    "title": "Code Editor Package for Visual Studio | FSM Unity Framework",
    "keywords": "Code Editor Package for Visual Studio About Visual Studio Editor The Visual Studio Editor package provides the Unity Editor with support for Unity-specific features from the Visual Studio Tools for Unity extension in Visual Studio and Visual Studio for Mac. These include IntelliSense auto-complete suggestions, C# editing, and debugging. Installation This package is a built-in package and installed by default. Note: If you’re using a version of the Unity Editor before 2019.4, you’ll need to install this package through the package manager. Requirements This version of the Visual Studio Editor package is compatible with the following versions of the Unity Editor: 2019.4 and later To use this package, you must have the following third-party products installed: On Windows: Visual Studio 2019 version 16.9 or newer with Visual Studio Tools for Unity 4.0.9 or newer. On macOS: Visual Studio for Mac 2019 version 8.9 or newer with Visual Studio Tools for Unity 2.0.9 or newer. For more information about using Visual Studio with Unity, see Microsoft’s Visual Studio Tools for Unity documentation. Submitting issues This package is maintained by Microsoft and Unity. Submit issues directly from Visual Studio and Visual Studio for Mac from the Help > Submit Feedback > Report a Problem menu. Unity will make this package accessible to the public on GitHub in the future."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/using-visual-studio-editor.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/Documentation~/using-visual-studio-editor.html",
    "title": "Using the Visual Studio Editor package | FSM Unity Framework",
    "keywords": "Using the Visual Studio Editor package To use the package, go to Edit > Preferences > External Tools > External Script Editor and select the version of Visual Studio you have installed. When you select this option, the window reloads and displays settings that control production of .csproj files. Generate .csproj files Each setting in the table below enables or disables the production of .csproj files for a different type of package.When you click Regenerate project files, Unity updates the existing .csproj files and creates the necessary new ones based on the settings you choose. These settings control whether to generate .csproj files for any installed packages. For more information on how to install packages, see Adding and removing packages. Property Description Embedded packages Any package that appears under your project’s Packages folder is an embedded package. An embedded package is not necessarily built-in; you can create your own packages and embed them inside your project. This setting is enabled by default. For more information on embedded packages, see Embedded dependencies. Local packages Any package that you install from a local repository stored on your machine, but from outside of your Unity project. This setting is enabled by default. Registry packages Any package that you install from either the official Unity registry or a custom registry. Packages in the Unity registry are available to install directly from the Package Manager. For more information about the Unity package registry, see The Package Registry section of the Unity Package Manager documentation. For information on how to create and use custom registries in addition to the Unity registry, see Scoped package registries. Git packages Any package you install directly from a Git repository using a URL. Built-in packages Any package that is already installed as part of the default Unity installation. Tarball packages Any package you install from a GZip tarball archive on the local machine, outside of your Unity project. Unknown packages Any package which Unity cannot determine an origin for. This could be because the package doesn’t list its origin, or that Unity doesn’t recognize the origin listed. Player projects For each player project, generate an additional .csproj file named ‘originalProjectName.Player.csproj’. This allows different project types to have their code included in Visual Studio’s systems, such as assembly definitions or testing suites."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "MIT License Copyright (c) 2019 Unity Technologies Copyright (c) 2019 Microsoft Corporation. All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/ThirdPartyNotices.html": {
    "href": "Library/PackageCache/com.unity.ide.visualstudio@2.0.22/ThirdPartyNotices.html",
    "title": "| FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the license(s) indicated below: Component Name: VSWhere License Type: \"MIT\" The MIT License (MIT) Copyright (C) Microsoft Corporation. All rights reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Component Name: benbuck/EnvDTE License Type: Zero-Clause BSD Permission to use, copy, modify, and/or distribute this software for any purpose with or without fee is hereby granted. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE."
  },
  "Library/PackageCache/com.unity.ide.vscode@1.2.5/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ide.vscode@1.2.5/CHANGELOG.html",
    "title": "Code Editor Package for Visual Studio Code | FSM Unity Framework",
    "keywords": "Code Editor Package for Visual Studio Code [1.2.5] - 2022-02-07 Introduce OnGeneratedCSProjectFiles, OnGeneratedCSProject and OnGeneratedSlnSolution callbacks. Always use forward slash in source paths Analyzers use absolute paths Ruleset files for roslyn analyzers Extra snap search paths on Ubuntu Specific c# language version for specific unity versions No longer hide .gitignore in VSCode file explorer [1.2.3] - 2020-10-23 Remove workaround for VSCode omnisharp (as of https://github.com/OmniSharp/omnisharp-vscode/issues/4113 we no longer need to disable the referenceoutputassemblies). [1.2.2] - 2020-09-04 VSC-14 - synchronize solution file when adding new assembly [1.2.1] - 2020-05-15 Source filtering adds support for asmref [1.2.0] - 2020-03-04 Do not reference projects that has not been generated (case 1211057) Only open files that exists (case 1188394) Add individual toggle buttons for generating csprojects for packages Add support for Roslyn analyzers in project generation through csc.rsp and compiled assembly references Remove Release build target from csproj and sln [1.1.4] - 2020-01-02 Delta project generation, only recompute the csproj files whose script modified. [1.1.3] - 2019-10-22 Exe version of vscode will use Normal ProcessWindowStyle while cmd will use Hidden [1.1.2] - 2019-08-30 Fixing OSX open command arguments [1.1.1] - 2019-08-19 Support for Player Project. Generates specific csproj files containing files, reference, defines, etc. that will show how the assembly will be compiled for a target platform. [1.1.0] - 2019-08-07 Adds support for choosing extensions to be opened with VSCode. This can be done through the GUI in Preferences. Avoids opening all extensions after the change in core unity. [1.0.7] - 2019-05-15 Fix various OSX specific issues. Generate project on load if they are not generated. Fix path recognition. [1.0.6] - 2019-04-30 Ensure asset database is refreshed when generating csproj and solution files. [1.0.5] - 2019-04-27 Add support for generating all csproj files. [1.0.4] - 2019-04-18 Fix relative package paths. Fix opening editor on mac. Add %LOCALAPPDATA%/Programs to the path of install paths. [1.0.3] - 2019-01-01 This is the first release of Unity Package vscode_editor. Using the newly created api to integrate Visual Studio Code with Unity."
  },
  "Library/PackageCache/com.unity.ide.vscode@1.2.5/CONTRIBUTING.html": {
    "href": "Library/PackageCache/com.unity.ide.vscode@1.2.5/CONTRIBUTING.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing All contributions are subject to the Unity Contribution Agreement(UCA) By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions. Once you have a change ready following these ground rules. Simply make a pull request"
  },
  "Library/PackageCache/com.unity.ide.vscode@1.2.5/Documentation~/README.html": {
    "href": "Library/PackageCache/com.unity.ide.vscode@1.2.5/Documentation~/README.html",
    "title": "Code Editor Package for Visual Studio Code | FSM Unity Framework",
    "keywords": "Code Editor Package for Visual Studio Code This package is not intended to be modified by users. Nor does it provide any api intended to be included in user projects."
  },
  "Library/PackageCache/com.unity.ide.vscode@1.2.5/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ide.vscode@1.2.5/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "MIT License Copyright (c) 2019 Unity Technologies Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to the input system package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. Due to package verification, the latest version below is the unpublished version and the date is meaningless. however, it has to be formatted properly to pass verification tests. [1.7.0] - 2023-08-14 Added Preliminary support for visionOS. Show a list of Derived Bindings underneath the Binding Path editor to show all controls that matched. Changed Changed the InputAction constructors so it generates an ID for the action and the optional binding parameter. This is intended to improve the serialization of input actions on behaviors when created through API when the property drawer in the Inspector window does not have a chance to generate an ID. Fixed Fixed missing prefab errors in InputDeviceTester project (case ISXB-420). Fixed serialization migration in the Tracked Pose Driver component causing bindings to clear when prefabs are used in some cases (case ISXB-512, case ISXB-521). Fixed Tracked Pose Driver to use Transform.SetLocalPositionAndRotation when available to improve performance. Based on the user contribution from DevDunk in a forum post. Fixed the Clone methods of InputAction and InputActionMap so it copies the Initial State Check flag (InputAction.wantsInitialStateCheck) of input actions. Fixed the \"Release tests throws exception in InputSystem\" bug (case ISXB-581). Fixed issues with generating Precompiled Layouts for devices which are not defined in a namespace Fixed an issue where some controls like QuaternionControl could not be included in a Precompiled Layout because the generated code could not access a setter on child control properties. [1.6.3] - 2023-07-11 Fixed Fixed warning in USS file [1.6.2] - 2023-07-10 Added Enabled displayIndex support for Unity 2022.3. Fixed Fixed UI clicks not registering when OS provides multiple input sources for the same event, e.g. on Samsung Dex (case ISX-1416, ISXB-342). Fixed unstable integration test Integration_CanSendAndReceiveEvents by ignoring application focus on integration tests. (case ISX-1381) Fixed broken \"Listen\" button in Input actions editor window with Unity dark skin (case ISXB-536). [1.6.1] - 2023-05-26 Fixed Fixed issue with compiling in Unity 2022.1 and with XR Toolkit by guarding the experimental UITK Asset Editor code completely. [1.6.0] - 2023-05-25 Added Added internal InputSystemProvider class for the new InputForUI internal module. InputForUI allows the UIToolkit to have a single dependency for input events, regardless of using the new input system or the legacy input system. Added InputSystem.customBindingPathValidators interface to allow showing warnings in the InputAsset Editor for specific InputBindings and draw custom UI in the properties panel. Added InputSystem.runInBackground to be used internally by specific platforms packages. Allows telling the input system that a specific platform runs in background. It allows fixing of case UUM-6744. Added new UIToolkit version of the InputActionsAsset editor. Currently this is incomplete (view-only) and the existing editor is still used by default. Added displayIndex field to the Touch struct to expose the index of the display that was touched. Changed Changed XR Layout build behavior to create Axis2D control devices with StickControl type instead of Vector2Control. Fixed Fixed BindingPath String-Comparison to be culture and case insensitive (case ISXB-449). Fixed custom processor display in the input action asset UI after entering/exiting play mode (previously they got hidden) (case ISXB-445). [1.5.1] - 2023-03-15 Fixed Fixed unclosed profiler marker in InvokeCallbacksSafe_AnyCallbackReturnsTrue which would lead to eventually broken profiler traces in some cases like using PlayerInput (case ISXB-393). Fixed InputAction.bindings.count not getting correctly updated after removing bindings with Erase(). Fixed an issue where connecting a gamepad in the editor with certain settings will cause memory and performance to degrade (case UUM-19480). Fixed issue leading to a stack overflow crash during device initialization in InsertControlBitRangeNode (case ISXB-405). Fixed the issue where saving and loading override bindings to JSON would set unassigned overrides (that were null) to assigned overrides (as an empty string \"\"). [1.5.0] - 2023-01-24 Added Added support for reading Tracking State in TrackedPoseDriver to constrain whether the input pose is applied to the Transform. This should be used when the device supports valid flags for the position and rotation values, which is the case for XR poses. Added InputSettings.shortcutKeysConsumeInput. This allows programmatic access to opt-in to the enhanced shortcut key behaviour (case ISXB-254)). Significantly optimized cost of ReadValue/ReadUnprocessedValueFromState/WriteValueIntoState for some control types. Optimization is opt-in for now, please call InputSystem.settings.SetInternalFeatureFlag(\"USE_OPTIMIZED_CONTROLS\", true); in your project to enable it. You can observe which controls are optimized by looking at new optimized column in device debugger. You will need to call a new InputControl.ApplyParameterChanges() method if the code is changing AxisControl fields after initial setup is done. Added the ability to change the origin positioning and movement behaviour of the OnScreenStick (OnScreenStick.cs) via the new behaviour property. This currently supports three modes of operation, two of which are new in addition to the previous behaviour. Based on the user contribution from eblabs in #658. Significantly optimized cost of InputAction.ReadValue and InputControl.ReadValue calls by introducing caching behaviour to input controls. Input controls now keep track of whether their underlying state has been changed and only read the value from the underlying state and apply processors when absolutely necessary. Optimization is opt-in for now, please call InputSystem.settings.SetInternalFeatureFlag(\"USE_READ_VALUE_CACHING\", true); in your project to enable it. If there are issues try enabling InputSystem.settings.SetInternalFeatureFlag(\"PARANOID_READ_VALUE_CACHING_CHECKS\", true); and check in the console if there are any errors regarding caching. Added a note in the supported devices page about DualSense support for Android devices. Exposed displayIndex property for Pointer, Touchscreen, TouchControl, TouchState, Mouse, MouseState which enables look up of the logical screen associated with a pointer event via (display documentation)[https://docs.unity3d.com/ScriptReference/Display.html] Fixed Fixed composite bindings incorrectly getting a control scheme assigned when pasting into input asset editor with a control scheme selected. Fixed an issue on PS5 where device disconnected events that happen while the app is in the background are missed causing orphaned devices to hang around forever and exceptions when the same device is added again (case UUM-7842). Fixed Switch Pro, DualShock 4, DualSense gamepads becoming current on PC/macOS when no controls are changing (case ISXB-223)). Fixed an issue that made OnScreenStick unusable when used in conjunction with PlayerInput in Auto-Switch devices mode, or with any code that changes user/device pairing on unsued device activity being detected (case ISXB-48). Fixed issue where input events were being suppressed during interactive action rebinding even when when their controls were excluded (case ISXB-367). Removed unneeded check that could trigger a NotImplementedException when binding to a Usage (e.g. Submit) (case ISXB-373). Display a warning instead of throwing a NotImplementedException when loading binding overrides from json when some of the entries have become outdated (case ISXB-375). Actions Extended input action code generator (InputActionCodeGenerator.cs) to support optional registration and unregistration of callbacks for multiple callback instances via AddCallbacks(...) and RemoveCallbacks(...) part of the generated code. Contribution by Ramobo in #889. Changed Changed define requirements of Unity.InputSystem.TestFramework, so that it can be used by other packages without setting the com.unity.inputsystem package to be testable in the project manifest. [1.4.4] - 2022-11-01 Fixed Fixed ArgumentNullException when opening the Prefab Overrides window and selecting a component with an InputAction. Fixed {fileID: 0} getting appended to ProjectSettings.asset file when building a project (case ISXB-296). Fixed Type of instance in array does not match expected type assertion when using PlayerInput in combination with Control Schemes and Interactions (case ISXB-282). The InputActions consume their inputs behaviour for shortcut support introduced in v1.4 is opt-in now and can be enabled via the project settings (case ISXB-254)). Fixed Memory alignment issue with deserialized InputEventTraces that could cause infinite loops when playing back replays (case ISXB-317). Fixed an InvalidOperationException when using Hold interaction, and by extension any interaction that changes to performed state after a timeout (case ISXB-332). Fixed Given object is neither an InputAction nor an InputActionMap when using InputActionTrace on input action from an input action asset (case ISXB-29). Fixing devices not being removed if unplugged during domain reload (entering or exiting play mode) (case ISXB-232). [1.4.3] - 2022-09-23 Fixed Added missing script and gizmo icon for TrackedPoseDriver.cs component (case ISXB-262). Fix for mitigating symptoms reported in (case UUM-10774 effectively avoiding reenabling mouse, pen or touch devices in InputSystemPlugin.OnDestroy() if currently quitting the editor. The fix avoids editor crashing if closed when Simulator Window is open. Note that the actual issue needs a separate fix in Unity and this package fix is only to avoid running into the issue. Fixed an issue where Input Action name would not display correctly in Inspector if serialized as [SerializedProperty] within a class not derived from MonoBehavior (case ISXB-124. Fix an issue where users could end up with the wrong device assignments when using the InputUser API directly and removing a user (case ISXB-274). Fixed an issue where PlayerInput behavior description was not updated when changing action assset (case ISXB-286). Changed Readded OnDisable() member to MultiplayerEventSystem which was previously removed from the API Improved performance of HID descriptor parsing by moving json parsing to a simple custom predicitve parser instead of relying on Unity's json parsing. This should improve domain reload times when there are many HID devices connected to a machine. Changed Documentation improvements: New workflows and concepts pages. Reorganised table of contents. Improved some code samples. Updated screenshots. [1.4.2] - 2022-08-12 Changed Hide XR legacy HMD and controllers layouts from Editor UI dropdown. Fixed Fix UI sometimes ignoring the first mouse click event after losing and regaining focus (case ISXB-127. Fixed issue when using MultiplayerEventSystems where the visual state of UI controls would change due to constant toggling of CanvasGroup.interactable on and off (case ISXB-112). Fixed minor issue when renaming input actions where unique renaming would incorrectly consider the input action being renamed as a different action and not allow renaming of 'A' to 'a' without appending a unique integer for example (case ISXB-25). Fixed an issue where the Input Action asset icon would not be visible during asset creation (case ISXB-6). Fixed DualSense low frequency motor speed being always set to min value. Fixed an issue where ReadUnprocessedValueFromState in PoseControl always returning default values. Fix Player 1's UI controls stop working after second player joins (case ISXB-125)) [1.4.1] - 2022-05-30 Fixed Fixed composite touchscreen controls were not firing an action if screen was touched before enabling the action (case ISXB-98). [1.4.0] - 2022-04-10 Changed Button type InputActions now go to started when a button goes from a press to below the release threshold but not yet to 0. // Before: Set(Gamepad.current.rightTrigger, 0.7f); // Performed (pressed) Set(Gamepad.current.rightTrigger, 0.2f); // Canceled (released) Set(Gamepad.current.rightTrigger, 0.1f); // Started!! Set(Gamepad.current.rightTrigger, 0f); // Canceled // Now: Set(Gamepad.current.rightTrigger, 0.7f); // Performed (pressed) Set(Gamepad.current.rightTrigger, 0.2f); // Started (released but not fully) Set(Gamepad.current.rightTrigger, 0.1f); // <Nothing> Set(Gamepad.current.rightTrigger, 0f); // Canceled This also applies to PressInteraction when set to Press behavior. In effect, it means that a button will be in started or performed phase for as long as its value is not 0 and will only go to canceled once dropping to 0. Processors are now always applied when reading action values through InputAction.ReadValue<> or CallbackContext.ReadValue<>. Previously, if no bound control was actuated, ReadValue calls would return the default value for the action type but not run the value through the processors.(case 1293728). Made the following internal types public. These types can be useful when deconstructing raw events captured via InputEventTrace. UnityEngine.InputSystem.Android.LowLevel.AndroidAxis UnityEngine.InputSystem.Android.LowLevel.AndroidGameControllerState UnityEngine.InputSystem.Android.LowLevel.AndroidKeyCode Adding or removing a device no longer leads to affected actions being temporarily disabled (case 1379932). If, for example, an action was bound to <Gamepad>/buttonSouth and was enabled, adding a second Gamepad would lead to the action being temporarily disabled, then updated, and finally re-enabled. This was especially noticeable if the action was currently in progress as it would get cancelled and then subsequently resumed. Now, an in-progress action will get cancelled if the device of its active control is removed. If its active control is not affected, however, the action will keep going regardless of whether controls are added or removed from its InputAction.controls list. Installing the package for the first time will now set \"Active Input Handling\" to \"Both\" rather than \"Input System Package\". This means, that by default, both the old and the new input system will run side by side where supported. This can be manually switched by going to Edit >> Project Settings >> Player >> Active Input Handling. Fixed Fixed an issue where a layout-override registered via InputSystem.RegisterLayoutOverride(...) would cause the editor to malfunction or crash if the layout override had a name already used by an existing layout (case 1377685). Fixed an issue where attempting to replace an existing layout-override by using an existing layout-override name didn't work as expected and would instead aggregate overrides instead of replacing them when an override with the given name already exists. Fixed Switch Pro controller not working correctly in different scenarios (case 1369091, case 1190216, case 1314869). Fixed DualShock 4 controller not allowing input from other devices due to noisy input from its unmapped sensors (case 1365891). Fixed InputSystem.onAnyButtonPress so that it doesn't throw exceptions when trying to process non state or delta events (case 1376034). Fixed InputControlPath.Matches incorrectly reporting matches when only a prefix was matching. This would, for example, cause Keyboard.eKey to be matched by <Keyboard>/escape. Fix contributed by Fredrik Ludvigsen in #1485. Fixed OnScreenButton triggering NullReferenceException in combination with custom devices (case 1380790 ). Fixed no devices being available in Start and Awake methods if, in the player, any InputSystem API was accessed during the SubsystemRegistration phase (case 1392358). Fixed dropdown for \"Supported Devices\" in settings not showing all device layouts. Fixed \"STAT event with state format TOUC cannot be used with device 'Touchscreen:/Touchscreen'\" when more than max supported amount of fingers, currently 10, are present on the screen at a same time (case 1395648). Fixed mouse events not being timesliced when input system is switched to process input in fixed updates (case 1386738). Fixed missing tooltips in PlayerInputManagerEditor for the Player Limit and Fixed Splitscreen sizes labels (case 1396945). Fixed DualShock 4 controllers not working in some scenarios by adding support for extended mode HID reports (case 1281633, case 1409867). Fixed BackgroundBehavior.IgnoreFocus having no effect when Application.runInBackground was false (case 1400456). Fixed an issue where a device was left disabled when it was disconnected while an application was out-of-focus and then re-connected when in-focus (case 1404320). Actions Fixed InvalidCastException: Specified cast is not valid. being thrown when clicking on menu separators in the control picker (case 1388049). Fixed accessing InputActions directly during RuntimeInitializeOnLoad not initializing the input system as a whole and leading to exceptions (case 1378614). Fixed InputAction.GetTimeoutCompletionPercentage jumping to 100% completion early (case 1377009). Fixed d-pad inputs sometimes being ignored on actions that were binding to multiple controls (case 1389858). Fixed IndexOutOfRangeException when having multiple interactions on an action and/or binding in an action map other than the first of an asset (case 1392559). Fix contributed by Russell Quinn in #1483. Fixed AxisComposite not respecting processors applied to positive and negative bindings (case 1398942). This was a regression introduced in 1.0.0-pre.6. Fixed calling action.AddCompositeBinding(...).With(...) while action is enabled not correctly updating controls for part bindings of the composite. Fixed TwoModifiersComposite inadvertently not allowing controls other than ButtonControls being bound to its binding part. Added support for keyboard shortcuts and mutually exclusive use of modifiers. In short, this means that a \"Shift+B\" binding can now prevent a \"B\" binding from triggering. OneModifierComposite, TwoModifiersComposite, as well as the legacy ButtonWithOneModifierComposite and ButtonWithTwoModifiersComposite now require their modifiers to be pressed before (or at least simultaneously with) pressing the target button. This check is performed only if the target is a button. For a binding such as \"CTRL+MouseDelta\" the check is bypassed. It can also be manually bypassed via the overrideModifiersNeedToBePressedFirst. State change monitors on a device (IInputStateChangeMonitor) are now sorted by their monitorIndex and will trigger in that order. Actions are now automatically arranging their bindings to trigger in the order of decreasing \"complexity\". This metric is derived automatically. The more complex a composite a binding is part of, the higher its complexity. So, \"Shift+B\" has a higher \"complexity\" than just \"B\". If an binding of higher complexity \"consumes\" a given input, all bindings waiting to consume the same input will automatically get skipped. So, if a \"Shift+B\" binding composite consumes a \"B\" key press, a binding to \"B\" that is waiting in line will get skipped and not see the key press. If your project is broken by these changes, you can disable the new behaviors via a feature toggle in code: InputSystem.settings.SetInternalFeatureFlag(\"DISABLE_SHORTCUT_SUPPORT\", true); Added new APIs for getting and setting parameter values on interactions, processors, and composites. // Get parameter. action.GetParameterValue(\"duration\"); // Any \"duration\" value on any binding. action.GetParameterValue(\"tap:duration\"); // \"duration\" on \"tap\" interaction on any binding. action.GetParameterValue(\"tap:duration\", // \"duration\" on \"tap\" on binding in \"Gamepad\" group. InputBinding.MaskByGroup(\"Gamepad\")); // Set parameter. action.ApplyParameterOverride(\"duration\", 0.4f); action.ApplyParameterOverride(\"tap:duration\", 0.4f); action.ApplyParameterOverride(\"tap:duration\", 0.4f, InputBinding.MaskByGroup(\"Gamepad\")); // Can also apply parameter overrides at the level of // InputActionMaps and InputActionAssets with an effect // on all the bindings contained therein. asset.ApplyParameterOverride(\"scaleVector2:x\", 0.25f, new InputBinding(\"<Mouse>/delta\")); Added Added support for \"Hori Co HORIPAD for Nintendo Switch\", \"HORI Pokken Tournament DX Pro Pad\", \"HORI Wireless Switch Pad\", \"HORI Real Arcade Pro V Hayabusa in Switch Mode\", \"PowerA NSW Fusion Wired FightPad\", \"PowerA NSW Fusion Pro Controller (USB only)\", \"PDP Wired Fight Pad Pro: Mario\", \"PDP Faceoff Wired Pro Controller for Nintendo Switch\", \"PDP Faceoff Wired Pro Controller for Nintendo Switch\", \"PDP Afterglow Wireless Switch Controller\", \"PDP Rockcandy Wired Controller\". Added support for SteelSeries Nimbus+ gamepad on Mac (addition contributed by Mollyjameson). Added support for Game Core platforms to XR layouts, devices, and input controls. These classes were previously only enabled on platforms where ENABLE_VR is defined. Added a new DeltaControl control type that is now used for delta-style controls such as Mouse.delta and Mouse.scroll. Like StickControl, this control has individual up, down, left, and right controls (as well as x and y that it inherits from Vector2Control). This means it is now possible to directly bind to individual scroll directions (such as <Mouse>/scroll/up). Added the 'Cursor Lock Behavior' setting to InputSystemUIInputModule to control the origin point of UI raycasts when the cursor is locked. This enables the use of PhysicsRaycaster when the cursor is locked to the center of the screen (case 1395281). Added support for using the Unity Remote app with the Input System. Requires Unity 2021.2.18 or later. [1.3.0] - 2021-12-10 Changed The artificial ctrl, shift, and alt controls (which combine the left and right controls into one) on the keyboard can now be written to and no longer throw NotSupportedException when trying to do so (case 1340793). All devices are now re-synced/reset in next update after entering play mode, this is needed to read current state of devices before any intentional input is provided (case 1231907). Replaced UnityLinkerBuildPipelineData.inputDirectory with hardcoded Temp folder because inputDirectory is deprecated. Deprecated InputSettings.filterNoiseOnCurrent. Now noise filtering is always enabled. Device only will become .current if any non-noise control have changed state. A device reset (such as when focus is lost) on Touchscreen will now result in all ongoing touches getting cancelled instead of all touches being simply reset to default state. Calling InputTestFixture.Press, InputTestFixture.Set, etc. from within a [UnityTest] will no longer immediately process input. Instead, input will be processed like it normally would as part of the Unity player loop. Fixed Fixed writing values into the half-axis controls of sticks (such as Gamepad.leftStick.left) producing incorrect values on the stick (case 1336240). Fixed setting size of event trace in input debugger always growing back to largest size set before. Fixed successive clicks not getting triggered with TouchSimulation on when not moving the mouse in-between clicks (case 1330014). Fixed InputSystemUIInputModule stopping to listen for input when swapping InputActionAsset instances while input was disabled (case 1371332). Fixed InputSystemUIInputModule showing incorrect bindings after pressing the 'Fix UI Input Module' button in PlayerInput component(case 1319968). Fixed an issue where UI button clicks could be ignored by InputSystemUIInputModule if modifying on-screen devices from Update() callbacks (case 1365070). Fixed an issue with InputSystemUIInputModule that would cause UI to stop responding during play mode after changing a script file while Recompile and Continue mode is active, or by forcing a script recompile using RequestScriptCompilation(case 1324215). Fixed InputSystemUIInputModule inspector showing all action bindings as \"None\" when assigned a runtime created actions asset (case 1304943). Fixed a problem with UI Toolkit buttons remaining active when multiple fingers are used on a touchscreen, using InputSystemUIInputModule with pointerBehavior set to UIPointerBehavior.SingleUnifiedPointer. UI Toolkit will now always receive the same pointerId when that option is in use, regardless of the hardware component that produced the pointer event. (case 1369081). Fixed a problem with InputUser where devices would be removed and not added again after layout overrides preventing certain devices, e.g. gamepads to not work correctly when associated with action map bindings tied to PlayerInput (case 1347320). Fixed DualSense on iOS not inheriting from DualShockGamepad (case 1378308). Fixed a device becoming .current (e.g. Gamepad.current, etc) when sending a new state event that contains no control changes (case 1377952). Fixed calling IsPressed on an entire device returning true (case 1374024). Fixed HIDs having blackslashes in their vendor or product names leading to binding paths generated by interactive rebinding that failed to resolve to controls and thus lead to no input being received (case 1335465). Fixed InputSystem.RegisterLayoutOverride resulting in the layout that overrides are being applied to losing the connection to its base layout (case 1377719). Fixed Touch.activeTouches still registering touches after the app loses focus (case 1364017). Fixed MultiplayerEventSystem not preventing keyboard and gamepad/joystick navigation from one player's UI moving to another player's UI (case 1306361). This fix relies on a CanvasGroup being injected into each playerRoot and the interactable property of the group being toggled back and forth depending on which part of the UI is being updated. Fixed InputTestFixture incorrectly running input updates out of sync with the player loop (case 1341740). This had effects such as InputAction.WasPressedThisFrame() returning false expectedly. Fixed broken code example for state structs in Devices.md documentation (fix contributed by jeffreylanters). Fixed TrackedDeviceRaycaster not picking closest hit in scene (fix originally contributed by alexboost222). Actions Fixed opening a new project (or one that needs a full reimport) leading to several exceptions in the console if the most recently opened project was closed with a .inputactions editor open (case 1313185). Fixed incorrect indentation of input actions in the inspector (case 1285546). Fixed an issue where serialized InputAction properties would have display name \"Input Action\" in the Inspector window instead of their given name. (case 1367240). Fixed an issue where InputAction.Enable would not reuse memory allocated prior and thus lead to memory leaks (case 1367442). Fixed interactions such as Press not getting processed correctly when having multiple of them on different bindings of the same action and receiving simultaneous input on all of them (case 1364667). If, for example, you bind the A and S key on the same action, put a Press interaction on both, and then press both keys, interactions would get missed or got stuck. Fixed InputAction.IsPressed/WasPressed/WasReleased returning incorrect results when binding multiple buttons on the same action and pressing/releasing them simultaneously. Improved performance of looking up actions by name. Fixed InputAction.controls exhibiting bad performance when there were no controls bound to an action (case 1347829). Fixed interactions involving timeouts (such as HoldInteraction) performing erroneous delayed triggers on actions when input is composed of multiple controls (1251231). For example, if you bind Shift+B using a OneModifierComposite and put a HoldInteraction on the binding, then depending on the order in which the keys are pressed, you would sometimes see the action spuriously getting triggered when in fact no input was received. Fixed control schemes of bindings not getting updates when being pasted from one .inputactions asset into another (case 1276106). For example, if you copied a binding from an asset that had a \"Gamepad\" control scheme into an asset that had none, the resulting binding would be unusable. All associations with control schemes that do not exist in the target asset are now removed from bindings upon pasting. Fixed InputActionSetupExtensions.AddCompositeBinding not setting name of composite. [1.2.0] - 2021-10-22 Changed When exceptions occur in user code inside of Input System callbacks, the exception message is now printed first and details about the callback second. Previously a message similar to \"Exception ... while executing '...' callbacks\" was printed first and then followed by exception log. This was hiding the actual exception and created confusion. Fixed Fixed a performance issue on entering/exiting playmode where HID device capabilities JSON could be parsed multiple times for a single device(case 1362733). Fixed a problem where explicitly switching to the already active control scheme and device set for PlayerInput would cancel event callbacks for no reason when the control scheme switch would have no practical effect. This fix detects and skips device unpairing and re-pairing if the switch is detected to not be a change to scheme or devices. (case 1342297) Any unhandled exception in InputManager.OnUpdate failing latter updates with InvalidOperationException: Already have an event buffer set! Was OnUpdate() called recursively?. Instead the system will try to handle the exception and recover into a working state. Fixed an issue that broke the VirtualMouseInput component in the editor (case 1367553). Fixed a problem where only using runtimes that are not XR supported causes a compile error. This fix adds back in ENABLE_VR checks to prevent this case (case 1368300) Fixed input action for Android gamepad's right stick will be correctly invoked when only y axis is changing (case 1308637). Generic gamepad short display button names were incorrectly mapped on Switch (A instead of B, etc). Fixed an issue where resetting an action via InputAction.Reset() while being in disabled state would prevent the action from being enabled again. (case 1370732). Fixed \"Default constructor not found for type UnityEngine.InputSystem.iOS.LowLevel.iOSStepCounter\" any other potential exceptions due to classes, methods, fields and properties being stripped when managed stripping setting set to medium or high (case 1368761). Fixed an issue where InvalidOperationExceptions are thrown if an input for an action with multiple interactions is held while disconnecting the device(case 1354098). Fixed action.ReadValue and others returning invalid data when used from FixedUpdate or early update when running in play mode in the editor (case 1368559 case 1367556 case 1372830). Fixed current being null for sensors (Accelerometer.current, others) (case 1371204). Added Added support for PS5 DualSense controllers on Mac and Windows. Improved the user experience when creating single vs multi-touch touchscreen bindings in the Input Action Asset editor by making both options visible in the input action dropdown menu. Now it's not neccessary to be aware of the touch*/press path binding syntax (case 1357664). Added support for the Unity Remote app. NOTE: This unfortunately requires a change in the Unity native runtime. We are in the process of rolling out the change to Unity versions. A public build that receives the change will automatically enable the functionality in the Input System package. [1.1.1] - 2021-09-03 Fixed Fixed InvalidCastException: Specified cast is not valid. and InvalidOperationException: Already have an event buffer set! Was OnUpdate() called recursively? when upgrading from 1.1.0-pre.5 or earlier. If you experience this issue you can also restart the editor to resolve it. Fixed InputDeviceChange.Destroyed not being available, now it's correctly marked as obsolete instead. Removed documentation around platform user account management of InputUser which was ahead of actual backend support for the feature. [1.1.0] - 2021-08-27 Changed Modified the fix that landed in 1.1-preview.3 for any given control being added to an action only once. This caused a regression with some setups that, for example, bound the same control multiple times in a composite using processors to alter the value of the control. Internally, a control is now again allowed to feed into the same action through more than one binding. However, externally the control will be mentioned on the action's InputAction.controls list only once. Adding InputSystemUIInputModule from code now installs DefaultInputActions. This is equivalent to the default setup when adding the component in the editor (case 1259306). var go = new GameObject(); go.AddComponent<EventSystem>(); var uiModule = go.AddComponent<InputSystemUIInputModule>(); // uiModule.actionsAsset now has a DefaultInputActions() asset assigned to it and the various // action references point to its actions. InputSystemUIInputModule.UnassignActions has been added to remove all actions from the module en bloc. uiModule.UnassignActions(); Fixed Fixed an issue where mixing test cases based on InputTestFixture (using mocked InputSystem) and regular test cases (using real InputSystem) would lead to static state leaking between test cases causing random failures and unexpected/undefined behavior (case 1329015). Fixed InputSystemUIInputModule.AssignDefaultActions not assigning trackedDeviceOrientation and trackedDevicePosition. Fixed regression introduced by previous change where InputSystemUIInputModule would not disable actions correctly. Fixed InputAction.canceled not getting triggered reliably for InputActionType.PassThrough actions when InputSystem.ResetDevice was called. Fixed device resets (e.g. happening as part of focus changes) leading to only some actions bound to these devices getting cancelled instead of all of them. [1.1.0-pre.6] - 2021-08-23 Fixed Fixed pairing devices to existing InputUsers potentially corrupting list of paired devices from other InputUsers (case 1327628). Fixed duplication of control paths when viewing collections of InputControls in the inspector. Fix contributed by NibbleByte in 1354. Fixed StackOverflowException caused by calling InputSystem.Update from inside an input action callback such as InputAction.performed (case 1316000). Fixed InputTestFixture leaving all .current getters uninitialized after a test run (case 1329015). Fixed broken script references in Touch Samples project (case 1190598). Fixed PointerInput composite in TouchSamples project being registered only after scenes already loaded (case 1215048). Fixed InputControlExtensions.EnumerateChangedControls skipping over left, right, and down controls on PS4 controller's dpad (case 1315107). Fixed undo not working in Input System Package project settings pane (case 1291709). Fixed incorrect indexing in InputUser.OnDeviceChanged that could result in incorrect pairing of devices or IndexOutOfRangeException being thrown when removing, adding or reconfiguring a device. Fix contribution by Mikael Klages in #1359. Fixed incorrect indexing when sorting magnitude based on score in InputActionRebindingExtensions.RebindingOperation which could result in incorrect magnitudes for candidates. Contribution by Fredrik Ludvigsen in #1348. Fixed inconsistent ordering and execution when adding to or removing from the various callbacks in the API (such as InputSystem.onDeviceChange but also InputAction.started etc.) during the execution of a callback (case 1322530. Fixed inconsistent behavior of WebGL gamepad left/right stick. Up/Down controls were reverse of X/Y controls. (case 1348959) Fixed PlayerInputManagers join action not triggering when using a referenced InputAction (case 1260625). Fixed UI issue where pressing the wrong button was possible while quickly moving through a UI because the submit action fired on action press instead of action release (1333563). Fixed InvalidOperationException when opening a preset created from a .inputactions asset (case 1199544). Fixed a problem arising when combining InputSystemUIInputModule and PlayInput with SendMessage or BroadcastMessage callback behavior on the same game object or hierarchy which is an ambiguous input setup. This fix eliminates callbacks into InputSystemUIInputModule. Related to (1343712). Fixed inconsistent usage of ENABLE_PROFILER define together with Profiler.BeginSample/Profiler.EndSample by removing ENABLE_PROFILER macro check because BeginSample/EndSample are already conditional with [Conditional(\"ENABLE_PROFILER\")] (case 1350139). Remediated majority of performance issues with high frequency mice (>=1kHz poll rates) in release mode by merging consecutive mouse move events together (case 1281266), see the events documentation for more information. Fixed InputEventTrace replays skipping over empty frames and thus causing playback to happen too fast. Fixed \"Pointer should have exited all objects before being removed\" error when changing screen orientation on mobile. Controls such as mouse positions are no longer reset when focus is lost. Pressing a uGUI Button and then alt-tabbing away, letting go of the button, and then going back to the application will no longer trigger a button click. Fixed Input.onUnpairedDeviceActivity triggering from editor input. Fixed 'up' and 'down' controls on WebGLGamepad left and right sticks not being clamped correctly. Actions Fixed right-clicking in empty action map or action list not popping up context menu (case 1336426). Fixed binding paths being misaligned in UI when switching to text mode editing (case 1200107). Fixed \"Exception: Style.Draw may not be called with GUIContent that is null.\" error from PlayerInput inspector when having an action map with no actions (case 1317735). Fixed calling GetBindingDisplayString() on an InputAction with a composite binding leading to doubled up output (case 1321175). Fixed MultiTapInteraction not respecting InputSettings.multiTapDelayTime (case 1292754). Fixed changing values in Input System Package project settings not affecting default values displayed in .inputactions editor window (case 1292754). Fixed rebinding a part of a composite with RebindingOperation.WithTargetBinding not also changing the type of control being looked for (case 1272563). Fixed AxisComposite not respecting minValue and maxValue properties (case 1335838). Fixed ArgumentOutOfRangeException caused by IsPointerOverGameObject (case 1337354). PlayerInput no longer logs an error message when it is set to Invoke UnityEvents and can't find an action in the given .inputactions asset (case 1259577). Fixed HoldInteraction getting stuck when hold and release happens in same event (case 1346786). Fixed adding an action in the .inputactions editor automatically duplicating interactions and processors from the first action in the map. Fixed InputActionSetupExtensions.ChangeBinding when modifying binding from a different action than specified. Contribution by Fredrik Ludvigsen in #1348. Added Added InputSystem.runUpdatesInEditMode to enable processing of non-editor updates without entering playmode (only available for XR). Added a new \"UI vs Game Input\" sample to the package. The sample can be installed from the Unity Package Manager UI in the editor. The sample demonstrates how to deal with inputs that may both lead to UI actions as well as in-game actions. Added method SetMotorSpeedsAndLightBarColor as a workaround for setting both the light bar and motor speeds simultaneously on a DualShock 4 controller (case 1271119). Added the concept of \"soft\" and \"hard\" device resets. In general, resetting a device will reset its state to default values. Individual controls can be marked as dontReset to exclude them from resets. This makes the reset \"soft\" (default). // Perform a \"soft\" reset of the mouse. The mouse position will not be affected // but controls such as buttons will be reset. InputSystem.ResetDevice(Mouse.current); A \"hard\" reset can be forced through the API. This also resets dontReset controls. // Perform a \"hard\" reset of the mouse. The mouse position will also be reset to (0,0). InputSystem.ResetDevice(Mouse.current, alsoResetDontResetControls: true); Resets will lead to InputActions that are enabled and in-progress from controls that being reset, to be canceled. This will not perform actions even if they trigger on, for example, button release. InputDevice.canRunInBackground can now be force-set through layouts. // Force XInputWindows gamepads to not run in the background. InputSystem.RegisterLayoutOverride(@\" { \"\"name\"\": \"\"XInputWindowsNoCanRunInBackground\"\", \"\"extend\"\": \"\"XInputWindows\"\", \"\"runInBackground\"\": \"\"off\"\" } \"); Improved performance of Touchscreen by merging consecutive touch move events together. See the events documentation for more information. Actions Added a new InputAction.wantsInitialStateCheck property that allows toggling on initial state checks for Button and Pass-Through actions (implicitly enabled for Value actions). This allows responding immediately to controls that are already actuated when the action is enabled. Added new API for more easily listening for event changes. InputSystem.onEvent .ForDevice<Gamepad>() .Where(e => e.HasButtonPress()) .CallOnce(e => Debug.Log(\"Button pressed!)); Added new API to easily listen for button presses on any device. InputSystem.onAnyButtonPress .CallOnce(ctrl => Debug.Log($\"Button '{ctrl}' pressed\")); This is a simple wrapper around the new API mentioned above. Changed Application focus handling behavior has been reworked. When runInBackground is off, no action will be taken on focus loss. When focus comes back, all devices will receive a sync request. Those that don't support it will see a \"soft\" reset. When runInBackground is on (which, when running in the editor, is considered to always be the case), a new setting InputSettings.backgroundBehavior dictates how input is to be handled while the application does not have focus. The default setting of ResetAndDisableNonBackgroundDevices will soft-reset and disable all devices for which InputDevice.canRunInBackground is false. While in the background, devices that are flagged as canRunInBackground will keep running as in the foreground. In the editor, devices other than Pointer and Keyboard devices (i.e. anything not used to operate the editor UI) are now by default routing their input to the Game View regardless of focus. This also fixes the problem of gamepad sticks resetting to (0,0) on focus loss (case 1222305). A new setting InputSettings.gameViewFocus has been introduced to determine how Game View focused is handled in the editor with respect to input. Editor: Removed 'Lock Input to Game View' setting in the Input Debugger. The setting has been replaced by the new 'Game View Focus' project setting. InputSystem.defaultButtonPressPoint is now clamped to a minimum value of 0.0001 (case 1349002). InputDevice.OnConfigurationChanged can now be overridden in derived classes. InputSystemUIInputModule now defers removing pointers for touches by one frame. This is to ensure that IsPointerOverGameObject can meaningfully be queried for touches that have happened within the frame – even if by the time the method is called, a touch has technically already ended (case 1347048). More precisely, this means that whereas before a PointerExit and PointerUp was received in the same frame, a touch will now see a PointerUp in the frame of release but only see a PointerExit in the subsequent frame. Calling EventSystem.IsPointerOverGameObject() from within InputAction callbacks (such as InputAction.performed) will now result in a warning. UI updates after input and consumes input through InputActions as they are processed. Thus, querying UI state from within InputAction callbacks will query outdated UI state. Changed TrackedPoseDriver to use properties of type InputActionProperty rather than InputAction to allow more flexibility. Changed quickstart documentation sample to use the Update method instead of FixedUpdate to show a more correct usage of the wasPressedThisFrame API. [1.1.0-pre.5] - 2021-05-11 Fixes a problem with the package's manifest missing a dependency on the UI Elements module. [1.1.0-pre.4] - 2021-05-04 Changed The VirtualMouseInput component is now part of the Input System assembly. It was previously packaged with the Gamepad Mouse Cursor sample. The component has a different GUID from before, so existing setups that use the component from the sample are not broken. To use the built-in component you must explicitly switch over. InputTestFixture no longer deletes the GameObjects in the current scene in its TearDown (case 1286987). This was added for the sake of the Input System's own tests but should not have been in the public fixture. Generic Gamepad now has platform independent long button names. Previously it used different names if editor targeted PS4/Switch consoles (case 1321676). When creating a new control scheme with a name All Control Schemes, All Control Schemes1 will be created to avoid confusion with implicit All Control Schemes scheme (case 1217379). Display names of keyboard buttons are now passed through ToLower and ToTitleCase to enforce consistent casing between different platforms and keyboard layouts (case 1254705). Editor: All remaining InputUser instances are now removed automatically when exiting play mode. This means that all devices are automatically unpaired. In essence, like InputAction, InputUser is now considered a player-only feature. Events queued during event processing (i.e. InputSystem.Update()) are now processed in the same frame. This eliminates the 1-frame lag previously incurred by simulated input. Note that this does not extend to input queued outside of event processing but in the same frame. For example, input queued by the UI (such as by OnScreenButton and OnScreenStick) will still see a 1-frame lag as UI event processing happens later in the frame and outside of input event processing. Actions When removing/unplugging a device, it will now also be removed from the device list of InputActionMap.devices and InputActionAsset.devices. var gamepad = InputSystem.AddDevice<Gamepad>(); var actions = new MyGeneratedActions(); actions.devices = new[] { gamepad }; InputSystem.RemoveDevice(gamepad); // `actions.devices` is now an empty array. Adding an action to a InputActionMap that is part of an InputActionAsset now requires all actions in the asset to be disabled (case 1288335). This used to trigger an Assert at runtime but now properly throws an InvalidOperationException. Fixed Fixed inputs in game view sometimes not working when running in the editor, as initial focus state could end up being incorrect. Fixed bad performance in Input Debugger with high-frequency devices (e.g. 1+ KHz gaming mice). Before, high event volumes led to excessive refreshes of debugger data. Fixed compile error on tvOS due to step counter support for iOS added in 1.1.0-preview.3. Fixed PS4- and PS3-specific rightTriggerButton and leftTriggerButton controls not being marked as synthetic and thus conflicting with rightTrigger and leftTrigger input (case 1293734). This manifested itself, for example, when using interactive rebinding and seeing rightTriggerButton getting picked instead of the expected rightTrigger control. Fixed changes to usages of devices in remote player not being reflected in Input Debugger. Fixed exceptions and incorrect values with HIDs using 32-bit fields (case 1189859). This happened, for example, with vJoy installed. Fixed InputUser no longer sending InputUserChange.ControlsChanged when adding a new user after previously, all users were removed. Fix contributed by Sven Herrmann in 1292. Fixed AxisDeadzoneProcessor min/max values not being settable to 0 in editor UI (case 1293744). Fixed blurry icons in input debugger, asset editor, input settings (case 1299595). Fixed clickCount not being incremented correctly by InputSystemUIInputModule for successive mouse clicks (case 1317239). Fixed UI not working after additively loading scenes with additional InputSystemUIInputModule modules (case 1251720). Fixed no OnPointerExit received when changing UI state without moving pointer (case 1232705). Fixed reference to .inputactions of Player Prefab referenced by PlayerInputManager being destroyed on going into play mode, if the player prefab was a nested prefab (case 1319756). Fixed \"Scheme Name\" label clipped in \"Add Control Schema\" popup window ([case 1199560]https://issuetracker.unity3d.com/issues/themes-input-system-scheme-name-is-clipped-in-add-control-schema-window-with-inter-default-font)). Fixed InputSystem.QueueEvent calls from within InputAction callbacks getting dropped entirely (case 1297339). Fixed InputSystemUIInputModule being in invalid state when added from Awake to a game object when entering playmode (case 1323566). Fixed Keyboard.current becoming null after OnScreenButton is disabled or destroyed (case 1305016). Actions Fixed rebinding not working for any discrete control that was held when the rebinding operation started (case 1317225). Fixed bindings being added to every InputAction in a collection when editing a collection of InputActions in the inspector. (case 1258578) Fixed Retrieving array element that was out of bounds and SerializedProperty ... has disappeared! errors when deleting multiple action bindings in the input asset editor (case 1300506). Fixed delete key not working in the input actions editor (case 1282090). Fixed actions embedded into MonoBehaviours not showing bindings added directly from within constructors (case 1291334). public class MyMB : MonoBehaviour { // This would end up not showing the binding in the inspector. public InputAction action = new InputAction(binding: \"<Gamepad>/leftStick\"); Fixed tooltips not appearing for elements of the Input Actions editor window (case 1311595). Fixed NullReferenceException when reading values through InputAction.CallbackContext on a OneModifierComposite or TwoModifierComposite binding. Fixed multi-taps not working when multiple controls were bound to an action (case 1267805). When there were multiple controls bound to an action, this bug would get triggered by any interaction that did not result in a phase change on the action. Fixed runtime rebinds added as new bindings from leaking into .inputactions assets when exiting play mode (case 1190502) Fixed IndexOutOfRangeException and null elements in InputUser.lostDevices when an InputUser loses a devices from a control scheme with only optional devices (case 1275148). Fixed binding path selection windows not remembering navigation state when going up through hierarchy (case 1254981). Added Support for Device Simulator touchscreen input. Enabled XR device support on Magic Leap (Lumin). Added ability to force XR Support in a project by defining UNITY_INPUT_FORCE_XR_PLUGIN. Added a warning message to PlayerInputManager editor when the attached input action asset won't work with Join Players When Button Is Pressed behaviour due to missing control scheme device requirements (case 1265853). Added support for UI Toolkit with Unity 2021.1+. UITK is now supported as a UI solution in players. Input support for both Unity UI and UI Toolkit is based on the same InputSystemUIInputModule code path. More details in the manual. InputSystemUIInputModule now has an xrTrackingOrigin property. When assigned, this will transform all tracked device positions and rotations from it's local space into Unity's world space (case 1308480). Added InputSystemUIInputModule.GetLastRaycastResult. This returns the most recent raycast result and can be used to draw ray visualizations or get information on the most recent UI object hit. Added InputStateBlock support for kFormatSBit when working with floats (case 1258003). Added an API to parse control paths. var parsed = InputControlPath.Parse(\"<XRController>{LeftHand}/trigger\").ToArray(); Debug.Log(parsed.Length); // Prints 2. Debug.Log(parsed[0].layout); // Prints \"XRController\". Debug.Log(parsed[0].name); // Prints an empty string. Debug.Log(parsed[0].usages.First()); // Prints \"LeftHand\". Debug.Log(parsed[1].layout); // Prints null. Debug.Log(parsed[1].name); // Prints \"trigger\". Can, for example, be used with InputBinding.path. Added a new API-only setting in the form of InputSystem.settings.maxEventBytesPerUpdate. Puts an upper limit on the number of event bytes processed in a single update. If exceeded, any additional event data will get thrown away and an error will be issued. Set to 5MB by default. Added a new API-only setting called InputSystem.settings.maxQueuedEventsPerUpdate. This limits the number of events that can be queued during event processing using the InputSystem.QueueEvent method. This guards against infinite loops in the case where an action callback queues an event that causes the same action callback to be called again. Added InputSystemUIInputModule.AssignDefaultActions to assign default actions when creating ui module in runtime. Added UNITY_INCLUDE_TESTS define constraints to our test assemblies, which is 2019.2+ equivalent to \"optionalUnityReferences\": [\"TestAssemblies\"]. [1.1.0-preview.3] - 2021-02-04 Changed An upper limit of 1024 controls per device and 1kb of memory state per device has been introduced. This allows for certain optimizations. Should the limits prove too tight, they can be raised in the future. The most complex device we have at the moment (Touchscreen) has 242 controls and 616 bytes of state. TouchSimulation now disables the Pointer devices it reads input from. This is to address the problem of mouse input leading to both mouse and touch input happening concurrently. Instead, enabling touch simulation will now effectively replace mouse and pen input with touch input. Devices such Mouse and Pen will remain in place but will not get updated. Events received for them will be consumed by TouchSimulation. Enabled XR device support on Switch. Fixed Fixed Right stick to use AXIS.Z and AXIS.RZ for Android gamepads. Fixed triggers to always use Axis.Gas and Axis.Brake for Android gamepads. Fixed precompiled layouts such as FastKeyboard leading to build time regressions with il2cpp (case 1283676). Fixed InputDevice.canRunInBackground not being correctly set for VR devices (thus not allowing them to receive input while the application is not focused). Fixed InputUser.OnEvent and RebindingOperation.OnEvent exhibiting bad performance profiles and leading to multi-millisecond input update times (case 1253371). In our own measurements, InputUser.OnEvent is >9 times faster than before and RebindingOperation.OnEvent is ~2.5 times faster. Fixed PS4 controller not recognized on Mac when connected over Bluetooth (case 1286449). Fixed EnhancedTouch leaking NativeArray memory on domain reloads (case 1190150). Fixed TouchSimulation leading to \"Pointer should have exited all objects before being removed\" errors (case 1190150). Fixed multi-touch not working with InputSystemUIInputModule (case 1271942). This also manifested itself when using On-Screen Controls and not being able to use multiple controls at the same time (for example, in the Warriors demo). Fixed restart prompt after package installation not appearing on Unity 2020.2+ (case 1292513). Fixed action with multiple bindings getting stuck in Performed state when two or more controls are pressed at the same time (case 1295535). Regression introduced in 1.1-preview.2. Fixed Touch.activeTouches having incorrect touch phases after calling EnhancedTouch.Disable() and then EnhancedTouch.Enable() (case 1286865). Fixed compile errors related to XR/AR on console platforms. Actions Fixed actions not triggering correctly when multiple bindings on the same action were referencing the same control (case 1293808). Bindings will now \"claim\" controls during resolution. If several bindings on the same action resolve to the same control, only the first such binding will successfully resolve to the control. Subsequent bindings will only resolve to controls not already referenced by other bindings on the action. var action = new InputAction(); action.AddBinding(\"<Gamepad>/buttonSouth\"); action.AddBinding(\"<Gamepad>/buttonSouth\"); // Will be ignored. action.AddBinding(\"<Gamepad>/button*\"); // Will only receive buttonWest, buttonEast, and buttonNorth. This also means that InputAction.controls will now only contain any control at most once. Fixed JSON serialization of action maps not preserving empty binding paths (case 1231968). Added Added DualShock4GamepadAndroid and XboxOneGamepadAndroid layout for Android Added a new high-performance way to iterate over changed controls in an event. // Can optionally specify a magnitude threshold that controls must cross. // NOTE: This will note allocate GC memory. foreach (var control in eventPtr.EnumerateChangedControls(magnitudeThreshold: 0.1f)) Debug.Log($\"Control {control} changed state\"); This can be used, for example, to implement much more performant \"any button pressed?\" queries. InputSystem.onEvent += (eventPtr, device) => { // Ignore anything that is not a state event. var eventType = eventPtr.type; if (eventType != StateEvent.Type && eventType != DeltaStateEvent.Type) return; // Find all changed controls actuated above the button press threshold. foreach (var control in eventPtr.EnumerateChangedControls (device: device, magnitudeThreshold: InputSystem.settings.defaultButtonPressThreshold)) // Check if it's a button. if (control is ButtonControl button) Debug.Log($\"Button {button} was pressed\"); } Added support for Step Counter sensors for iOS. You need to enable Motion Usage under Input System settings before using the sensor. You can also manually add Privacy - Motion Usage Description to your application's Info.plist file. [1.1.0-preview.2] - 2020-10-23 Changed The submit and the cancel actions of the UI input module now trigger on release instead of press. This makes the behavior consistent with clicks triggering UI response on release rather than press. Removed the old \"Tanks\" demo (previously available from the samples shipped with the package). Added a new and improved demo project, which you can download from the InputSystem_Warriors GitHub repository. Actions Actions of type InputActionType.Button now respect button press (and release) points. Previously, button-type actions, when used without explicit \"Press\" interactions, would perform immediately when a bound control was actuated. Now, a button-type action will behave the same as if a \"Press\" interaction is applied with \"Trigger Behavior\" set to \"Press Only\". This means that a button-type action will now perform (and perform once only) when a control crosses the button press threshold defined in the global settings or, if present, locally on a ButtonControl. It will then stay performed and finally cancel only when the control falls back to or below the release threshold. InputAction.ReadValue<T>() now always returns default<T> when the action is canceled. This is to make it consistent with InputAction.CallbackContext.ReadValue<T>() which already returned default<T> when the action was canceled. In general, all APIs that read values will return default values when an action is in a phase other than Started or Performed. If multiple actions in different action maps but in the same .inputactions asset have the same name, calling InputActionAsset.FindAction() with just an action name will now return the first enabled action. If none of the actions are enabled, it will return the first action with a matching name as before (case 1207550). var map1 = new InputActionMap(\"map1\"); var map2 = new InputActionMap(\"map2\"); map1.AddAction(\"actionWithSameName\"); map2.AddAction(\"actionWithSameName\"); var asset = ScriptableObject.CreateInstance<InputActionAsset>(); asset.AddActionMap(map1); asset.AddActionMap(map2); map2[\"actionWithSameName\"].Enable(); var action = asset[\"actionWithSameName\"]; // Before: \"map1/actionWithSameName\" // Now: \"map2/actionWithSameName\" Fixed Fixed player build causing ProjectSettings.asset to be checked out in Perforce (case 1254502). Fixed player build corrupting preloaded asset list in PlayerSettings if it was modified by another build processor. Fixed remoting in Input Debugger not working for devices in the player that are created from generated layouts (such as XR devices). Fixed potential NullReferenceException in InputActionProperty when the InputActionReference is null. Fixed \"On-Screen Controls\" sample still using StandaloneInputModule and thus throwing InvalidOperationException when used with \"Active Input Handling\" set to \"Input System Package (New)\" (case 1201866). Fixed OnScreenButton leaving button controls in pressed state when disabled in-between receiving OnPointerDown and OnPointerUp. Usually manifested itself by having to click the button twice next time it was enabled. Fixed exiting out of play mode in the Unity Editor while a test run is in progress leading to the Input System permanently losing all its state until the editor is restarted (case 1251724). Fixed max values for Axis and Double controls stored as multi-bit fields being off by one (case 1223436). Fix contributed by jamre in 962. Thank you! Fixed debug assert in InputDeviceTester sample when simultaneously pressing two buttons on gamepad (case 1244988). Fixed use of UI Slider causing drag thresholds to no longer work (case 1275834). Fixed layout lists in Input Debugger not updating when removing layouts. Fixed device connects leading to different but similar device being reported as reconnected. Actions Fixed Action with multiple bindings becoming unresponsive after a Hold interaction was performed (case 1239551). Fixed NullReferenceException when Player Input component Create Action is pressed and saved (case 1245921). Fixed InputActionTrace.ActionEventPtr.ReadValueAsObject leading to InvalidCastException when trying to read values that came from composite bindings. Fixed not being able to stack a MultiTap on top of a Tap (case 1261462). Fixed rebinds triggered by the Enter key causing stuck Enter key states (case 1271591). Fixed Map index on trigger and IndexOutOfRangeException errors when using multiple Interactions on the same Action. (case 1253034). Fixed context menu in action editor not filtering out composites the same way that the + icon menu does. This led to, for example, a \"2D Vector\" composite being shown as an option for a button type action. Fixed initial state checks for composite bindings failing if performed repeatedly. For example, doing a ReadValue<Vector2> for a WASD binding would return an incorrect value after disabling the map twice while no input from the keyboard was received (case 1274977). Fixed \"Add Interaction\" menu in action editor not filtering out interactions with incompatible value types (case 1272772). Fixed PlayerInput no longer auto-switching control schemes if neverAutoSwitchControlSchemes was toggled off and back on after the component was first enabled (case 1232039). Fixed action map name being the same as .inputactions asset name leading to compile errors when Generate C# Class is used; now leads to import error (case 1212052). Fixed bindings not getting updated when binding by display name and there is no control with the given display name initially. // If at the time this action is enabled, there's no ä key on the keyboard, // this did not update properly later when switched to a layout that does have the key. var action = new InputAction(binding: \"<Keyboard>/#(ä)\"); Added Added tvOS documentation entries in 'Supported Input Devices' page. Actions Added \"release thresholds\" for buttons. Release points are now separated from press points by a percentage threshold. The threshold is defined by InputSettings.buttonReleaseThreshold. Thresholds are defined as percentages of press points. A release is thus defined as a button, after having reached a value of at least InputSettings.defaultButtonPressPoint (or whatever local press is used), falling back to a value equal to or less than InputSettings.buttonReleaseThreshold percent of the press point. This is intended to solve the problem of buttons flickering around button press points. The default threshold is set at 75%, that is, buttons release at 3/4 of the press point. Added new methods to the InputAction class: InputAction.IsPressed(): Whether a bound control has crossed the press threshold and has not yet fallen back below the release threshold. InputAction.WasPressedThisFrame(): Whether a bound control has crossed the press threshold this frame. InputAction.WasReleasedThisFrame(): Whether a bound control has fallen back below the release threshold this frame. InputAction.WasPerformedThisFrame(): Whether the action was performed at any point during the current frame. Equivalent to InputAction.triggered, which will be deprecated in the future. InputAction.Reset(): Forcibly reset the action state. Cancels the action, if it is currently in progress. Added InputAction.GetTimeoutCompletionPercentage to query the amount left to complete a currently ongoing interaction. // Let's say there's a hold interaction on a \"warp\" action. The user presses a button bound // to the action and then holds it. While the user holds the button, we want to know how much // longer the user will have to hold it so that we can display feedback in the UI. var holdCompleted = playerInput.actions[\"warp\"].GetTimeoutCompletionPercentage(); Added three new binding composite types: OneModifierComposite: This is a generalization of ButtonWithOneModifier (which is still available but now hidden from the UI) which also represents bindings such as \"SHIFT+1\" but now can be used to target bindings other than buttons (e.g. \"SHIFT+delta\"). TwoModifiersComposite: This is a generalization of ButtonWithTwoModifiers (which is still available but now hidden from the UI) which also represents bindings such as \"SHIFT+CTRL+1\" but now can be used to target bindings other than buttons (e.g. \"SHIFT+CTRL+delta\"). Vector3Composite: Works the same way Vector2Composite does. Adds a forward and backward binding in addition to up, down, left, and right. [1.1.0-preview.1] - 2020-08-20 The minimum version requirement for the Input System package has been moved up to 2019.4 LTS. Changed Actions Auto-generated C# files now have <auto-generated> headers so they get ignored by Rider code analysis. Auto-generated C# classes are now partial so that they can be manually extended. Deleting a composite binding with action.ChangeBinding(0).Erase() now also erases all the bindings that are part of the composite. Trigger binding resolution from within action callbacks (e.g. InputAction.performed) will now defer resolution until after the callback has completed. This fixes crashes such as case 1242406 where disabling PlayerInput from within an action callback led to an action's state being released while the action was still in a callback. Fixed Fixed input history on Android mono build by alligning memory of history records Fixed no input being processed when running a [UnityTest] over several frames. Before, this required calling InputSystem.Update manually. Fixed clicking on help page button in Unity inspector for Input System components not going to relevant manual pages. Fixed a bug that prevented DualShock controllers from working on tvOS. (case 1221223). GravitySensor, LinearAccelerationSensor, and AttitudeSensor not being initialized on iOS (case 1251382). Fixed compilation issues with XR and VR references when building to platforms that do not have complete XR and VR implementations. Fixed possible NullReferenceExceptions on ARMs with controls that receive automatic memory offsets. Fixed TouchControl.tapCount resetting to 0 when \"Script Debugging\" is enabled (case 1194636). Fixed Touch.activeTouches not having a TouchPhase.Began entry for touches that moved in the same frame that they began in (case 1230656). Fixed sequential taps causing touches to get stuck in Touch.activeTouches. Improved performance of Touch.activeTouches (most notably, a lot of time was spent in endlessly repetitive safety checks). Fixed EnhancedTouch APIs not indicating that they need to be enabled with EnhancedTouchSupport.Enable(). The APIs now throw InvalidOperationException when used without being enabled. Fixed memory corruption in InputEventTrace.AllocateEvent (case 1262496) Manifested itself, for example, as crashes when using InputActionTrace.SubscribeToAll. AxisControls and Vector2Controls' X and Y subcontrols on XR devices now have a minimum range of -1 and a maximum range of 1. This means they can now properly respond to modifiers and interactions in the binding system. Actions Fixed drag&drop reordering actions while having one control scheme selected causing bindings from other control schemes to be lost (case 122800). Fixed stack overflow in PlayerInput.SwitchCurrentActionMap when called from action callback (case 1232893). Fixed control picker ending up empty when listing devices in \"Supported Devices\" (case 1254150). Added Device layouts can now be \"precompiled\" for speed. Keyboard, Mouse, and Touchscreen are now included as precompiled layouts greatly reducing instantiation time and GC heap cost for these devices. For Touchscreen, this results in a >20x speed-up for InputSystem.AddDevice<Touchscreen>(). Added Pose Control layout. The Pose Control is used on XR Devices and wraps tracking state, position, rotation, and velocity information. Actions Can now save binding overrides as JSON strings and restore them from such using the newly added SaveBindingOverridesAsJson and LoadBindingOverridesFromJson extension methods. void SaveUserRebinds(PlayerInput player) { var rebinds = player.actions.SaveBindingOverridesAsJson(); PlayerPrefs.SetString(\"rebinds\", rebinds); } void LoadUserRebinds(PlayerInput player) { var rebinds = PlayerPrefs.GetString(\"rebinds\"); player.actions.LoadBindingOverridesFromJson(rebinds); } [1.0.0] - 2020-04-23 Fixed Fixed compilation issues in TrackedDeviceRaycaster when disabling built-in XR module. [1.0.0-preview.7] - 2020-04-17 Fixed VirtualMouseInput not moving the software cursor when set to HardwareCursorIsAvailable but not having a hardware cursor () Can now override built-in Android gamepad layouts. Previously, the input system would always choose its default defaults even after registering more specific layouts using InputSystem.RegisterLayout. InputControlPath.TryGetControlLayout no longer throws NotImplementedException for <Mouse>/scroll/x and similar paths where the layout is modifying a control it inherited from its base layout (thread). Fixed compilation errors when disabling built-in VR and XR modules. (case 1214248). Fixed compilation errors when disabling built-in Physics and Physics2D modules. (case 1191392). No longer throws NotImplementedException when matching against a field of InputDeviceDescription.capabilities when the value of the field used scientific notation. No longer incorrectly matches fields of InputDeviceDescription.capabilities by prefix only (i.e. previously it would find the field \"foo\" when actually looking for \"foobar\"). Input device debugger window slowing editor to a crawl when opened on PS4 DualShock controller. InputUser.UnpairDevices() corrupting user device list. Actions Controls are now re-resolved after adding or removing bindings from actions (case 1218544). Can now have spaces and special characters in action names when using PlayerInput with the SendMessages or BroadcastMessages behavior. Previously, an incorrect method name was generated (fix contributed by BHSPitMonkey in #1022; case 1214519). Adding a new action now sets expectedControlType to Button as expected (case 1221015). Player joins with PlayerInputManager from button presses no longer fail if there are multiple devices of the same type present and the join was not on the first gamepad (case 226920). PlayerInputEditor no longer leads to the player's InputActionAsset mistakenly getting replaced with a clone when the inspector is open on a PlayerInput component (case 1228636). The control picker in the .inputactions editor will no longer incorrectly filter out layouts such as Xbox One Gamepad (on XB1) when using them in control schemes. Also, it will no longer filter out controls from base layouts (such as Gamepad) (case 1219415). RebindOperations will no longer pick controls right away that are already actuated above the magnitude threshold when the operation starts. Instead, these controls will have to change their actuation from their initial level such that they cross the magnitude threshold configured in the operation (case 1215784). Newly added actions and action maps are now scrolled to when there are more items than fit into view. Previously newly added item was appended but outside of the visible area. Actions and bindings in the .inputactions editor are no longer force-expanded on every domain reload and whenever a new action or binding is added. The importer for .inputactions assets will now check out from version control the generated .cs file when overwriting it – which only happens if the contents differ (case 1222972). The editor for .inputactions assets will now check out from version control the asset before saving it. Drag-reordering action maps no longer throws \"Should have drop target\" asserts in the console (case 1229146). Drag-reordering actions no longer changes action IDs of some of the existing actions (case 1231233). References to InputActionReference objects created by the importer for .inputactions files are no longer broken when the action referenced by the object is renamed (case 1229145). NOTE: This fix does not apply to existing InputActionReference instances. The problem was inherent in the internal file IDs generated for actions – which were affected by action and map names. Thus, changing the name of an action or map would change the resulting file ID of the InputActionReference. However, changing file IDs will break any existing reference to the object. Thus we had to preserve the existing InputActionReference objects under their original file ID. We hide them in the Project Browser, however. The ones that are visible now have the new, fixed file IDs. To switch existing InputActionReference properties to the new file IDs, simply replace them with the newly created InputActionReference. Changed InputDevice.all has been deprecated due to the confusion it creates with other getters like Gamepad.all. Use InputSystem.devices instead (case 1231216). In the same vein, we added a new Joystick.all getter that works the same as Gamepad.all. Changed UI Package to be optional dependency. Removing the package will now disable all UI relevant Input code. [1.0.0-preview.6] - 2020-03-06 Changed InputSystemUIInputModule.trackedDeviceSelect has been removed. Use InputSystemUIInputModule.leftClick instead. InputSystemUIInputModule.repeatDelay has been renamed to moveRepeatDelay and repeatRate has been renamed to moveRepeatRate. Fixed Fixed CS0109 warning being generated during player build due to use of new with the PlayerInput.camera property (case 1174688). Fixed a number of issues in InputSystemUIInputModule. Fixed GC heap garbage when click-dragging. Fixed number of pointer states growing indefinitely if OS did not reuse touch IDs. Fixed lastPress on PointerEventData getting lost. Fixed button press-and-release happening in same frame resulting in no UI input. Fixed clicks initiated from non-pointer devices resulting in pointer inputs with (0,0) positions. Fixed huge screen deltas on pointer events from tracked devices. Fixed touch input not sending pointer exit events (case 1213550). Fixed TrackedDeviceRaycaster not setting screenPosition in RaycastResult. Actions Mixing the enabling&disabling of single actions (as, for example, performed by InputSystemUIInputModule) with enabling&disabling of entire action maps (as, for example, performed by PlayerInput) no longer leaves to unresponsive input and \"should not reach here\" assertions (forum thread). Leaving play mode no longer leaves state change monitors lingering around from enabled actions. Enabling action maps with bindings that do not refer to an existing action in the map no longer leads to asserts and exceptions when input on the bindings is received (case 1213085). PressInteraction no longer misses the next button press if it gets reset from within the performed callback (case 1205285). InputBinding.DisplayStringOptions.DontIncludeInteractions is now properly respected. Reading the value of a composite binding no longer causes processors from the last active part binding to be applied rather than the processors of the composite itself, if any (case 1207082). Fixed InputSystem.onActionChange getting invoked too many times on binding changes. Added InputSystemUIInputModule now sends pointer events using a new ExtendedPointerEventData instead of using the base PointerEventData class. This surfaces additional input data in pointer events. Added InputSystemUIInputModule.pointerBehavior to allow dictating how the UI will resolve concurrent input from multiple pointers. Actions Added InputAction.CallbackContext.ReadValueAsButton. [1.0.0-preview.5] - 2020-02-14 Changed We've changed the rules that govern how action phases have to progress: This is a breaking change! The primary effect is additional callbacks getting triggered. Before: There were no enforced rules about how an action would go through InputAction.started, InputAction.performed, and InputAction.canceled. Which of the callbacks were triggered and in what order depended on a number of factors, the biggest influencer of which were the different interactions that could be applied to actions (like Press or Hold). This made for unpredictable and frequently surprising results. In addition, it led to bugs where, for example, adding a Press interaction to the Click action of InputSystemUIInputModule would cause the click state to get stuck because the click action would never cancel. Now: The system will now always trigger InputAction.started first. If this is not done explicitly, it happens implicitly. Likewise, the system will now always trigger InputAction.canceled before going back to waiting state. Like with InputAction.started, if this isn't done explicitly, it will happen implicitly. This implies that InputAction.canceled no longer signifies an action getting aborted because it stopped after it started but before it performed. It now simply means \"the action has ended\" whether it actually got performed or not. In-between InputAction.started and InputAction.canceled, InputAction.performed may be triggered arbitrary many times (including not at all). While late in the cycle for 1.0, we've opted to make this change now in order to fix a range of bugs and problems we've observed that people encountered because of the previous behavior of the system. Related to the change above, the behavior of PressInteraction has been tweaked and now is the following: Press Only: Starts and immediately performs when pressed, then stays performed and cancels when button is released. Release Only: Starts when button is pressed and then performs and immediately cancels when the button is released. Press And Release: Starts and immediately performs when button is pressed, then stays performed and performs again and immediately cancels when button is released. Vector2Composite now has a mode parameter which can be used to choose between DigitalNormalized (the default), Digital (same as DigitalNormalized but does not normalize the resulting vector), and Analog (uses float input values as is). Vector2Composite.normalize has been deprecated. Note that it will not work together with Analog. The parameter will be removed in the future. Fixed XR controllers and HMDs have proper display names in the UI again. This regressed in preview.4 such that all XR controllers were displayed as just \"XR Controller\" in the UI and all HMDs were displayed as \"XR HMD\". InputSystemUIInputModule no longer generates GC heap garbage every time mouse events are processed. Fixed a bug where an internal array helper method was corrupting array contents leading to bugs in both InputUser and Touch. Fixed exception when saving changes to an Input Action asset and the parent directory has been renamed. (case 1207527) Actions The regression in 1.0.0-preview.4 of PlayerInputManager not joining players correctly if a scheme has more than one device requirement has been fixed. This most notably manifested itself with keyboard+mouse control schemes. PlayerInputManager will no longer join players when control schemes are used and none of the schemes produces a successful match based on the devices available for the join. When no action map is selected in action editor, plus icon to add an action is now disabled; formerly threw an exception when clicked (case 1199562). Removing a callback from actions from the callback itself no longer throws ArgumentOutOfRangeException (case 1192972). \"Invalid user\" ArgumentException when turning the same PlayerInput on and off (case 1198889). The list of device requirements for a control scheme in the action editor no longer displays devices with their internal layout name rather than their external display name. StackOverflowException when Invoke Unity Events is selected in PlayerInput and it cannot find an action (#1033). HoldInteraction now stays performed after timer has expired and cancels only on release of the control (case 1195498). Foldouts in the various action UIs now properly toggle their expansion state when clicked in Unity 2019.3+ (case 1213781). Added We've added a new Simple Multiplayer sample which demonstrates a simple, bare-bones local multiplayer setup. We've also added a Gamepad Mouse Cursor sample that shows how to drive a UI mouse cursor using the gamepad. The sample contains a reusable VirtualMouseInput component that does most of the work. Added a Deselect On Background Click option to InputSystemUIInputModule. This allows toggling the behavior off where clicking the mouse and not hitting a GameObject will automatically clear the current selection -- which will break keyboard and gamepad navigation. [1.0.0-preview.4] - 2020-01-24 This release includes a number of Quality-of-Life improvements for a range of common problems that users have reported. Added To aid in debugging issues, we've extended the system's event tracing and replay functionality to allow persisting and replaying arbitrary input event streams. InputEventTrace now has APIs to persist the events to disk and to load them back in from previously persisted event streams. The same API can be used to persist in arbitrary C# Stream instances, not just in file streams. // Write. myTrace.WriteTo(\"file.inputtrace\"); // Read. InputEventTrace.LoadFrom(\"file.inputtrace\"); InputEventTrace now has built-in replay functionality. myTrace.Replay().PlayAllFramesOneByOne(); The event trace in device windows of the Input Debugger has been extended with controls to save and load traces. We've added a new InputRecording sample which has a reusable MonoBehaviour component that can be used to capture and replay device activity. Keyboard now has a FindKeyOnCurrentKeyboardLayout method to look up key controls by their display names. Keyboards now have synthetic controls that combine left and right variants of modifier keys. This means that you can bind to just \"shift\" now, for example, instead of having to bind to both \"left shift\" and \"right shift\". new InputAction(binding: \"<Keyboard>/shift\"); The controls are also available as properties on Keyboard. if (Keyboard.current.shiftKey.isPressed) /* ... */; // Is equivalent to: if (Keyboard.current.leftShiftKey.isPressed || Keyboard.current.rightShiftKey.isPressed) /* ... */; Actions PlayerInput now has a new Controls Changed event/message which is triggered when the control setup of the player changes (e.g. when switching control schemes). public void OnControlsChanged() { // Update UI display hints, for example... } We've added APIs to simplify turning bindings into strings suitable for display in UIs. // Takes things such as currently bound controls and active binding masks into account // and can handle composites. action.GetBindingDisplayString(); Related to this, custom binding composites can now be annotated with the new DisplayStringFormat attribute to control how composites as a whole are turned into display strings. [DisplayStringFormat(\"{button}+{stick}\")] public class MyComposite : InputBindingComposite<Vector2> { [InputControl(layout = \"Button\")] public int button; [InputControl(layout = \"Stick\")] public int stick; } InputActionRebindingExtension.RebindingOperation has a new configuration method WithMatchingEventsBeingSuppressed which allows suitable input events to automatically be swallowed while a rebind is ongoing. This greatly helps with not having something else respond to input while a rebind is in progress. We've added two new samples: Rebinding UI: Demonstrates how to create a rebinding screen using the Input System's APIs. The sample also includes a reusable prefab you can use directly in your projects to quickly put rebinding screens together. In-Game Hints: Demonstrates how to show context-sensitive help that respects the current control scheme. Changed The logic for resetting devices on focus loss has changed somewhat: When focus is lost, all devices are forcibly reset to their default state. As before, a RequestResetCommand for each device is also sent to the backend but regardless of whether the device responds or not, the input state for the device will be overwritten to default. Noisy controls are exempted from resets. The assumption here is that noisy controls most often represent sensor readings of some kind (e.g. tracking data) and snapping the values back to their default will usually If Application.runInBackground is true, all devices that return true from InputDevice.canRunInBackground are exempted from resets entirely. This, for example, allows XR devices to continue running regardless of focus change. This fixes problems such as keyboard keys getting stuck when alt-tabbing between applications (case 1206199). InputControlExtensions.GetStatePtrFromStateEvent no longer throws InvalidOperationException when the state format for the event does not match that of the device. It simply returns null instead (same as when control is found in the event's state). InputEventTrace instances are no longer disposed automatically from their finalizer but MUST be disposed of explicitly using Dispose(). This is to allow event traces to survive domain reloads. If they are disposed of automatically during finalizers, even if they survive the reload, the next GC will cause traces to be deallocated. Actions InputActionRebindingExtensions.PerformInteractiveRebinding has been greatly enhanced to apply a wide range of default configurations to the rebind. This greatly reduces the need to manually configure the resulting rebind. // Start a rebind with the default configuration. myAction.PerformInteractiveRebinding().Start(); Pointer position input will be ignored by default. If not a suitable binding target itself, <Keyboard>/escape will automatically be made to quit the rebind. Events with control input not explicitly matching exclusions will now get suppressed. This prevents input actions from getting triggered while a rebind is in progress. The expected control type is automatically adjusted if a part binding of a composite is targeted by the rebind (e.g. if the action expects a Vector2 but the part binding expects a Button, the rebind switches automatically to Button). If the targeted binding is part of a control scheme, controls will automatically be restricted to match the device requirements of the control scheme. For example, if the binding belongs to a \"Keyboard&Mouse\" scheme that has <Keyboard> and a <Mouse> requirement, the rebind will ignore input on gamepads. As before, you can always create a RebindingOperation from scratch yourself or wipe/alter the configuration returned by PerformInteractiveRebinding however you see fit. Control schemes can now handle ambiguity. This means that, for example, you can now have one control scheme for generic gamepads and another control scheme specifically for PS4 controllers and the system will reliably pick the PS4 scheme when a PS4 controller is used and fall back to the generic gamepad scheme otherwise. While this is exposed as a new score property on InputControlScheme.MatchResult, no code changes are necessary to take advantage of this feature. PlayerInput.active has been renamed to PlayerInput.inputIsActive to avoid ambiguities with GameObject activation. Fixed InputUser in combination with touchscreens no longer throws InvalidOperationException complaining about incorrect state format. In a related change, InputControlExtensions.GetStatePtrFromStateEvent now works with touch events, too. Stack overflow in InputTestFixture.currentTime getter. Input that occurs in-between pressing the play button and the game starting no longer leaks into the game (case 1191342). This usually manifested itself as large accumulated mouse deltas leading to such effects as the camera immediately jerking around on game start. Removing a device no longer has the potential of corrupting state change monitors (and thus actions getting triggered) from other devices. This bug led to input being missed on a device once another device had been removed. TrackedDevice layout is no longer incorrectly registered as Tracked Device. Event traces in the input debugger are no longer lost on domain reloads. IndexOutOfRangeException being thrown when looking up controls on XR devices. Actions Clicking the \"Replace with InputSystemUIInputModule\" button in the inspector when looking at StandaloneInputModule, the resulting operation is now undoable and will properly dirty the scene. [1.0.0-preview.3] - 2019-11-14 Fixed Fixed wrong event handlers getting removed when having three or more handlers on an event (case 1196143). This was an bug in an internal data structure that impacted a number of code paths that were using the data structure. Fixed LayoutNotFoundException being thrown when InputControlPath.ToHumanReadableString referenced a layout that could not be found. [1.0.0-preview.2] - 2019-11-04 Changed Automatic conversion of window coordinates in EditorWindow code is now performed regardless of focus or the setting of Lock Input to Game View in the input debugger. Fixed Fixed touch taps triggering when they shouldn't on Android. Fixed custom devices registered from [InitializeOnLoad] code being lost on domain reload (case 1192379). This happened when there were multiple pieces of [InitializeOnLoad] code that accessed the input system in the project and the RegisterLayout for the custom device happened to not be the first in sequence. OpenVR touchpad controls (touchpadClicked & touchpadPressed) now report accurate data. Actions Fixed missing keyboard bindings in DefaultInputActions.inputactions for navigation in UI. Fixed using C# reserved names in .inputactions assets leading to compile errors in generated C# classes (case 1189861). Assigning a new InputActionAsset to a InputSystemUIInputModule will no longer look up action names globally but rather only look for actions that are located in action maps with the same name. Previously, if you e.g. switched from one asset where the point action was bound to UI/Point to an asset that had no UI action map but did have an action called Point somewhere else, it would erroneously pick the most likely unrelated Point action for use by the UI. Fixed missing custom editors for AxisDeadzoneProcessor and StickDeadzoneProcessor that link min and max values to input settings. Fixed actions ending up being disabled if switching to a control scheme that has no binding for the action (case 1187377). Fixed part of composite not being bound leading to subsequent part bindings not being functional (case 1189867). Fixed PlayerInput not pairing devices added after it was enabled when not having control schemes. This problem would also show in the SimpleDemo sample when having the CustomDeviceUsages sample installed as well. Gamepads would not get picked up in that case. Fixed ArgumentNullException when adding a device and a binding in an action map had an empty path (case 1187163). Fixed bindings that are not associated with any control scheme not getting enabled with other control schemes as they should. Added Added a new EditorWindow Demo sample that illustrates how to use the input system in editor UI code. [1.0.0-preview.1] - 2019-10-11 Changed Generated action wrappers now won't Destroy the generated Asset in a finalizer, but instead implement IDisposable. Added back XR layouts (except for Magic Leap) that were removed for 1.0-preview. We removed these layouts under the assumption that they would almost concurrently become available in the respective device-specific XR packages. However, this did not work out as expected and the gap here turned out to be more than what we anticipated. To deal with this gap, we have moved the bulk of the XR layouts back and will transition things gradually as support in device-specific packages becomes publicly available. Fixed Fixed a bug where the Input Settings Window might throw exceptions after assembly reload. Correctly implemented IsPointerOverGameObject method for InputSystemUIInputModule. Several bugs with layout overrides registered with (InputSystem.RegisterLayoutOverrides). In 1.0-preview, layout overrides could lead to corruption of the layout state and would also not be handled correctly by the various editor UIs. Selecting a layout in the input debugger no longer selects its first child item, too. Fixed XR devices reporting noise as valid user input (should fix problem of control schemes involving VR devices always activating when using PlayerInput). Fixed tap/swipe gesture detection in touch samples. Actions Fixed a bug where multiple composite bindings for the same controls but on different action maps would throw exceptions. Fixed anyKey not appearing in control picker for Keyboard. The text on the \"Listen\" button is no longer clipped off on 2019.3. Controls bound to actions through composites no longer show up as duplicates in the input debugger. Fixed \"Create Actions...\" on PlayerInput creating an asset with an incorrect binding for taps on Touchscreens. NOTE: If you have already created an .inputactions asset with this mechanism, update \"tap [Touchscreen]\" to \"Primary Touch/Tap\" to fix the problem manually. Fixed Invoke CSharp Events when selected in PlayerInput not triggering PlayerInput.onActionTriggered. Fixed duplicating multiple items at the same time in the action editor duplicating them repeatedly. Added Will now recognize Xbox One and PS4 controllers connected to iOS devices correctly as Xbox One and PS4 controllers. Added a new sample called \"Custom Device Usages\" that shows how to use a layout override on Gamepad to allow distinguishing two gamepads in bindings based on which player the gamepad is assigned to. Added abstract TrackedDevice input device class as the basis for various kinds of tracked devices. [1.0.0-preview] - 2019-09-20 Fixed Will now close Input Action Asset Editor windows from previous sessions when the corresponding action was deleted. Fixed an issue where Stick Controls could not be created in Players built with medium or high code stripping level enabled. Fixed incorrect default state for axes on some controllers. Actions Fixed CallbackContext.ReadValue throwing when invoked during device removal Changed Added [0.9.6-preview] - 2019-09-06 Fixed Exceptions in scenes of Visualizers sample if respective device was not present on system (e.g. in PenVisualizer if no pen was present in system). Fixed exception in Input Action Asset Editor window when typing whitespace into the search field. Fixed control scheme popup window in input action asset editor window showing in the correct screen position on windows. Actions Setting timeouts from IInputInteraction.Process not working as expected when processing happened in response to previous timeout expiring (#714). Pending timeouts on a device not being removed when device was removed. Changed Replaced HIDSupport.shouldCreateHID event with a new HIDSupport.supportedHIDUsages property, which takes an array of supported usages. Added Actions Added PlayerInput.neverAutoSwitchControlSchemes to disable logic that automatically enables control scheme switching when there is only a single PlayerInput in the game. Added PlayerInput.SwitchControlScheme to switch schemes manually. [0.9.5-preview] - 2019-08-29 Fixed Don't pass events for null devices (for devices which have not been created) to InputSystem.onEvent callbacks. Will close debugger input state windows, when the state is no longer valid instead of throwing exceptions. Fixed pointer coordinates in editor windows for non-mouse pointing devices. Fixed using the input system in il2cpp when managed stripping level is set higher then \"Low\". Device debugger window will still show when reading from specific controls throws exceptions. Offsets and sizes for elements on Linux joysticks are now computed correctly. Joysticks now have a deadzone processor on the stick itself. Up/down/left/right on sticks are now deadzoned just like X and Y on sticks are. Removed toplevel X and Y controls on HIDs when there is a Stick/X and Stick/Y added for the device. HID fallback can now deal with sticks that have X and Y controls of different sizes and sitting in non-contiguous locations in the HID input report. Button 1 on HID joysticks will now correctly come out as the trigger control. Previously, the trigger control on the joystick was left pointing to random state. Actions Binding paths now show the same way in the action editor UI as they do in the control picker. For example, where before a binding to <XInputController>/buttonSouth was shown as rightShoulder [XInputController], the same binding will now show as A [Xbox Controller]. When deleting a control scheme, bindings are now updated. A dialog is presented that allows choosing between deleting the bindings or just unassigning them from the control scheme. When renaming a control scheme, bindings are now updated. Previously the old name was in place on bindings. Control scheme names can no longer be set to empty strings. PlayerInput.Instantiate now correctly sets up a given control scheme, if specified. When passing a controlScheme: argument, the result used to be a correctly assigned control scheme at the InputUser level but no restrictions being actually applied to the bindings, i.e. every single binding was active regardless of the specified control scheme. NullReferenceExceptions during event processing from RebindingOperation. Changed InputUser.onUnpairedDeviceUsed now receives a 2nd argument which is the event that triggered the callback. Also, the callback is now triggered BEFORE the given event is processed rather than after the event has already been written to the device. This allows updating the pairing state of the system before input is processed. In practice, this means that, for example, if the user switches from keyboard&mouse to gamepad, the initial input that triggered the switch will get picked up right away. InputControlPath.ToHumanReadableString now takes display names from registered InputControlLayout instances into account. This means that the method can now be used to generate strings to display in rebinding UIs. AxisControl.clamp is now an enum-valued property rather than a bool. Can now perform clamping before normalization. Actions When switching devices/controls on actions, the system will no longer subsequently force an initial state check on all actions. Instead, every time an action's bindings get re-resolved, the system will simply cancel all on-going actions and then re-enable them the same way it would happen by manually calling InputAction.Enable. Removed non-functional InputControlScheme.baseScheme API and basedOn serialized property. This was never fully implemented. Added Can right-click devices in Input Debugger (also those under \"Unsupported\") and select \"Copy Device Description\" to copy the internal InputDeviceDescription of the device in JSON format to the system clipboard. This information is helpful for us to debug problems related to specific devices. If a device description has been copied to the clipboard, a new menu \"Paste Device Description as Device\" entry in the \"Options\" menu of the input debugger appears. This instantiates the device from the description as if it was reported locally by the Unity runtime. [0.9.3-preview] - 2019-08-15 Fixed XInputController and XboxOneGamepad no longer have two extraneous, non-functional \"menu\" and \"view\" buttons. Fixed InputUser.onUnpairedDeviceUser ignoring input on controls that do not support EvaluateMagnitude. This led to situations, for example, where PlayerInput would not initialize a control scheme switch from a <Mouse>/delta binding as the delta X and Y axes do not have min&max limits and thus return -1 from EvaluateMagnitude. Fixed available processor list not updated right away when changing the action type in the Input Action editor window. Actions NullReferenceException when the input debugger is open with actions being enabled. When selecting a device to add to a control scheme, can now select devices with specific usages, too (e.g. \"LeftHand\" XRController). Changed Removed timesliceEvents setting - and made this tied to the update mode instead. We now always time slice when using fixed updates, and not when using dynamic updates. When adding a composite, only ones compatible with the value type of the current action are shown. This will, for example, no longer display a 2D Vector composite as an option on a floating-point button action. The InputState.onChange callback now receives a second argument which is the event (if any) that triggered the state change on the device. Added InputSystemUIInputModule can now track multiple pointing devices separately, to allow multi-touch input - required to allow control of multiple On-Scree controls at the same time with different fingers. Two new composite bindings have been added. ButtonWithOneModifier can be used to represent shortcut-like bindings such as \"CTRL+1\". ButtonWithTwoModifiers can be used to represent shortcut-like bindings such as \"CTRL+SHIFT+1\". [0.9.2-preview] - 2019-08-09 Fixed A RebindingOperation will now fall back to the default path generation behavior if the callback provided to OnGeneratePath returns null. Fixed the Input Action editor window throwing exceptions when trying to view action properties. Actions PlayerInput will now copy overrides when creating duplicate actions. It is now possible to use an empty binding path with a non empty override path. It is now possible to use set an empty override path to disable a binding. It is not possible to query the effectively used path of a binding using effectivePath. Actions embedded into MonoBehaviour components can now have their properties edited in the inspector. Previously there was no way to get to the properties in this workflow. There is a gear icon now on the action that will open the action properties. Changed Added Added a new sample to the package called SimpleDemo. You can install the sample from the package manager. See the README.md file for details about the sample. [0.9.1-preview] - 2019-08-08 Fixed Fixed GC heap garbage being caused by triggered by event processing. This meant that every processing of input would trigger garbage being allocated on the managed heap. The culprit was a peculiarity in the C# compiler which caused a struct in InputEventPtr.IsA to be allocated on the heap. The bindings selection popup window will now show child controls matching the current action type even if the parent control does not match. Fixed duration values reported for Hold and Press interactions. DualShock 3 on macOS: Fixed actions bound to the dpad control performing correctly. Fixed non-present touchpad button control being triggered incorrectly. Fixed compile issues with switch classes on standalone Linux. Leak of unmanaged memory in InputControlList. Actions Fixed actions not updating their set of controls when the usages of a device are changed. Composite bindings with the default interaction will now correctly cancel when the composite is released, even if there are multiple composite bindings on the action. Changed MouseState, KeyboardState, and GamepadState have been made public again. PlayerInput and PlayerInputManager have been moved from the UnityEngine.InputSystem.PlayerInput namespace to UnityEngine.InputSystem. The signature of InputSystem.onEvent has changed. The callback now takes a second argument which is the device the given event is sent to (null if there's no corresponding InputDevice). // Before: InputSystem.onEvent += eventPtr => { var device = InputSystem.GetDeviceById(eventPtr.deviceId); //... }; // Now: InputSystem.onEvent += (eventPtr, device) => { //... }; The signatures of InputSystem.onBeforeUpdate and InputSystem.onAfterUpdate have changed. The callbacks no longer receive an InputUpdateType argument. Use InputState.currentUpdateType in case you need to know the type of update being run. InputUpdateType has been moved to the UnityEngine.InputSystem.LowLevel namespace. InputSystem.Update(InputUpdateType) has been removed from the public API. The way input devices are built internally has been streamlined. InputDeviceBuilder is now internal. It is no longer necessary to access it to look up child controls. Simply use InputControl.GetChildControl instead. To build a device without adding it to the system, call the newly added InputDevice.Build method. InputDevice.Build<Mouse>(); InputSystem.SetLayoutVariant has been removed. Layout variants can no longer be set retroactively but must be decided on as part of device creation. InputSystem.RegisterControlProcessor has been renamed to just InputSystem.RegisterProcessor. Actions InputAction.ReadValue<TValue>() is longer correlated to InputAction.triggered. It simply returns the current value of a bound control or composite while the action is being interacted with. InputInteractionContext.PerformedAndGoBackToWaiting has been renamed to just InputInteractionContext.Performed. Actions Individual composite part bindings can now no longer have interactions assigned to them as that never made any sense. Added Devices can now have more than one usage. Call InputSystem.AddDeviceUsage(device,usage) to add additional usages to a device. Call InputSystem.RemoveDeviceUsage(device,usage) to remove existing usages from a device. InputSystem.SetDeviceUsage(device,usage) still exists. It will clear all existing usages from the given device. A new VisualizerSamples sample that can be installed through the package manager. Contains two components InputControlVisualizer and InputActionVisualizer that help visualizing/debugging control/device and action activity through in-game overlays. A few sample scenes illustrate how to use them. Actions Added InputAction.ReadValueAsObject API. Added InputAction.activeControl API. [0.9.0-preview] - 2019-07-18 Fixed Validate all parameters on public APIs. Fixed an internal bug in InlinedArray.RemoveAtByMovingTailWithCapacity, which could cause data corruption. Fixed Xbox controller support on macOS il2cpp. Fixed issue of Xbox gamepads on Windows desktop not being able to navigate left and down in a UI. Allow using InputSystem package if the XR, VR or Physics modules are disabled for smaller builds. Fixed documentation landing page and table of contents. Fixed tracked devices assigning pointer ids for UI pointer events correctly. Adjusted some UI Elements to fit the Unity 19.3 font. Fixed NullReferenceException being thrown when project changes. Fixed duplicate devices showing in the \"Supported Devices\" popup when using a search filter. Fixed an error when adding new bindings in the Input Actions editor window when a filter was applied. Fixed scroll wheel handling in InputSystemUIInputModule not being smooth. Fixed compile errors from Switch Pro controller code on Linux. Actions Fixed CallbackContext.control referencing the composite member control which was actually actuated for this trigger for composite bindings. Generated C# wrappers for .inputactions assets are no longer placed in Assets/Assets/ folder on Windows. Added Touch support has been reworked and extended. Touchscreen.touch[0..9] are now bindable from the control picker. Touchscreen.primaryTouch is now a separate control which tracks the primary touch on the screen. The controls Touchscreen inherits from Pointer (such as position, phase, and delta) are now tied to Touchscreen.primaryTouch and allow for Touchscreen to function as a generic Pointer (like Mouse and Pen). Touchscreen.press (renamed from Touchscreen.button) is now a working, synthetic button that is down whenever at least one finger is on the screen. Recording of start time and start position has been added to touches. TouchControl.startPosition gives the starting position of the touch. TouchControl.startTime gives the starting time of the touch. Tap detection has been added to Touchscreen. Tap time (i.e. time within which a press-and-release must be completed for a tap to register) corresponds to InputSettings.defaultTapTime. Tap release must happen within a certain radius of first contact. This is determined by a new setting InputSettings.tapRadius. TouchControl.tap is a new button control that triggers then the touch is tapped. Note that this happens instantly when a touch ends. The button will go to 1 and immediately go back to 0. This means that polling the button in Update, for example, will never trigger a tap. Either use actions to observe the button or use the Touch API from EnhancedTouch to poll taps. Touchscreen.activeTouches has been removed. Use Touch.activeTouches from the new enhanced touch API instead for more reliable touch tracking. Touchscreen.allTouchControls has been renamed to Touchscreen.touches. A new EnhancedTouch plugin has been added which offers an enhanced Touch and Finger API to reliably track touches and fingers across updates. This obsoletes the need to manually track touch IDs and phases and gives access to individual touch history. Touch can be simulated from mouse or pen input now. To enable simulation, call TouchSimulation.Enable() or put the TouchSimulation MonoBehaviour in your scene. Also, in the input debugger, you can now enable touch simulation from the \"Options\" dropdown. Changing state has been decoupled from events. While input events are the primary means by which to trigger state changes, anyone can perform state changes manually now from anywhere. InputState.Change(gamepad.leftStick, new Vector2(123, 234)); This change makes it possible to update state from state and thus synthesize input data from other input coming in. A new API for recording state changes over time has been added. var history = new InputStateHistory(\"<Gamepad>/leftStick\"); history.StartRecording(); //... foreach (var record in history) Debug.Log(record); Added support for generic joysticks on WebGL (which don't use the standard gamepad mapping). Added support for DualShock 3 gamepads on desktops. Added support for Nintendo Switch Pro Controllers on desktops. Actions Actions now also have a polling API! InputAction.triggered is true if the action was performed in the current frame. InputAction.ReadValue<TValue>() yields the last value that started, performed, or cancelled (whichever came last) was called with. If the action is disabled, returns default(TValue). For InputActionType.Button type actions, returns 1.0f if triggered==true and 0.0f otherwise. Generated C# wrappers for .inputactions can now placed relative to the .inputactions file by specifying a path starting with './' (e.g. ./foo/bar.cs). Changed The system no longer supports processing input in BOTH fixed and dynamic updates. Instead, a choice has to be made whether to process input before each FixedUpdate() or before each Update(). Rationale: the existing code that supported having both updates receive input independently still had several holes and became increasingly complex and brittle. Our solution was based on not actually processing input twice but on channeling input concurrently into both the state of both updates. Together with the fact that specific inputs have to reset (and possibly accumulate) correctly with respect to their update time slices, this became increasingly hard to do right. This, together with the fact that we've come to increasingly question the value of this feature, led us to removing the capability while preserving the ability to determine where input is processed. NOTE: Timeslicing is NOT affected by this. You can still switch to ProcessEventInFixedUpdates and get events timesliced to individual FixedUpdate periods according to their timestamps. InputSettings.UpdateMode.ProcessEventsInBothFixedAndDynamicUpdate has been removed. InputSettings.UpdateMode.ProcessEventsInDynamicUpdateOnly has been renamed to InputSettings.UpdateMode.ProcessEventsInDynamicUpdate and is now the default. InputSettings.UpdateMode.ProcessEventsInFixedUpdateOnly has been renamed to InputSettings.UpdateMode.ProcessEventsInFixedUpdate. Added icons for PlayerInput, PlayerInputManager, InputSystemUIInputModule and MultiplayerEventSystem components. Changed Keyboard IME properties (imeEnabled, imeCursorPosition) to methods (SetIMEEnabled, SetIMECursorPosition). Added getters to all IInputRuntime properties. Replace some GetXxx methods in our API with xxx properties. Pointer.phase has been removed and PointerPhase has been renamed to TouchPhase. Phases are now specific to touch. PointerPhaseControl has been renamed to TouchPhaseControl. Pointer.button has been renamed to Pointer.press and now is a control that indicates whether the pointer is in \"press down\" state. For mouse, corresponds to left button press. For pen, corresponds to tip contact. For touch, corresponds to primary touch contact (i.e. whether any finger is down). The state change monitor APIs (IInputStateChangeMonitor and friends) have been moved out of InputSystem into a new static class InputState in UnityEngine.Experimental.Input.LowLevel. Rationale: These APIs are fairly low-level and not of general interest so having them out of InputSystem reduces the API surface visible to most users. InputDeviceChange.StateChanged has been removed and is now a separate callback InputState.onChange. Rationale: The other InputDeviceChange notifications are low-frequency whereas StateChanged is high-frequency. Putting them all on the same callback made adding a callback to InputSystem.onDeviceChange unnecessarily expensive. IInputStateCallbackReceiver has been rewritten from scratch. Now has two simple methods OnNextUpdate and OnEvent. If implemented by a device, the device now has completely control over changing its own state. Use the InputState.Change methods to affect state changes while trigger state change monitors (e.g. for actions) correctly. Simplified handling of XR input in InputSystemUIInputModule by having only one set of actions for all XR devices. We now use the same hierarchical device picker in the \"Add Control Scheme\" popup, which is already used in the \"Input Settings\" window. Made all IInputStateTypeInfo implementations internal, as these did not offer value to the user. Made all IInputDeviceCommandInfo implementations internal, as these did not offer value to the user. Removed ReadWriteArray, which was only used for making RebindingOperation.scores editable, which did not add any value. Removed PrimitiveValueOrArray, as non of it's functionality over PrimitiveValue was implemented. Made all InputProcessor implementation internal, as access to these types is exposed only through text mode representations. Removed CurveProcessor as it was not implemented. Renamed XInputControllerOSX to a more descriptive XboxGamepadMacOS. Actions InputAction.continuous has been removed. Running logic every frame regardless of input can easily be achieved in game code. The way action behavior is configured has been simplified. The previous roster of toggles has been replaced with two settings: Action Type: Determines the behavior of the action. Choices are Value, Button, and PassThrough. Control Type: Determines the type of control (and implicitly the type of value) the action is looking for if the action is a Value or PassThrough action. The previous Initial State Check toggle is now implicit in the action type now. Value actions perform an initial state check (i.e. trigger if their control is already actuated when the action is enabled). Other types of actions don't. The previous Pass Through toggle is now rolled into the action type. [0.2.10-preview] - 2019-05-17 Added Added a MultiplayerEventSystem class, which allows you use multiple UI event systems to control different parts of the UI by different players. InputSystemUIInputModule now lets you specify an InputActionAsset in the actionsAsset property. If this is set, the inspector will populate all actions from this asset. If you have a PlayerInput component on the same game object, referencing the same InputActionAsset, the PlayerInput component will keep the actions on the InputSystemUIInputModule in synch, allowing easy setup of multiplayer UI systems. Changed StickControl.x and StickControl.y are now deadzoned, i.e. have AxisDeadzone processors on them. This affects all gamepads and joysticks. NOTE: The deadzoning is independent of the stick. Whereas the stack has a radial deadzones, x and y have linear deadzones. This means that leftStick.ReadValue().x is not necessary equal to leftStick.x.ReadValue(). This change also fixes the problem of noise from sticks not getting filtered out and causing devices such as the PS4 controller to constantly make itself Gamepad.current. Redesigned UIActionInputModule Added a button in the inspector to automatically assign actions from an input action asset based on commonly used action names. Will now populate actions with useful defaults. Removed clickSpeed property - will use native click counts from the OS where available instead. Removed sendEventsWhenInBackground property. Hiding Touches and TrackedDevices until we decide how to handle them. Remove moveDeadzone property as it is made redundant by the action's dead zone. Removed UIActionInputModuleEnabler component, UIActionInputModule will now enable itself. Changed default button press point to 0.5. Changed all constants in public API to match Unity naming conventions (\"Constant\" instead of \"kConstant\"). Changed namespace from UnityEngine.Experimental.Input to UnityEngine.InputSystem. Generated wrapper code now has nicer formatting. Renamed UIActionInputModule to InputSystemUIInputModule. Nicer icons for InputActionAssets and InputActions and for Button and generic controls. Change all public API using IntPtr to use unsafe pointer types instead. PlayerInput will no longer disable any actions not in the currently active action map when disabling input or switching action maps. Change some public fields into properties. Input System project settings are now called \"Input System Package\" in the project window instead of \"Input (NEW)\". Removed Plugins from all namespaces. Rename \"Cancelled\" -> \"Canceled\" (US spelling) in all APIs. Fixed Adding devices to \"Supported Devices\" in input preferences not allowing to select certain device types (like \"Gamepad\"). Fixed scrolling in UIActionInputModule. Fixed compiling the input system package in Unity 19.2 with ugui being moved to a package now. In the Input System project settings window, you can no longer add a supported device twice. Actions Custom inspector for PlayerInput no longer adds duplicates of action events if Invoke Unity Events notification behavior is selected. Fixed Hold interactions firing immediately before the duration has passed. Fixed editing bindings or processors for InputAction fields in the inspector (Changes wouldn't persist before). Fixed exception message when calling CallbackContext.ReadValue<TValue>() for an action with a composite binding with TValue not matching the composite's value type. Added Actions PlayerInput can now handle .inputactions assets that have no control schemes. Will pair all devices mentioned by any of the bindings except if already paired to another player. [0.2.8-preview] - 2019-04-23 Added Added a clickCount control to the Mouse class, which specifies the click count for the last mouse click (to allow distinguishing between single-, double- and multi-clicks). Support for Bluetooth Xbox One controllers on macOS. Actions New API for changing bindings on actions // Several variations exist that allow to look up bindings in various ways. myAction.ChangeBindingWithPath(\"<Gamepad>/buttonSouth\") .WithPath(\"<Keyboard>/space\"); // Can also replace the binding wholesale. myAction.ChangeBindingWithPath(\"<Keyboard>/space\") .To(new InputBinding { ... }); // Can also remove bindings programmatically now. myAction.ChangeBindingWithPath(\"<Keyboard>/space\").Erase(); Changed Joystick.axes and Joystick.buttons have been removed. Generated wrapper code for Input Action Assets are now self-contained, generating all the data from code and not needing a reference to the asset; InputActionAssetReference has been removed. The option to generate interfaces on wrappers has been removed, instead we always do this now. The option to generate events on wrappers has been removed, we felt that this no longer made sense. Will now show default values in Input Action inspector if no custom values for file path, class name or namespace have been provided. InputSettings.runInBackground has been removed. This should now be supported or not on a per-device level. Most devices never supported it in the first place, so a global setting did not seem to be useful. Several new Sensor-based classes have been added. Various existing Android sensor implementations are now based on them. InputControlLayoutAttribute is no longer inherited. Rationale: A class marked as a layout will usually be registered using RegisterLayout. A class derived from it will usually be registered the same way. Because of layout inheritance, properties applied to the base class through InputControlLayoutAttribute will affect the subclass as intended. Not inheriting the attribute itself, however, now allows having properties such as isGenericTypeOfDevice which should not be inherited. Removed acceleration, orientation, and angularVelocity controls from DualShockGamepad base class. They are still on DualShockGamepadPS4. The reason is that ATM we do not yet support these controls other than on the PS4. The previous setup pretended that these controls work when in fact they don't. Marking a control as noisy now also marks all child controls as noisy. The input system now defaults to ignoring any HID devices with usage types not known to map to game controllers. You can use HIDSupport.supportedUsages to enable specific usage types. In the Input Settings window, asset selection has now been moved to the \"gear\" popup menu. If no asset is created, we now automatically create one. In the inspector for Input Settings assets, we now show a button to go to the Input Settings window, and a button to make the asset active if it isn't. Tests are now no longer part of the com.unity.inputsystem package. The InputTestFixture class still is for when you want to write input-related tests for your project. You can reference the Unity.InputSystem.TestFixture assembly when you need to do that. Implemented adding usages to and removing them from devices. Actions A number of changes have been made to the control picker UI in the editor. The button to pick controls interactively (e.g. by pressing a button on a gamepad) has been moved inside the picker and renamed to \"Listen\". It now works as a toggle that puts the picker into a special kind of 'search' mode. While listening, suitable controls that are actuated will be listed in the picker and can then be picked from. Controls are now displayed with their nice names (e.g. \"Cross\" instead of \"buttonSouth\" in the case of the PS4 controller). Child controls are indented instead of listed in \"parent/child\" format. The hierarchy of devices has been rearranged for clarity. The toplevel groups of \"Specific Devices\" and \"Abstract Devices\" are now merged into one hierarchy that progressively groups devices into more specific groups. Controls now have icons displayed for them. There is new support for binding to keys on the keyboard by their generated character rather than by their location. At the toplevel of the Keyboard device, you now have the choice of either binding by keyboard location or binding by generated/mapped character. Binding by location shows differences between the local keyboard layout and the US reference layout. The control path language has been extended to allow referencing controls by display name. <Keyboard>/#(a) binds to the control on a Keyboard with the display name a. continuous flag is now ignored for Press and Release interactions, as it did not make sense. Reacting to controls that are already actuated when an action is enabled is now an optional behavior rather than the default behavior. This is a breaking change. Essentially, this change reverts back to the behavior before 0.2-preview. To reenable the behavior, toggle \"Initial State Check\" on in the UI or set the initialStateCheck property in code. The reason for the change is that having the behavior on by default made certain setups hard to achieve. For example, if <Keyboard>/escape is used in one action map to toggle into the main menu and in another action map to toggle out of it, then the previous behavior would immediately exit out of the menu if escape was still pressed from going into the menu. We have come to believe that wanting to react to the current state of a control right away is the less often desirable behavior and so have made it optional with a separate toggle. Processors and Interactions are now shown in a component-inspector-like fashion in the Input Action editor window, allowing you to see the properties of all items at once. The various InputAction.lastTriggerXXX APIs have been removed. Rationale: They have very limited usefulness and if you need the information, it's easy to set things up in order to keep track of it yourself. Also, we plan on having a polling API for actions in the future which is really what the lastActionXXX APIs were trying to (imperfectly) solve. Tap, SlowTap, and MultiTap interactions now respect button press points. Tap, SlowTap, and MultiTap interactions now have improved parameter editing UIs. Fixed Input Settings configured in the editor are now transferred to the built player correctly. Time slicing for fixed updates now works correctly, even when pausing or dropping frames. Make sure we Disable any InputActionAsset when it is being destroyed. Otherwise, callbacks which were not cleaned up would could cause exceptions. DualShock sensors on PS4 are now marked as noisy (#494). IL2CPP causing issues with XInput on windows and osx desktops. Devices not being available yet in MonoBehavior.Awake, MonoBehaviour.Start, and MonoBehaviour.OnEnable in player or when entering play mode in editor. Fixed a bug where the event buffer used by InputEventTrace could get corrupted. Actions Actions and bindings disappearing when control schemes have spaces in their names. InputActionRebindingExceptions.RebindOperation can now be reused as intended; used to stop working properly the first time a rebind completed or was cancelled. Actions bound to multiple controls now trigger correctly when using PressInteraction set to ReleaseOnly (#492). PlayerInput no longer fails to find actions when using UnityEvents (#500). The \"{...}\" format for referencing action maps and actions using GUIDs as strings has been obsoleted. It will still work but adding the extra braces is no longer necessary. Drag&dropping bindings between other bindings that came before them in the list no longer drops the items at a location one higher up in the list than intended. Editing name of control scheme in editor not taking effect except if hitting enter key. Saving no longer causes the selection of the current processor or interaction to be lost. This was especially annoying when having \"Auto-Save\" on as it made editing parameters on interactions and processors very tedious. In locales that use decimal separators other than '.', floating-point parameters on composites, interactions, and processors no longer lead to invalid serialized data being generated. Fix choosing \"Add Action\" in action map context menu throwing an exception. The input action asset editor window will no longer fail saving if the asset has been moved. The input action asset editor window will now show the name of the asset being edited when asking for saving changes. Clicking \"Cancel\" in the save changes dialog for the input action asset editor window will now cancel quitting the editor. Fixed pasting or dragging a composite binding from one action into another. In the action map editor window, switching from renaming an action to renaming an action map will no longer break the UI. Fixed calling Enable/Disable from within action callbacks sometimes leading to corruption of state which would then lead to actions not getting triggered (#472). Fixed setting of \"Auto-Save\" toggle in action editor getting lost on domain reload. Fixed blurry icons in editor for imported .inputactions assets and actions in them. Press and Release interactions will now work correctly if they have multiple bound controls. Release interactions will now invoke a Started callback when the control is pressed. Made Vector2 composite actions respect the press points of button controls used to compose the value. [0.2.6-preview] - 2019-03-20 NOTE: The UI code for editing actions has largely been rewritten. There may be regressions. NOTE: The minimum version requirement for the new input system has been bumped to 2019.1 Added Support gamepad vibration on Switch. Added support for Joysticks on Linux. Actions Added ability to change which part of a composite a binding that is part of the composite is assigned to. Part bindings can now be freely duplicated or copy-pasted. This allows having multiple bindings for \"up\", for example. Changing part assignments retroactively allows to freely edit the composite makeup. Can now drag&drop multiple items as well as drop items onto others (equivalent to cut&paste). Holding ALT copies data instead of moving it. Edits to control schemes are now undoable. Control schemes are now sorted alphabetically. Can now search by binding group (control scheme) or devices directly from search box. g:Gamepad filters bindings to those in the \"Gamepad\" group. d:Gamepad filters bindings to those from Gamepad-compatible devices. Changed The input debugger will no longer automatically show remote devices when the profiler is connected. Instead, use the new menu in debugger toolbar to connect to players or to enable/disable remote input debugging. \"Press and Release\" interactions will now invoke the performed callback on both press and release (instead of invoking performed and cancel, which was inconsistent with other behaviors). Actions Bindings have GUIDs now like actions and maps already did. This allows to persistently and uniquely identify individual bindings. Replaced UI overlay while rebinding interactively with cancellable progress bar. Interactive rebinding now cancels automatically after 4 seconds without suitable input. Bindings that are not assigned to any control scheme are now visible when a particular control scheme is selected. Bindings not assigned to any control scheme are active in ALL control schemes. The change makes this visible in the UI now. When a specific control scheme is selected, these bindings are affixed with {GLOBAL} for added visibility. When filtering by devices from a control scheme, the filtering now takes layout inheritance into account. So, a binding to a control on Pointer will now be shown when the filter is Mouse. The public control picker API has been revised. The simplest way to add control picker UI to a control path is to add an InputControlAttribute to the field. // In the inspector, shows full UI to select a control interactively // (including interactive picking through device input). [InputControl(layout = \"Button\")] private string buttonControlPath; Processors of incompatible types will now be ignored instead of throwing an exception. Fixed Remote connections in input debugger now remain connected across domain reloads. Don't incorrectly create non-functioning devices if a physical device implements multiple incompatible logical HID devices (such as the MacBook keyboard/touch pad and touch bar). Removed non-functioning sort triangles in event list in Input Debugger device windows. Sort events in input debugger window by id rather then by timestamp. Make parsing of float parameters support floats represented in \"e\"-notation and \"Infinity\". Input device icons in input debugger window now render in appropriate resolution on retina displays. Fixed Xbox Controller on macOS reporting negative values for the sticks when represented as dpad buttons. InputSettings.UpdateMode.ProcessEventsManually now correctly triggers updates when calling InputSystem.Update(InputUpdateType.Manual). Actions Pasting or duplicating an action in an action map asset will now assign a new and unique ID to the action. \"Add Action\" button being active and triggering exceptions when no action map had been added yet. Fixed assert when generating C# class and make sure it gets imported correctly. Generate directories as needed when generating C# class, and allow path names without \"Assets/\" path prefix. Allow binding dpad controls to actions of type \"Vector2\". Fixed old name of action appearing underneath rename overlay. Fixed inspector UIs for on-screen controls throwing exceptions and being non-functional. Fixed deleting multiple items at same time in action editor leading to wrong items being deleted. Fixed copy-pasting actions not preserving action properties other than name. Fixed memory corruptions coming from binding resolution of actions. InputActionAssetReferences in ScriptableObjects will continue to work after domain reloads in the editor. Fixed startTime and duration properties of action callbacks. [0.2.1-preview] - 2019-03-11 Changed NativeUpdateCallback API update to match Unity 2018.3.8f1 [0.2.0-preview] - 2019-02-12 This release contains a number of fairly significant changes. The focus has been on further improving the action system to make it easier to use as well as to make it work more reliably and predictably. NOTE: There are some breaking changes. Please see the \"Changed\" section below. Changed Removed Unity 2018.2 support code. Removed .NET 3.5 support code. Started using C# 7. IInputControlProcessor<TValue> has been replaced with InputProcessor and InputProcessor<TValue> base classes. IInputBindingComposite has been replaced with an InputBindingComposite base class and the IInputBindingComposite<TValue> interface has been merged with the InputBindingComposite<TValue> class which had already existed. InputUser.onUnpairedDeviceUser will now notify for each actuated control until the device is paired or there are no more actuated controls. SensitivityProcessor has been removed. The approach needs rethinking. What SensitivityProcessor did caused more problems than it solved. State monitors no longer have their timeouts removed automatically when they fire. This makes it possible to have a timeout that is removed only in response to a specific state change. Events for devices that implement IInputStateCallbacks (such as Touchscreen) are allowed to go back in time. Avoids the problem of having to order events between multiple fingers correctly or seeing events getting rejected. PenState.Button is now PenButton. Removed TouchPositionTransformProcessor, was used only by Android, the position transformation will occur in native backend in 2019.x Actions: Bindings that have no interactions on them will trigger differently now. This is a breaking change. Previously, these bindings would trigger performed on every value change including when going back to their default value. This is why you would see two calls of performed with a button; one when the button was pressed, another when it was depressed. Now, a binding without an interaction will trigger started and then performed when a bound control is actuated. Thereafter, the action will remain in Started phase. For as long as the control is actuated, every value change will trigger performed again. When the control stops being actuated, it will trigger cancelled and the action will remain in Waiting state. Control actuation is defined as a control having a magnitude (see InputControl.EvaluateMagnitude) greater than zero. If a control does not support magnitudes (returns -1 from EvaluateMagnitude), then the control is considered actuated when it changes state away from its default state. To restore the previous behavior, simply change code like myAction.performed += MyCallback; to myAction.performed += MyCallback; myAction.cancelled += MyCallback; Alternatively, enable passThrough mode on an action. This effectively restores the previous default behavior of actions. new InputAction(binding: \"<Gamepad>/leftTrigger\") { passThrough = true }; As part of the aforementioned change, the following interactions have been removed as they are no longer relevant: StickInteraction: Can simply be removed from bindings. The new default behavior obsoletes the need for what StickInteraction did. Use started to know then the stick starts being actuated, performed to be updated on movements, and cancelled to know when the stick goes back into rest position. PressAndReleaseInteraction: Can simply be removed from bindings. The default behavior with no interaction encompasses press and release detection. Use started to know then a button is pressed and cancelled to know when it is released. To set a custom button press point, simply put an AxisDeadzoneProcessor on the binding. PressInteraction has been completely rewritten. Trigger behavior can be set through behavior parameter and now provides options for observing just presses (PressOnly), just releases (ReleaseOnly), or both presses and releases (PressAndRelease). Also, the interaction now operates on control actuation rather than reading out float values directly. This means that any control that supports magnitudes can be used. Also supports continuous mode now. If bound controls are already actuated when an action is enabled, the action will now trigger in the next input update as if the control had just been moved from non-actuated to actuated state. In other words, if e.g. you have a binding to the A button of the gamepad and the A button is already pressed when the action is first enabled, then the action associated with the A button will trigger as if the button had just been pressed. Previously, it required releasing and re-pressing the button first -- which, together with certain interactions, could lead to actions ending up in a confused state. When an action is disabled, it will now cancel all ongoing interactions, if any (i.e. you will see InputAction.cancelled being called). Note that unlike the above-mentioned callbacks that happen when an action starts out with a control already actuated, the cancellation callbacks happen immediately rather than in the next input update. Actions that at runtime are bound to multiple controls will now perform conflict resolution, if necessary. This applies only if an action actually receives multiple concurrent actuations from controls. When ambiguity is detected, the greatest amount of actuation on any of the controls gets to drive the action. In practice, this means that as long as any of the controls bound to an action is actuated, the action will keep going. This resolves ambiguities when an action has primary and secondary bindings, for examples, or when an action is bound to multiple different devices at the same time. Composite bindings count as single actuations regardless of how many controls participate in the composite. This behavior can be bypassed by setting the action to be pass-through. Action editor now closes when asset is deleted. If there are unsaved changes, asks for confirmation first. Interactions and processors in the UI are now filtered based on the type of the action (if set) and sorted by name. Renamed \"Axis\" and \"Dpad\" composites to \"1D Axis\" and \"2D Vector\" composite. The old names can still be used and existing data will load as expected. DpadComposite got renamed to Vector2Composite; AxisComposite is unchanged. InputInteractionContext.controlHasDefaultValue has been replaced with InputInteractionContext.ControlIsActuated(). InputActionChange.BindingsHaveChangedWhileEnabled has been reworked and split in two: InputActionChange.BoundControlsAboutToChange: Bindings have been previously resolved but are about to be re-resolved. InputActionChange.BoundControlsChanged: Bindings have been resolved on one or more actions. Actions internally now allocate unmanaged memory. Disposing should be taken care of automatically (though you can manually Dispose as well). If you see errors in the console log about unmanaged memory being leaked, please report the bug. All execution state except for C# heap objects for processors, interactions, and composites has been collapsed into a single block of unmanaged memory. Actions should now be able to re-resolve efficiently without allocating additional GC memory. Added PlayerInput component which simplifies setting up individual player input actions and device pairings. PlayerInputManager component which simplifies player joining and split-screen setups. InputDevice.all (equivalent to InputSystem.devices) InputControl.IsActuated() can be used to determine whether control is currently actuated (defined as extension method in InputControlExtensions). Can now read control values from buffers as objects using InputControl.ReadValueFromBufferAsObject. This allows reading a value stored in memory without having to know the value type. New processors: ScaleProcessor ScaleVector2Processor ScaleVector3Processor InvertVector2Processor InvertVector3Processor NormalizeVector2Processor NormalizeVector3Processor Added MultiTapInteraction. Can be used to listen for double-taps and the like. Can get total and average event lag times through InputMetrics.totalEventLagTime and InputMetrics.averageEventLagTime. Mouse.forwardButton and Mouse.backButton. The input debugger now shows users along with their paired devices and actions. See the documentation Added third and fourth barrel buttons on Pen. Actions: Actions have a new continuous mode that will cause the action to trigger continuously even if there is no input. See the documentation for details. Actions have a new pass-through mode. In this mode an action will bypass any checks on control actuation and let any input activity on the action directly flow through. See the documentation for details. Can now add interactions and processors directly to actions. This is functionally equivalent to adding the respective processors and/or interactions to every binding on the action. Can now change the type of a composite retroactively. Values can now be read out as objects using InputAction.CallbackContext.ReadValueAsObject(). Allocates GC memory. Should not be used during normal gameplay but is very useful for testing and debugging. Added auto-save mode for .inputactions editor. Processors, interactions, and composites can now define their own parameter editor UIs by deriving from InputParameterEditor. This solves the problem of these elements not making it clear that the parameters usually have global defaults and do not need to be edited except if local overrides are necessary. Can now set custom min and max values for axis composites. var action = new InputAction(); action.AddCompositeBinding(\"Axis(minValue=0,maxValue=2)\") .With(\"Positive\", \"<Keyboard>/a\") .With(\"Negative\", \"<Keyboard>/d\"); \"C# Class File\" property on .inputactions importer settings now has a file picker next to it. InputActionTrace has seen various improvements. Recorded data will now stay valid even if actions are rebound to different controls. Can listen to all actions using InputActionTrace.SubscribeToAll. InputActionTrace now maintains a list of subscriptions. Add subscriptions with SubscribeTo and remove a subscription with UnsubscribeFrom. See the documentation for details. Fixes Fixed support for Unity 2019.1 where we landed a native API change. InputUser.UnpairDevicesAndRemoveUser() corrupting device pairings of other InputUsers Control picker in UI having no devices if list of supported devices is empty but not null IndexOutOfRangeException when having multiple action maps in an asset (#359 and #358). Interactions timing out even if there was a pending event that would complete the interaction in time. Action editor updates when asset is renamed or moved. Exceptions when removing action in last position of action map. Devices marked as unsupported in input settings getting added back on domain reload. Fixed Pen causing exceptions and asserts. Composites that assign multiple bindings to parts failing to set up properly when parts are assigned out of order (#410). Known Issues Input processing in edit mode on 2019.1 is sporadic rather than happening on every editor update. [0.1.2-preview] - 2018-12-19 NOTE: The minimum version requirement for the new input system has been bumped to 2018.3. The previous minum requirement of 2018.2 is no longer supported. Also, we have dropped support for the .NET 3.5 runtime. The new .NET 4 runtime is now required to use the new input system. We've started working on documentation. The current work-in-progress can be found on GitHub. Changed InputConfiguration has been replaced with a new InputSettings class. InputConfiguration.lockInputToGame has been moved to InputEditorUserSettings.lockInputToGameView. This setting is now persisted as a local user setting. InputSystem.updateMask has been replaced with InputSettings.updateMode. InputSystem.runInBackground has been moved to InputSettings.runInBackground. Icons have been updated for improved styling and now have separate dark and light skin versions. Lock Input To Game and Diagnostics Mode are now persisted as user settings Brought back .current getters and added InputSettings.filterNoiseOnCurrent to control whether noise filtering on the getters is performed or not. Removed old and outdated Doxygen-generated API docs. Added InputSystem.settings contains the current input system settings. A new UI has been added to \"Edit >> Project Settings...\" to edit input system settings. Settings are stored in a user-controlled asset in any location inside Assets/. Multiple assets can be used and switched between. Joystick HIDs are now supported on Windows, Mac, and UWP. Can now put system into manual update mode (InputSettings.updateMode). In this mode, events will not get automatically processed. To process events, call InputSystem.Update(). Added shortcuts to action editor window (requires 2019.1). Added icons for .inputactions assets. Fixed InputSystem.devices not yet being initialized in MonoBehaviour.Start when in editor. Known Issues Input settings are not yet included in player builds. This means that at the moment, player builds will always start out with default input settings. There have been reports of some stickiness to buttons on 2019.1 alpha builds. We are looking at this now. [0.0.14-preview] - 2018-12-11 Changed Pointer.delta no longer has SensitivityProcessor on it. The processor was causing many issues with mouse deltas. It is still available for adding it manually to action bindings but the processor likely needs additional work. Fixed Core: Invalid memory accesses when using .NET 4 runtime Mouse.button not being identical to Mouse.leftButton DualShock not being recognized when connected via Bluetooth Actions: Parameters disappearing on processors and interactions in UI when edited Parameters on processors and interactions having wrong value type in UI (e.g. int instead of float) RebindingOperation calling OnComplete() after being cancelled Misc: Documentation no longer picked up as assets in user project [0.0.13-preview] - 2018-12-05 First release from stable branch."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/ActionAssets.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/ActionAssets.html",
    "title": "Input Action Assets | FSM Unity Framework",
    "keywords": "Input Action Assets Creating Action Assets Editing Action Assets Using Action Assets An Input Action Asset is an Asset which contains Input Actions and their associated Bindings and Control Schemes. These Assets have the .inputactions file extension and are stored in a plain JSON format. Creating Input Action Assets To create an Asset that contains Input Actions in Unity, right-click in the Project window or go to Assets > Create > Input Actions from Unity's main menu. Editing Input Action Assets To bring up the Action editor, double-click an .inputactions Asset in the Project Browser, or select the Edit Asset button in the Inspector for that Asset. You can have more than one editor window open at the same time, but not for the same Asset. The Action editor appears as a separate window, which you can also dock into Unity's main UI. Note: For details about how Action Maps, Actions, and Bindings work, see documentation on Actions. By default, Unity doesn't save edits you make in the Action Asset window when you save the Project. To save your changes, select Save Asset in the window's toolbar. To discard your changes, close the window and choose Don't Save when prompted. Alternatively, you can toggle auto-saving on by enabling the Auto-Save checkbox in the toolbar. This saves any changes to that Asset. Note: This setting affects all .inputactions Assets, and persists across Unity Editor sessions. The Action editor window is divided into three panes: The left pane lists the Action Maps in the Asset. Each Action Map is a collection of Actions that you can enable or disable in bulk. The middle pane contains the Actions in the currently selected Action Map, and the bindings associated with each Action. The right pane contains the properties of the currently selected Action or Binding. Use the following keyboard shortcuts to quickly trigger common operations: Shortcut (Mac) Shortcut (Windows) Description ⌘X, ⌘C, ⌘V Ctrl-X, Ctrl-C, Ctrl-V Cut, Copy, and Paste. Can be used on Actions, Action Maps, and Bindings. ⌘D Ctrl-D Duplicate. Can be used on Actions, Action Maps, and Bindings. ⌘⌫ Del Delete. Can be used on Actions, Action Maps, and Bindings. ⌥S Alt-S Save. ⌥M Alt-M Add Action Map. ⌥A Alt-A Add Action. ⌥B Alt-B Add Binding. Tip: You can search for Devices and/or Control Schemes directly from the search box. For example, \"d:gamepad\" filters for bindings to gamepad Devices, whereas \"g:gamepad\" filters for bindings in the \"gamepad\" Control Scheme. Matching is case-insensitive and matches any partial name. Editing Action Maps To add a new Action Map, select the Add (+) icon in the header of the Action Map column. To rename an existing Action Map, either long-click the name, or right-click the Action Map and select Rename from the context menu. Note that Action Map names can't contain slashes (/). To delete an existing Action Map, either right-click it and select Delete from the context menu, or use the Delete key (Windows) / ⌘⌫ (Mac). To duplicate an existing Action Map, either right-click it and select Duplicate from the context menu, or use Ctrl-D (Windows) / ⌘D (Mac). Editing Actions To add a new Action, select the Add (+) icon in the header of the Action column. To rename an existing Action, either long-click the name, or right-click the Action Map and select Rename from the context menu. To delete an existing Action, either right-click it and select Delete from the context menu, or use the Delete key (Windows) / ⌘⌫ (Mac). To duplicate an existing Action, either right-click it and select Duplicate from the context menu, or use Ctrl-D (Windows) / ⌘D (Mac). If you select an Action, you can edit it's properties in the right-hand pane of the window: Editing Bindings To add a new Binding, select the Add (+) icon on the action you want to add it to, and select the binding type from the menu that appears. To delete an existing Binding, either right-click it and select Delete from the context menu, or use the Delete key (Windows) / ⌘⌫ (Mac). To duplicate an existing Binding, either right-click it and select Duplicate from the context menu, or use Ctrl-D (Windows) / ⌘D (Mac). If you select a Binding, you can edit its properties in the right-hand pane of the window: Picking Controls The most important property of any Binding is the control path it's bound to. To edit it, open the Path drop-down list. This displays a Control picker window. In the Control picker window, you can explore a tree of Input Devices and Controls that the Input System recognizes, and bind to these Controls. Unity filters this list by the Action's Control Type property. For example, if the Control type is Vector2, you can only select a Control that generates two-dimensional values, like a stick. The Device and Control tree is organized hierarchically from generic to specific. For example, the Gamepad Control path <Gamepad>/buttonSouth matches the lower action button on any gamepad. Alternatively, if you navigate to Gamepad > More Specific Gamepads and select PS4 Controller, and then choose the Control path <DualShockGamepad>/buttonSouth, this only matches the \"Cross\" button on PlayStation gamepads, and doesn't match any other gamepads. Instead of browsing the tree to find the Control you want, it's easier to let the Input System listen for input. To do that, select the Listen button. At first, the list of Controls is empty. Once you start pressing buttons or actuating Controls on the Devices you want to bind to, the Control picker window starts listing any Bindings that match the controls you pressed. Select any of these Bindings to view them. Finally, you can choose to manually edit the Binding path, instead of using the Control picker. To do that, select the T button next to the Control path popup. This changes the popup to a text field, where you can enter any Binding string. This also allows you to use wildcard (*) characters in your Bindings. For example, you can use a Binding path such as <Touchscreen>/touch*/press to bind to any finger being pressed on the touchscreen, instead of manually binding to <Touchscreen>/touch0/press, <Touchscreen>/touch1/press and so on. Editing Composite Bindings Composite Bindings are Bindings consisting of multiple parts, which form a Control together. For instance, a 2D Vector Composite uses four buttons (left, right, up, down) to simulate a 2D stick input. See the Composite Bindings documentation to learn more. To create a Composite Binding, in the Input Action Asset editor window, select the Add (+) icon on the Action you want to add it to, and select the Composite Binding type from the popup menu. Important: The set of Composites displayed in the menu is filtered based on the value type of the Action. This means that, for example, if the Action is set to type \"Button\", then only Composites able of returning values of type float will be shown. This creates multiple Binding entries for the Action: one for the Composite as a whole, and then, one level below that, one for each Composite part. The Composite itself doesn't have a Binding path property, but its individual parts do, and you can edit these parts like any other Binding. Once you bind all the Composite's parts, the Composite can work together as if you bound a single control to the Action. To change the type of a Composite retroactively, select the Composite, then select the new type from the Composite Type drop-down in the Properties pane. To can change the part of the Composite to which a particular Binding is assigned, use the Composite Part drop-down in the Binding's properties. Multiple Bindings can be assigned to the same part. You can also duplicate individual part Bindings: right-click the Binding, then select Duplicate to create new part Bindings for the Composite. This can be used, for example, to create a single Composite for both \"WASD\" style controls and arrow keys. Editing Control Schemes Input Action Assets can have multiple Control Schemes, which let you enable or disable different sets of Bindings for your Actions for different types of Devices. To see the Control Schemes in the Input Action Asset editor window, open the Control Scheme drop-down list in the top left of the window. This menu lets you add or remove Control Schemes to your Asset. If the Asset contains any Control Schemes, you can select a Control Scheme, and then the window only shows bindings that belong to that Scheme. If you select a binding, you can now pick the Control Schemes for which this binding should be active in the Properties view to the left of the window. When you add a new Control Scheme, or select an existing Control Scheme, and then select Edit Control Scheme…, you can edit the name of the Control Scheme and which devices the Scheme should be active for. Using Input Action Assets Auto-generating script code for Actions One of the most convenient ways to work with .inputactions Assets in scripts is to automatically generate a C# wrapper class for them. This removes the need to manually look up Actions and Action Maps using their names, and also provides an easier way to set up callbacks. To enable this option, tick the Generate C# Class checkbox in the importer properties in the Inspector of the .inputactions Asset, then select Apply. You can optionally choose a path name, class name, and namespace for the generated script, or keep the default values. This generates a C# script that simplifies working with the Asset. using UnityEngine; using UnityEngine.InputSystem; // IGameplayActions is an interface generated from the \"gameplay\" action map // we added (note that if you called the action map differently, the name of // the interface will be different). This was triggered by the \"Generate Interfaces\" // checkbox. public class MyPlayerScript : MonoBehaviour, IGameplayActions { // MyPlayerControls is the C# class that Unity generated. // It encapsulates the data from the .inputactions asset we created // and automatically looks up all the maps and actions for us. MyPlayerControls controls; public void OnEnable() { if (controls == null) { controls = new MyPlayerControls(); // Tell the \"gameplay\" action map that we want to get told about // when actions get triggered. controls.gameplay.SetCallbacks(this); } controls.gameplay.Enable(); } public void OnDisable() { controls.gameplay.Disable(); } public void OnUse(InputAction.CallbackContext context) { // 'Use' code here. } public void OnMove(InputAction.CallbackContext context) { // 'Move' code here. } } Note: To regenerate the .cs file, right-click the .inputactions asset in the Project Browser and choose \"Reimport\". Using Action Assets with PlayerInput The Player Input component provides a convenient way to handle input for one or multiple players. It requires you to set up all your Actions in an Input Action Asset, which you can then assign to the Player Input component. The Player Input component can then automatically handle activating Action Maps and selecting Control Schemes for you. Modifying Input Action Assets at runtime There are several ways to modify an Input Action Asset at runtime. Any modifications that you make during Play mode to an Input Action Asset do not persist in the Input Action Asset after you exit Play mode. This means you can test your application in a realistic manner in the Editor without having to worry about inadvertently modifying the asset. For examples on how to modify an Input Action Asset, see the documentation on Creating Actions in code and Changing Bindings."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/ActionBindings.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/ActionBindings.html",
    "title": "Input Bindings | FSM Unity Framework",
    "keywords": "Input Bindings Composite Bindings 1D Axis 2D Vector 3D Vector One Modifier Two Modifiers Writing custom Composites Working with Bindings Looking up Bindings Changing Bindings Applying overrides Erasing Bindings Adding Bindings Setting parameters Interactive rebinding Saving and loading rebinds Restoring original Bindings Displaying Bindings Control schemes Details Binding resolution Binding resolution while Actions are enabled Choosing which Devices to use Conflicting inputs Initial state check An InputBinding represents a connection between an Action and one or more Controls identified by a Control path. An Action can have an arbitrary number of Bindings pointed at it. Multiple Bindings can reference the same Control. Each Binding has the following properties: Property Description path Control path that identifies the control(s) from which the Action should receive input. Example: \"<Gamepad>/leftStick\" overridePath Control path that overrides path. Unlike path, overridePath is not persistent, so you can use it to non-destructively override the path on a Binding. If it is set to something other than null, it takes effect and overrides path. To get the path which is currently in effect (that is, either path or overridePath), you can query the effectivePath property. action The name or ID of the Action that the Binding should trigger. Note that this can be null or empty (for instance, for composites). Not case-sensitive. Example: \"fire\" groups A semicolon-separated list of Binding groups that the Binding belongs to. Can be null or empty. Binding groups can be anything, but are mostly used for Control Schemes. Not case-sensitive. Example: \"Keyboard&Mouse;Gamepad\" interactions A semicolon-separated list of Interactions to apply to input on this Binding. Note that Unity appends Interactions applied to the Action itself (if any) to this list. Not case-sensitive. Example: \"slowTap;hold(duration=0.75)\" processors A semicolon-separated list of Processors to apply to input on this Binding. Note that Unity appends Processors applied to the Action itself (if any) to this list. Not case-sensitive. Processors on Bindings apply in addition to Processors on Controls that are providing values. For example, if you put a stickDeadzone Processor on a Binding and then bind it to <Gamepad>/leftStick, you get deadzones applied twice: once from the deadzone Processor sitting on the leftStick Control, and once from the Binding. Example: \"invert;axisDeadzone(min=0.1,max=0.95)\" id Unique ID of the Binding. You can use it to identify the Binding when storing Binding overrides in user settings, for example. name Optional name of the Binding. Identifies part names inside Composites. Example: \"Positive\" isComposite Whether the Binding acts as a Composite. isPartOfComposite Whether the Binding is part of a Composite. To query the Bindings to a particular Action, you can use InputAction.bindings. To query a flat list of Bindings for all Actions in an Action Map, you can use InputActionMap.bindings. Composite Bindings Sometimes, you might want to have several Controls act in unison to mimic a different type of Control. The most common example of this is using the W, A, S, and D keys on the keyboard to form a 2D vector Control equivalent to mouse deltas or gamepad sticks. Another example is to use two keys to form a 1D axis equivalent to a mouse scroll axis. This is difficult to implement with normal Bindings. You can bind a ButtonControl to an action expecting a Vector2, but doing so results in an exception at runtime when the Input System tries to read a Vector2 from a Control that can deliver only a float. Composite Bindings (that is, Bindings that are made up of other Bindings) solve this problem. Composites themselves don't bind directly to Controls; instead, they source values from other Bindings that do, and then synthesize input on the fly from those values. To see how to create Composites in the editor UI, see documentation on editing Composite Bindings. To create composites in code, you can use the AddCompositeBinding syntax. myAction.AddCompositeBinding(\"Axis\") .With(\"Positive\", \"<Gamepad>/rightTrigger\") .With(\"Negative\", \"<Gamepad>/leftTrigger\"); Each Composite consists of one Binding that has InputBinding.isComposite set to true, followed by one or more Bindings that have InputBinding.isPartOfComposiste set to true. In other words, several consecutive entries in InputActionMap.bindings or InputAction.bindings together form a Composite. Note that each composite part can be bound arbitrary many times. // Make both shoulders and triggers pull on the axis. myAction.AddCompositeBinding(\"Axis\") .With(\"Positive\", \"<Gamepad>/rightTrigger\") .With(\"Positive\", \"<Gamepad>/rightShoulder\") .With(\"Negative\", \"<Gamepad>/leftTrigger\"); .With(\"Negative\", \"<Gamepad>/leftShoulder\"); Composites can have parameters, just like Interactions and Processors. myAction.AddCompositeBinding(\"Axis(whichSideWins=1)\"); There are currently five Composite types that come with the system out of the box: 1D-Axis, 2D-Vector, 3D-Vector, One Modifier and Two Modifiers. Additionally, you can add your own types of Composites. 1D axis A Composite made of two buttons: one that pulls a 1D axis in its negative direction, and another that pulls it in its positive direction. Implemented in the AxisComposite class. The result is a float. myAction.AddCompositeBinding(\"1DAxis\") // Or just \"Axis\" .With(\"Positive\", \"<Gamepad>/rightTrigger\") .With(\"Negative\", \"<Gamepad>/leftTrigger\"); The axis Composite has two part bindings. Part Type Description positive Button Controls pulling in the positive direction (towards maxValue). negative Button Controls pulling in the negative direction, (towards minValue). You can set the following parameters on an axis Composite: Parameter Description whichSideWins What happens if both positive and negative are actuated. See table below. minValue The value returned if the negative side is actuated. Default is -1. maxValue The value returned if the positive side is actuated. Default is 1. If Controls from both the positive and the negative side are actuated, then the resulting value of the axis Composite depends on the whichSideWin parameter setting. WhichSideWins Description (0) Neither Neither side has precedence. The Composite returns the midpoint between minValue and maxValue as a result. At their default settings, this is 0. This is the default value for this setting. (1) Positive The positive side has precedence and the Composite returns maxValue. (2) Negative The negative side has precedence and the Composite returns minValue. Note: There is no support yet for interpolating between the positive and negative over time. 2D vector A Composite that represents a 4-way button setup like the D-pad on gamepads. Each button represents a cardinal direction. Implemented in the Vector2Composite class. The result is a Vector2. This Composite is most useful for representing up-down-left-right controls, such as WASD keyboard input. myAction.AddCompositeBinding(\"2DVector\") // Or \"Dpad\" .With(\"Up\", \"<Keyboard>/w\") .With(\"Down\", \"<Keyboard>/s\") .With(\"Left\", \"<Keyboard>/a\") .With(\"Right\", \"<Keyboard>/d\"); // To set mode (2=analog, 1=digital, 0=digitalNormalized): myAction.AddCompositeBinding(\"2DVector(mode=2)\") .With(\"Up\", \"<Gamepad>/leftStick/up\") .With(\"Down\", \"<Gamepad>/leftStick/down\") .With(\"Left\", \"<Gamepad>/leftStick/left\") .With(\"Right\", \"<Gamepad>/leftStick/right\"); The 2D vector Composite has four part Bindings. Part Type Description up Button Controls representing (0,1) (+Y). down Button Controls representing (0,-1) (-Y). left Button Controls representing (-1,0) (-X). right Button Controls representing (1,0) (+X). In addition, you can set the following parameters on a 2D vector Composite: Parameter Description mode Whether to treat the inputs as digital or as analog controls. If this is set to Mode.DigitalNormalized, inputs are treated as buttons (off if below defaultButtonPressPoint and on if equal to or greater). Each input is 0 or 1 depending on whether the button is pressed or not. The vector resulting from the up/down/left/right parts is normalized. The result is a diamond-shaped 2D input range. If this is set to Mode.Digital, the behavior is essentially the same as Mode.DigitalNormalized except that the resulting vector is not normalized. Finally, if this is set to Mode.Analog, inputs are treated as analog (i.e. full floating-point values) and, other than down and left being inverted, values will be passed through as is. The default is Mode.DigitalNormalized. Note: There is no support yet for interpolating between the up/down/left/right over time. 3D vector A Composite that represents a 6-way button where two combinations each control one axis of a 3D vector. Implemented in the Vector3Composite class. The result is a Vector3. myAction.AddCompositeBinding(\"3DVector\") .With(\"Up\", \"<Keyboard>/w\") .With(\"Down\", \"<Keyboard>/s\") .With(\"Left\", \"<Keyboard>/a\") .With(\"Right\", \"<Keyboard>/d\"); // To set mode (2=analog, 1=digital, 0=digitalNormalized): myAction.AddCompositeBinding(\"3DVector(mode=2)\") .With(\"Up\", \"<Gamepad>/leftStick/up\") .With(\"Down\", \"<Gamepad>/leftStick/down\") .With(\"Left\", \"<Gamepad>/leftStick/left\") .With(\"Right\", \"<Gamepad>/leftStick/right\"); The 3D vector Composite has four part Bindings. Part Type Description up Button Controls representing (0,1,0) (+Y). down Button Controls representing (0,-1,0) (-Y). left Button Controls representing (-1,0,0) (-X). right Button Controls representing (1,0,0) (+X). forward Button Controls representing (0,0,1) (+Z). backward Button Controls representing (0,0,-1) (-Z). In addition, you can set the following parameters on a 3D vector Composite: Parameter Description mode Whether to treat the inputs as digital or as analog controls. If this is set to Mode.DigitalNormalized, inputs are treated as buttons (off if below defaultButtonPressPoint and on if equal to or greater). Each input is 0 or 1 depending on whether the button is pressed or not. The vector resulting from the up/down/left/right/forward/backward parts is normalized. If this is set to Mode.Digital, the behavior is essentially the same as Mode.DigitalNormalized except that the resulting vector is not normalized. Finally, if this is set to Mode.Analog, inputs are treated as analog (that is, full floating-point values) and, other than down, left, and backward being inverted, values will be passed through as they are. The default is Analog. One Modifier A Composite that requires the user to hold down a \"modifier\" button in addition to another control from which the actual value of the Binding is determined. This can be used, for example, for Bindings such as \"SHIFT+1\". Implemented in the OneModifierComposite class. The buttons can be on any Device, and can be toggle buttons or full-range buttons such as gamepad triggers. The result is a value of the same type as the controls bound to the binding part. // Add binding for \"CTRL+1\". myAction.AddCompositeBinding(\"OneModifier\") .With(\"Binding\", \"<Keyboard>/1\") .With(\"Modifier\", \"<Keyboard>/ctrl\") // Add binding to mouse delta such that it only takes effect // while the ALT key is down. myAction.AddCompositeBinding(\"OneModifier\") .With(\"Binding\", \"<Mouse>/delta\") .With(\"Modifier\", \"<Keyboard>/alt\"); The button with one modifier Composite has two part Bindings. Part Type Description modifier Button Modifier that has to be held for binding to come through. If the user holds any of the buttons bound to the modifier at the same time as the button that triggers the action, the Composite assumes the value of the modifier Binding. If the user does not press any button bound to the modifier, the Composite remains at default value. binding Any The control(s) whose value the Composite assumes while the user holds down the modifier button. This Composite has no parameters. Two Modifiers A Composite that requires the user to hold down two \"modifier\" buttons in addition to another control from which the actual value of the Binding is determined. This can be used, for example, for Bindings such as \"SHIFT+CTRL+1\". Implemented in the TwoModifiersComposite class. The buttons can be on any Device, and can be toggle buttons or full-range buttons such as gamepad triggers. The result is a value of the same type as the controls bound to the binding part. myAction.AddCompositeBinding(\"TwoModifiers\") .With(\"Button\", \"<Keyboard>/1\") .With(\"Modifier1\", \"<Keyboard>/leftCtrl\") .With(\"Modifier1\", \"<Keyboard>/rightCtrl\") .With(\"Modifier2\", \"<Keyboard>/leftShift\") .With(\"Modifier2\", \"<Keyboard>/rightShift\"); The button with two modifiers Composite has three part Bindings. Part Type Description modifier1 Button The first modifier the user must hold alongside modifier2, for binding to come through. If the user does not press any button bound to the modifier1, the Composite remains at default value. modifier2 Button The second modifier the user must hold alongside modifier1, for binding to come through. If the user does not press any button bound to the modifier2, the Composite remains at default value. binding Any The control(s) whose value the Composite assumes while the user presses both modifier1 and modifier2 at the same time. This Composite has no parameters. Writing custom Composites You can define new types of Composites, and register them with the API. Unity treats these the same as predefined types, which the Input System internally defines and registers in the same way. To define a new type of Composite, create a class based on InputBindingComposite<TValue>. IMPORTANT: Composites must be stateless. This means that you cannot store local state that changes depending on the input being processed. For stateful processing on Bindings, see interactions. // Use InputBindingComposite<TValue> as a base class for a composite that returns // values of type TValue. // NOTE: It is possible to define a composite that returns different kinds of values // but doing so requires deriving directly from InputBindingComposite. #if UNITY_EDITOR [InitializeOnLoad] // Automatically register in editor. #endif // Determine how GetBindingDisplayString() formats the composite by applying // the DisplayStringFormat attribute. [DisplayStringFormat(\"{firstPart}+{secondPart}\")] public class CustomComposite : InputBindingComposite<float> { // Each part binding is represented as a field of type int and annotated with // InputControlAttribute. Setting \"layout\" restricts the controls that // are made available for picking in the UI. // // On creation, the int value is set to an integer identifier for the binding // part. This identifier can read values from InputBindingCompositeContext. // See ReadValue() below. [InputControl(layout = \"Button\")] public int firstPart; [InputControl(layout = \"Button\")] public int secondPart; // Any public field that is not annotated with InputControlAttribute is considered // a parameter of the composite. This can be set graphically in the UI and also // in the data (e.g. \"custom(floatParameter=2.0)\"). public float floatParameter; public bool boolParameter; // This method computes the resulting input value of the composite based // on the input from its part bindings. public override float ReadValue(ref InputBindingCompositeContext context) { var firstPartValue = context.ReadValue<float>(firstPart); var secondPartValue = context.ReadValue<float>(secondPart); //... do some processing and return value } // This method computes the current actuation of the binding as a whole. public override float EvaluateMagnitude(ref InputBindingCompositeContext context) { // Compute normalized [0..1] magnitude value for current actuation level. } static CustomComposite() { // Can give custom name or use default (type name with \"Composite\" clipped off). // Same composite can be registered multiple times with different names to introduce // aliases. // // NOTE: Registering from the static constructor using InitializeOnLoad and // RuntimeInitializeOnLoadMethod is only one way. You can register the // composite from wherever it works best for you. Note, however, that // the registration has to take place before the composite is first used // in a binding. Also, for the composite to show in the editor, it has // to be registered from code that runs in edit mode. InputSystem.RegisterBindingComposite<CustomComposite>(); } [RuntimeInitializeOnLoadMethod(RuntimeInitializeLoadType.BeforeSceneLoad)] static void Init() {} // Trigger static constructor. } The Composite should now appear in the editor UI when you add a Binding, and you can now use it in scripts. myAction.AddCompositeBinding(\"custom(floatParameter=2.0)\") .With(\"firstpart\", \"<Gamepad>/buttonSouth\") .With(\"secondpart\", \"<Gamepad>/buttonNorth\"); To define a custom parameter editor for the Composite, you can derive from InputParameterEditor<TObject>. #if UNITY_EDITOR public class CustomParameterEditor : InputParameterEditor<CustomComposite> { public override void OnGUI() { EditorGUILayout.Label(\"Custom stuff\"); target.floatParameter = EditorGUILayout.FloatField(\"Some Parameter\", target.floatParameter); } } #endif Working with Bindings Looking up Bindings You can retrieve the bindings of an action using its InputAction.bindings property which returns a read-only array of InputBinding structs. // Get bindings of \"fire\" action. var fireBindings = playerInput.actions[\"fire\"].bindings; Also, all the bindings for all actions in an InputActionMap are made available through the InputActionMap.bindings property. The bindings are associated with actions through an action ID or action name stored in the InputBinding.action property. // Get all bindings in \"gameplay\" action map. var gameplayBindings = playerInput.actions.FindActionMap(\"gameplay\").bindings; You can also look up specific the indices of specific bindings in InputAction.bindings using the InputActionRebindingExtensions.GetBindingIndex method. // Find the binding in the \"Keyboard\" control scheme. playerInput.actions[\"fire\"].GetBindingIndex(group: \"Keyboard\"); // Find the first binding to the space key in the \"gameplay\" action map. playerInput.FindActionMap(\"gameplay\").GetBindingIndex( new InputBinding { path = \"<Keyboard>/space\" }); Finally, you can look up the binding that corresponds to a specific control through GetBindingIndexForControl. This way, you can, for example, map a control found in the controls array of an InputAction back to an InputBinding. // Find the binding that binds LMB to \"fire\". If there is no such binding, // bindingIndex will be -1. var fireAction = playerInput.actions[\"fire\"]; var bindingIndex = fireAction.GetBindingIndexForControl(Mouse.current.leftButton); if (binding == -1) Debug.Log(\"Fire is not bound to LMB of the current mouse.\"); Changing Bindings In general, you can change existing bindings via the InputActionSetupExtensions.ChangeBinding method. This returns an accessor that can be used to modify the properties of the targeted InputBinding. Note that most of the write operations of the accessor are destructive. For non-destructive changes to bindings, see Applying Overrides. // Get write access to the second binding of the 'fire' action. var accessor = playerInput.actions['fire'].ChangeBinding(1); // You can also gain access through the InputActionMap. Each // map contains an array of all its bindings (see InputActionMap.bindings). // Here we gain access to the third binding in the map. accessor = playerInput.actions.FindActionMap(\"gameplay\").ChangeBinding(2); You can use the resulting accessor to modify properties through methods such as WithPath or WithProcessors. playerInput.actions[\"fire\"].ChangeBinding(1) // Change path to space key. .WithPath(\"<Keyboard>/space\"); You can also use the accessor to iterate through bindings using PreviousBinding and NextBinding. // Move accessor to previous binding. accessor = accessor.PreviousBinding(); // Move accessor to next binding. accessor = accessor.NextBinding(); If the given binding is a composite, you can address it by its name rather than by index. // Change the 2DVector composite of the \"move\" action. playerInput.actions[\"move\"].ChangeCompositeBinding(\"2DVector\") // playerInput.actions[\"move\"].ChangeBinding(\"WASD\") Applying overrides You can override aspects of any Binding at run-time non-destructively. Specific properties of InputBinding have an override variant that, if set, will take precedent over the property that they shadow. All override properties are of type String. Property Override Description path overridePath Replaces the Control path that determines which Control(s) are referenced in the binding. If overridePath is set to an empty string, the binding is effectively disabled. Example: \"<Gamepad>/leftStick\" processors overrideProcessors Replaces the processors applied to the binding. Example: \"invert,normalize(min=0,max=10)\" interactions overrideInteractions Replaces the interactions applied to the binding. Example: \"tap(duration=0.5)\" NOTE: The override property values will not be saved along with the Actions (for example, when calling InputActionAsset.ToJson()). See Saving and loading rebinds for details about how to persist user rebinds. To set the various override properties, you can use the ApplyBindingOverride APIs. // Rebind the \"fire\" action to the left trigger on the gamepad. playerInput.actions[\"fire\"].ApplyBindingOverride(\"<Gamepad>/leftTrigger\"); In most cases, it is best to locate specific bindings using APIs such as GetBindingIndexForControl and to then apply the override to that specific binding. // Find the \"Jump\" binding for the space key. var jumpAction = playerInput.actions[\"Jump\"]; var bindingIndex = jumpAction.GetBindingIndexForControl(Keyboard.current.spaceKey); // And change it to the enter key. jumpAction.ApplyBindingOverride(bindingIndex, \"<Keyboard>/enter\"); Erasing Bindings You can erase a binding by calling Erase on the binding accessor. // Erase first binding on \"fire\" action. playerInput.actions[\"fire\"].ChangeBinding(0).Erase(); // Erase \"2DVector\" composite. This will also erase the part // bindings of the composite. playerInput.actions[\"move\"].ChangeCompositeBinding(\"2DVector\").Erase(); // Can also do this by using the name given to the composite binding. playerInput.actions[\"move\"].ChangeCompositeBinding(\"WASD\").Erase(); // Erase first binding in \"gameplay\" action map. playerInput.actions.FindActionMap(\"gameplay\").ChangeBinding(0).Erase(); Adding Bindings New bindings can be added to an Action using AddAction or AddCompositeBinding. // Add a binding for the left mouse button to the \"fire\" action. playerInput.actions[\"fire\"].AddBinding(\"<Mouse>/leftButton\"); // Add a WASD composite binding to the \"move\" action. playerInput.actions[\"move\"] .AddCompositeBinding(\"2DVector\") .With(\"Up\", \"<Keyboard>/w\") .With(\"Left\", \"<Keyboard>/a\") .With(\"Down\", \"<Keyboard>/s\") .With(\"Right\", \"<Keyboard>/d\"); Setting parameters A Binding may, either through itself or through its associated Action, lead to processor, interaction, and/or composite objects being created. These objects can have parameters you can configure through in the Binding properties view of the Action editor or through the API. This configuration will give parameters their default value. // Create an action with a \"Hold\" interaction on it. // Set the \"duration\" parameter to 4 seconds. var action = new InputAction(interactions: \"hold(duration=4)\"); You can query the current value of any such parameter using the GetParameterValue API. // This returns a PrimitiveValue?. It will be null if the // parameter is not found. Otherwise, it is a PrimitiveValue // which can be converted to a number or boolean. var p = action.GetParameterValue(\"duration\"); Debug.Log(\"'duration' is set to: \" + p.Value); The above looks for the parameter on any object found on any of the bindings on the action. You can restrict either or both to a more narrow set. // Retrieve the value of the \"duration\" parameter specifically of a // \"Hold\" interaction and only look on bindings in the \"Gamepad\" group. action.GetParameterValue(\"hold:duration\", InputBinding.MaskByGroup(\"Gamepad\")); Alternatively, you can use an expression parameter to encapsulate both the type and the name of the parameter you want to get the value of. This has the advantage of not needing a string parameter but rather references both the type and the name of the parameter in a typesafe way. // Retrieve the value of the \"duration\" parameter of TapInteraction. // This version returns a float? instead of a PrimitiveValue? as it // sees the type of \"duration\" at compile-time. action.GetParameterValue((TapInteraction x) => x.duration); To alter the current value of a parameter, you can use what is referred to as a \"parameter override\". You can apply these at the level of an individual InputAction, or at the level of an entire InputActionMap, or even at the level of an entire InputActionAsset. Such overrides are stored internally and applied automatically even on bindings added later. To add an override, use the ApplyParameterOverride API or any of its overloads. // Set the \"duration\" parameter on all bindings of the action to 4. action.ApplyParameterOverride(\"duration\", 4f); // Set the \"duration\" parameter specifically for \"tap\" interactions only. action.ApplyParameterOverride(\"tap:duration\", 0.5f); // Set the \"duration\" parameter on tap interactions but only for bindings // in the \"Gamepad\" group. action.ApplyParameterOverride(\"tap:duration\", 0.5f, InputBinding.MaskByGroup(\"Gamepad\"); // Set tap duration for all bindings in an action map. map.ApplyParameterOverride(\"tap:duration\", 0.5f); // Set tap duration for all bindings in an entire asset. asset.ApplyParameterOverride(\"tap:duration\", 0.5f); // Like for GetParameterValue, overloads are available that take // an expression instead. action.ApplyParameterOverride((TapInteraction x) => x.duration, 0.4f); map.ApplyParameterOverride((TapInteraction x) => x.duration, 0.4f); asset.ApplyParameterOverride((TapInteraction x) => x.duration, 0.4f); The new value will be applied immediately and affect all composites, processors, and interactions already in use and targeted by the override. Note that if multiple parameter overrides are applied – especially when applying some directly to actions and some to maps or assets –, there may be conflicts between which override to apply. In this case, an attempt is made to chose the \"most specific\" override to apply. // Let's say you have an InputAction `action` that is part of an InputActionAsset asset. var map = action.actionMap; var asset = map.asset; // And you apply a \"tap:duration\" override to the action. action.ApplyParameterOverride(\"tap:duration\", 0.6f); // But also apply a \"tap:duration\" override to the action specifically // for bindings in the \"Gamepad\" group. action.ApplyParameterOverride(\"tap:duration\", 1f, InputBinding.MaskByGroup(\"Gamepad\")); // And finally also apply a \"tap:duration\" override to the entire asset. asset.ApplyParameterOverride(\"tap:duration\", 0.3f); // Now, bindings on `action` in the \"Gamepad\" group will use a value of 1 for tap durations, // other bindings on `action` will use 0.6, and every other binding in the asset will use 0.3. You can use parameter overrides, for example, to scale mouse delta values on a \"Look\" action. // Set up an example \"Look\" action. var look = new InputAction(\"look\", type: InputActionType.Value); look.AddBinding(\"<Mouse>/delta\", groups: \"KeyboardMouse\", processors: \"scaleVector2\"); look.AddBinding(\"<Gamepad>/rightStick\", groups: \"Gamepad\", processors: \"scaleVector2\"); // Now you can adjust stick sensitivity separately from mouse sensitivity. look.ApplyParameterOverride(\"scaleVector2:x\", 0.5f, InputBinding.MaskByGroup(\"KeyboardMouse\")); look.ApplyParameterOverride(\"scaleVector2:y\", 0.5f, InputBinding.MaskByGroup(\"KeyboardMouse\")); look.ApplyParameterOverride(\"scaleVector2:x\", 2f, InputBinding.MaskByGroup(\"Gamepad\")); look.ApplyParameterOverride(\"scaleVector2:y\", 2f, InputBinding.MaskByGroup(\"Gamepad\")); // Alternative to using groups, you can also apply overrides directly to specific binding paths. look.ApplyParameterOverride(\"scaleVector2:x\", 0.5f, new InputBinding(\"<Mouse>/delta\")); look.ApplyParameterOverride(\"scaleVector2:y\", 0.5f, new InputBinding(\"<Mouse>/delta\")); NOTE: Parameter overrides are not persisted along with an asset. Interactive rebinding Note: To download a sample project which demonstrates how to set up a rebinding user interface with Input System APIs, open the Package Manager, select the Input System Package, and choose the sample project \"Rebinding UI\" to download. Runtime rebinding allows users of your application to set their own Bindings. To allow users to choose their own Bindings interactively, use the InputActionRebindingExtensions.RebindingOperation class. Call the PerformInteractiveRebinding() method on an Action to create a rebinding operation. This operation waits for the Input System to register any input from any Device which matches the Action's expected Control type, then uses InputBinding.overridePath to assign the Control path for that Control to the Action's Bindings. If the user actuates multiple Controls, the rebinding operation chooses the Control with the highest magnitude. IMPORTANT: You must dispose of InputActionRebindingExtensions.RebindingOperation instances via Dispose(), so that they don't leak memory on the unmanaged memory heap. void RemapButtonClicked(InputAction actionToRebind) { var rebindOperation = actionToRebind .PerformInteractiveRebinding().Start(); } The InputActionRebindingExtensions.RebindingOperation API is highly configurable to match your needs. For example, you can: Choose expected Control types (WithExpectedControlType()). Exclude certain Controls (WithControlsExcluding()). Set a Control to cancel the operation (WithCancelingThrough()). Choose which Bindings to apply the operation on if the Action has multiple Bindings (WithTargetBinding(), WithBindingGroup(), WithBindingMask()). Refer to the scripting API reference for InputActionRebindingExtensions.RebindingOperation for a full overview. Note that PerformInteractiveRebinding() automatically applies a set of default configurations based on the given action and targeted binding. Saving and loading rebinds You can serialize override properties of Bindings by serializing them as JSON strings and restoring them from these. Use SaveBindingOverridesAsJson to create these strings and LoadBindingOverridesFromJson to restore overrides from them. // Store player rebinds in PlayerPrefs. var rebinds = playerInput.actions.SaveBindingOverridesAsJson(); PlayerPrefs.SetString(\"rebinds\", rebinds); // Restore player rebinds from PlayerPrefs (removes all existing // overrides on the actions; pass `false` for second argument // in case you want to prevent that). var rebinds = PlayerPrefs.GetString(\"rebinds\"); playerInput.actions.LoadBindingOverridesFromJson(rebinds); Restoring original Bindings You can remove Binding overrides and thus restore defaults by using RemoveBindingOverride or RemoveAllBindingOverrides. // Remove binding overrides from the first binding of the \"fire\" action. playerInput.actions[\"fire\"].RemoveBindingOverride(0); // Remove all binding overrides from the \"fire\" action. playerInput.actions[\"fire\"].RemoveAllBindingOverrides(); // Remove all binding overrides from a player's actions. playerInput.actions.RemoveAllBindingOverrides(); Displaying Bindings It can be useful for the user to know what an Action is currently bound to (taking any potentially active rebindings into account) while rebinding UIs, and for on-screen hints while the app is running. You can use InputBinding.effectivePath to get the currently active path for a Binding (which returns overridePath if set, or otherwise returns path). The easiest way to retrieve a display string for an action is to call InputActionRebindingExtensions.GetBindingDisplayString which is an extension method for InputAction. // Get a binding string for the action as a whole. This takes into account which // bindings are currently active and the actual controls bound to the action. m_RebindButton.GetComponentInChildren<Text>().text = action.GetBindingDisplayString(); // Get a binding string for a specific binding on an action by index. m_RebindButton.GetComponentInChildren<Text>().text = action.GetBindingDisplayString(1); // Look up binding indices with GetBindingIndex. var bindingIndex = action.GetBindingIndex(InputBinding.MaskByGroup(\"Gamepad\")); m_RebindButton.GetComponentInChildren<Text>().text = action.GetBindingDisplayString(bindingIndex); You can also use this method to replace the text string with images. // Call GetBindingDisplayString() such that it also returns information about the // name of the device layout and path of the control on the device. This information // is useful for reliably associating imagery with individual controls. // NOTE: The first argument is the index of the binding within InputAction.bindings. var bindingString = action.GetBindingDisplayString(0, out deviceLayout, out controlPath); // If it's a gamepad, look up an icon for the control. Sprite icon = null; if (!string.IsNullOrEmpty(deviceLayout) && !string.IsNullOrEmpty(controlPath) && InputSystem.IsFirstLayoutBasedOnSecond(deviceLayout, \"Gamepad\")) { switch (controlPath) { case \"buttonSouth\": icon = aButtonIcon; break; case \"dpad/up\": icon = dpadUpIcon; break; //... } } // If you have an icon, display it instead of the text. var text = m_RebindButton.GetComponentInChildren<Text>(); var image = m_RebindButton.GetComponentInChildren<Image>(); if (icon != null) { // Display icon. text.gameObject.SetActive(false); image.gameObject.SetActive(true); image.sprite = icon; } else { // Display text. text.gameObject.SetActive(true); image.gameObject.SetActive(false); text.text = bindingString; } Additionally, each Binding has a ToDisplayString method, which you can use to turn individual Bindings into display strings. There is also a generic formatting method for Control paths, InputControlPath.ToHumanReadableString, which you can use with arbitrary Control path strings. Note that the Controls a Binding resolves to can change at any time, and the display strings for controls might change dynamically. For example, if the user switches the currently active keyboard layout, the display string for each individual key on the Keyboard might change. Control Schemes A Binding can belong to any number of Binding groups. Unity stores these on the InputBinding class as a semicolon-separated string in the InputBinding.groups property, and you can use them for any arbitrary grouping of bindings. To enable different sets of binding groups for an InputActionMap or InputActionAsset, you can use the InputActionMap.bindingMask/InputActionAsset.bindingMask property. The Input System uses this to implement the concept of grouping Bindings into different InputControlSchemes. Control Schemes use Binding groups to map Bindings in an InputActionMap or InputActionAsset to different types of Devices. The PlayerInput class uses these to enable a matching Control Scheme for a new user joining the game, based on the Device they are playing on. Details Binding resolution When the Input System accesses the Controls bound to an Action for the first time, the Action resolves its Bindings to match them to existing Controls on existing Devices. In this process, the Action calls InputSystem.FindControls<>() (filtering for devices assigned to the InputActionMap, if there are any) for the Binding path of each of the Action's bindings. This creates a list of resolved Controls that are now bound to the Action. Note that a single Binding path can match multiple Controls: A specific Device path such as <DualShockGamepad>/buttonEast matches the \"Circle\" button on a PlayStation controller. If you have multiple PlayStation controllers connected, it resolves to the \"Circle\" button on each of these controllers. An abstract Device path such as <Gamepad>/buttonEast matches the right action button on any connected gamepad. If you have a PlayStation controller and an Xbox controller connected, it resolves to the \"Circle\" button on the PlayStation controller, and to the \"B\" button on the Xbox controller. A Binding path can also contain wildcards, such as <Gamepad>/button*. This matches any Control on any gamepad with a name starting with \"button\", which matches all the four action buttons on any connected gamepad. A different example: */{Submit} matches any Control tagged with the \"Submit\" usage on any Device. If there are multiple Bindings on the same Action that all reference the same Control(s), the Control will effectively feed into the Action multiple times. This is to allow, for example, a single Control to produce different input on the same Action by virtue of being bound in a different fashion (composites, processors, interactions, etc). However, regardless of how many times a Control is bound on any given action, it will only be mentioned once in the Action's array of controls. To query the Controls that an Action resolves to, you can use InputAction.controls. You can also run this query if the Action is disabled. To be notified when binding resolution happens, you can listen to InputSystem.onActionChange which triggers InputActionChange.BoundControlsAboutToChange before modifying Control lists and triggers InputActionChange.BoundControlsChanged after having updated them. Binding resolution while Actions are enabled In certain situations, the Controls bound to an Action have to be updated more than once. For example, if a new Device becomes usable with an Action, the Action may now pick up input from additional controls. Also, if Bindings are added, removed, or modified, Control lists will need to be updated. This updating of Controls usually happens transparently in the background. However, when an Action is enabled and especially when it is in progress, there may be a noticeable effect on the Action. Adding or removing a device – either globally or to/from the device list of an Action – will remain transparent except if an Action is in progress and it is the device of its active Control that is being removed. In this case, the Action will automatically be cancelled. Modifying the binding mask or modifying any of the Bindings (such as through rebinding or by adding or removing bindings) will, however, lead to all enabled Actions being temporarily disabled and then re-enabled and resumed. Choosing which Devices to use Note: InputUser and PlayerInput make use of this facility automatically. They set InputActionMap.devices automatically based on the Devices that are paired to the user. By default, Actions resolve their Bindings against all Devices present in the Input System (that is, InputSystem.devices). For example, if there are two gamepads present in the system, a Binding to <Gamepad>/buttonSouth picks up both gamepads and allows the Action to be used from either. You can override this behavior by restricting InputActionAssets or individual InputActionMaps to a specific set of Devices. If you do this, Binding resolution only takes the Controls of the given Devices into account. var actionMap = new InputActionMap(); // Restrict the action map to just the first gamepad. actionMap.devices = new[] { Gamepad.all[0] }; Conflicting inputs There are two situations where a given input may lead to ambiguity: Several Controls are bound to the same Action and more than one is feeding input into the Action at the same time. Example: an Action that is bound to both the left and right trigger on a Gamepad and both triggers are pressed. The input is part of a sequence of inputs and there are several possible such sequences. Example: one Action is bound to the B key and another Action is bound to Shift-B. Multiple, concurrently used Controls Note: This section does not apply to PassThrough Actions as they are by design meant to allow multiple concurrent inputs. For a Button or Value Action, there can only be one Control at any time that is \"driving\" the Action. This Control is considered the activeControl. When an Action is bound to multiple Controls, the activeControl at any point is the one with the greatest level of \"actuation\", that is, the largest value returned from EvaluateMagnitude. If a Control exceeds the actuation level of the current activeControl, it will itself become the active Control. The following example demonstrates this mechanism with a Button Action and also demonstrates the difference to a PassThrough Action. // Create a button and a pass-through action and bind each of them // to both triggers on the gamepad. var buttonAction = new InputAction(type: InputActionType.Button, binding: \"<Gamepad>/*Trigger\"); var passThroughAction = new InputAction(type: InputActionType.PassThrough, binding: \"<Gamepad>/*Trigger\"); buttonAction.performed += c => Debug.Log(\"${c.control.name} pressed (Button)\"); passThroughAction.performed += c => Debug.Log(\"${c.control.name} changed (Pass-Through)\"); buttonAction.Enable(); passThroughAction.Enable(); // Press the left trigger all the way down. // This will trigger both buttonAction and passThroughAction. Both will // see leftTrigger becoming the activeControl. Set(gamepad.leftTrigger, 1f); // Will log // \"leftTrigger pressed (Button)\" and // \"leftTrigger changed (Pass-Through)\" // Press the right trigger halfway down. // This will *not* trigger or otherwise change buttonAction as the right trigger // is actuated *less* than the left one that is already driving action. // However, passThrough action is not performing such tracking and will thus respond // directly to the value change. It will perform and make rightTrigger its activeControl. Set(gamepad.rightTrigger, 0.5f); // Will log // \"rightTrigger changed (Pass-Through)\" // Release the left trigger. // For buttonAction, this will mean that now all controls feeding into the action have // been released and thus the button releases. activeControl will go back to null. // For passThrough action, this is just another value change. So, the action performs // and its active control changes to leftTrigger. Set(gamepad.leftTrigger, 0f); // Will log // \"leftTrigger changed (Pass-Through)\" For composite bindings, magnitudes of the composite as a whole rather than for individual Controls are tracked. However, activeControl will stick track individual Controls from the composite. Disabling Conflict Resolution Conflict resolution is always applied to Button and Value type Actions. However, it can be undesirable in situations when an Action is simply used to gather any and all inputs from bound Controls. For example, the following Action would monitor the A button of all available gamepads: var action = new InputAction(type: InputActionType.PassThrough, binding: \"<Gamepad>/buttonSouth\"); action.Enable(); By using the Pass-Through Action type, conflict resolution is bypassed and thus, pressing the A button on one gamepad will not result in a press on a different gamepad being ignored. Multiple input sequences (such as keyboard shortcuts) Note: The mechanism described here only applies to Actions that are part of the same InputActionMap or InputActionAsset. Inputs that are used in combinations with other inputs may also lead to ambiguities. If, for example, the b key on the Keyboard is bound both on its own as well as in combination with the shift key, then if you first press shift and then b, the latter key press would be a valid input for either of the Actions. The way this is handled is that Bindings will be processed in the order of decreasing \"complexity\". This metric is derived automatically from the Binding: A binding that is not part of a composite is assigned a complexity of 1. A binding that is part of a composite is assigned a complexity equal to the number of part bindings in the composite. In our example, this means that a OneModifier composite Binding to Shift+B has a higher \"complexity\" than a Binding to B and thus is processed first. Additionally, the first Binding that results in the Action changing phase will \"consume\" the input. This consuming will result in other Bindings to the same input not being processed. So in our example, when Shift+B \"consumes\" the B input, the Binding to B will be skipped. The following example illustrates how this works at the API level. // Create two actions in the same map. var map = new InputActionMap(); var bAction = map.AddAction(\"B\"); var shiftbAction = map.AddAction(\"ShiftB\"); // Bind one of the actions to 'B' and the other to 'SHIFT+B'. bAction.AddBinding(\"<Keyboard>/b\"); shiftbAction.AddCompositeBinding(\"OneModifier\") .With(\"Modifier\", \"<Keyboard>/shift\") .With(\"Binding\", \"<Keyboard>/b\"); // Print something to the console when the actions are triggered. bAction.performed += _ => Debug.Log(\"B action performed\"); shiftbAction.performed += _ => Debug.Log(\"SHIFT+B action performed\"); // Start listening to input. map.Enable(); // Now, let's assume the left shift key on the keyboard is pressed (here, we manually // press it with the InputTestFixture API). Press(Keyboard.current.leftShiftKey); // And then the B is pressed. This is a valid input for both // bAction as well as shiftbAction. // // What will happen now is that shiftbAction will do its processing first. In response, // it will *perform* the action (i.e. we see the `performed` callback being invoked) and // thus \"consume\" the input. bAction will stay silent as it will in turn be skipped over. Press(keyboard.bKey); Initial state check After an Action is enabled, it will start reacting to input as it comes in. However, at the time the Action is enabled, one or more of the Controls that are bound to an action may already have a non-default state at that point. Using what is referred to as an \"initial state check\", an Action can be made to respond to such a non-default state as if the state change happened after the Action was enabled. The way this works is that in the first input update after the Action was enabled, all its bound controls are checked in turn. If any of them has a non-default state, the Action responds right away. This check is implicitly enabled for Value actions. If, for example, you have a Move Action bound to the left stick on the gamepad and the stick is already pushed in a direction when Move is enabled, the character will immediately start walking. By default, Button and Pass-Through type Actions, do not perform this check. A button that is pressed when its respective Action is enabled first needs to be released and then pressed again for it to trigger the Action. However, you can manually enable initial state checks on these types of Actions using the checkbox in the editor:"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Actions.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Actions.html",
    "title": "Actions | FSM Unity Framework",
    "keywords": "Actions Actions Overview Creating Actions Creating Actions using the Action editor Creating Actions by embedding them in MonoBehaviours Loading Actions from JSON Creating Actions in code Default Actions Using Actions Responding to Actions Action callbacks InputActionMap.actionTriggered callback InputSystem.onActionChange callback Polling Actions InputActionTrace Action types Value Button Pass-Through Debugging Actions Using Actions with multiple players Related pages: Assets Bindings Interactions Actions allow you to separate the logical meaning of an input (the things your user can do in your game or app, such as move, jump, crouch) and the device-specific controls (for example, pressing a button or moving a gamepad stick). Without Actions, the meaning of your input and the device controls end up hard-coded together in your scripts, which although quick to implement, is not very flexible and therefore not always desirable. For example, here the moveVector variable is hard-coded to read values from the right stick of a gamepad: // without Actions, you end up hard-coding device controls // like this: (not always desirable) moveVector = gamepad.rightStick.ReadValue<Vector2>(); When you use Actions, you do not need to refer to specific devices or their controls in your code. Instead the Action's binding defines which controls on which device is used to perform the action, and your code becomes simpler. In this example, moveAction would contain a reference to an Action, defined either in code, or in an Action Asset: // using Actions removes the need to refer directly to device controls moveAmount = moveAction.ReadValue<Vector2>(); You can then use the visual editor (either in the inspector, or in the Actions Asset editor) to establish the mapping between the Action and one or more device controls. For example in this screenshot, the \"move\" action is bound to the left gamepad stick, and the keyboard's arrow keys. This also makes it easier to let players customize bindings at runtime. !Note Actions are a game-time only feature. You can't use them in EditorWindow code. For an overview of the terms and terminology used on this page, see Concepts. Overview There are three key classes for Actions in the API: Class Description InputActionAsset An Asset that contains one or more Action Maps and, optionally, a sequence of Control Schemes. For more information on how to create, edit, and work with these Assets, see Action Assets. InputActionMap A named collection of Actions. InputAction A named Action that triggers callbacks in response to input. Actions use InputBinding to refer to the inputs they collect. For more information about Bindings and how to use them, see Action Bindings. Each Action has a name (InputAction.name), which must be unique within the Action Map that the Action belongs to, if any (see InputAction.actionMap). Each Action also has a unique ID (InputAction.id), which you can use to reference the Action. The ID remains the same even if you rename the Action. Each Action Map has a name (InputActionMap.name), which must be unique within the Action Asset that the Action Map belongs to, if any (see InputActionMap.asset). Each Action Map also has a unique ID (InputActionMap.id), which you can use to reference the Action Map. The ID remains the same even if you rename the Action Map. Creating Actions You can create Actions in any of the following ways: Use the dedicated editor for Input Action Assets. Embed them in MonoBehaviour components, then set up bindings in the Inspector. Manually load them from JSON. Create them entirely in code, including setting up the bindings. Creating Actions using the Action editor For information on how to create and edit Input Action Assets in the dedicated editor, see Action Assets. This is the recommended workflow if you want to organise all your input actions and bindings together into a single Asset, which is often the case for many types of game or app. Creating Actions by embedding them in MonoBehaviours As an alternative to using an Action Asset, You can embed individual Input Action and Input Action Maps as fields directly inside MonoBehaviour components, like this: using UnityEngine; using UnityEngine.InputSystem; public class ExampleScript : MonoBehaviour { public InputAction move; public InputAction jump; } The result is similar to using an Action Asset, except the Actions are defined in the GameObject's properties and saved as Scene or Prefab data, instead of in a dedicated Asset. When you embed actions in a MonoBehaviour and assign that MonoBehaviour to a GameObject, the GameObject's Inspector window displays an interface similar to the Actions Asset window, which allows you to set up the bindings for those actions. For example: The visual editors work similarly to the Action Asset editor. To add or remove Actions or Bindings, click the Add (+) or Remove (-) icon in the header. To edit Bindings, double-click them. To edit Actions, double-click them in an Action Map, or click the gear icon on individual Action properties. You can also right-click entries to bring up a context menu, and you can drag them. Hold the Alt key and drag an entry to duplicate it. You must manually enable and disable Actions and Action Maps that are embedded in MonoBehaviour components. public class MyBehavior : MonoBehaviour { // ... void Awake() { fireAction.performed += OnFire; lookAction.performed += OnLook; gameplayActions[\"fire\"].performed += OnFire; } void OnEnable() { fireAction.Enable(); lookAction.Enable(); gameplayActions.Enable(); } void OnDisable() { fireAction.Disable(); lookAction.Disable(); gameplayActions.Disable(); } } Loading Actions from JSON You can load Actions as JSON in the form of a set of Action Maps or as a full InputActionAsset. This also works at runtime in the Player. // Load a set of action maps from JSON. var maps = InputActionMap.FromJson(json); // Load an entire InputActionAsset from JSON. var asset = InputActionAsset.FromJson(json); Creating Actions in code You can manually create and configure Actions entirely in code, including assigning the bindings. This also works at runtime in the Player. For example: // Create free-standing Actions. var lookAction = new InputAction(\"look\", binding: \"<Gamepad>/leftStick\"); var moveAction = new InputAction(\"move\", binding: \"<Gamepad>/rightStick\"); lookAction.AddBinding(\"<Mouse>/delta\"); moveAction.AddCompositeBinding(\"Dpad\") .With(\"Up\", \"<Keyboard>/w\") .With(\"Down\", \"<Keyboard>/s\") .With(\"Left\", \"<Keyboard>/a\") .With(\"Right\", \"<Keyboard>/d\"); // Create an Action Map with Actions. var map = new InputActionMap(\"Gameplay\"); var lookAction = map.AddAction(\"look\"); lookAction.AddBinding(\"<Gamepad>/leftStick\"); // Create an Action Asset. var asset = ScriptableObject.CreateInstance<InputActionAsset>(); var gameplayMap = new InputActionMap(\"gameplay\"); asset.AddActionMap(gameplayMap); var lookAction = gameplayMap.AddAction(\"look\", \"<Gamepad>/leftStick\"); Any action that you create in this way during Play mode do not persist in the Input Action Asset after you exit Play mode. This means you can test your application in a realistic manner in the Editor without having to worry about inadvertently modifying the asset. Default Actions An asset called DefaultInputActions.inputactions containing a default setup of Actions comes with the Input System Package. You can reference this asset directly in your projects like any other Unity asset. However, the asset is also available in code form through the DefaultInputActions class. void Start() { // Create an instance of the default actions. var actions = new DefaultInputActions(); actions.Player.Look.performed += OnLook; actions.Player.Move.performed += OnMove; actions.Enable(); } Using Actions For an Action to do something, you must first enable it. You can do this either by individually enabling Actions, or by enabling them in bulk through Action Maps. The second method is more efficient in all scenarios. // Enable a single action. lookAction.Enable(); // Enable an en entire action map. gameplayActions.Enable(); When you enable an Action, the Input System resolves its bindings, unless it has done so already, or if the set of devices that the Action can use has not changed. For more details about this process, see the documentation on binding resolution. You can't change certain aspects of the configuration, such Action Bindings, while an Action is enabled. To stop Actions or Action Maps from responding to input, call Disable. While enabled, an Action actively monitors the Control(s) it's bound to. If a bound Control changes state, the Action processes the change. If the Control's change represents an Interaction change, the Action creates a response. All of this happens during the Input System update logic. Depending on the update mode selected in the input settings, this happens once every frame, once every fixed update, or manually if updates are set to manual. Responding to Actions An Action doesn't represent an actual response to input by itself. Instead, an Action informs your code that a certain type of input has occurred. Your code then responds to this information. There are several ways to do this: Each Action has a started, performed, and canceled callback. Each Action Map has an actionTriggered callback. The Input System has a global InputSystem.onActionChange callback. You can poll the current state of an Action whenever you need it. InputActionTrace can record changes happening on Actions. There are also two higher-level, more streamlined ways of picking up input from Actions: use PlayerInput, or generate script code that wraps around the Input Actions. Action callbacks Every Action has a set of distinct phases it can go through in response to receiving input. Phase Description Disabled The Action is disabled and can't receive input. Waiting The Action is enabled and is actively waiting for input. Started The Input System has received input that started an Interaction with the Action. Performed An Interaction with the Action has been completed. Canceled An Interaction with the Action has been canceled. You can read the current phase of an action using InputAction.phase. The Started, Performed, and Canceled phases each have a callback associated with them: var action = new InputAction(); action.started += ctx => /* Action was started */; action.performed += ctx => /* Action was performed */; action.canceled += ctx => /* Action was canceled */; Each callback receives an InputAction.CallbackContext structure, which holds context information that you can use to query the current state of the Action and to read out values from Controls that triggered the Action (InputAction.CallbackContext.ReadValue). Note: The contents of the structure are only valid for the duration of the callback. In particular, it isn't safe to store the received context and later access its properties from outside the callback. When and how the callbacks are triggered depends on the Interactions present on the respective Bindings. If the Bindings have no Interactions that apply to them, the default Interaction applies. InputActionMap.actionTriggered callback Instead of listening to individual actions, you can listen on an entire Action Map for state changes on any of the Actions in the Action Map. var actionMap = new InputActionMap(); actionMap.AddAction(\"action1\", \"<Gamepad>/buttonSouth\"); actionMap.AddAction(\"action2\", \"<Gamepad>/buttonNorth\"); actionMap.actionTriggered += context => { ... }; The argument received is the same InputAction.CallbackContext structure that you receive through the started, performed, and canceled callbacks. Note: The Input System calls InputActionMap.actionTriggered for all three of the individual callbacks on Actions. That is, you get started, performed, and canceled all on a single callback. InputSystem.onActionChange callback Similar to InputSystem.onDeviceChange, your app can listen for any action-related change globally. InputSystem.onActionChange += (obj, change) => { // obj can be either an InputAction or an InputActionMap // depending on the specific change. switch (change) { case InputActionChange.ActionStarted: case InputActionChange.ActionPerformed: case InputActionChange.ActionCanceled: Debug.Log($\"{((InputAction)obj).name} {change}\"); break; } } Polling Actions Instead of using callbacks, it might be simpler sometimes to poll the value of an Action where you need it in your code. You can poll the current value of an Action using InputAction.ReadValue<>(): public InputAction moveAction; public float moveSpeed = 10.0f; public Vector2 position; void Start() { moveAction.Enable(); } void Update() { var moveDirection = moveAction.ReadValue<Vector2>(); position += moveDirection * moveSpeed * Time.deltaTime; } Note that the value type has to correspond to the value type of the control that the value is being read from. To determine whether an action was performed in the current frame, you can use InputAction.WasPerformedThisFrame(): private InputAction action; void Start() { // Set up an action that triggers when the A button is // held for 1 second. action = new InputAction( type: InputActionType.Button, binding: \"<Gamepad>/buttonSouth\", interactions: \"hold(duration=1)\"); action.Enable(); } void Update() { if (action.WasPerformedThisFrame()) Debug.Log(\"A button on gamepad was held for one second\"); } Finally, there are three methods you can use to poll for button presses and releases: Method Description InputAction.IsPressed() True if the level of actuation on the action has crossed the press point and did not yet fall to or below the release threshold. InputAction.WasPressedThisFrame() True if the level of actuation on the action has, at any point during the current frame, reached or gone above the press point. InputAction.WasReleasedThisFrame() True if the level of actuation on the action has, at any point during the current frame, gone from being at or above the press point to at or below the release threshold. Example: public PlayerInput playerInput; public void Update() { // IsPressed if (playerInput.actions[\"up\"].IsPressed()) transform.Translate(0, 10 * Time.deltaTime, 0); // WasPressedThisFrame if (playerInput.actions[\"teleport\"].WasPressedThisFrame()) Teleport(); // WasReleasedThisFrame if (playerInput.actions[\"submit\"].WasReleasedThisFrame()) ConfirmSelection(); } InputActionTrace You can trace Actions to generate a log of all activity that happened on a particular set of Actions. To do so, use InputActionTrace. This behaves in a similar way to InputEventTrace for events. Note: InputActionTrace allocates unmanaged memory and needs to be disposed of so that it doesn't create memory leaks. var trace = new InputActionTrace(); // Subscribe trace to single Action. // (Use UnsubscribeFrom to unsubscribe) trace.SubscribeTo(myAction); // Subscribe trace to entire Action Map. // (Use UnsubscribeFrom to unsubscribe) trace.SubscribeTo(myActionMap); // Subscribe trace to all Actions in the system. trace.SubscribeToAll(); // Record a single triggering of an Action. myAction.performed += ctx => { if (ctx.ReadValue<float>() > 0.5f) trace.RecordAction(ctx); }; // Output trace to console. Debug.Log(string.Join(\",\\n\", trace)); // Walk through all recorded Actions and then clear trace. foreach (var record in trace) { Debug.Log($\"{record.action} was {record.phase} by control {record.control}\"); // To read out the value, you either have to know the value type or read the // value out as a generic byte buffer. Here, we assume that the value type is // float. Debug.Log(\"Value: \" + record.ReadValue<float>()); // If it's okay to accept a GC hit, you can also read out values as objects. // In this case, you don't have to know the value type. Debug.Log(\"Value: \" + record.ReadValueAsObject()); } trace.Clear(); // Unsubscribe trace from everything. trace.UnsubscribeFromAll(); // Release memory held by trace. trace.Dispose(); Once recorded, a trace can be safely read from multiple threads as long as it is not concurrently being written to and as long as the Action setup (that is, the configuration data accessed by the trace) is not concurrently being changed on the main thread. Action types Each Action can be one of three different Action types. You can select the Action type in the Input Action editor window, or by specifying the type parameter when calling the InputAction() constructor. The Action type influences how the Input System processes state changes for the Action. The default Action type is Value. Value This is the default Action type. Use this for any inputs which should track continuous changes to the state of a Control. Value type actions continuously monitor all the Controls which are bound to the Action, and then choose the one which is the most actuated to be the Control driving the Action, and report the values from that Control in callbacks, triggered whenever the value changes. If a different bound Control actuated more, then that Control becomes the Control driving the Action, and the Action starts reporting values from that Control. This process is called conflict resolution. This is useful if you want to allow different Controls to control an Action in the game, but only take input from one Control at the same time. When the Action initially enables, it performs an initial state check of all bound Controls. If any of them is actuated, the Action then triggers a callback with the current value. Button This is very similar to Value, but Button type Actions can only be bound to ButtonControl Controls, and don't perform an initial state check like Value Actions do (see the Value section above). Use this for inputs that trigger an Action once every time they are pressed. The initial state check is usually not useful in such cases, because it can trigger actions if the button is still held down from a previous press when the Action was enabled. Pass-Through Pass-Through Actions bypass the conflict resolution process described above for Value Actions and don't use the concept of a specific Control driving the Action. Instead, any change to any bound Control triggers a callback with that Control's value. This is useful if you want to process all input from a set of Controls. Debugging Actions To see currently enabled Actions and their bound Controls, use the Input Debugger. You can also use the InputActionVisualizer component from the Visualizers sample to get an on-screen visualization of an Action's value and Interaction state in real-time. Using Actions with multiple players You can use the same Action definitions for multiple local players (for example, in a local co-op game). For more information, see documentation on the Player Input Manager component."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Architecture.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Architecture.html",
    "title": "Architecture | FSM Unity Framework",
    "keywords": "Architecture The Input System has a layered architecture that consists of a low-level layer and a high-level layer. Native backend The foundation of the Input System is the native backend code. This is platform-specific code which collects information about available Devices and input data from Devices. This code is not part of the Input System package, but is included with Unity itself. It has implementations for each runtime platform supported by Unity. This is why some platform-specific input bugs can only be fixed by an update to Unity, rather than a new version of the Input System package. The Input System interfaces with the native backend using events that the native backend sends. These events notify the system of the creation and removal of Input Devices, as well as any updates to the Device states. For efficiency and to avoid creating any garbage, the native backend reports these events as a simple buffer of raw, unmanaged memory containing a stream of events. The Input System can also send data back to the native backend in the form of commands sent to Devices, which are also buffers of memory that the native backend interprets. These commands can have different meanings for different Device types and platforms. Input System (low-level) The low-level Input System code processes and interprets the memory from the event stream that the native backend provides, and dispatches individual events. The Input System creates Device representations for any newly discovered Device in the event stream. The low-level code sees a Device as a block of raw, unmanaged memory. If it receives a state event for a Device, it writes the data from the state event into the Device's state representation in memory, so that the state always contains an up-to-date representation of the Device and all its Controls. The low-level system code also contains structs which describe the data layout of commonly known Devices. Input System (high-level) The high-level Input System code interprets the data in a Device's state buffers by using layouts, which describe the data layout of a Device and its Controls in memory. The Input System creates layouts from either the pre-defined structs of commonly known Devices supplied by the low level system, or dynamically at runtime, as in the case of generic HIDs. Based on the information in the layouts, the Input System then creates Control representations for each of the Device's controls, which let you read the state of each individual Control in a Device. As part of the high-level system, you can also build another abstraction layer to map Input Controls to your application mechanics. Use Actions to bind one or more Controls to an input in your application. The Input System then monitors these Controls for state changes, and notifies your game logic using callbacks. You can also specify more complex behaviors for your Actions using Processors (which perform processing on the input data before sending it to you) and Interactions (which let you specify patterns of input on a Control to listen to, such as multi-taps)."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Concepts.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Concepts.html",
    "title": "Input System Concepts | FSM Unity Framework",
    "keywords": "Input System Concepts This page introduces the concepts that relate to working with the Input System. When you become familiar with these concepts, you will be able to understand the difference between the workflows available within the Input System, and choose which workflow best suits your project. Basic concepts These basic concepts and terms refer to the steps in the sequence of events that occur when a user sends input to your game or app. The Input System provides features which implement these steps, or you can choose to implement some of them yourself. Concept Description User The person playing your game or using your app, by holding or touching the input device and providing input. Input Device Often referred to just as a \"device\" within the context of input. A physical piece of hardware, such as a keyboard, gamepad, mouse, or touchscreen which allows the user to send input into Unity. Controls The separate individual parts of an input device which each send input values into Unity. For example, a gamepad’s controls comprise multiple buttons, sticks and triggers, and a mouse’s controls include the two X and Y sensors on the underside, and the various buttons and scroll wheels on the top side. Interactions These describe different ways of using the controls on a device. For example, pressing a button down, releasing a button, a long press, or a double tap. Interactions can be thought of as \"patterns of input\". The Input System provides ways of identifying and responding to different types of interaction. Actions These are things a user can do in your game or app as a result of input, regardless of what device or control they use to perform it. Actions generally have conceptual names that you choose to suit your project, and should usually be verbs. For example \"Run\", \"Jump\" \"Crouch\", \"Use\", \"Start\", \"Quit\". The Input System can help you manage and edit your actions, or you can implement them yourself. Action Asset An asset type which allows you to define and configure groups of actions as a set. The Action Asset UI allows you to bind controls, group related actions into Action Maps, and specify which controls belong to different Control Schemes. Embedded Actions Actions defined directly as fields in your scripts (as opposed to in an Action Asset). These types of action are the same as those defined in an Action Asset, and their inspector UI allows you to bind controls. However, because they’re defined as individual fields in your script, you do not benefit from the Action Asset’s ability to group Actions together into Action Maps and Control Schemes. Binding A connection defined between an Action and one or more Controls. For example, in a car racing game, pressing the right shoulder button on a controller might be bound to the action \"Change Gear Up\". The Action Asset and Embedded Actions both provide a similar UI to create and edit bindings. Extended concepts These concepts relate to more advanced handling of input, and aren't necessary to understand straight away if you are implementing simple input, or learning about the system for the first time. Concept Description Processor An operation that the Input System applies to an input value. For example, an \"invert\" Processor inverts a floating-point value. Phase The current state of an Interaction, for example, \"Started\", \"Performed\", or \"Canceled\". Control Scheme Allows you to define mappings of Bindings to different Control Schemes, and to switch your Action Maps between different Control Schemes to enable different subsets of Bindings for your Actions. Control Schemes can have associated Device types, so that the game can automatically enable them for users when using that type of Device. Action Map A named collection of Actions. You can simultaneously enable or disable all Actions in an action map, so it is useful to group Actions in Action Maps by the context in which they are relevant (for example: \"gameplay\")."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Contributing.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Contributing.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing The full source code for the Input System is available on GitHub. This is also where most of the Input System's development happens. Note: This includes the full source code for the managed/C# part of the system. At this point, the native, platform-specific C++ backends are still closed-source and require a source code license. Reporting bugs To report documentation problems, please use the feedback section at the bottom of the page containing the problem. To report bugs related to the Input System Please follow Unity's standard bug reporting guidelines. Don't forget to submit a Project that the developer who picks up your report can use to reproduce the issue. Be sure to mention that the bug is specific to the Input System package in the description, so it gets forwarded to the correct team at Unity. Discussion To ask questions or discuss the Input System, see the dedicated section on Unity's forum. This is also the best place to post feature requests."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Controls.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Controls.html",
    "title": "Controls | FSM Unity Framework",
    "keywords": "Controls Hierarchies Types Usages Paths State Actuation Noisy Controls Synthetic Controls Performance Optimization An Input Control represents a source of values. These values can be of any structured or primitive type. The only requirement is that the type is blittable. Note: Controls are for input only. Output and configuration items on Input Devices are not represented as Controls. Each Control is identified by a name (InputControl.name) and can optionally have a display name (InputControl.displayName) that differs from the Control name. For example, the right-hand face button closest to the touchpad on a PlayStation DualShock 4 controller has the control name \"buttonWest\" and the display name \"Square\". Additionally, a Control might have one or more aliases which provide alternative names for the Control. You can access the aliases for a specific Control through its InputControl.aliases property. Finally, a Control might also have a short display name which can be accessed through the InputControl.shortDisplayName property. For example, the short display name for the left mouse button is \"LMB\". Control hierarchies Controls can form hierarchies. The root of a Control hierarchy is always a Device. The setup of hierarchies is exclusively controlled through layouts. You can access the parent of a Control using InputControl.parent, and its children using InputControl.children. To access the flattened hierarchy of all Controls on a Device, use InputDevice.allControls. Control types All controls are based on the InputControl base class. Most concrete implementations are based on InputControl<TValue>. The Input System provides the following types of controls out of the box: Control Type Description Example AxisControl A 1D floating-point axis. Gamepad.leftStick.x ButtonControl A button expressed as a floating-point value. Whether the button can have a value other than 0 or 1 depends on the underlying representation. For example, gamepad trigger buttons can have values other than 0 and 1, but gamepad face buttons generally can't. Mouse.leftButton KeyControl A specialized button that represents a key on a Keyboard. Keys have an associated keyCode and, unlike other types of Controls, change their display name in accordance to the currently active system-wide keyboard layout. See the Keyboard documentation for details. Keyboard.aKey Vector2Control A 2D floating-point vector. Pointer.position Vector3Control A 3D floating-point vector. Accelerometer.acceleration QuaternionControl A 3D rotation. AttitudeSensor.attitude IntegerControl An integer value. Touchscreen.primaryTouch.touchId StickControl A 2D stick control like the thumbsticks on gamepads or the stick control of a joystick. Gamepad.rightStick DpadControl A 4-way button control like the D-pad on gamepads or hatswitches on joysticks. Gamepad.dpad TouchControl A control that represents all the properties of a touch on a touch screen. Touchscreen.primaryTouch You can browse the set of all registered control layouts in the input debugger. Control usages A Control can have one or more associated usages. A usage is a string that denotes the Control's intended use. An example of a Control usage is Submit, which labels a Control that is commonly used to confirm a selection in the UI. On a gamepad, this usage is commonly found on the buttonSouth Control. You can access a Control's usages using the InputControl.usages property. Usages can be arbitrary strings. However, a certain set of usages is very commonly used and comes predefined in the API in the form of the CommonUsages static class. Check out the CommonUsages scripting API page for an overview. Control paths Example: <Gamepad>/leftStick/x means \"X Control on left stick of gamepad\". The Input System can look up Controls using textual paths. Bindings on Input Actions rely on this feature to identify the Control(s) they read input from. However, you can also use them for lookup directly on Controls and Devices, or to let the Input System search for Controls among all devices using InputSystem.FindControls. var gamepad = Gamepad.all[0]; var leftStickX = gamepad[\"leftStick/x\"]; var submitButton = gamepad[\"{Submit}\"]; var allSubmitButtons = InputSystem.FindControls(\"*/{Submit}\"); Control paths resemble file system paths. Each path consists of one or more components separated by a forward slash: component/component... Each component uses a similar syntax made up of multiple fields. Each field is optional, but at least one field must be present. All fields are case-insensitive. <layoutName>{usageName}controlName#(displayName) The following table explains the use of each field: Field Description Example <layoutName> Requires the Control at the current level to be based on the given layout. The actual layout of the Control may be the same or a layout based on the given layout. <Gamepad>/buttonSouth {usageName} Works differently for Controls and Devices. When used on a Device (the first component of a path), it requires the device to have the given usage. See Device usages for more details. For looking up a Control, the usage field is currently restricted to the path component immediately following the Device (the second component in the path). It finds the Control on the Device that has the given usage. The Control can be anywhere in the Control hierarchy of the Device. Device: <XRController>{LeftHand}/trigger Control: <Gamepad>/{Submit} controlName Requires the Control at the current level to have the given name. Takes both \"proper\" names (InputControl.name) and aliases (InputControl.aliases) into account. This field can also be a wildcard (*) to match any name. MyGamepad/buttonSouth */{PrimaryAction} (match PrimaryAction usage on Devices with any name) #(displayName) Requires the Control at the current level to have the given display name (i.e. InputControl.displayName). The display name may contain whitespace and symbols. <Keyboard>/#(a) (matches the key that generates the \"a\" character, if any, according to the current keyboard layout). <Gamepad>/#(Cross) You can access the literal path of a given control via its InputControl.path property. If needed, you can manually parse a control path into its components using the InputControlPath.Parse(path) API. var parsed = InputControlPath.Parse(\"<XRController>{LeftHand}/trigger\").ToArray(); Debug.Log(parsed.Length); // Prints 2. Debug.Log(parsed[0].layout); // Prints \"XRController\". Debug.Log(parsed[0].name); // Prints an empty string. Debug.Log(parsed[0].usages.First()); // Prints \"LeftHand\". Debug.Log(parsed[1].layout); // Prints null. Debug.Log(parsed[1].name); // Prints \"trigger\". Control state Each Control is connected to a block of memory that is considered the Control's \"state\". You can query the size, format, and location of this block of memory from a Control through the InputControl.stateBlock property. The state of Controls is stored in unmanaged memory that the Input System handles internally. All Devices added to the system share one block of unmanaged memory that contains the state of all the Controls on the Devices. A Control's state might not be stored in the natural format for that Control. For example, the system often represents buttons as bitfields, and axis controls as 8-bit or 16-bit integer values. This format is determined by the combination of platform, hardware, and drivers. Each Control knows the format of its storage and how to translate the values as needed. The Input System uses layouts to understand this representation. You can access the current state of a Control through its ReadValue method. Gamepad.current.leftStick.x.ReadValue(); Each type of Control has a specific type of values that it returns, regardless of how many different types of formats it supports for its state. You can access this value type through the InputControl.valueType property. Reading a value from a Control might apply one or more value Processors. See documentation on Processors for more information. Recording state history You might want to access the history of value changes on a Control (for example, in order to compute exit velocity on a touch release). To record state changes over time, you can use InputStateHistory or InputStateHistory<TValue>. The latter restricts Controls to those of a specific value type, which in turn simplifies some of the API. // Create history that records Vector2 control value changes. // NOTE: You can also pass controls directly or use paths that match multiple // controls (e.g. \"<Gamepad>/<Button>\"). // NOTE: The unconstrained InputStateHistory class can record changes on controls // of different value types. var history = new InputStateHistory<Vector2>(\"<Touchscreen>/primaryTouch/position\"); // To start recording state changes of the controls to which the history // is attached, call StartRecording. history.StartRecording(); // To stop recording state changes, call StopRecording. history.StopRecording(); // Recorded history can be accessed like an array. for (var i = 0; i < history.Count; ++i) { // Each recorded value provides information about which control changed // value (in cases state from multiple controls is recorded concurrently // by the same InputStateHistory) and when it did so. var time = history[i].time; var control = history[i].control; var value = history[i].ReadValue(); } // Recorded history can also be iterated over. foreach (var record in history) Debug.Log(record.ReadValue()); Debug.Log(string.Join(\",\\n\", history)); // You can also record state changes manually, which allows // storing arbitrary histories in InputStateHistory. // NOTE: This records a value change that didn't actually happen on the control. history.RecordStateChange(Touchscreen.current.primaryTouch.position, new Vector2(0.123f, 0.234f)); // State histories allocate unmanaged memory and need to be disposed. history.Dispose(); For example, if you want to have the last 100 samples of the left stick on the gamepad available, you can use this code: var history = new InputStateHistory<Vector2>(Gamepad.current.leftStick); history.historyDepth = 100; history.StartRecording(); Control actuation A Control is considered actuated when it has moved away from its default state in such a way that it affects the actual value of the Control. You can query whether a Control is currently actuated using IsActuated. // Check if leftStick is currently actuated. if (Gamepad.current.leftStick.IsActuated()) Debug.Log(\"Left Stick is actuated\"); It can be useful to determine not just whether a Control is actuated at all, but also the amount by which it is actuated (that is, its magnitude). For example, for a Vector2Control this would be the length of the vector, whereas for a button it is the raw, absolute floating-point value. In general, the current magnitude of a Control is always >= 0. However, a Control might not have a meaningful magnitude, in which case it returns -1. Any negative value should be considered an invalid magnitude. You can query the current amount of actuation using EvaluateMagnitude. // Check if left stick is actuated more than a quarter of its motion range. if (Gamepad.current.leftStick.EvaluateMagnitude() > 0.25f) Debug.Log(\"Left Stick actuated past 25%\"); There are two mechanisms that most notably make use of Control actuation: Interactive rebinding (InputActionRebindingExceptions.RebindOperation) uses it to select between multiple suitable Controls to find the one that is actuated the most. Conflict resolution between multiple Controls that are bound to the same action uses it to decide which Control gets to drive the action. Noisy Controls The Input System can label a Control as \"noisy\". You can query this using the InputControl.noisy property. Noisy Controls are those that can change value without any actual or intentional user interaction required. A good example of this is a gravity sensor in a cellphone. Even if the cellphone is perfectly still, there are usually fluctuations in gravity readings. Another example are orientation readings from an HMD. If a Control is marked as noisy, it means that: The Control is not considered for interactive rebinding. InputActionRebindingExceptions.RebindingOperation ignores the Control by default (you can bypass this using WithoutIgnoringNoisyControls). If enabled in the Project Settings, the system performs additional event filtering, then calls InputDevice.MakeCurrent. If an input event for a Device contains no state change on a Control that is not marked noisy, then the Device will not be made current based on the event. This avoids, for example, a plugged in PS4 controller constantly making itself the current gamepad (Gamepad.current) due to its sensors constantly feeding data into the system. When the application loses focus and Devices are reset as a result, the state of noisy Controls will be preserved as is. This ensures that sensor readinds will remain at their last value rather than being reset to default values. Note: If any Control on a Device is noisy, the Device itself is flagged as noisy. Parallel to the input state and the default state that the Input System keeps for all Devices currently present, it also maintains a noise mask in which only bits for state that is not noise are set. This can be used to very efficiently mask out noise in input. Synthetic Controls A synthetic Control is a Control that doesn't correspond to an actual physical control on a device (for example the left, right, up, and down child Controls on a StickControl). These Controls synthesize input from other, actual physical Controls and present it in a different way (in this example, they allow you to treat the individual directions of a stick as buttons). Whether a given Control is synthetic is indicated by its InputControl.synthetic property. The system considers synthetic Controls for interactive rebinding but always favors non-synthetic Controls. If both a synthetic and a non-synthetic Control that are a potential match exist, the non-synthetic Control wins by default. This makes it possible to interactively bind to <Gamepad>/leftStick/left, for example, but also makes it possible to bind to <Gamepad>/leftStickPress without getting interference from the synthetic buttons on the stick. Performance Optimization Avoiding defensive copies Use InputControl<T>.value instead of InputControl<T>.ReadValue to avoid creating a copy of the control state on every call, as the former returns the value as ref readonly while the latter always makes a copy. Note that this optimization only applies if the call site assigns the return value to a variable that has been declared 'ref readonly'. Otherwise a copy will be made as before. Additionally, be aware of defensive copies that can be allocated by the compiler when it is unable to determine that it can safely use the readonly reference i.e. if it can't determine that the reference won't be changed, it will create a defensive copy for you. For more details, see https://learn.microsoft.com/en-us/dotnet/csharp/write-safe-efficient-code#use-ref-readonly-return-statements. Control Value Caching When the 'USE_READ_VALUE_CACHING' internal feature flag is set, the Input System will switch to an optimized path for reading control values. This path efficiently marks controls as 'stale' when they have been actuated and subsequent calls to InputControl<T>.ReadValue will only apply control processing when absolutely necessary. Control processing in this case can mean any hard-coded processing that might exist on the control, such as with AxisControl which has built-in inversion, normalisation, scaling etc, or any processors that have been applied to the controls' processor stack. This can have a significant positive impact on performance, especially when using complex composite input actions with many composite parts, such as a movement input action that could be bound to W, A, S, and D on the keyboard, two gamepad sticks and a DPad. This feature is not enabled by default as it can result in the following minor behavioural changes: Some control processors use global state. Without cached value optimizations, it is possible to read the control value, change the global state, read the control value again, and get a new value due to the fact that the control processor runs on every call. With cached value optimizations, reading the control value will only ever return a new value if the physical control has been actuated. Changing the global state of a control processor will have no effect otherwise. Writing to device state using low-level APIs like InputControl<T>.WriteValueIntoState does not set the stale flag and subsequent calls to InputControl<T>.value will not reflect those changes. After changing properties on AxisControl the ApplyParameterChanges has to be called to invalidate cached value. Processors that need to run on every read can set their respective caching policy to EvaluateOnEveryRead. That will disable caching on controls that are using such processor. If there are any non-obvious inconsistencies, 'PARANOID_READ_VALUE_CACHING_CHECKS' internal feature flag can be enabled to compare cached and uncached value on every read and log an error if they don't match. Optimized control read value When the 'USE_OPTIMIZED_CONTROLS' internal feature flag is set, the Input System will use faster way to use state memory for some controls instances. Most controls are flexible with regards to memory representation, like AxisControl can be one bit, multiple bits, a float, etc, or in Vector2Control where x and y can have different memory representation. Yet for most controls there are common memory representation patterns, for example AxisControl are floats or single bytes, or some Vector2Control are two consequitive floats in memory. If a control is matching a common representation we can bypass reading children control and cast memory directly to the common representation. For example if Vector2Control is two consequitive floats in memory we can bypass reading x and y separately and just cast whole state memory to Vector2, this only works if x and y don't need any processing applied to them. Optimized controls compute a potential memory representation in InputControl.CalculateOptimizedControlDataType(), store it InputControl.optimizedControlDataType and then inside ReadUnprocessedValueFromState used it to decide to cast memory directly instead of reading every children control on it's own to reconstruct the controls state. InputControl.ApplyParameterChanges() should be called after changes to ensure InputControl.optimizedControlDataType is updated to the correct value when configuration changes after InputControl.FinishSetup() was called, like value of AxisControl.invert flips or other cases."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Debugging.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Debugging.html",
    "title": "Debugging | FSM Unity Framework",
    "keywords": "Debugging Debugging Input Debugger Debugging Devices Debugging Actions Debugging users and PlayerInput Debugging layouts Debugging remotely Input visualizers InputControlVisualizer InputActionVisualizer Device Simulator Unity Remote (iOS, Android) Other tips: When something isn't working as expected, the quickest way to troubleshoot what's wrong is the Input Debugger in the Unity Editor. The Input Debugger provides access to the activity of the Input System in both the Editor and the connected Players. To open the Input Debugger, go to Window > Analysis > Input Debugger from Unity's main menu. Input Debugger The Input Debugger displays a tree breakdown of the state of the Input System. Item Description Devices A list of all Input Devices that are currently in the system, and a list of unsupported/unrecognized Devices. Layouts A list of all registered Control and Device layouts. This is the database of supported hardware, and information on how to represent a given piece of input hardware. Actions Only visible in Play mode, and only if at least one Action is enabled. A list of all currently enabled Actions, and the Controls they are bound to. See Debugging Actions. Users Only visible when one or more InputUser instances exist. See documentation on user management. A list of all currently active users, along with their active Control Schemes and Devices, all their associated Actions, and the Controls these Actions are bound to. Note that PlayerInput uses InputUser to run. When using PlayerInput components, each player has an entry in this list. See Debugging users and PlayerInput. Settings The currently active Input System settings. Metrics Statistics about Input System resource usage. Debugging Devices In the Input Debugger window, navigate to the Devices list and double-click any Input Device. This opens a window that displays information about the Device, including real-time state information for its Controls. The top of the Device window displays general information about the specific Device, such as name, manufacturer, and serial number. The Controls section lists the Device's Controls and their individual states. This is useful when debugging input issues, because you can verify whether the data that the Input System receives from the Input Device is what you expect it to be. There are two buttons at the top of this panel: HID Descriptor: Only displayed for devices that use the HID protocol to connect. This opens a window that displays the detailed HID specifications for the Device and each of it's logical controls. State: Display the current state of the Device in a new window. This is identical to the information displayed in this view, but doesn't update in real time, so you can take a snapshot of input state data and take the time to inspect it as needed. The Events section lists all input events generated by the Device. You can double-click any event in the list to inspect the full Device state at the time the event occurred. To get a side-by-side difference between the state of the Device at different points in time, select multiple events, right-click them, and click Compare from the context menu. Debugging Actions The Input Debugger window lists all enabled Actions in the Actions list. This list only appears if at least one Action is active and the Editor is in Play mode. If an Action has actively bound Controls, you can click the arrow next to the Action to see a list of the Controls. This is useful to debug whether your Bindings correctly map to the Controls you want them to bind to. See documentation on Binding resolution for more information about how Unity maps Bindings to Controls. Note: Actions that belong to InputUsers don't appear here. They appear in the Users list instead. Debugging users and PlayerInput When there are InputUser instances (if you use PlayerInput, each PlayerInput instance implicitly creates one), the Input Debugger's Users list displays each instance along with its paired Devices and active Actions. The listed Devices and Actions work the same way as those displayed in the Devices and Actions lists in the debugging window. Debugging layouts The Layouts list in the Input Debugger window displays a breakdown of all registered Control and Device layouts. This is the database of supported hardware and the knowledge of how to represent a given piece of input hardware. It's useful when you want to create a new Device mapping and see how the Input System represents it. Debugging remotely You can connect the Input Debugger to a Player that runs on a remote computer or device. This makes it possible to observe input activity from the Player in the Editor. This connection uses the PlayerConnection mechanism, which is the same one the Unity profiler uses to connect to a Player. Note: At the moment, debugging input in Players is restricted to seeing Devices and events from connected Players. There is no support for seeing other input-related data such as Actions and input users from Players. To see remote Devices from built Players, open the Input Debugger window's Remote Devices drop-down list. This list displays the remote Player instance you can connect to (if there are any). The same list appears in the Profiler and Console windows, and any connections are shared between those windows. If any Player(s) are connected, you can enable Show remote devices in the same drop-down list. If Players are connected, and Show remote devices is enabled, the Devices list in the Input Debugger window splits into a Local section and a Remote section. The Remote section displays any Input Device from any connected Player, and lets you inspect Device state and events in real time, as if it were a local Device. Input visualizers The Input System package comes with a Visualizers sample, which provides various components which let you monitor the state of various Input System elements in real time using on-screen visualizers. To install the sample, navigate to the Input System package in the Package Manager window (see Installation), and next to the Visualizers sample, click Import in project. The sample provides two visualizer components: InputControlVisualizer Visualizes the current state of a single Control in real time. You can have multiple Control visualizers to visualize the state of multiple Controls. Check the GamepadVisualizer, MouseVisualizer, or PenVisualizer Scenes in the sample for examples. InputActionVisualizer Visualizes the current state of a single Action in real time. You can have multiple Action visualizers to visualize the state of multiple Actions. This can also display the current value of the Action and the Control currently driving the Action, and track the state of Interactions over time. Check the SimpleControlsVisualizer Scene in the sample for examples. Device Simulator When Device Simulator window is in use, mouse and pen inputs on the simulated device screen are turned into touchscreen inputs. Device Simulator uses its own touchscreen device, which it creates and destroys together with the Device Simulator window. To prevent conflicts between simulated touchscreen inputs and native mouse and pen inputs, Device Simulator disables all native mouse and pen devices. Unity Remote The Unity Remote is an app available for iOS and Android which allows using a mobile device for input while running in the Unity Editor. You can find details about the app and how to install it in the Unity manual. If you would like to try out the Unity Remote app, you can install the \"Unity Remote\" sample that is provided with the Input System package. Note: Joysticks/gamepads are not yet supported over the Unity Remote. No joystick/gamepad input from the mobile device will come through in the editor. Note: This requires Unity 2021.2.18 or later. When in play mode in the Editor and connected to the Unity Remote app, you will see a number of Devices have been added with the InputDevice.remote flag set to true: Touchscreen Accelerometer If a gyro is present on the mobile device: Gyroscope AttitudeSensor LinearAccelerationSensor GravitySensor These Devices can be used just like local Devices. They will receive input from the connected mobile device which in turn will receive the rendered output of the game running in the editor. The Accelerometer device will automatically be enabled and will not need you to call InputSystem.EnableDevice explicitly. Setting the sampling frequency on the accelerometer from the Unity Remote using Sensor.samplingFrequency has no effect. The remaining sensors listed above will need to be explicitly enabled via InputSystem.EnableDevice just like local sensors. Setting the sampling frequency on these sensors from the Unity Remote using Sensor.samplingFrequency will be relayed to the device but note that setting the frequency on one of them will set it for all of them. Touch coordinates from the device will be translated to the screen coordinates of the Game View inside the Editor. Other tips: To record events flowing through the system, use this code: // You can also provide a device ID to only // trace events for a specific device. var trace = new InputEventTrace(); trace.Enable(); var current = new InputEventPtr(); while (trace.GetNextEvent(ref current)) { Debug.Log(\"Got some event: \" + current); } // Also supports IEnumerable. foreach (var eventPtr in trace) Debug.Log(\"Got some event: \" + eventPtr); // Trace consumes unmanaged resources. Make sure you dispose it correctly to avoid memory leaks. trace.Dispose(); To see events as they're processed, use this code: InputSystem.onEvent += (eventPtr, device) => { // Can handle events yourself, for example, and then stop them // from further processing by marking them as handled. eventPtr.handled = true; };"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Devices.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Devices.html",
    "title": "Devices | FSM Unity Framework",
    "keywords": "Devices Devices Device descriptions Capabilities Matching Hijacking the matching process Device lifecycle Device creation Device removal Device resets Device syncs Device enabling and disabling Background and focus change behavior Domain reloads in the Editor Native Devices Disconnected Devices Device IDs Device usages Device commands Sending commands to Devices Adding custom device Commands Device state State changes Monitoring state changes Synthesizing state Working with Devices Monitoring Devices Adding and removing Devices Creating custom Devices Step 1: The state struct Step 2: The Device class Step 3: The Update method Step 4: Device registration and creation Step 5: current and all (optional) Step 6: Device Commands (Optional) Physically, Input Devices represent devices attached to the computer, which a user can use to control the app. Logically, Input Devices are the top-level container for Controls. The InputDevice class is itself a specialization of InputControl. See supported Devices to see what kind of Devices the Input System currently supports. To query the set of all currently present Devices, you can use InputSystem.devices. Device descriptions An InputDeviceDescription describes a Device. The Input System uses this primarily during the Device discovery process. When a new Device is reported (by the runtime or by the user), the report contains a Device description. Based on the description, the system then attempts to find a Device layout that matches the description. This process is based on Device matchers. After a Device has been created, you can retrieve the description it was created from through the InputDevice.description property. Every description has a set of standard fields: Field Description interfaceName Identifier for the interface/API that is making the Device available. In many cases, this corresponds to the name of the platform, but there are several more specific interfaces that are commonly used: HID, RawInput, XInput. This field is required. deviceClass A broad categorization of the Device. For example, \"Gamepad\" or \"Keyboard\". product Name of the product as reported by the Device/driver itself. manufacturer Name of the manufacturer as reported by the Device/driver itself. version If available, provides the version of the driver or hardware for the Device. serial If available, provides the serial number for the Device. capabilities A string in JSON format that describes Device/interface-specific capabilities. See the section on capabilities. Capabilities Aside from a number of standardized fields, such as product and manufacturer, a Device description can contain a capabilities string in JSON format. This string describes characteristics which help the Input System to interpret the data from a Device, and map it to Control representations. Not all Device interfaces report Device capabilities. Examples of interface-specific Device capabilities are HID descriptors. WebGL, Android, and Linux use similar mechanisms to report available Controls on connected gamepads. Matching InputDeviceMatcher instances handle matching an InputDeviceDescription to a registered layout. Each matcher loosely functions as a kind of regular expression. Each field in the description can be independently matched with either a plain string or regular expression. Matching is not case-sensitive. For a matcher to apply, all of its individual expressions have to match. To matchers to any layout, call InputSystem.RegisterLayoutMatcher. You can also supply them when you register a layout. // Register a new layout and supply a matcher for it. InputSystem.RegisterLayoutMatcher<MyDevice>( matches: new InputDeviceMatcher() .WithInterface(\"HID\") .WithProduct(\"MyDevice.*\") .WithManufacturer(\"MyBrand\"); // Register an alternate matcher for an already registered layout. InputSystem.RegisterLayoutMatcher<MyDevice>( new InputDeviceMatcher() .WithInterface(\"HID\") If multiple matchers are matching the same InputDeviceDescription, the Input System chooses the matcher that has the larger number of properties to match against. Hijacking the matching process You can overrule the internal matching process from outside to select a different layout for a Device than the system would normally choose. This also makes it possible to quickly build new layouts. To do this, add a custom handler to the InputSystem.onFindControlLayoutForDevice event. If your handler returns a non-null layout string, then the Input System uses this layout. Device lifecycle Device creation Once the system has chosen a layout for a device, it instantiates an InputDevice and populates it with InputControls as the layout dictates. This process is internal and happens automatically. Note: You can't create valid InputDevices and InputControls by manually instantiating them with new. To guide the creation process, you must use layouts. After the Input System assembles the InputDevice, it calls FinishSetup on each control of the device and on the device itself. Use this to finalize the setup of the Controls. After an InputDevice is fully assembled, the Input System adds it to the system. As part of this process, the Input System calls MakeCurrent on the Device, and signals InputDeviceChange.Added on InputSystem.onDeviceChange. The Input System also calls InputDevice.OnAdded. Once added, the InputDevice.added flag is set to true. To add devices manually, you can call one of the InputSystem.AddDevice methods such as InputSystem.AddDevice(layout). // Add a gamepad. This bypasses the matching process and creates a device directly // with the Gamepad layout. InputSystem.AddDevice<Gamepad>(); // Add a device such that the matching process is employed: InputSystem.AddDevice(new InputDeviceDescription { interfaceName = \"XInput\", product = \"Xbox Controller\", }); When a device is added, the Input System automatically issues a sync request on the device. This instructs the device to send an event representing its current state. Whether this request succeeds depends on the whether the given device supports the sync command. Device removal When a Device is disconnected, it is removed from the system. A notification appears for InputDeviceChange.Removed (sent via InputSystem.onDeviceChange) and the Devices are removed from the devices list. The system also calls InputDevice.OnRemoved. The InputDevice.added flag is reset to false in the process. Note that Devices are not destroyed when removed. Device instances remain valid and you can still access them in code. However, trying to read values from the controls of these Devices leads to exceptions. Device resets Resetting a Device resets its Controls to their default state. You can do this manually using InputSystem.ResetDevice: InputSystem.ResetDevice(Gamepad.current); There are two types of resets as determined by the second parameter to InputSystem.ResetDevice: Type Description \"Soft\" Resets This is the default. With this type, only controls that are not marked as dontReset are reset to their default value. This excludes controls such as Pointer.position from resets and thus prevents mouse positions resetting to (0,0). \"Hard\" Resets In this type, all controls are reset to their default value regardless of whether they have dontReset set or not. Resetting Controls this way is visible on Actions. If you reset a Device that is currently driving one or more Action, the Actions are cancelled. This cancellation is different from sending an event with default state. Whereas the latter may inadvertently perform Actions (e.g. a button that was pressed would not appear to have been released), a reset will force clean cancellation. Resets may be triggered automatically by the Input System depending on application focus. Device syncs A Device may be requested to send an event with its current state through RequestSyncCommand. It depends on the platform and type of Device whether this is supported or not. A synchronization request can be explicitly sent using InputSystem.TrySyncDevice. If the device supports sync requests, the method returns true and an InputEvent will have been queued on the device for processing in the next update. Synchronization requests are also automatically sent by the Input System in certain situations. See Background and focus change behavior for more details. Device enabling and disabling When a Device is added, the Input System sends it an initial QueryEnabledStateCommand to find out whether the device is currently enabled or not. The result of this is reflected in the InputDevice.enabled property. When disabled, no events other than removal (DeviceRemoveEvent) and configuration change (DeviceConfigurationEvent) events are processed for a Device, even if they are sent. A Device can be manually disabled and re-enabled via InputSystem.DisableDevice and InputSystem.EnableDevice respectively. Note that sensors start in a disabled state by default, and you need to enable them in order for them to generate events. The Input System may automatically disable and re-enable Devices in certain situations, as detailed in the next section. Background and focus change behavior In general, input is tied to application focus. This means that Devices do not receive input while the application is not in the foreground and thus no Actions will receive input either. When the application comes back into focus, all devices will receive a sync request to have them send their current state (which may have changed while the application was in the background) to the application. Devices that do not support sync requests will see a soft reset that resets all Controls not marked as dontReset to their default state. On platforms such as iOS and Android, that do not support running Unity applications in the background, this is the only supported behavior. If the application is configured to run while in the background (that is, not having focus), input behavior can be selected from several options. This is supported in two scenarios: In Unity's Player Settings you can explicity enable Run In Background for specific players that support it (such as Windows or Mac standalone players). Note that in these players this setting is always enabled automatically in development players. In the editor, application focus is tied to focus on the Game View. If no Game View is focused, the application is considered to be running in the background. However, while in play mode, the editor will always keep running the player loop regardless of focus on the Game View window. This means that in the editor, Run In Background is considered to always be enabled. If the application is configured this way to keep running while in the background, the player loop and thus the Input System, too, will keep running even when the application does not have focus. What happens with respect to input then depends on two factors: On the ability of individual devices to receive input while the application is not running in the foreground. This is only supported by a small subset of devices and platforms. VR devices (TrackedDevice) such as HMDs and VR controllers generally support this. To find out whether a specific device supports this, you can query the InputDevice.canRunInBackground property. This property can also be forced to true or false via a Device's layout. On two settings you can find in the project-wide Input Settings. Specifically, InputSettings.backgroundBehavior: and InputSettings.editorInputBehaviorInPlayMode: The table below shows a detailed breakdown of how input behaviors vary based on these two settings and in relation to the Run In Background player setting in Unity. Note: InputDevice.canRunInBackground is overridden by the editor in certain situations (see table below). In general, the value of the property does not have to be the same between the editor and the player and depends on the specific platform and device. The following table shows the full matrix of behaviors according to the Input Settings and whether the game is running in the editor or in the player. Domain reloads in the Editor The Editor reloads the C# application domain whenever it reloads and recompiles scripts, or when the Editor goes into Play mode. This requires the Input System to reinitialize itself after each domain reload. During this process, the Input System attempts to recreate devices that were instantiated before the domain reload. However, the state of each Device doesn't carry across, which means that Devices reset to their default state on domain reloads. Note that layout registrations do not persist across domain reloads. Instead, the Input System relies on all registrations to become available as part of the initialization process (for example, by using [InitializeOnLoad] to run registration as part of the domain startup code in the Editor). This allows you to change registrations and layouts in script, and the change to immediately take effect after a domain reload. Native Devices Devices that the native backend reports are considered native (as opposed to Devices created from script code). To identify these Devices, you can check the InputDevice.native property. The Input System remembers native Devices. For example, if the system has no matching layout when the Device is first reported, but a layout which matches the device is registered later, the system uses this layout to recreate the Device. You can force the Input System to use your own layout when the native backend discovers a specific Device, by describing the Device in the layout, like this: { \"name\" : \"MyGamepad\", \"extend\" : \"Gamepad\", \"device\" : { // All strings in here are regexs and case-insensitive. \"product\" : \"MyController\", \"manufacturer\" : \"MyCompany\" } } Note: You don't have to restart Unity in order for changes in your layout to take effect on native Devices. The Input System applies changes automatically on every domain reload, so you can just keep refining a layout and your Device is recreated with the most up-to-date version every time scripts are recompiled. Disconnected Devices If you want to get notified when Input Devices disconnect, subscribe to the InputSystem.onDeviceChange event, and look for events of type InputDeviceChange.Disconnected. The Input System keeps track of disconnected Devices in InputSystem.disconnectedDevices. If one of these Devices reconnects later, the Input System can detect that the Device was connected before, and reuses its InputDevice instance. This allows the PlayerInputManager to reassign the Device to the same user again. Device IDs Each Device that is created receives a unique numeric ID. You can access this ID through InputDevice.deviceId. All IDs are only used once per Unity session. Device usages Like any InputControl, a Device can have usages associated with it. You can query usages with the usages property, and useInputSystem.SetDeviceUsage() to set them. Usages can be arbitrary strings with arbitrary meanings. One common case where the Input System assigns Devices usages is the handedness of XR controllers, which are tagged with the \"LeftHand\" or \"RightHand\" usages. Device commands While input events deliver data from a Device, commands send data back to the Device. The Input System uses these to retrieve specific information from the Device, to trigger functions on the Device (such as rumble effects), and for a variety of other needs. Sending commands to Devices The Input System sends commands to the Device through InputDevice.ExecuteCommand<TCommand>. To monitor Device commands, use InputSystem.onDeviceCommand. Each Device command implements the IInputDeviceCommandInfo interface, which only requires the typeStatic property to identify the type of the command. The native implementation of the Device should then understand how to handle that command. One common case is the \"HIDO\" command type which is used to send HID output reports to HIDs. Adding custom device Commands To create custom Device commands (for example, to support some functionality for a specific HID), create a struct that contains all the data to be sent to the Device, and add a typeStatic property to make that struct implement the IInputDeviceCommandInfo interface. To send data to a HID, this property should return \"HIDO\". You can then create an instance of this struct and populate all its fields, then use InputDevice.ExecuteCommand<TCommand> to send it to the Device. The data layout of the struct must match the native representation of the data as the device interprets it. Device state Like any other type of Control, each Device has a block of memory allocated to it which stores the state of all the Controls associated with the Device. State changes State changes are usually initiated through state events from the native backend, but you can use InputControl<>.WriteValueIntoState() to manually overwrite the state of any Control. Monitoring state changes You can use InputState.AddChangeMonitor() to register a callback to be called whenever the state of a Control changes. The Input System uses the same mechanism to implement input Actions. Synthesizing state The Input System can synthesize a new state from an existing state. An example of such a synthesized state is the press button Control that Touchscreen inherits from Pointer. Unlike a mouse, which has a physical button, for Touchscreen this is a synthetic Control that doesn't correspond to actual data coming in from the Device backend. Instead, the Input System considers the button to be pressed if any touch is currently ongoing, and released otherwise. To do this, the Input System uses InputState.Change, which allows feeding arbitrary state changes into the system without having to run them through the input event queue. The Input System incorporates state changes directly and synchronously. State change monitors still trigger as expected. Working with Devices Monitoring Devices To be notified when new Devices are added or existing Devices are removed, use InputSystem.onDeviceChange. InputSystem.onDeviceChange += (device, change) => { switch (change) { case InputDeviceChange.Added: // New Device. break; case InputDeviceChange.Disconnected: // Device got unplugged. break; case InputDeviceChange.Connected: // Plugged back in. break; case InputDeviceChange.Removed: // Remove from Input System entirely; by default, Devices stay in the system once discovered. break; default: // See InputDeviceChange reference for other event types. break; } } InputSystem.onDeviceChange delivers notifications for other device-related changes as well. See the InputDeviceChange enum for more information. Adding and removing Devices To manually add and remove Devices through the API, use InputSystem.AddDevice() and InputSystem.RemoveDevice(). This allows you to create your own Devices, which can be useful for testing purposes, or for creating virtual Input Devices which synthesize input from other events. As an example, see the on-screen Controls that the Input System provides. The Input Devices used for on-screen Controls are created entirely in code and have no native representation. Creating custom Devices Note: This example deals only with Devices that have fixed layouts (that is, you know the specific model or models that you want to implement). This is different from an interface such as HID, where Devices can describe themselves through the interface and take on a wide variety of forms. A fixed Device layout can't cover self-describing Devices, so you need to use a layout builder to build Device layouts from information you obtain at runtime. There are two main situations in which you might need to create a custom Device: You have an existing API that generates input, and which you want to reflect into the Input System. You have an HID that the Input System ignores, or that the Input system auto-generates a layout for that doesn't work well enough for your needs. For the second scenario, see Overriding the HID Fallback. The steps below deal with the first scenario, where you want to create a new Input Device entirely from scratch and provide input to it from a third-party API. Step 1: The state struct The first step is to create a C# struct that represents the form in which the system receives and stores input, and also describes the InputControl instances that the Input System must create for the Device in order to retrieve its state. // A \"state struct\" describes the memory format that a Device uses. Each Device can // receive and store memory in its custom format. InputControls then connect to // the individual pieces of memory and read out values from them. // // If it's important for the memory format to match 1:1 at the binary level // to an external representation, it's generally advisable to use // LayoutLind.Explicit. [StructLayout(LayoutKind.Explicit, Size = 32)] public struct MyDeviceState : IInputStateTypeInfo { // You must tag every state with a FourCC code for type // checking. The characters can be anything. Choose something that allows // you to easily recognize memory that belongs to your own Device. public FourCC format => new FourCC('M', 'Y', 'D', 'V'); // InputControlAttributes on fields tell the Input System to create Controls // for the public fields found in the struct. // Assume a 16bit field of buttons. Create one button that is tied to // bit #3 (zero-based). Note that buttons don't need to be stored as bits. // They can also be stored as floats or shorts, for example. The // InputControlAttribute.format property determines which format the // data is stored in. If omitted, the system generally infers it from the value // type of the field. [InputControl(name = \"button\", layout = \"Button\", bit = 3)] public ushort buttons; // Create a floating-point axis. If a name is not supplied, it is taken // from the field. [InputControl(layout = \"Axis\")] public short axis; } The Input System's layout mechanism uses InputControlAttribute annotations to add Controls to the layout of your Device. For details, see the layout system documentation. With the state struct in place, you now have a way to send input data to the Input System and store it there. The next thing you need is an InputDevice that uses your custom state struct and represents your custom Device. Step 2: The Device class Next, you need a class derived from one of the InputDevice base classes. You can either base your Device directly on InputDevice, or you can pick a more specific Device type, like Gamepad. This example assumes that your Device doesn't fit into any of the existing Device classes, so it derives directly from InputDevice. // InputControlLayoutAttribute attribute is only necessary if you want // to override the default behavior that occurs when you register your Device // as a layout. // The most common use of InputControlLayoutAttribute is to direct the system // to a custom \"state struct\" through the `stateType` property. See below for details. [InputControlLayout(displayName = \"My Device\", stateType = typeof(MyDeviceState))] public class MyDevice : InputDevice { // In the state struct, you added two Controls that you now want to // surface on the Device, for convenience. The Controls // get added to the Device either way. When you expose them as properties, // it is easier to get to the Controls in code. public ButtonControl button { get; private set; } public AxisControl axis { get; private set; } // The Input System calls this method after it constructs the Device, // but before it adds the device to the system. Do any last-minute setup // here. protected override void FinishSetup() { base.FinishSetup(); // NOTE: The Input System creates the Controls automatically. // This is why don't do `new` here but rather just look // the Controls up. button = GetChildControl<ButtonControl>(\"button\"); axis = GetChildControl<AxisControl>(\"axis\"); } } Step 3: The Update method You now have a Device in place along with its associated state format. You can call the following method to create a fully set-up Device with your two Controls on it: InputSystem.AddDevice<MyDevice>(); However, this Device doesn't receive input yet, because you haven't added any code that generates input. To do that, you can use InputSystem.QueueStateEvent or InputSystem.QueueDeltaStateEvent from anywhere, including from a thread. The following example uses IInputUpdateCallbackReceiver, which, when implemented by any InputDevice, adds an OnUpdate() method that automatically gets called during InputSystem.onBeforeUpdate and provides input events to the current input update. Note: If you already have a place where input for your device becomes available, you can skip this step and queue input events from there instead of using IInputUpdateCallbackReceiver. public class MyDevice : InputDevice, IInputUpdateCallbackReceiver { //... public void OnUpdate() { // In practice, this would read out data from an external // API. This example uses some empty input. var state = new MyDeviceState(); InputSystem.QueueStateEvent(this, state); } } Step 4: Device registration and creation You now have a functioning device, but you haven't registered it (added it to the system) yet. This means you can't see the device when, for example, you create bindings in the Action editor. You can register your device type with the system from within the code that runs automatically as part of Unity's startup. To do so, modify the definition of MyDevice like so: // Add the InitializeOnLoad attribute to automatically run the static // constructor of the class after each C# domain load. #if UNITY_EDITOR [InitializeOnLoad] #endif public class MyDevice : InputDevice, IInputUpdateCallbackReceiver { //... static MyDevice() { // RegisterLayout() adds a \"Control layout\" to the system. // These can be layouts for individual Controls (like sticks) // or layouts for entire Devices (which are themselves // Controls) like in our case. InputSystem.RegisterLayout<MyDevice>(); } // You still need a way to trigger execution of the static constructor // in the Player. To do this, you can add the RuntimeInitializeOnLoadMethod // to an empty method. [RuntimeInitializeOnLoadMethod(RuntimeInitializeLoadType.BeforeSceneLoad)] private static void InitializeInPlayer() {} } This registers the Device type with the system and makes it available in the Control picker. However, you still need a way to add an instance of the Device when it is connected. In theory, you could call InputSystem.AddDevice<MyDevice>() somewhere, but in a real-world setup you likely have to correlate the Input Devices you create with their identities in the third-party API. It might be tempting to do something like this: public class MyDevice : InputDevice, IInputUpdateCallbackReceiver { //... // This does NOT work correctly. public ThirdPartyAPI.DeviceId externalId { get; set; } } and then set that on the Device after calling AddDevice<MyDevice>. However, this doesn't work as expected in the Editor, because the Input System requires Devices to be created solely from their InputDeviceDescription in combination with the chosen layout (and layout variant). In addition, the system supports a fixed set of mutable per-device properties such as device usages (that is, InputSystem.SetDeviceUsage() and related methods). This allows the system to easily recreate Devices after domain reloads in the Editor, as well as to create replicas of remote Devices when connecting to a Player. To comply with this requirement, you must cast that information provided by the third-party API into an InputDeviceDescription and then use an InputDeviceMatcher to match the description to our custom MyDevice layout. This example assumes that the third-party API has two callbacks, like this: public static ThirdPartyAPI { // This example assumes that the argument is a string that // contains the name of the Device, and that no two Devices // have the same name in the external API. public static Action<string> deviceAdded; public static Action<string> deviceRemoved; } You can hook into those callbacks and create and destroy devices in response. // This example uses a MonoBehaviour with [ExecuteInEditMode] // on it to run the setup code. You can do this many other ways. [ExecuteInEditMode] public class MyDeviceSupport : MonoBehaviour { protected void OnEnable() { ThirdPartyAPI.deviceAdded += OnDeviceAdded; ThirdPartyAPI.deviceRemoved += OnDeviceRemoved; } protected void OnDisable() { ThirdPartyAPI.deviceAdded -= OnDeviceAdded; ThirdPartyAPI.deviceRemoved -= OnDeviceRemoved; } private void OnDeviceAdded(string name) { // Feed a description of the Device into the system. In response, the // system matches it to the layouts it has and creates a Device. InputSystem.AddDevice( new InputDeviceDescription { interfaceName = \"ThirdPartyAPI\", product = name }); } private void OnDeviceRemoved(string name) { var device = InputSystem.devices.FirstOrDefault( x => x.description == new InputDeviceDescription { interfaceName = \"ThirdPartyAPI\", product = name, }); if (device != null) InputSystem.RemoveDevice(device); } // Move the registration of MyDevice from the // static constructor to here, and change the // registration to also supply a matcher. protected void Awake() { // Add a match that catches any Input Device that reports its // interface as \"ThirdPartyAPI\". InputSystem.RegisterLayout<MyDevice>( matches: new InputDeviceMatcher() .WithInterface(\"ThirdPartyAPI\")); } } Step 5: current and all (optional) For convenience, you can quickly access the last used device of a given type, or list all devices of a specific type. To do this, add support for a current and for an all getter to the API of MyDevice. public class MyDevice : InputDevice, IInputCallbackReceiver { //... public static MyDevice current { get; private set; } public static IReadOnlyList<MyDevice> all => s_AllMyDevices; private static List<MyDevice> s_AllMyDevices = new List<MyDevice>(); public override void MakeCurrent() { base.MakeCurrent(); current = this; } protected override void OnAdded() { base.OnAdded(); s_AllMyDevices.Add(this); } protected override void OnRemoved() { base.OnRemoved(); s_AllMyDevices.Remove(this); } } Step 6: Device Commands (Optional) A final, but optional, step is to add support for Device commands. A \"device command\" is that opposite of input. In other words, it consists of data traveling to the input device, which might also return data as part of the operation (much like a function call). You can use this to communicate with the backend of the device in order to query configuration, or to initiate effects such as haptics. At the moment there isn't a proper interface available for this, however there are still some scenarios that can be solved with the current interfaces. E.g. the following shows, when implementing a non-hardware-backed device (simulated device), how to simulate hardware reporting that the device can be run in the background and supports sync commands. This is useful to prevent the device from cancelling Actions when application focus is lost and restored. For more info see Device syncs public class MyDevice : InputDevice, IInputCallbackReceiver { //... protected override unsafe long ExecuteCommand(InputDeviceCommand* commandPtr) { var type = commandPtr->type; if (type == RequestSyncCommand.Type) { // Report that the device supports the sync command and has handled it. // This will prevent device reset during focus changes. result = InputDeviceCommand.GenericSuccess; return true; } if (type == QueryCanRunInBackground.Type) { // Notify that the device supports running in the background. ((QueryCanRunInBackground*)commandPtr)->canRunInBackground = true; result = InputDeviceCommand.GenericSuccess; return true; } result = default; return false; } }"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/EditorFeatures.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/EditorFeatures.html",
    "title": "Input System Editor Features | FSM Unity Framework",
    "keywords": "Input System Editor Features This section describes how the Input System integrates with the Unity Editor, which allows you to read input in edit mode, debug input values, and set up automated input tests. Using Input in the Editor Unlike Unity's old Input Manager, the Input System package allows you to read input from within Editor window code as well. (Read more) The Input Debugger When something isn't working as expected, the quickest way to troubleshoot what's wrong is the Input Debugger in the Unity Editor. The Input Debugger provides access to the activity of the Input System in both the Editor and the connected Players. (Read more) Automated Input Testing The Input System has built-in support for writing automated input tests. You can drive input entirely from code, without any dependencies on platform backends and physical hardware devices. The automated input tests you write consider the generated input to be the same as input generated at runtime by actual platform code. (Read more)"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Events.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Events.html",
    "title": "Input events | FSM Unity Framework",
    "keywords": "Input events Types of events State events Device events Text events Working with events Listening to events Reading state events Creating events Capturing events Processing events Merging of events The Input System is event-driven. All input is delivered as events, and you can generate custom input by injecting events. You can also observe all source input by listening in on the events flowing through the system. Note: Events are an advanced, mostly internal feature of the Input System. Knowledge of the event system is mostly useful if you want to support custom Devices, or change the behavior of existing Devices. Input events are a low-level mechanism. Usually, you don't need to deal with events if all you want to do is receive input for your app. Events are stored in unmanaged memory buffers and not converted to C# heap objects. The Input System provides wrapper APIs, but unsafe code is required for more involved event manipulations. Note that there are no routing mechanism. The runtime delivers events straight to the Input System, which then incorporates them directly into the Device state. Input events are represented by the InputEvent struct. Each event has a set of common properties: Property Description type FourCC code that indicates what type of event it is. eventId Unique numeric ID of the event. time Timestamp of when the event was generated. This is on the same timeline as Time.realtimeSinceStartup. deviceId ID of the Device that the event targets. sizeInBytes Total size of the event in bytes. You can observe the events received for a specific input device in the input debugger. Types of events State events A state event contains the input state for a Device. The Input System uses these events to feed new input to Devices. There are two types of state events: StateEvent ('STAT') DeltaStateEvent ('DLTA') StateEvent contains a full snapshot of the entire state of a Device in the format specific to that Device. The stateFormat field identifies the type of the data in the event. You can access the raw data using the state pointer and stateSizeInBytes. A DeltaStateEvent is like a StateEvent, but only contains a partial snapshot of the state of a Device. The Input System usually sends this for Devices that require a large state record, to reduce the amount of memory it needs to update if only some of the Controls change their state. To access the raw data, you can use the deltaState pointer and deltaStateSizeInBytes. The Input System should apply the data to the Device's state at the offset defined by stateOffset. Device events Device events indicate a change that is relevant to a Device as a whole. If you're interested in these events, it is usually more convenient to subscribe to the higher-level InputSystem.onDeviceChange event rather then processing InputEvents yourself. There are three types of Device events: DeviceRemoveEvent ('DREM') DeviceConfigurationEvent ('DCFG') DeviceResetEvent ('DRST') DeviceRemovedEvent indicates that a Device has been removed or disconnected. To query the device that has been removed, you can use the common deviceId field. This event doesn't have any additional data. DeviceConfigurationEvent indicates that the configuration of a Device has changed. The meaning of this is Device-specific. This might signal, for example, that the layout used by the keyboard has changed or that, on a console, a gamepad has changed which player ID(s) it is assigned to. You can query the changed device from the common deviceId field. This event doesn't have any additional data. DeviceResetEvent indicates that a device should get reset. This will trigger InputSystem.ResetDevice to be called on the Device. Text events Keyboard devices send these events to handle text input. If you're interested in these events, it's usually more convenient to subscribe to the higher-level callbacks on the Keyboard class rather than processing InputEvents yourself. There are two types of text events: TextEvent ('TEXT') IMECompositionEvent ('IMES') Working with events Listening to events If you want to do any monitoring or processing on incoming events yourself, subscribe to the InputSystem.onEvent callback. InputSystem.onEvent += (eventPtr, device) => { Debug.Log($\"Received event for {device}\"); }; An IObservable interface is provided to more conveniently process events. // Wait for first button press on a gamepad. InputSystem.onEvent .ForDevice<Gamepad>() .Where(e => e.HasButtonPress()) .CallOnce(ctrl => Debug.Log($\"Button {ctrl} pressed\")); To enumerate the controls that have value changes in an event, you can use InputControlExtensions.EnumerateChangedControls. InputSystem.onEvent .Call(eventPtr => { foreach (var control in eventPtr.EnumerateChangedControls()) Debug.Log($\"Control {control} changed value to {control.ReadValueFromEventAsObject(eventPtr)}\"); }; This is significantly more efficient than manually iterating over InputDevice.allControls and reading out the value of each control from the event. Reading state events State events contain raw memory snapshots for Devices. As such, interpreting the data in the event requires knowledge about where and how individual state is stored for a given Device. The easiest way to access state contained in a state event is to rely on the Device that the state is meant for. You can ask any Control to read its value from a given event rather than from its own internally stored state. For example, the following code demonstrates how to read a value for Gamepad.leftStick from a state event targeted at a Gamepad. InputSystem.onEvent += (eventPtr, device) => { // Ignore anything that isn't a state event. if (!eventPtr.IsA<StateEvent>() && !eventPtr.IsA<DeltaStateEvent>()) return; var gamepad = device as Gamepad; if (gamepad == null) { // Event isn't for a gamepad or device ID is no longer valid. return; } var leftStickValue = gamepad.leftStick.ReadValueFromEvent(eventPtr); }; Creating events Anyone can create and queue new input events against any existing Device. Queueing an input event is thread-safe, which means that event generation can happen in background threads. Note: Unity allocates limited memory to events that come from background threads. If background threads produce too many events, queueing an event from a thread blocks the thread until the main thread flushes out the background event queue. Note that queuing an event doesn't immediately consume the event. Event processing happens on the next update (depending on InputSettings.updateMode, it is triggered either manually via InputSystem.Update, or automatically as part of the Player loop). Sending state events For Devices that have a corresponding \"state struct\" describing the state of the device, the easiest way of sending input to the Device is to simply queue instances of those structs: // Mouse. InputSystem.QueueStateEvent(Mouse.current, new MouseState { position = new Vector2(123, 234) }); // Keyboard. InputSystem.QueueStateEvent(Keyboard.current, new KeyboardState(Key.LeftCtrl, Key.A)); Touchscreen is somewhat special in that it expects its input to be in TouchState format. // Start touch. InputSystem.QueueStateEvent(Touchscreen.current, new TouchState { touchId = 1, phase = TouchPhase.Began, position = new Vector2(123, 234) }); // Move touch. InputSystem.QueueStateEvent(Touchscreen.current, new TouchState { touchId = 1, phase = TouchPhase.Moved, position = new Vector2(234, 345) }); // End touch. InputSystem.QueueStateEvent(Touchscreen.current, new TouchState { touchId = 1, phase = TouchPhase.Ended, position = new Vector2(123, 234) }); IMPORTANT: Touch IDs cannot be 0! A valid touch must have a non-zero touch ID. Concurrent touches must each have a unique ID. After a touch has ended, its ID can be reused – although it is recommended to not do so. If the exact format of the state used by a given Device is not known, the easiest way to send input to it is to simply create a StateEvent from the Device itself: // `StateEvent.From` creates a temporary buffer in unmanaged memory that holds // a state event large enough for the given device and contains a memory // copy of the device's current state. InputEventPtr eventPtr; using (StateEvent.From(myDevice, out eventPtr)) { ((AxisControl) myDevice[\"myControl\"]).WriteValueIntoEvent(0.5f, eventPtr); InputSystem.QueueEvent(eventPtr); } Alternatively, you can send events for individual Controls. // Send event to update leftStick on the gamepad. InputSystem.QueueDeltaStateEvent(Gamepad.current.leftStick, new Vector2(0.123f, 0.234f); Note that delta state events only work for Controls that are both byte-aligned and a multiple of 8 bits in size in memory. You can't send a delta state event for a button Control that is stored as a single bit, for example. Capturing Events NOTE: To download a sample project which contains a reusable MonoBehaviour called InputRecorder, which can capture and replay input from arbitrary devices, open the Package Manager, select the Input System Package, and choose the sample project \"Input Recorder\" to download. You can use the InputEventTrace class to record input events for later processing: var trace = new InputEventTrace(); // Can also give device ID to only // trace events for a specific device. trace.Enable(); //... run stuff var current = new InputEventPtr(); while (trace.GetNextEvent(ref current)) { Debug.Log(\"Got some event: \" + current); } // Also supports IEnumerable. foreach (var eventPtr in trace) Debug.Log(\"Got some event: \" + eventPtr); // Trace consumes unmanaged resources. Make sure to dispose. trace.Dispose(); Dispose event traces after use, so that they do not leak memory on the unmanaged (C++) memory heap. You can also write event traces out to files/streams, load them back in, and replay recorded streams. // Set up a trace with such that it automatically grows in size as needed. var trace = new InputEventTrace(growBuffer: true); trace.Enable(); // ... capture some input ... // Write trace to file. trace.WriteTo(\"mytrace.inputtrace.\"); // Load trace from same file. var loadedTrace = InputEventTrace.LoadFrom(\"mytrace.inputtrace\"); You can replay captured traces directly from InputEventTrace instances using the Replay method. // The Replay method returns a ReplayController that can be used to // configure and control playback. var controller = trace.Replay(); // For example, to not replay the events as is but rather create new devices and send // the events to them, call WithAllDevicesMappedToNewInstances. controller.WithAllDevicessMappedToNewInstances(); // Replay all frames one by one. controller.PlayAllFramesOnyByOne(); // Replay events in a way that tries to simulate original event timing. controller.PlayAllEventsAccordingToTimestamps(); Processing events Events are collected on a queue by the Unity runtime. This queue is regularly flushed out and the events on it processed. Events can be added to the queue manually by calling InputSystem.QueueEvent. Each time input is processed, InputSystem.Update is called implicitly by the Unity runtime. The interval at which this happens is determined by the \"Update Mode\" configured in the settings. By default, input is processed in each frame before MonoBehaviour.Update methods are called. If the setting is changed to process input in fixed updates, then this changes to input being processed each time before MonoBehaviour.FixedUpdate methods are called. Normally, when input is processed, all outstanding input events on the queue will be consumed. There are two exceptions to this, however. When using UpdateMode.ProcessEventsInFixedUpdate, the Input System attempts to associate events with the timeslice of the corresponding FixedUpdate . This is based on the timestamps of the events and a \"best effort\" at calculating the corresponding timeslice of the current FixedUpdated . The other exception are BeforeRender updates. These updates are run after fixed or dynamic updates but before rendering and used used exclusively to update devices such as VR headsets that need the most up-to-date tracking data. Other input is not consumed from such updates and these updates are only enabled if such devices are actually present. BeforeRender updates are not considered separate frames as far as input is concerned. Note: Manually calling InputSystem.Update is strongly advised against except within tests employing InputTestFixture or when explicitly setting the system to manual update mode. Methods such as InputAction.WasPerformedThisFrame and InputAction.WasPerformedThisFrame operate implicitly based on the [InputSystem.Update] cadence described above. Meaning, that they refer to the state as per the last fixed/dynamic/manual update happened. You can query the current/last update type and count from InputState. Merging of events Input system uses event mering to reduce amount of events required to be processed. This greatly improves performance when working with high refresh rate devices like 8000 Hz mice, touchscreens and others. For example let's take a stream of 7 mouse events coming in the same update: Mouse Mouse Mouse Mouse Mouse Mouse Mouse Event no1 Event no2 Event no3 Event no4 Event no5 Event no6 Event no7 Time 1 Time 2 Time 3 Time 4 Time 5 Time 6 Time 7 Pos(10,20) Pos(12,21) Pos(13,23) Pos(14,24) Pos(16,25) Pos(17,27) Pos(18,28) Delta(1,1) Delta(2,1) Delta(1,2) Delta(1,1) Delta(2,1) Delta(1,2) Delta(1,1) BtnLeft(0) BtnLeft(0) BtnLeft(0) BtnLeft(1) BtnLeft(1) BtnLeft(1) BtnLeft(1) To reduce workload we can skip events that are not encoding button state changes: Mouse Mouse Mouse Time 3 Time 4 Time 7 Event no3 Event no4 Event no7 Pos(13,23) Pos(14,24) Pos(18,28) Delta(3,3) Delta(1,1) Delta(4,4) BtnLeft(0) BtnLeft(1) BtnLeft(1) In that case we combine no1, no2, no3 together into no3 and accumulate the delta, then we keep no4 because it stores the transition from button unpressed to button pressed, and it's important to keep the exact timestamp of such transition. Later we combine no5, no6, no7 together into no7 because it is the last event in the update. Currently this approach is implemented for: FastMouse, combines events unless buttons or clickCount differ in MouseState. Touchscreen, combines events unless touchId, phaseId or flags differ in TouchState. You can disable merging of events by: InputSystem.settings.disableRedundantEventsMerging = true;"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Gamepad.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Gamepad.html",
    "title": "Gamepad Support | FSM Unity Framework",
    "keywords": "Gamepad Support Gamepad Support Controls Deadzones Polling Rumble Pausing, resuming, and stopping haptics PlayStation controllers Xbox controllers Switch controllers Cursor Control A Gamepad is narrowly defined as a Device with two thumbsticks, a D-pad, and four face buttons. Additionally, gamepads usually have two shoulder and two trigger buttons. Most gamepads also have two buttons in the middle. A gamepad can have additional Controls, such as a gyro, which the Device can expose. However, all gamepads are guaranteed to have at least the minimum set of Controls described above. Gamepad support guarantees the correct location and functioning of Controls across platforms and hardware. For example, a PS4 DualShock controller layout should look identical regardless of which platform it is supported on. A gamepad's south face button should always be the lowermost face button. NOTE: Generic HID gamepads will not be surfaced as Gamepad devices but rather be created as generic joysticks. This is because the Input System cannot guarantee correct mapping of buttons and axes on the controller (the information is simply not available at the HID level). Only HID gamepads that are explicitly supported by the Input System (like the PS4 controller) will come out as gamepads. Note that you can set up the same kind of support for specific HID gamepads yourself (see \"Overriding the HID Fallback\"). NOTE: In case you want to use the gamepad for driving mouse input, there is a sample called Gamepad Mouse Cursor you can install from the package manager UI when selecting the Input System package. The sample demonstrates how to set up gamepad input to drive a virtual mouse cursor. Controls Every gamepad has the following Controls: Control Type Description leftStick StickControl Thumbstick on the left side of the gamepad. Deadzoned. Provides a normalized 2D motion vector. X is [-1..1] from left to right, Y is [-1..1] from bottom to top. Has up/down/left/right buttons for use like a D-pad. rightStick StickControl Thumbstick on the right side of the gamepad. Deadzoned. Provides a normalized 2D motion vector. X is [-1..1] from left to right, Y is [-1..1] from bottom to top. Has up/down/left/right buttons for use like a D-pad. dpad DpadControl The D-pad on the gamepad. buttonNorth ButtonControl The upper button of the four action buttons, which are usually located on the right side of the gamepad. Labelled \"Y\" on Xbox controllers and \"Triangle\" on PlayStation controllers. buttonSouth ButtonControl The lower button of the four action buttons, which are usually located on the right side of the gamepad. Labelled \"A\" on Xbox controllers and \"Cross\" on PlayStation controllers. buttonWest ButtonControl The left button of the four action buttons, which are usually located on the right side of the gamepad. Labelled \"X\" on Xbox controllers and \"Square\" on PlayStation controllers. buttonEast ButtonControl The right button of the four action buttons, which are usually located on the right side of the gamepad. Labelled \"B\" on Xbox controllers and \"Circle\" on PlayStation controllers. leftShoulder ButtonControl The left shoulder button. rightShoulder ButtonControl The right shoulder button. leftTrigger ButtonControl The left trigger button. rightTrigger ButtonControl The right trigger button. startButton ButtonControl The start button. selectButton ButtonControl The select button. leftStickButton ButtonControl The button pressed when the user presses down the left stick. rightStickButton ButtonControl The button pressed when the user presses down the right stick. Note: Buttons are also full floating-point axes. For example, the left and right triggers can function as buttons as well as full floating-point axes. You can also access gamepad buttons using the indexer property on Gamepad and the GamepadButton enumeration: Gamepad.current[GamepadButton.LeftShoulder]; Gamepads have both both Xbox-style and PS4-style aliases on buttons. For example, the following four accessors all retrieve the same \"north\" face button: Gamepad.current[GamepadButton.Y] Gamepad.current[\"Y\"] Gamepad.current[GamepadButton.Triangle] Gamepad.current[\"Triangle\"] Deadzones Deadzones prevent accidental input due to slight variations in where gamepad sticks come to rest at their centre point. They allow a certain small inner area where the input is considered to be zero even if it is slightly off from the zero position. To add a deadzone to gamepad stick, put a stick deadzone Processor on the sticks, like this: { \"name\" : \"MyGamepad\", \"extend\" : \"Gamepad\", \"controls\" : [ { \"name\" : \"leftStick\", \"processors\" : \"stickDeadzone(min=0.125,max=0.925)\" }, { \"name\" : \"rightStick\", \"processors\" : \"stickDeadzone(min=0.125,max=0.925)\" } ] } You can do the same in your C# state structs. public struct MyDeviceState { [InputControl(processors = \"stickDeadzone(min=0.125,max=0.925)\"] public StickControl leftStick; [InputControl(processors = \"stickDeadzone(min=0.125,max=0.925)\"] public StickControl rightStick; } The gamepad layout already adds stick deadzone processors which take their min and max values from InputSettings.defaultDeadzoneMin and InputSettings.defaultDeadzoneMax. Polling On Windows (XInput controllers only), Universal Windows Platform (UWP), and Switch, Unity polls gamepads explicitly rather than deliver updates as events. You can control polling frequency manually. The default polling frequency is 60 Hz. Use InputSystem.pollingFrequency to get or set the frequency. // Poll gamepads at 120 Hz. InputSystem.pollingFrequency = 120; Increased frequency should lead to an increased number of events on the respective Devices. The timestamps provided on the events should roughly follow the spacing dictated by the polling frequency. Note, however, that the asynchronous background polling depends on OS thread scheduling and can vary. Rumble The Gamepad class implements the IDualMotorRumble interface that allows you to control the left and right motor speeds. In most common gamepads, the left motor emits a low-frequency rumble, and the right motor emits a high-frequency rumble. // Rumble the low-frequency (left) motor at 1/4 speed and the high-frequency // (right) motor at 3/4 speed. Gamepad.current.SetMotorSpeeds(0.25f, 0.75f); Note: Only the following combinations of Devices/OSes currently support rumble: PS4, Xbox, and Switch controllers, when connected to their respective consoles. Only supported if you install console-specific input packages in your Project. PS4 controllers, when connected to Mac or Windows/UWP computers. Xbox controllers on Windows. Pausing, resuming, and stopping haptics IDualMotorRumble is based on IHaptics, which is the base interface for any haptics support on any Device. You can pause, resume, and reset haptic feedback using the PauseHaptics, ResumeHaptics, and ResetHaptics methods respectively. In certain situations, you might want to globally pause or stop haptics for all Devices. For example, if the player enters an in-game menu, you can pause haptics while the player is in the menu, and then resume haptics once the player resumes the game. You can use the corresponding methods on InputSystem to achieve this result. These methods work the same way as Device-specific methods, but affect all Devices: // Pause haptics globally. InputSystem.PauseHaptics(); // Resume haptics globally. InputSystem.ResumeHaptics(); // Stop haptics globally. InputSystem.ResetHaptics(); The difference between PauseHaptics and ResetHaptics is that the latter resets haptics playback state on each Device to its initial state, whereas PauseHaptics preserves playback state in memory and only stops playback on the hardware. PlayStation controllers PlayStation controllers are well supported on different Devices. The Input System implements these as different derived types of the DualShockGamepad base class, which derives from Gamepad): DualShock3GamepadHID: A DualShock 3 controller connected to a desktop computer using the HID interface. Currently only supported on macOS. Doesn't support rumble. DualShock4GamepadHID: A DualShock 4 controller connected to a desktop computer using the HID interface. Supported on macOS, Windows, UWP, and Linux. DualSenseGamepadHID: A DualSense controller connected to a desktop computer using the HID interface. Supported on macOS, Windows. DualShock4GampadiOS: A DualShock 4 controller connected to an iOS Device via Bluetooth. Requires iOS 13 or higher. SetLightBarColor(Color): Used to set the color of the light bar on the controller. Note that, due to limitations in the USB driver and/or the hardware, only one IOCTL (input/output control) command can be serviced at a time. SetLightBarColor(Color) and SetMotorSpeeds(Single, Single) functionality on Dualshock 4 is implemented using IOCTL commands, and so if either method is called in quick succession, it is likely that only the first command will successfully complete. The other commands will be dropped. If there is a need to set both lightbar color and rumble motor speeds at the same time, use the SetMotorSpeedsAndLightBarColor(Single, Single, Color) method. Note: Unity supports PlayStation controllers on WebGL in some browser and OS configurations, but treats them as basic Gamepad or Joystick Devices, and doesn't support rumble or any other DualShock-specific functionality. Unity doesn't support connecting a PlayStation controller to a desktop machine using the DualShock 4 USB Wireless Adaptor. Use USB or Bluetooth to connect it. Xbox controllers Xbox controllers are well supported on different Devices. The Input System implements these using the XInputController class, which derives from Gamepad. On Windows and UWP, Unity uses the XInput API to connect to any type of supported XInput controller, including all Xbox One or Xbox 360-compatible controllers. These controllers are represented as an XInputController instance. You can query the XInputController.subType property to get information about the type of controller (for example, a wheel or a gamepad). On other platforms Unity, uses derived classes to represent Xbox controllers: XboxGamepadMacOS: Any Xbox or compatible gamepad connected to a Mac via USB using the Xbox Controller Driver for macOS. XboxOneGampadMacOSWireless: An Xbox One controller connected to a Mac via Bluetooth. Only the latest generation of Xbox One controllers supports Bluetooth. These controllers don't require any additional drivers in this scenario. XboxOneGampadiOS: An Xbox One controller connected to an iOS Device via Bluetooth. Requires iOS 13 or higher. Note: XInput controllers on Mac currently require the installation of the Xbox Controller Driver for macOS. This driver only supports USB connections, and doesn't support wireless dongles. However, the latest generation of Xbox One controllers natively support Bluetooth. Macs natively support these controllers as HIDs without any additional drivers when connected via Bluetooth. Unity supports Xbox controllers on WebGL in some browser and OS configurations, but treats them as basic Gamepad or Joystick Devices, and doesn't support rumble or any other Xbox-specific functionality. Switch controllers The Input System support Switch Pro controllers on desktop computers via the SwitchProControllerHID class, which implements basic gamepad functionality. Note: This support does not currently work for Switch Pro controllers connected via wired USB. Instead, the Switch Pro controller must be connected via Bluetooth. This is due to the controller using a prioprietary communication protocol on top of HID which does not allow treating the controller like any other HID. Note: Switch Joy-Cons are not currently supported on desktop. Cursor Control To give gamepads and joysticks control over a hardware or software cursor, you can use the VirtualMouseInput component. See VirtualMouseInput component in the UI section of the manual."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/HID.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/HID.html",
    "title": "HID Support | FSM Unity Framework",
    "keywords": "HID Support Human Interface Device (HID) is a specification to describe peripheral user input devices connected to computers via USB or Bluetooth. HID is commonly used to implement devices such as gamepads, joysticks, or racing wheels. The Input System directly supports HID (connected via both USB and Bluetooth) on Windows, MacOS, and the Universal Windows Platform (UWP). The system might support HID on other platforms, but not deliver input through HID-specific APIs. For example, on Linux, the system supports gamepad and joystick HIDs through SDL, but doesn't support other HIDs. Every HID comes with a device descriptor. To browse through the descriptor of an HID from the Input Debugger, click the HID Descriptor button in the device debugger window. To specify the type of the device, the HID descriptor reports entry numbers in the HID usage tables, and a list of all controls on the device, along with their data ranges and usages. The Input System handles HIDs in one of two ways: The system has a known layout for the specific HID. If the system does not have a known layout, it auto-generates one for the HID. Auto-generated layouts By default, the Input System creates layouts and Device representations for any HID which reports its usage as GenericDesktop/Joystick, GenericDesktop/Gamepad, or GenericDesktop/MultiAxisController (see the HID usage table specifications for more information). To change the list of supported usages, set HIDSupport.supportedHIDUsages. When the Input System automatically creates a layout for an HID, it always reports these Devices as Joysticks, represented by the Joystick device class. The first elements with a reported HID usage of GenericDesktop/X and GenericDesktop/Y together form the joystick's stick Control. The system then adds Controls for all further HID axis or button elements, using the Control names reported by the HID specification. The Input System assigns the first control with an HID usage of Button/Button 1 to the joystick's trigger Control. The auto-generated layouts represent a \"best effort\" on the part of the Input System. The way Human Interface Devices describe themselves in accordance with the HID standard is too ambiguous in practice, so generated layouts might lead to Controls that don't work as expected. For example, while the layout builder can identify hat switches and D-pads, it can often only make guesses as to which direction represents which. The same goes for individual buttons, which generally aren't assigned any meaning in HID. The best way to resolve the situation of HIDs not working as expected is to add a custom layout, which bypasses auto-generation altogether. See Creating a custom device layout for details. HID output HIDs can support output (for example, to toggle lights or force feedback motors on a gamepad). Unity controls output by sending HID Output Report commands to a Device. Output reports use Device-specific data formats. To use HID Output Reports, call InputDevice.ExecuteCommand to send a command struct with the typeStatic property set as \"HIDO\" to a Device. The command struct contains the Device-specific data sent out to the HID. Creating a custom device layout Often, when using the layouts auto-generated for HIDs, the result isn't ideal. Controls don't receive proper names specific to the Device, some Controls might not work as expected, and some Controls that use vendor-specific formats might not appear altogether. The best way to deal with this is to override the HID fallback and set up a custom Device layout specifically for your Device. This overrides the default auto-generation and gives you full control over how the Device is exposed. Below are three example workflows showing different ways to achieve this. Example 1 - Use an existing C# InputDevice Example 2 - Create your own InputDevice class Example 3 - A more complex example using the PS4 DualShock Controller Custom Device Workflow Example 1 - Use an existing C# InputDevice If you want to use one of the existing C# InputDevice classes in code to interface with your Device, you can build on an existing layout using JSON: { \"name\" : \"MyDevice\", \"extend\" : \"Gamepad\", // Or some other thing \"controls\" : [ { \"name\" : \"firstButton\", \"layout\" : \"Button\", \"offset\" : 0, \"bit\": 0, \"format\" : \"BIT\", }, { \"name\" : \"secondButton\", \"layout\" : \"Button\", \"offset\" : 0, \"bit\": 1, \"format\" : \"BIT\", }, { \"name\" : \"axis\", \"layout\" : \"Axis\", \"offset\" : 4, \"format\" : \"FLT\", \"parameters\" : \"clamp=true,clampMin=0,clampMax=1\" } ] } You then register your layout with the system and then instantiate it: InputSystem.RegisterControlLayout(myDeviceJson); var device = InputSystem.AddDevice(\"MyDevice\"); Custom Device Workflow Example 2 - Create your own InputDevice class Alternatively, you can create your own InputDevice class and state layouts in C#. public struct MyDeviceState : IInputStateTypeInfo { // FourCC type codes are used to identify the memory layouts of state blocks. public FourCC format => new FourCC('M', 'D', 'E', 'V'); [InputControl(name = \"firstButton\", layout = \"Button\", bit = 0)] [InputControl(name = \"secondButton\", layout = \"Button\", bit = 1)] public int buttons; [InputControl(layout = \"Analog\", parameters=\"clamp=true,clampMin=0,clampMax=1\")] public float axis; } [InputState(typeof(MyDeviceState)] public class MyDevice : InputDevice { public ButtonControl firstButton { get; private set; } public ButtonControl secondButton { get; private set; } public AxisControl axis { get; private set; } protected override void FinishSetup(InputControlSetup setup) { firstButton = setup.GetControl<ButtonControl>(this, \"firstButton\"); secondButton = setup.GetControl<ButtonControl>(this, \"secondButton\"); axis = setup.GetControl<AxisControl>(this, \"axis\"); base.FinishSetup(setup); } } To create an instance of your Device, register it as a layout and then instantiate it: InputSystem.RegisterControlLayout(\"MyDevice\", typeof(MyDevice)); InputSystem.AddDevice(\"MyDevice\"); Custom Device Workflow Example 3 - PS4 DualShock Controller This example workflow uses the same technique as the previous example, but provides more detail by using the PS4 DualShock controller as a more complex device to set up. The following example assumes that the Input System doesn't already have a custom layout for the PS4 DualShock controller, and that you want to add such a layout. In this example, you want to expose the controller as a Gamepad and you roughly know the HID data format used by the Device. Tip: If you don't know the format of a given HID you want to support, you can open the Input Debugger with the Device plugged in and pop up both the debugger view for the Device and the window showing the HID descriptor. Then, you can go through the Controls one by one, see what happens in the debug view, and correlate that to the Controls in the HID descriptor. You can also double-click individual events and compare the raw data coming in from the Device. If you select two events in the event trace, you can then right-click them and choose Compare to open a window that shows only the differences between the two events. Step 1: The state struct The first step is to describe in detail what format that input data for the Device comes in, as well as the InputControl instances that should read out individual pieces of information from that data. The HID input reports from the PS4 controller look approximately like this: struct PS4InputReport { byte reportId; // #0 byte leftStickX; // #1 byte leftStickY; // #2 byte rightStickX; // #3 byte rightStickY; // #4 byte dpad : 4; // #5 bit #0 (0=up, 2=right, 4=down, 6=left) byte squareButton : 1; // #5 bit #4 byte crossButton : 1; // #5 bit #5 byte circleButton : 1; // #5 bit #6 byte triangleButton : 1; // #5 bit #7 byte leftShoulder : 1; // #6 bit #0 byte rightShoulder : 1; // #6 bit #1 byte leftTriggerButton : 2;// #6 bit #2 byte rightTriggerButton : 2;// #6 bit #3 byte shareButton : 1; // #6 bit #4 byte optionsButton : 1; // #6 bit #5 byte leftStickPress : 1; // #6 bit #6 byte rightStickPress : 1; // #6 bit #7 byte psButton : 1; // #7 bit #0 byte touchpadPress : 1; // #7 bit #1 byte padding : 6; byte leftTrigger; // #8 byte rightTrigger; // #9 } You can translate this into a C# struct: // We receive data as raw HID input reports. This struct // describes the raw binary format of such a report. [StructLayout(LayoutKind.Explicit, Size = 32)] struct DualShock4HIDInputReport : IInputStateTypeInfo { // Because all HID input reports are tagged with the 'HID ' FourCC, // this is the format we need to use for this state struct. public FourCC format => new FourCC('H', 'I', 'D'); // HID input reports can start with an 8-bit report ID. It depends on the device // whether this is present or not. On the PS4 DualShock controller, it is // present. We don't really need to add the field, but let's do so for the sake of // completeness. This can also help with debugging. [FieldOffset(0)] public byte reportId; // The InputControl annotations here probably look a little scary, but what we do // here is relatively straightforward. The fields we add we annotate with // [FieldOffset] to force them to the right location, and then we add InputControl // to attach controls to the fields. Each InputControl attribute can only do one of // two things: either it adds a new control or it modifies an existing control. // Given that our layout is based on Gamepad, almost all the controls here are // inherited from Gamepad, and we just modify settings on them. [InputControl(name = \"leftStick\", layout = \"Stick\", format = \"VC2B\")] [InputControl(name = \"leftStick/x\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5\")] [InputControl(name = \"leftStick/left\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0,clampMax=0.5,invert\")] [InputControl(name = \"leftStick/right\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0.5,clampMax=1\")] [InputControl(name = \"leftStick/y\", offset = 1, format = \"BYTE\", parameters = \"invert,normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5\")] [InputControl(name = \"leftStick/up\", offset = 1, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0,clampMax=0.5,invert\")] [InputControl(name = \"leftStick/down\", offset = 1, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0.5,clampMax=1,invert=false\")] [FieldOffset(1)] public byte leftStickX; [FieldOffset(2)] public byte leftStickY; [InputControl(name = \"rightStick\", layout = \"Stick\", format = \"VC2B\")] [InputControl(name = \"rightStick/x\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5\")] [InputControl(name = \"rightStick/left\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0,clampMax=0.5,invert\")] [InputControl(name = \"rightStick/right\", offset = 0, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0.5,clampMax=1\")] [InputControl(name = \"rightStick/y\", offset = 1, format = \"BYTE\", parameters = \"invert,normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5\")] [InputControl(name = \"rightStick/up\", offset = 1, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0,clampMax=0.5,invert\")] [InputControl(name = \"rightStick/down\", offset = 1, format = \"BYTE\", parameters = \"normalize,normalizeMin=0,normalizeMax=1,normalizeZero=0.5,clamp,clampMin=0.5,clampMax=1,invert=false\")] [FieldOffset(3)] public byte rightStickX; [FieldOffset(4)] public byte rightStickY; [InputControl(name = \"dpad\", format = \"BIT\", layout = \"Dpad\", sizeInBits = 4, defaultState = 8)] [InputControl(name = \"dpad/up\", format = \"BIT\", layout = \"DiscreteButton\", parameters = \"minValue=7,maxValue=1,nullValue=8,wrapAtValue=7\", bit = 0, sizeInBits = 4)] [InputControl(name = \"dpad/right\", format = \"BIT\", layout = \"DiscreteButton\", parameters = \"minValue=1,maxValue=3\", bit = 0, sizeInBits = 4)] [InputControl(name = \"dpad/down\", format = \"BIT\", layout = \"DiscreteButton\", parameters = \"minValue=3,maxValue=5\", bit = 0, sizeInBits = 4)] [InputControl(name = \"dpad/left\", format = \"BIT\", layout = \"DiscreteButton\", parameters = \"minValue=5, maxValue=7\", bit = 0, sizeInBits = 4)] [InputControl(name = \"buttonWest\", displayName = \"Square\", bit = 4)] [InputControl(name = \"buttonSouth\", displayName = \"Cross\", bit = 5)] [InputControl(name = \"buttonEast\", displayName = \"Circle\", bit = 6)] [InputControl(name = \"buttonNorth\", displayName = \"Triangle\", bit = 7)] [FieldOffset(5)] public byte buttons1; [InputControl(name = \"leftShoulder\", bit = 0)] [InputControl(name = \"rightShoulder\", bit = 1)] [InputControl(name = \"leftTriggerButton\", layout = \"Button\", bit = 2)] [InputControl(name = \"rightTriggerButton\", layout = \"Button\", bit = 3)] [InputControl(name = \"select\", displayName = \"Share\", bit = 4)] [InputControl(name = \"start\", displayName = \"Options\", bit = 5)] [InputControl(name = \"leftStickPress\", bit = 6)] [InputControl(name = \"rightStickPress\", bit = 7)] [FieldOffset(6)] public byte buttons2; [InputControl(name = \"systemButton\", layout = \"Button\", displayName = \"System\", bit = 0)] [InputControl(name = \"touchpadButton\", layout = \"Button\", displayName = \"Touchpad Press\", bit = 1)] [FieldOffset(7)] public byte buttons3; [InputControl(name = \"leftTrigger\", format = \"BYTE\")] [FieldOffset(8)] public byte leftTrigger; [InputControl(name = \"rightTrigger\", format = \"BYTE\")] [FieldOffset(9)] public byte rightTrigger; [FieldOffset(30)] public byte batteryLevel; } Step 2: The InputDevice Next, you need an InputDevice to represent your device. Because you're dealing with a gamepad, you must create a new subclass of Gamepad. For simplicity, this example ignores the fact that there is a DualShockGamepad class that the actual DualShockGamepadHID is based on. // Using InputControlLayoutAttribute, we tell the system about the state // struct we created, which includes where to find all the InputControl // attributes that we placed on there. This is how the Input System knows // what controls to create and how to configure them. [InputControlLayout(stateType = typeof(DualShock4HIDInputReport)] public DualShock4GamepadHID : Gamepad { } Step 3: Registering the Device The last step is to register your new type of Device and set up the Input System so that when a PS4 controller is connected, the Input System generates your custom Device instead of using the default HID fallback. This only requires a call to InputSystem.RegisterLayout<T>, giving it an InputDeviceMatcher that matches the description for a PS4 DualShock HID. In theory, you can place this call anywhere, but the best point for registering layouts is generally during startup. Doing so ensures that your custom layout is visible to the Unity Editor and therefore exposed, for example, in the Input Control picker. You can insert your registration into the startup sequence by modifying the code for your DualShock4GamepadHID Device as follows: [InputControlLayout(stateType = typeof(DualShock4HIDInputReport)] #if UNITY_EDITOR [InitializeOnLoad] // Make sure static constructor is called during startup. #endif public DualShock4GamepadHID : Gamepad { static DualShock4GamepadHID() { // This is one way to match the Device. InputSystem.RegisterLayout<DualShock4GamepadHID>( new InputDeviceMatcher() .WithInterface(\"HID\") .WithManufacturer(\"Sony.+Entertainment\") .WithProduct(\"Wireless Controller\")); // Alternatively, you can also match by PID and VID, which is generally // more reliable for HIDs. InputSystem.RegisterLayout<DualShock4GamepadHID>( matches: new InputDeviceMatcher() .WithInterface(\"HID\") .WithCapability(\"vendorId\", 0x54C) // Sony Entertainment. .WithCapability(\"productId\", 0x9CC)); // Wireless controller. } // In the Player, to trigger the calling of the static constructor, // create an empty method annotated with RuntimeInitializeOnLoadMethod. [RuntimeInitializeOnLoadMethod(RuntimeInitializeLoadType.BeforeSceneLoad)] static void Init() {} } Your custom layout now picks up any Device that matches the manufacturer and product name strings, or the vendor and product IDs in its HID descriptor. The Input System now represents a DualShock4GamepadHID Device instance. For more information, you can also read the Device matching documentation."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/HowDoI.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/HowDoI.html",
    "title": "How do I…? | FSM Unity Framework",
    "keywords": "How do I…? A collection of frequently asked questions, and where to find their answers in the documentation. How do I...? check if a specific key or button was pressed this frame? check if any key or button was pressed find all connected gamepads? find the gamepad that the player is currently using? know when a new device was plugged in? create my own custom devices? create a simple \"Fire\" type action? Use the same techniques shown for the \"Jump\" action in the Workflows section require a button to be held down for some duration before triggering an action? use a \"positive\" and a \"negative\" button to drive an axis? create a UI to rebind input in my game? set up an Action to specifically target the left-hand XR controller? make my left-hand XR controller my right-hand one? get all current touches from the touchscreen? deal with my gamepad data arriving in a format different from GamepadState? force the Input System to use my own layout when the native backend discovers a specific Device? add deadzoning to my gamepad sticks? give my head tracking an extra update before rendering? record events flowing through the system? see events as they're processed? see what Devices I have and what state they're in?"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Installation.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Installation.html",
    "title": "Installation guide | FSM Unity Framework",
    "keywords": "Installation guide Installing the package Enabling the new input backends Installing samples This guide describes how to install and activate the Input System package for your Unity Project. Note: The new Input System requires Unity 2019.4+ and the .NET 4 runtime. It doesn't work in projects using the old .NET 3.5 runtime. Installing the package To install the new Input System, open Unity's package manager (menu: Window > Package Manager). Select the Input System package from the list, then click Install. Enabling the new input backends By default, Unity's classic Input Manager (UnityEngine.Input) is active and support for the new Input System is inactive. This allows existing Unity Projects to keep working as they are. When you install the Input System package, Unity will ask whether you want to enable the new backends. If you click Yes, Unity will enable the new backends and disable the old backends, and the Editor will restart. You can find the corresponding setting in the Player settings (menu: Edit > Project Settings > Player), under Active Input Handling. You can change this setting at any time. Doing so will restart the Editor. Note: You can enable both the old and the new system at the same time. To do so, set Active Input Handling to Both. When the new input backends are enabled, the ENABLE_INPUT_SYSTEM=1 C# #define is added to builds. Similarly, when the old input backends are enabled, the ENABLE_LEGACY_INPUT_MANAGER=1 C# #define is added. Because both can be enabled at the same time, it is possible for both defines to be 1 at the same time. Installing samples The Input System package comes with a number of samples. You can install these directly from the Package Manager window in Unity (menu: Window > Package Manager). To see the list, select the Input System package in the Package Manager window. Click Import into Project next to a sample to copy it into the current Project. For a more comprehensive demo project for the Input System, see the InputSystem_Warriors GitHub repository."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Interactions.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Interactions.html",
    "title": "Interactions | FSM Unity Framework",
    "keywords": "Interactions Interactions Operation Multiple Controls on an Action Multiple Interactions on a Binding Timeouts Using Interactions Interactions applied to Bindings Interactions applied to Actions Predefined Interactions Default Interaction Press Hold Tap SlowTap MultiTap Writing custom Interactions An Interaction represents a specific input pattern. For example, a hold is an Interaction that requires a Control to be held for at least a minimum amount of time. Interactions drive responses on Actions. You can place them on individual Bindings or an Action as a whole, in which case they apply to every Binding on the Action. At runtime, when a particular interaction completes, this triggers the Action. Operation An Interaction has a set of distinct phases it can go through in response to receiving input. Phase Description Waiting The Interaction is waiting for input. Started The Interaction has been started (that is, it received some of its expected input), but is not complete yet. Performed The Interaction is complete. Canceled The Interaction was interrupted and aborted. For example, the user pressed and then released a button before the minimum time required for a hold Interaction to complete. Not every Interaction triggers every phase, and the pattern in which specific Interactions trigger phases depends on the Interaction type. While Performed is typically the phase that triggers the actual response to an Interaction, Started and Canceled can be useful for providing UI feedback while the Interaction is in progress. For example, when a hold is Started, the app can display a progress bar that fills up until the hold time has been reached. If, however, the hold is Canceled before it completes, the app can reset the progress bar to the beginning. The following example demonstrates this kind of setup with a fire Action that the user can tap to fire immediately, or hold to charge: var fireAction = new InputAction(\"fire\"); fireAction.AddBinding(\"<Gamepad>/buttonSouth\") // Tap fires, slow tap charges. Both act on release. .WithInteractions(\"tap;slowTap\"); fireAction.started += context => { if (context.Interaction is SlowTapInteraction) ShowChargingUI(); }; fireAction.performed += context => { if (context.Interaction is SlowTapInteraction) ChargedFire(); else Fire(); }; fireAction.canceled += _ => HideChargingUI(); Multiple Controls on an Action If you have multiple Controls bound to a Binding or an Action which has an Interaction, then the Input System first applies the Control conflict resolution logic to get a single value for the Action, which it then feeds to the Interaction logic. Any of the bound Controls can perform the Interaction. Multiple Interactions on a Binding If multiple Interactions are present on a single Binding or Action, then the Input System checks the Interactions in the order they are present on the Binding. The code example above illustrates this example. The Binding on the fireAction Action has two Interactions: WithInteractions(\"tap;slowTap\"). The tap Interaction gets a first chance at interpreting the input from the Action. If the button is pressed, the Action calls the Started callback on the tap Interaction. If the user keeps holding the button, the tap Interaction times out, and the Action calls the Canceled callback for the tap Interaction and starts processing the slow tap Interaction (which now receives a Started callback). At any one time, only one Interaction can be \"driving\" the action (that is, it gets to determine the action's current phase). If an Interaction higher up in the stack cancels, Interactions lower down in the stack can take over. Timeouts Interactions might need to wait a certain time for a specific input to occur or to not occur. An example of this is the Hold interaction which, after a button is pressed, has to wait for a set duration until the \"hold\" is complete. To do this, an interaction installs a timeout using SetTimeout. It can be useful to know how much of a timeout is left for an interaction to complete. For example, you might want to display a bar in the UI that is charging up while the interaction is waiting to complete. To query the percentage to which a timeout has completed, use GetTimeoutCompletionPercentage. // Returns a value between 0 (inclusive) and 1 (inclusive). var warpActionCompletion = playerInput.actions[\"warp\"].GetTimeoutCompletionPercentage(); Note that each Interaction can have its own separate timeout (but only a single one at any one time). If multiple interactions are in effect, then GetTimeoutCompletionPercentage will only use the timeout of the one interaction that is currently driving the action. Some Interactions might involve multiple timeouts in succession. In this case, knowing only the completion of the currently running timeout (if any) is often not useful. An example is MultiTapInteraction, which involves a timeout on each individual tap, as well as a timeout in-between taps. The Interaction is complete only after a full tap sequence has been performed. An Interaction can use SetTotalTimeoutCompletionTime to inform the Input System of the total time it will run timeouts for. Using Interactions You can install Interactions on Bindings or Actions. Interactions applied to Bindings When you create Bindings for your Actions, you can choose to add Interactions to the Bindings. If you're using Input Action Assets, you can add any Interaction to your Bindings in the Input Action editor. Once you created some Bindings, select the Binding you want to add Interactions to, so that the right pane of the window shows the properties for that Binding. Next, click on the plus icon on the Interactions foldout to open a list of all available Interactions types. Choose an Interaction type to add an Interaction instance of that type. The Interaction now appears in the Interactions foldout. If the Interaction has any parameters, you can now edit them here as well: To remove an Interaction, click the minus button next to it. To change the order of Interactions, click the up and down arrows. If you create your Bindings in code, you can add Interactions like this: var Action = new InputAction(); action.AddBinding(\"<Gamepad>/leftStick\") .WithInteractions(\"tap(duration=0.8)\"); Interactions applied to Actions Applying Interactions directly to an Action is equivalent to applying them to all Bindings for the Action. It is thus more or less a shortcut that avoids manually adding the same Interaction(s) to each of the Bindings. If Interactions are applied both to an Action and to its Bindings, then the effect is the same as if the Action's Interactions are appended to the list of Interactions on each of the Bindings. This means that the Binding's Interactions are applied first, and then the Action's Interactions are applied after. You can add and edit Interactions on Actions in the Input Action Assets editor window the same way as you would do for Bindings: select an Action to Edit, then add the Interactions in the right window pane. If you create your Actions in code, you can add Interactions like this: var Action = new InputAction(Interactions: \"tap(duration=0.8)\"); Predefined Interactions The Input System package comes with a set of basic Interactions you can use. If an Action has no Interactions set, the system uses its default Interaction. Note: The built-in Interactions operate on Control actuation and don't use Control values directly. The Input System evaluates the pressPoint parameters against the magnitude of the Control actuation. This means you can use these Interactions on any Control which has a magnitude, such as sticks, and not just on buttons. The following diagram shows the behavior of the built-in Interactions for a simple button press. Default Interaction If you haven't specifically added an Interaction to a Binding or its Action, the default Interaction applies to the Binding. Value type Actions have the following behavior: As soon as a bound Control becomes actuated, the Action goes from Waiting to Started, and then immediately to Performed and back to Started. One callback occurs on InputAction.started, followed by one callback on InputAction.performed. For as long as the bound Control remains actuated, the Action stays in Started and triggers Performed whenever the value of the Control changes (that is, one call occurs to InputAction.performed). When the bound Control stops being actuated, the Action goes to Canceled and then back to Waiting. One call occurs to InputAction.canceled. Button type Actions have the following behavior: As soon as a bound Control becomes actuated, the Action goes from Waiting to Started. One callback occurs on InputAction.started. If a Control then reaches or exceeds the button press threshold, the Action goes from Started to Performed. One callback occurs on InputAction.performed. The default value of the button press threshold is defined in the input settings. However, an individual control can override this value. Once the Action has Performed, if all Controls then go back to a level of actuation at or below the release threshold, the Action goes from Performed to Canceled. One call occurs to InputAction.canceled. If the Action never went to Performed, it will go to Canceled as soon as all Controls are released. One call occurs to InputAction.canceled. PassThrough type Actions have a simpler behavior. The Input System doesn't try to track bound Controls as a single source of input. Instead, it triggers a Performed callback for each value change. Callback InputActionType.Value InputActionType.Button InputActionType.PassThrough started Control(s) changed value away from the default value. Button started being pressed but has not necessarily crossed the press threshold yet. First Control actuation after Action was enabled. performed Control(s) changed value. Button was pressed to at least the button press threshold. Control changed value. canceled Control(s) are no longer actuated. Button was released. If the button was pressed above the press threshold, the button has now fallen to or below the release threshold. If the button was never fully pressed, the button is now back to completely unpressed. Action is disabled. Press You can use a PressInteraction to explicitly force button-like interactions. Use the behavior parameter to select if the Interaction should trigger on button press, release, or both. Parameters Type Default value pressPoint float InputSettings.defaultButtonPressPoint behavior PressBehavior PressOnly Callbacks/behavior PressOnly ReleaseOnly PressAndRelease started Control magnitude crosses pressPoint Control magnitude crosses pressPoint Control magnitude crosses pressPoint performed Control magnitude crosses pressPoint Control magnitude goes back below pressPoint - Control magnitude crosses pressPoint or - Control magnitude goes back below pressPoint canceled not used not used not used Hold A HoldInteraction requires the user to hold a Control for duration seconds before the Input System triggers the Action. Parameters Type Default value duration float InputSettings.defaultHoldTime pressPoint float InputSettings.defaultButtonPressPoint To display UI feedback when a button starts being held, use the started callback. action.started += _ => ShowGunChargeUI(); action.performed += _ => FinishGunChargingAndHideChargeUI(); action.cancelled += _ => HideChargeUI(); Callbacks started Control magnitude crosses pressPoint. performed Control magnitude held above pressPoint for >= duration. canceled Control magnitude goes back below pressPoint before duration (that is, the button was not held long enough). Tap A TapInteraction requires the user to press and release a Control within duration seconds to trigger the Action. Parameters Type Default value duration float InputSettings.defaultTapTime pressPoint float InputSettings.defaultButtonPressPoint Callbacks started Control magnitude crosses pressPoint. performed Control magnitude goes back below pressPoint before duration. canceled Control magnitude held above pressPoint for >= duration (that is, the tap was too slow). SlowTap A SlowTapInteraction requires the user to press and hold a Control for a minimum duration of duration seconds, and then release it, to trigger the Action. Parameters Type Default value duration float InputSettings.defaultSlowTapTime pressPoint float InputSettings.defaultButtonPressPoint Callbacks started Control magnitude crosses pressPoint. performed Control magnitude goes back below pressPoint after duration. canceled Control magnitude goes back below pressPoint before duration (that is, the tap was too fast). MultiTap A MultiTapInteraction requires the user to press and release a Control within tapTime seconds tapCount times, with no more then tapDelay seconds passing between taps, for the Interaction to trigger. You can use this to detect double-click or multi-click gestures. Parameters Type Default value tapTime float InputSettings.defaultTapTime tapDelay float 2 * tapTime tapCount int 2 pressPoint float InputSettings.defaultButtonPressPoint Callbacks started Control magnitude crosses pressPoint. performed Control magnitude went back below pressPoint and back up above it repeatedly for tapCount times. canceled - After going back below pressPoint, Control magnitude did not go back above pressPoint within tapDelay time (that is, taps were spaced out too far apart). or - After going back above pressPoint, Control magnitude did not go back below pressPoint within tapTime time (that is, taps were too long). Writing custom Interactions You can also write a custom Interaction to use in your Project. You can use custom Interactions in the UI and code the same way you use built-in Interactions. Add a class implementing the IInputInteraction interface, like this: // Interaction which performs when you quickly move an // axis all the way from extreme to the other. public class MyWiggleInteraction : IInputInteraction { public float duration = 0.2; void Process(ref InputInteractionContext context) { if (context.timerHasExpired) { context.Canceled(); return; } switch (context.phase) { case InputActionPhase.Waiting: if (context.Control.ReadValue<float>() == 1) { context.Started(); context.SetTimeout(duration); } break; case InputActionPhase.Started: if (context.Control.ReadValue<float>() == -1) context.Performed(); break; } } // Unlike processors, Interactions can be stateful, meaning that you can keep a // local state that mutates over time as input is received. The system might // invoke the Reset() method to ask Interactions to reset to the local state // at certain points. void Reset() { } } Now, you need to tell the Input System about your Interaction. Call this method in your initialization code: InputSystem.RegisterInteraction<MyWiggleInteraction>(); Your new Interaction is now available in the Input Action Asset Editor window. You can also add it in code like this: var Action = new InputAction(Interactions: \"MyWiggle(duration=0.5)\");"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Joystick.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Joystick.html",
    "title": "Joystick support | FSM Unity Framework",
    "keywords": "Joystick support The Input System currently has limited support for joysticks as generic HIDs only. The system attempts to identify Controls based on the information provided in the HID descriptor of the Device, but it might not always be accurate. These Devices often work best when you allow the user to manually remap the Controls. To better support specific joysticks Devices, you can also provide your own custom mappings for those Devices. Unity might extend the Input System to include some mappings for common devices in the future. See the manual page on HID for more information. Controls The Input System supports Generic HID Input Devices which are recognized as joysticks via the Joystick class. Joystick Devices can have any number of Controls as reported by the Device's HID descriptor, but the Input System always tries to at least match these common Controls: Control Type Description stick StickControl The main stick of the joystick. trigger ButtonControl The primary trigger of the joystick."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Keyboard.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Keyboard.html",
    "title": "Keyboard support | FSM Unity Framework",
    "keywords": "Keyboard support The Keyboard class defines a Device with a set of key Controls defined by the Key enumeration. The location of individual keys is agnostic to keyboard layout. This means that, for example, the A key is always the key to the right of the Caps Lock key, regardless of where the currently active keyboard layout places the key that generates an a character, or whether or not the layout doesn't have a key assigned to that character. To query which (if any) character is generated by a given key, use the key Control's displayName property. The value of this property changes automatically when the OS changes the keyboard layout. You can look up keys based on the character they produce using Control paths. For example, you can query the key that produces the producing the a character from Keyboard using Keyboard.current[\"#(a)\"]. Note Keyboards usually have hardware limitations on both the number of simultaneous keypresses they report, and the combinations of keys they support. This means that certain simultaneous keypresses may not register correctly. For example, a given keyboard might report a simultaneous press of the \"QWERT\" keys correctly, but might not report \"QWERA\" correctly. At the moment, the new Input System doesn't support on-screen keyboards. For now, please Unity's existing API in UnityEngine.TouchScreenKeyboard. At the moment, Unity platform backends generally do not support distinguishing between multiple keyboards. While the Input System supports having many Keyboard devices at any point, platform backends generally only report a single keyboard and route input from all attached keyboards to the one keyboard device. Controls To retrieve a key from Keyboard, you can use one of these methods: Use the key's accessor property, such Keyboard.spaceKey. Use Keyboard's indexer and the Key enumeration (for example, keyboard[Key.Space]). The scripting API reference for the Keyboard class lists all the properties for the individual key Controls. Two special Controls, anyKey and imeSelected, don't directly map to individual keys. anyKey is a synthetic button Control which reports whether any key on the keyboard is pressed, and imeSelected reports whether or not IME text processing is enabled. In addition, Keyboard's indexer and the Key has three synthetic controls that represent combinations of modifier keys: Control Description shiftKey A button that is pressed if leftShiftKey and/or rightShiftKey is pressed. ctrlKey A button that is pressed if leftCtrlKey and/or rightCtrlKey is pressed. altKey A button that is pressed if leftAltKey and/or rightAltKey is pressed. Text input As a best practice, you shouldn't manually translate text input from key presses by trying to string together the characters corresponding to the keys. Instead, to listen to text input, hook into Keyboard.onTextInput. This delivers character-by-character input as reported by the platform, including input from on-screen keyboards. Note that the text input API doesn't allocate GC memory because it doesn't deliver fully composed strings. IME Some writing systems, such as some East-Asian scripts, are too complex to represent all characters as individual keys on a keyboard. For these layouts, operating systems implement IMEs (Input Method Editors) to allow composing input strings by other methods, for instance by typing several keys to produce a single character. Unity's UI frameworks for text input support IME without any additional configuration. If you want to build your own UI for text input, the Keyboard class allows you to work with input from IMEs using the following APIs: imeSelected is a virtual input Control that reports whether IME text processing is enabled. SetIMEEnabled() is a method which lets you turn IME processing on or off. Typically, IME processing is useful when the user wants to edit text, but not so much when the user is using the keyboard to control a game. SetIMECursorPosition(). IMEs might open system windows that let users interactively edit the text they want to input. Typically, these open next to the text editing UI. You can use the SetIMECursorPosition() method to tell the OS where that is. onIMECompositionChange is an event you can subscribe to in order to receive all updates to the IME composition string. The composition string is the text input the user is currently editing using an IME. Typically, any UI dealing with text input displays this text (with some visual indication of it being actively edited, usually an underline) at the current text input cursor position. Keyboard layouts You can query the name of the current keyboard layout using Keyboard.keyboardLayout. Layout names are platform-specific. There is no support for setting keyboard layouts from your application. To monitor keyboard layout changes, hook into InputSystem.onDeviceChange and check for InputDeviceChange.ConfigurationChanged on a Keyboard device. To find the key control that corresponds to a specific display character sequence, call Keyboard.FindKeyOnCurrentKeyboardLayout. // Find key that generates a 'q' character according to the current keyboard layout. Keyboard.current.FindKeyOnCurrentKeyboardLayout(\"q\");"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/KnownLimitations.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/KnownLimitations.html",
    "title": "Known Limitations | FSM Unity Framework",
    "keywords": "Known Limitations The following is a list of known limitations that the Input System currently has. Compatibility with other Unity features Input processing in the background is tied to Application.runInBackground (i.e. the \"Run In Background\" setting in \"Player Preferences\") which, however, Unity always forces to true in development players. This means that in development players, input will always be processed, even if the app is in the background. Of course, this only pertains to platforms where the player can actually run in the background (iOS and Android are thus unaffected). PlayerInput split-screen support does not work with Cinemachine virtual cameras. The Input System cannot generate input for IMGUI. UI Toolkit can be used with InputSystemUIInputModule but only pointer (mouse, pen, touch) and gamepad input is supported at the moment. XR support is coming. Also, UI Toolkit support currently requires use of an EventSystem setup in order to interface the Input System with UITK. uGUI After enabling, the UI will not react to a pointer's position until the position is changed. The new input system cannot yet feed text input into uGUI and TextMesh Pro input field components. This means that text input ATM is still picked up directly and internally from the Unity native runtime. The UI will not consume input such that it will not also trigger in-game actions. Device support Currently, devices whose input sources depend on application focus (generally, keyboards and pointers but can be any device depending on platform) will not automatically sync their current state when the app loses and subsequently regains focus. This means that, for example, if the W key is held when application comes back into the foreground, the key needs to be depressed and pressed again for the input to come through. This is being worked on. (Desktop) We do not yet support distinguishing input from multiple pointers (mouse, pen, touch) or keyboards. There will be a single Mouse, Pen, Touch, and Keyboard device. (Windows) Pen input will not work with Wacom devices if \"Windows Ink\" support is turned off. (Windows) HID input is not currently supported in 32-bit players. This means that devices such as the PS4 controller will not work in 32-bit standalone players. Use the 64-bit standalone player instead. (Android) We only support a single Touchscreen at the moment. (Stadia) The Stadia controller is only supported in the Stadia player at the moment. In the editor, use the generic Gamepad for bindings and use any Xbox or PS4 controller for testing. Joy-Cons are only supported on Switch. Sensors in the PS4 controller are currently only supported on PS4. NOTE: Support for NDA platforms is distributed as separate packages due to licensing restrictions. The packages, at this point, are made available separately to licensees for download and installation. Features Supported by Old Input Manager MonoBehaviour mouse methods (OnMouseEnter, OnMouseDrag, etc) will not be called by the Input System. Unity Remote doesn't currently support the Input System. This is being worked on."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Layouts.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Layouts.html",
    "title": "Layouts | FSM Unity Framework",
    "keywords": "Layouts Layout formats Layout from type Layout from JSON Generated layouts Layout inheritance Control items Layout overrides Precompiled layouts Creating a precompiled layout Layouts are the central mechanism by which the Input System learns about types of Input Devices and Input Controls. Each layout represents a specific composition of Input Controls. By matching the description of a Device to a layout, the Input System is able to create the correct type of Device and interpret the incoming input data correctly. Note: Layouts are an advanced, mostly internal feature of the Input System. Knowledge of the layout system is mostly useful if you want to support custom Devices or change the behavior of existing Devices. A layout describes a memory format for input, and the Input Controls to build in order to read and write data to or from that memory. The Input System ships with a large set of layouts for common Control types and common Devices. For other Device types, the system automatically generates layouts based on the Device description that the Device's interface reports. You can browse the set of currently understood layouts from the Input Debugger. A layout has two primary functions: Describe a certain memory layout containing input data. Assign names, structure, and meaning to the Controls operating on the data. A layout can either be for a Control on a Device (for example, Stick), or for a Device itself (that is, anything based on InputDevice). The Input System only loads layouts when they are needed (usually, when creating a new Device). To manually load a layout, you can use InputSystem.LoadLayout. This returns an InputControlLayout instance, which contains the final, fully merged (that is, containing any information inherited from base layouts and/or affected by layout overrides) structure of the layout. You can register new layouts through InputSystem.RegisterLayout. Layout formats You can add new layouts layouts in one of three ways. Represented by C# structs and classes. In JSON format. Built on the fly at runtime using layout builders. Layout from type In its most basic form, a layout can be expressed by a C# class derived from: InputControl for a Control layout. InputDevice for a Device layout. // The InputControlLayout attribute is not strictly necessary here. // However, you can use it to set additional properties (such as // a custom display name for the layout). [InputControlLayout] public class MyDevice : InputDevice { public AxisControl axis { get; private set; } public ButtonControl button { get; private set; } protected override void FinishSetup(InputDeviceBuilder builder) { base.FinishSetup(builder); axis = builder.GetControl<AxisControl>(\"axis\"); button = builder.GetControl<ButtonControl>(\"button\"); } } You can then register the layout with InputSystem.RegisterLayout. This works the same for Control and for Device layouts. // Note: This should generally be done from InitializeOnLoad/ // RuntimeInitializeOnLoad code. InputSystem.RegisterLayout<MyDevice>(); When the layout is instantiated, the system looks at every field and property defined directly in the type to potentially turn it into one or more Control items. If the field or property is annotated with InputControlAttribute, the system applies the attribute's properties to the Control item. Some special defaults apply in this case: If no offset is set, and the attribute is applied to a field, offset defaults to the offset of the field. If no name is set, it defaults to the name of the property/field. If no layout is set, the system infers it from the type of the field/property. If the field or property has a struct type which implements IInputStateTypeInfo, the field is considered to be an embedded state struct and the system recurses into the field or property to gather Controls from it. Otherwise, if the type of the field or property is based on InputControl, the system adds a Control item similar to case 1, where the member is annotated with InputControlAttribute. Using a state structure When you implement support for a new Input Device, there's usually an existing data format in which the Input System receives input for the Device. The easiest way to add support for the data format is to describe it with a C# struct annotated with InputControlAttribute. public struct MyDeviceState : IInputStateTypeInfo { public FourCC format => new FourCC('M', 'D', 'E', 'V'); [InputControl(name = \"button1\", layout = \"Button\", bit = 0)] [InputControl(name = \"button2\", layout = \"Button\", bit = 1)] [InputControl(name = \"dpad\", layout = \"Dpad\", bit = 2, sizeInBits = 4)] [InputControl(name = \"dpad/up\", bit = 2)] [InputControl(name = \"dpad/down\", bit = 3)] [InputControl(name = \"dpad/left\", bit = 4)] [InputControl(name = \"dpad/right\", bit = 5)] public int buttons; [InputControl(layout = \"Stick\")] public Vector2 stick; [InputControl(layout = \"Axis\")] // Automatically converts from byte to float. public byte trigger; } // The Device must be directed to the state struct we have created. [InputControlLayout(stateType = typeof(MyDeviceState)] public class MyDevice : InputDevice { } Layout from JSON You can also create a layout from a JSON string that contains the same information. This is mostly useful if you want to be able to store and transfer layout information separate from your code - for instance, if you want to be able to add support for new Devices dynamically without making a new build of your application. You can use InputControlLayout.ToJson() and InputControlLayout.FromJson() to convert layouts to and from the format. The same layout as above looks like this in JSON format: { \"name\": \"MyDevice\", \"format\": \"MDEV\", \"controls\": [ { \"name\": \"button1\", \"layout\": \"Button\", \"offset\": 0, \"bit\": 0, }, { \"name\": \"button2\", \"layout\": \"Button\", \"offset\": 0, \"bit\": 1, }, { \"name\": \"dpad\", \"layout\": \"Dpad\", \"offset\": 0, \"bit\": 2, \"sizeInBits\": 4, }, { \"name\": \"dpad/up\", \"offset\": -1, \"bit\": 2, }, { \"name\": \"dpad/down\", \"offset\": -1, \"bit\": 3, }, { \"name\": \"dpad/left\", \"offset\": -1, \"bit\": 4, }, { \"name\": \"dpad/right\", \"offset\": -1, \"bit\": 5, }, { \"name\": \"stick\", \"layout\": \"Stick\", \"offset\": 4, \"format\": \"VEC2\", }, { \"name\": \"trigger\", \"layout\": \"Axis\", \"offset\": 12, \"format\": \"BYTE\", } ] } Generated layouts Finally, the Input System can also build layouts on the fly in code. This is useful for Device interfaces such as HID that supply descriptive information for each Device. To build layouts dynamically in code, you can use the InputControlLayout.Builder API. Here's the same layout from the previous examples constructed programmatically: var builder = new InputControlLayout.Builder() .WithName(\"MyDevice\") .WithFormat(\"MDEV\"); builder.AddControl(\"button1\") .WithLayout(\"Button\") .WithByteOffset(0) .WithBitOffset(0); builder.AddControl(\"button2\") .WithLayout(\"Button\") .WithByteOffset(0) .WithBitOffset(1); builder.AddControl(\"dpad\") .WithLayout(\"Dpad\") .WithByteOffset(0) .WithBitOffset(2) .WithSizeInBits(4); builder.AddControl(\"dpad/up\") .WithByteOffset(-1) .WithBitOffset(2); builder.AddControl(\"dpad/down\") .WithByteOffset(-1) .WithBitOffset(3); builder.AddControl(\"dpad/left\") .WithByteOffset(-1) .WithBitOffset(4); builder.AddControl(\"dpad/right\") .WithByteOffset(-1) .WithBitOffset(5); builder.AddControl(\"stick\") .WithLayout(\"Stick\") .WithByteOffset(4) .WithFormat(\"VEC2\"); builder.AddControl(\"trigger\") .WithLayout(\"Axis\") .WithByteOffset(12) .WithFormat(\"BYTE\"); var layout = builder.Build(); Layout inheritance You can derive a layout from an existing layout. This process is based on merging the information from the derived layout on top of the information that the base layout contains. For layouts defined as types, the base layout is the layout of the base type (if any). For layouts defined in JSON, you can specify the base layout in the extends property of the root node. For layouts created in code using InputControlLayout.Builder, you can specify a base layout using InputControlLayout.Builder.Extend(). Control items Each layout is comprised of zero or more Control items. Each item either describes a new Control, or modifies the properties of an existing Control. The latter can also reach down into the hierarchy and modify properties of a Control added implicitly as a child by another item. // Add a dpad Control. [InputControl(layout = \"Dpad\")] // And now modify the properties of the \"up\" Control that was added by the // \"Dpad\" layout above. [InputControl(name = \"dpad/up\", displayName = \"DPADUP\")] public int buttons; The following table details the properties that a Control item can have. These can be set as properties on InputControlAttribute, as properties on the Control in JSON, or through methods on InputControlLayout.Builder.ControlBuilder. Property Description name Name of the Control. By default, this is the name of the field/property that InputControlAttribute is applied to. displayName Display name of the Control (for use in UI strings). shortDisplayName Short display name of the Control (for use in UI strings). layout Layout to use for the Control. variants Variants of the Control. aliases Aliases for the Control. These are alternative names the Control can be referred by. usages Usages of the Control. offset The byte offset at which the state for the Control is found. bit The bit offset at which the state of the Control is found within its byte. sizeInBits The total size of the Control's state, in bits. arraySize If this is set to a non-zero value, the system will create an array of Controls of this size. parameters Any parameters to be passed to the Control. The system will apply these to any fields the Control type might have, such as AxisControl.scaleFactor. processors Processors to apply to the Control. noisy Whether the Control is to be considered noisy. synthetic Whether the Control is to be considered synthetic. defaultState Default initial value of the state memory Control. useStateFrom For synthetic Controls, used to synthesize Control state. minValue The minimum value the Control can report. Used for evaluating Control magnitude. maxValue The maximum value the Control can report. Used for evaluating Control magnitude. dontReset When a device \"soft\" reset is performed, the state of this control will not be reset. This is useful for controls such as pointer positions which should not go to (0,0) on a reset. When a \"hard\" reset is performed, the control will still be reset to its default value. Layout overrides You can non-destructively change aspects of an existing layout using layout overrides. You can call InputSystem.RegisterLayoutOverride to register a layout as an override of its base layout. The system then adds any property present in the override to the base layout or to existing properties. // Add an extra Control to the \"Mouse\" layout const string json = @\" { \"\"name\"\" : \"\"Overrides\"\", \"\"extend\"\" : \"\"Mouse\"\", \"\"controls\"\" : [ { \"\"name\"\" : \"\"extraControl\"\", \"\"layout\"\" : \"\"Button\"\" } ] } \"; InputSystem.RegisterLayoutOverride(json); Precompiled layouts Building a device at runtime from an InputControlLayout is a slow process. The layout instance itself has to be built (which might involve reflection) and then interpreted in order to put the final InputDevice instance together. This process usually involves the loading of multiple InputControlLayout instances, each of which might be the result of merging multiple layouts together (if the layout involves inheritance or overrides). You can speed up this process up by \"baking\" the final form of a layout into a \"precompiled layout\". A precompiled layout is generated C# code that, when run, will build the corresponding device without relying on loading and interpreting an InputControlLayout. Aside from running faster, this will also create far less garbage and will not involve C# reflection (which generally causes runtime overhead by inflating the number of objects internally kept by the C# runtime). NOTE: Precompiled layouts must be device layouts. It is not possible to precompile the layout for an InputControl. Creating a precompiled layout The first step in setting up a precompiled layout is to generate it. To do so, open the Input Debugger, navigate to the layout you want to precompile within the Layouts branch, right-click it, and select Generate Precompiled Layout. Unity will ask you where to store the generated code. Pick a directory in your project, enter a file name, and click Save. Once generated, you can register the precompiled layout with the Input System using InputSystem.RegisterPrecompiledLayout. The method expects a string argument containing metadata for the precompiled layout. This string is automatically emitted as a const inside the generated class. InputSystem.RegisterPrecompiledLayout<MyPrecompiledDevice>(MyPrecompiledDevice.metadata); IMPORTANT: It is very important that this method is called with all relevant layout registrations being in the same state as at the time the layout was precompiled. There is no internal check whether the precompiled layout will still generate an identical result to the non-precompiled version. Once registered, a precompiled layout is automatically used whenever the layout that the precompiled layout is based on is instantiated. // Let's assume you have a custom device class. public class MyDevice : InputDevice { // Setters for your control getters need to have at least `protected` // or `internal` access so the precompiled version can use them. [InputControl] public ButtonControl button { get; protected set; } // This method will *NOT* be invoked by the precompiled version. Instead, all the lookups // performed here will get hardcoded into the generated C# code. protected override void FinishSetup() { base.FinishSetup(); button = GetChildControl<ButtonControl>(\"button1\"); } } // You register the device as a layout somewhere during startup. InputSystem.RegisterLayout<MyDevice>(); // And you register a precompiled version of it then as well. InputSystem.RegisterPrecompiledLayout<PrecompiledMyDevice>(PrecompiledMyDevice.metadata); // Then the following will implicitly use the precompiled version. InputSystem.AddDevice<MyDevice>(); A precompiled layout will automatically be unregistered in the following cases: A layout override is applied to one of the layouts used by the precompiled Device. This also extends to controls used by the Device. A layout with the same name as one of the layouts used by the precompiled Device is registered (which replaces the layout already registered under the name). A processor is registered that replaces a processor used by the precompiled Device. This causes the Input System to fall back to the non-precompiled version of the layout. Note also that a precompiled layout will not be used for layouts derived from the layout the precompiled version is based on. In the example above, if someone derives a new layout from MyDevice, the precompiled version is unaffected (it will not be unregistered) but is also not used for the newly created type of device. // Let's constinue from the example above and assume that sometime // later, someone replaces the built-in button with an extended version. InputSystem.RegisterLayout<ExtendedButtonControl>(\"Button\"); // PrecompiledMyDevice has implicitly been removed now, because the ButtonControl it uses // has now been replaced with ExtendedButtonControl. If needed, you can add #if checks to the generated code, if needed. The code generator will scan the start of an existing file for a line starting with #if and, if found, preserve it in newly generated code and generate a corresponding #endif at the end of the file. Similarly, you can change the generated class from public to internal and the modifier will be preserved when regenerating the class. Finally, you can also modify the namespace in the generated file with the change being preserved. The generated class is marked as partial, which means you can add additional overloads and other code by having a parallel, partial class definition. // The next line will be preserved when regenerating the precompiled layout. A // corresponding #endif will be emitted at the end of the file. #if UNITY_EDITOR || UNITY_STANDALONE // If you change the namespace to a different one, the name of the namespace will be // preserved when you regenerate the precompiled layout. namepace MyNamespace { // If you change `public` to `internal`, the change will be preserved // when regenerating the precompiled layout. public partial class PrecompiledMyDevice : MyDevice { //... The namespace of the generated layout will correspond to the"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Migration.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Migration.html",
    "title": "Migrating from the old input system | FSM Unity Framework",
    "keywords": "Migrating from the old input system This guide provides a list of APIs in UnityEngine.Input (and other related APIs in UnityEngine) and their corresponding APIs in the new Input System. Not all APIs have a corresponding version in the new API yet. Note: All of the new APIs are in the UnityEngine.InputSystem namespace. The namespace is omitted here for brevity. UnityEngine.InputSystem is referenced in full for easy disambiguation. Note that it is possible to have code for the old and the new input system at the same time using conditional compilation. When the new input system is enabled in the player preferences (see here), the ENABLE_INPUT_SYSTEM preprocessor directive is available. #if ENABLE_INPUT_SYSTEM // New input system backends are enabled. #endif #if ENABLE_LEGACY_INPUT_MANAGER // Old input backends are enabled. #endif // NOTE: Both can be true at the same time as it is possible to select \"Both\" // under \"Active Input Handling\". UnityEngine.Input UnityEngine.Input.acceleration Use Accelerometer.current.acceleration.ReadValue(). UnityEngine.Input.accelerationEventCount See UnityEngine.Input.accelerationEvents. UnityEngine.Input.accelerationEvents Acceleration events aren't made available separately from other input events. The following code traces all input events on the Accelerometer.current device. private InputEventTrace trace; void StartTrace() { InputSystem.EnableDevice(Accelerometer.current); trace = new InputEventTrace(Accelerometer.current); trace.Enable(); } void Update() { foreach (var e in trace) { //... } trace.Clear(); } UnityEngine.Input.anyKey Use InputSystem.onAnyButtonPress. InputSystem.onAnyButtonPress .CallOnce(ctrl => Debug.Log($\"Button {ctrl} pressed!\")); UnityEngine.Input.anyKeyDown Use Keyboard.current.anyKey.wasUpdatedThisFrame UnityEngine.Input.backButtonLeavesApp No corresponding API yet. UnityEngine.Input.compass No corresponding API yet. UnityEngine.Input.compensateSensors Use InputSystem.settings.compensateForScreenOrientation. UnityEngine.Input.compositionCursorPos Use Keyboard.current.SetIMECursorPosition(myPosition). UnityEngine.Input.compositionString Subscribe to the Keyboard.onIMECompositionChange event: var compositionString = \"\"; Keyboard.current.onIMECompositionChange += composition => { compositionString = composition.ToString(); }; UnityEngine.Input.deviceOrientation No corresponding API yet. UnityEngine.Input.gyro The UnityEngine.Gyroscope class is replaced by multiple separate sensor Devices in the new Input System: Gyroscope to measure angular velocity. GravitySensor to measure the direction of gravity. AttitudeSensor to measure the orientation of the device. Accelerometer to measure the total acceleration applied to the device. LinearAccelerationSensor to measure acceleration applied to the device, compensating for gravity. UnityEngine.Input.gyro.attitude Use AttitudeSensor.current.orientation.ReadValue(). UnityEngine.Input.gyro.enabled // Get: Gyroscope.current.enabled // Set: InputSystem.EnableDevice(Gyroscope.current); InputSystem.DisableDevice(Gyroscope.current); Note: The new Input System replaces UnityEngine.Gyroscope with multiple separate sensor devices. Substitute Gyroscope with other sensors in the sample as needed. See UnityEngine.Input.gyro section for details. UnityEngine.Input.gyro.gravity Use GravitySensor.current.gravity.ReadValue(). UnityEngine.Input.gyro.rotationRate Use Gyroscope.current.angularVelocity.ReadValue(). UnityEngine.Input.gyro.rotationRateUnbiased No corresponding API yet. UnityEngine.Input.gyro.updateInterval Use Sensor.samplingFrequency: Gyroscope.current.samplingFrequency = 1.0f / updateInterval; Note: samplingFrequency is in Hz, not in seconds as updateInterval, so you need to divide 1 by the value. The new Input System replaces UnityEngine.Gyroscope with multiple separate sensor devices. Substitute Gyroscope with other sensors in the sample as needed. See UnityEngine.Input.gyro for details. UnityEngine.Input.gyro.userAcceleration Use LinearAccelerationSensor.current.acceleration.acceleration.ReadValue(). UnityEngine.Input.imeCompositionMode No corresponding API yet. UnityEngine.Input.imeIsSelected // Get: Keyboard.current.imeSelected // Set: Keyboard.current.SetIMEEnabled(true); UnityEngine.Input.inputString Subscribe to the Keyboard.onTextInput event: Keyboard.current.onTextInput += character => /* ... */; UnityEngine.Input.location No corresponding API yet. UnityEngine.Input.mousePosition Use Mouse.current.position.ReadValue(). Note: Mouse simulation from touch isn't implemented yet. UnityEngine.Input.mousePresent Use Mouse.current != null. UnityEngine.Input.multiTouchEnabled No corresponding API yet. UnityEngine.Input.simulateMouseWithTouches No corresponding API yet. UnityEngine.Input.stylusTouchSupported No corresponding API yet. UnityEngine.Input.touchCount Use InputSystem.EnhancedTouch.Touch.activeTouches.Count Note: Enable enhanced touch support first by calling InputSystem.EnhancedTouchSupport.Enable(). UnityEngine.Input.touches Use InputSystem.EnhancedTouch.Touch.activeTouches. Note: Enable enhanced touch support first by calling InputSystem.EnhancedTouch.Enable(). UnityEngine.Input.touchPressureSupported No corresponding API yet. UnityEngine.Input.touchSupported Use Touchscreen.current != null. UnityEngine.Input.GetAccelerationEvent See UnityEngine.Input.accelerationEvents. UnityEngine.Input.GetAxis There is no global setup that corresponds exactly to virtual axis setups in the old Input Manager settings. Instead, you can create sets of Input Actions as independent Assets, or put them directly on your C# components. For example, if you want to recreate the following axis configuration: Option A: Put Input Actions on your component Declare one or more fields of type InputAction. public class MyComponent : MonoBehaviour { public InputAction fireAction; Configure a response to the Action. void Awake() { fireAction.performed += ctx => Fire(); } void Fire() { //... } Put the component on a GameObject. You can now configure Bindings in the Inspector window. Click the plus sign on the Bindings list to add Bindings, and double-click the Bindings to pick Controls to bind to. Enable and disable the Action as needed. void OnEnable() { fireAction.Enable(); } void OnDisable() { fireAction.Disable(); } Option B: Create an Input Action Asset Create an Input Action Asset (right-click in the Project browser and select Create > Input Actions). Give the Asset a name. Double-click the Asset to open the Input Actions editor window. In the Action Maps column, click the plus sign to add a new Action Map. Double-click the New Action Map name to rename the set. Use a descriptive name, such as gameplay. In the Actions column, click the plus sign to add a new Action. Double-click the Action to give it a name. Add Bindings to the Action. To do this, click the plus sign on the Action and choose a Binding type from the list. Select the Binding and click on the Path button in the right column to pick Controls to bind to. Click Save Asset. Your Input Action editor should now look like this: Check the Generate C# Wrapper Class checkbox in the Inspector window for the Asset, then click Apply. Your Inspector should now look like this: Add an instance of the generated C# wrapper class to your component. public class MyComponent : MonoBehaviour { MyControls controls; Create the instance and hook up a response to the fire Action. public void Awake() { controls = new MyControls(); controls.gameplay.fire.performed += ctx => Fire(); } Enable and disable the Action as appropriate. public void OnEnable() { controls.Enable(); } public void OnDisable() { controls.Disable(); } Hints To force button-like behavior on the control referenced in a Binding, add a press Interaction to it. You can access the Control that triggered an Action from the callback. Through it, you can also query its current value. fireAction.performed += ctx => { var control = ctx.control; // Grab control. var value = ctx.GetValue<float>(); // Read value from control. // Can do control-specific checks. var button = control as ButtonControl; if (button != null && button.wasPressedThisFrame) /* ... */; } UnityEngine.Input.GetAxisRaw Not directly applicable. You can use InputControl<>.ReadUnprocessedValue() to read unprocessed values from any control. UnityEngine.Input.GetButton Use InputAction.IsPressed. if (playerInput.actions[\"fire\"].IsPressed() && Time.time - m_LastFireTime >= kFireRate) Fire(); UnityEngine.input.GetButtonDown Use InputAction.WasPressedThisFrame. if (playerInput.actions[\"fire\"].WasPressedThisFrame()) Fire(); UnityEngine.input.GetButtonUp Use InputAction.WasReleasedThisFrame. if (playerInput.actions[\"fire\"].WasReleasedThisFrame()) StopFiring(); UnityEngine.Input.GetJoystickNames There is no API that corresponds to this exactly. Here are various ways to discover connected Devices: // Query a list of all connected Devices (does not allocate; read-only access). InputSystem.devices // Get notified when a Device is added or removed. InputSystem.onDeviceChange += (device, change) => { if (change == InputDeviceChange.Added || change == InputDeviceChange.Removed) { Debug.Log($\"Device '{device}' was {change}\"); } } // Find all gamepads and joysticks. var devices = InputSystem.devices; for (var i = 0; i < devices.Count; ++i) { var device = devices[i]; if (device is Joystick || device is Gamepad) { Debug.Log(\"Found \" + device); } } UnityEngine.Input.GetKey Use ButtonControl.isPressed on the corresponding key: // Using KeyControl property directly. Keyboard.current.spaceKey.isPressed Keyboard.current.aKey.isPressed // etc. // Using Key enum. Keyboard.current[Key.Space].isPressed // Using key name. ((KeyControl)Keyboard.current[\"space\"]).isPressed Note: The Input System identifies keys by physical layout, not according to the current language mapping of the keyboard. To query the name of the key according to the language mapping, use KeyControl.displayName. UnityEngine.Input.GetKeyDown Use ButtonControl.wasPressedThisFrame on the corresponding key: // Using KeyControl property directly. Keyboard.current.spaceKey.wasPressedThisFrame Keyboard.current.aKey.wasPressedThisFrame // etc. // Using Key enum. Keyboard.current[Key.Space].wasPressedThisFrame // Using key name. ((KeyControl)Keyboard.current[\"space\"]).wasPressedThisFrame Note: The Input System identifies keys by physical layout, not according to the current language mapping of the keyboard. To query the name of the key according to the language mapping, use KeyControl.displayName. UnityEngine.Input.GetKeyUp Use ButtonControl.wasReleasedThisFrame on the corresponding key: // Using KeyControl property directly. Keyboard.current.spaceKey.wasReleasedThisFrame Keyboard.current.aKey.wasReleasedThisFrame // etc. // Using Key enum. Keyboard.current[Key.Space].wasReleasedThisFrame // Using key name. ((KeyControl)Keyboard.current[\"space\"]).wasReleasedThisFrame Note: The Input System identifies keys by physical layout, not according to the current language mapping of the keyboard. To query the name of the key according to the language mapping, use KeyControl.displayName. UnityEngine.Input.GetMouseButton Use ButtonControl.isPressed on the corresponding mouse button: Mouse.current.leftButton.isPressed Mouse.current.rightButton.isPressed Mouse.current.middleButton.isPressed // You can also go through all buttons on the mouse (does not allocate). var controls = Mouse.current.allControls; for (var i = 0; i < controls.Count; ++i) { var button = controls[i] as ButtonControl; if (button != null && button.isPressed) /* ... */; } // Or look up controls by name. ((ButtonControl)Mouse.current[\"leftButton\"]).isPressed UnityEngine.Input.GetMouseButtonDown Use ButtonControl.wasPressedThisFrame on the corresponding mouse button: Mouse.current.leftButton.wasPressedThisFrame Mouse.current.rightButton.wasPressedThisFrame Mouse.current.middleButton.wasPressedThisFrame UnityEngine.Input.GetMouseButtonUp Use ButtonControl.wasReleasedThisFrame on the corresponding mouse button: Mouse.current.leftButton.wasReleasedThisFrame Mouse.current.rightButton.wasReleasedThisFrame Mouse.current.middleButton.wasReleasedThisFrame UnityEngine.Input.GetTouch Use InputSystem.EnhancedTouch.Touch.activeTouches[i] Note: Enable enhanced touch support first by calling InputSystem.EnhancedTouch.Enable(). UnityEngine.Input.IsJoystickPreconfigured Not needed. Devices which derive from Gamepad always correctly implement the mapping of axes and buttons to the corresponding InputControl members of the Gamepad class. UnityEngine.Input.ResetInputAxes Not directly applicable. UnityEngine.TouchScreenKeyboard No corresponding API yet. Use TouchScreenKeyboard for now."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Mouse.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Mouse.html",
    "title": "Mouse support | FSM Unity Framework",
    "keywords": "Mouse support The Input System represents mouse input with the Mouse Device layout that the Mouse class implements. Mice are based on the Pointer layout. To query the last used or last added mouse, use Mouse.current. var mouse = Mouse.current; Note: The Input System does not currently support: Input from multiple mice at the platform level. Identifying the current display a mouse is on. Controls In addition to the Controls inherited from Pointer, Mouse devices implement the following Controls: Control Type Description leftButton ButtonControl The left mouse button. Same as the inherited Pointer.press. rightButton ButtonControl The right mouse button. middleButton ButtonControl The middle mouse button. forwardButton ButtonControl Used for other mouse buttons where applicable. backButton ButtonControl Used for other mouse buttons where applicable. clickCount IntegerControl A Control which lets you read the number of consecutive clicks the last mouse click belonged to, as reported by the OS. Use this to distinguish double- or multi-clicks. scroll Vector2Control The input from the mouse scrolling control expressed as a delta in pixels since the last frame. Can come from a physical scroll wheel, or from touchpad gestures. Cursor warping On desktop platforms (Windows, Mac, Linux, and UWP), you can move the mouse cursor via code. Note that this moves the system's actual mouse cursor, not just Unity's internally-stored mouse position. This means that the user sees the cursor jumping to a different position, which is generally considered to be bad UX practice. It's advisable to only do this if the cursor is hidden (see the Cursor API documentation for more information). To move the cursor to a different position, use Mouse.WarpCursorPosition. The coordinates are expressed as Unity screen coordinates, just like Mouse.position. Mouse.current.WarpCursorPosition(new Vector2(123, 234)); Note: If the cursor is locked, warping the mouse position is only temporary and Unity resets the cursor to the center of the window every frame."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/OnScreen.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/OnScreen.html",
    "title": "On-screen Controls | FSM Unity Framework",
    "keywords": "On-screen Controls You can use on-screen Controls to simulate Input Devices with UI widgets that the user interacts with on the screen. The most prominent example is the use of stick and button widgets on touchscreens to emulate a joystick or gamepad. There are currently two Control types implemented out of the box: buttons and sticks. You can implement custom Controls by extending the base OnScreenControl class (see documentation on writing custom on screen Controls to learn more). Note: On-screen Controls don't have a predefined visual representation. It's up to you to set up the visual aspect of a Control (for example, by adding a sprite or UI component to the GameObject). On-screen Controls take care of the interaction logic and of setting up and generating input from interactions. Each on-screen Control uses a Control path to reference the Control that it should report input as. For example, the following on-screen button reports input as the right shoulder button of a gamepad: The collection of on-screen Controls present in a Scene forms one or more Input Devices. The Input System creates one Input Device for each distinct type of Device the Controls reference. For example, if one on-screen button references <Gamepad>/buttonSouth and another on-screen button references <Keyboard>/a, the Input System creates both a Gamepad and a Keyboard. This happens automatically when the components are enabled. When disabled, the Input System automatically removes the Devices again. To query the Control (and, implicitly, the Device) that an on-screen Control feeds into, you can use the OnScreenControl.control property. Note: This design allows you to use on-screen Controls to create input for arbitrary Input Devices, in addition to joysticks and gamepads. On-screen buttons To create an on-screen button: Add a UI Button object. Add the OnScreenButton component to it. Set the Control Path to refer to a ButtonControl (for example, <Gamepad>/buttonSouth). The type of device referenced by the control path determines the type of virtual device created by the component. The OnScreenButton component requires the target Control to be a Button Control. OnScreenButton sets the target Control value to 1 when it receives a pointer-down (IPointerDownHandler.OnPointerDown) event, or 0 when it receives a pointer-up (IPointerUpHandler.OnPointerUp) event. On-screen sticks To create an on-screen stick: Create a UI Image object. Add the OnScreenStick component to it. Set the Control Path to refer to a Vector2Control (for example, <Gamepad>/leftStick). The type of device referenced by the control path determines the type of virtual device created by the component. The OnScreenStick component requires the target Control to be a Vector2 Control. OnScreenStick starts the movement of the stick Control when it receives a pointer-down (IPointerDownHandler.OnPointerDown) event, and stops it when it receives a pointer-up (IPointerUpHandler.OnPointerUp) event. In-between, the stick moves according to the pointer being dragged (IDragHandler.OnDrag) within a box centered on the pointer-down screen point, and with an edge length defined in the component's Movement Range property. A movement range of 50, for example, means that the stick's on-screen area is 25 pixels up, down, left, and right of the pointer-down point on screen. If you want to be notified when the user starts and/or stops touching the on-screen stick, implement IPointerDownHandler and/or IPointerUpHandler on a component and add it to the stick GameObject. Isolated mode The OnScreenStick simulates input events from the device specified in the OnScreenControl.control property. To the Input System itself, these are normal events and can cause the paired device to change in games and applications where dynamic device switching is used, for example when the PlayerInput component is used with the PlayerInput.neverAutoSwitchControlSchemes) propety set to false. As the stick is dragged around, the paired device will alternate between the device that owns the pointer (mouse, touch, pen etc) and the device from the control path, which can result in jittery movement of the on-screen stick. Use Isolated Input Actions to fix this. This mode uses a local set of Input Action instances to drive interaction with the stick, and not the actions defined in the UI. The downside of this mode is that pointer actions will be duplicated in both the on-screen stick component and any Input Action Assets being used to drive the UI. Note that if a set of bindings is not specified for the Pointer Down Action and Pointer Move Actions, the following defaults will be used: Pointer Down Action <Mouse>/leftButton <Pen>/tip <Touchscreen>/touch*/press <XRController>/trigger Pointer Move Action <Mouse>/position <Pen>/position <Touchscreen>/touch*/position Writing custom on-screen Controls You can add support for new types of Input Controls by extending OnScreenControl. An easy example to follow is OnScreenButton. [AddComponentMenu(\"Input/On-Screen Button\")] public class OnScreenButton : OnScreenControl, IPointerDownHandler, IPointerUpHandler { public void OnPointerUp(PointerEventData data) { SendValueToControl(0.0f); } public void OnPointerDown(PointerEventData data) { SendValueToControl(1.0f); } [InputControl(layout = \"Button\")] [SerializeField] private string m_ControlPath; protected override string controlPathInternal { get => m_ControlPath; set => m_ControlPath = value; } }"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Pen.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Pen.html",
    "title": "Pen, tablet, and stylus support | FSM Unity Framework",
    "keywords": "Pen, tablet, and stylus support Pen support comprises both tablets on desktops (such as the various tablets produced by Wacom), and styluses on mobile devices (such as the stylus on the Samsung Note, the Apple Pencil on iOS, or the Surface Pen on the Microsoft Surface line of notebooks). Pens generally offer pressure sensitivity, in-range detection (being able to control the cursor while not yet touching the tablet/screen surface), and often the ability to flip the pen for eraser-like behavior. Pens are represented by the Pen Device layout implemented by the Pen class. Pens are based on the Pointer layout. You can query the last used or last added pen with Pen.current. Note: Pen/tablet support is currently implemented on Windows, UWP, iOS, and Android. macOS is supported in Unity 2020.1+. Some devices support tracking multiple pens independently. Unity's Input System doesn't support this currently. iOS: The double-tap interaction on the side of the Apple Pencil is not surfaced as input at the moment. Also, no in-range detection is supported and inRange will remain at its default value. Controls In addition to the Controls inherited from Pointer, pen Devices implement the following Controls: Control Type Description tip ButtonControl Whether the tip of the pen touches the surface. Same as the inherited Pointer.press. eraser ButtonControl Whether the eraser/back end of the pen touches the surface. firstBarrelButton ButtonControl Whether the first button on the barrel of the pen is pressed. secondBarrelButton ButtonControl Whether the second button on the barrel of the pen is pressed. thirdBarrelButton ButtonControl Whether the third button on the barrel of the pen is pressed. fourthBarrelButton ButtonControl Whether the forth button on the barrel of the pen is pressed. inRange ButtonControl Whether the pen is currently in detection range of the tablet. If unsupported, this control will remain at a value of 1. tilt Vector2Control Tilt of the pen relative to the surface. twist AxisControl Rotation of the pen around its own axis. Only supported on a limited number of pens, such as the Wacom Art Pen. Pressure, tilt, and twist Pressure: You can access the pen's current pressure via Pen.pressure, where 0 means no pressure, and 1 means maximum pressure. However, pressure can go over 1 if the system applies a custom pressure curve where a pressure value of 1 doesn't require pressing the pen down all the way to the maximum force the hardware supports. If a pen doesn't support different pressure levels, the pressure Control always returns 1. Tilt: If supported, the Pen.tilt Control represents the angle at which the pen tilts towards the tablet or screen surface. The X and Y axes correspond to the respective screen axes. A value of 1 on either axis means that the pen is fully parallel to the tablet or screen surface on that axis. A value of 0 means that the pen is perpendicular to the tablet or screen surface on that axis. If a pen doesn't support tilt angles, Pen.tilt is always (0,0). Twist: Some pens also support twist detection (the pen rotating around its own axis). If supported, Pen.twist represents the current rotation, where 0 means that the pen is facing up towards the Y axis, and values close to 1 mean that the pen is fully rotated clockwise around its own axis. In-range detection A pen might not need to touch the tablet or screen surface in order to be able to control the cursor. You can use the inRange button Control to determine whether the pen is currently in detection range. If inRange reports as pressed, the pen registers with the tablet or screen. For Devices that don't support this feature, inRange always reports as pressed. Barrel buttons Pen Devices often have one or multiple buttons on the side of the pen. These are represented by the firstBarrelButton, secondBarrelButton, thirdBarrelButton, and fourthBarrelButton where applicable."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/PlayerInput.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/PlayerInput.html",
    "title": "The PlayerInput component | FSM Unity Framework",
    "keywords": "The PlayerInput component The Input System provides two related components that simplify how you set up and work with input: the Player Input component and the Player Input Manager component. The Player Input component represents a single player, and that player's associated Input Actions, whereas the Player Input Manager component handles setups that allow for several concurrent users (for example, player lobbies and split-screen gameplay in a game). The Player Input component Above, the Player Input component as visible in the inspector, with an Actions Asset called \"ExampleActions\" assigned. Each PlayerInput instance represents a separate player or user. You can use multiple PlayerInput instances at the same time (although not on the same GameObject) to represent local multiplayer setups. The Input System pairs each player to a unique set of Devices that the player uses exclusively, but you can also manually pair Devices in a way that enables two or more players to share a Device (for example, left/right keyboard splits or hot seat use). Each PlayerInput corresponds to one InputUser. You can use PlayerInput.user to query the InputUser from the component. Note: These components are built on top of the public Input System API. As such, they don't do anything that you can't program yourself. They are meant primarily as an easy, out-of-the-box setup that eliminates much of the need for custom scripting. Getting started To get started using the Player Input component, use the following steps: Add a Player Input component to a GameObject. This would usually be the GameObject that represents the player in your game. Create an Input Actions asset. You can do this by clicking the \"Create Actions...\" button which is visible in the Player Input component inspector if you have not yet assigned actions to it (shown below). When creating the actions asset, Unity asks you where to create the new Asset. Choose a name and folder inside the Assets folder of your Project (or just accept the defaults) and select Okay. This creates a new .inputactions Asset in your Project, connects it to the Player Input component, and brings up the editor window for the Input Action asset. Configure the Actions Asset so that it contains the actions you want your users to be able to perform, bound to the controls they should use to perform them. Set up Action responses, by selecting a Behaviour type from the Behaviour menu. The Behaviour type you select affects how you should implement the methods that handle your Action responses. See the notification behaviors section further down for details. Configuring the Player Input component You can use the following properties to configure PlayerInput: Property Description Actions The set of Input Actions associated with the player. To receive input, each player must have an associated set of Actions. See documentation on Actions for details. Default Control Scheme Which Control Scheme (from what is defined in Actions) to enable by default. Default Action Map Which Action Map in Actions to enable by default. If set to None, then the player starts with no Actions being enabled. Camera The individual camera associated with the player. This is only required when employing split-screen setups and has no effect otherwise. Behavior How the PlayerInput component notifies game code about things that happen with the player. See documentation on notification behaviors. Actions To receive input, each player must have an associated set of Input Actions. When you create these via the Player Input inspector's Create Actions button, the Input System creates a default set of Actions. However, the Player Input component places no restrictions on the arrangement of Actions. The Player Input component automatically handles enabling and disabling Actions, and also handles installing callbacks on the Actions. When multiple Player Input components use the same Actions, the components automatically create private copies of the Actions. When first enabled, the Player Input component enables all Actions from the the Default Action Map. If no default Action Map exists, the Player Input component does not enable any Actions. To manually enable Actions, you can call Enable and Disable on the Action Maps or Actions, like you would do without PlayerInput. To check which Action Map is currently enabled, or to switch to a different one, use the PlayerInput.currentActionMap property. To switch Action Maps with an Action Map name, you can also call PlayerInput.SwitchCurrentActionMap. To disable a player's input, call PlayerInput.DeactivateInput. To re-enable it, call PlayerInput.ActivateInput. The latter enables the default Action Map, if it exists. When PlayerInput is disabled, it automatically disables the currently active Action Map (PlayerInput.currentActionMap) and disassociate any Devices paired to the player. See the notification behaviors section below for how to be notified when player triggers an Action. When using Send Messages or Broadcast Messages When the notification behavior of PlayerInput is set to Send Messages or Broadcast Messages, you can set your app to respond to Actions by defining methods in components like so: public class MyPlayerScript : MonoBehaviour { // \"jump\" action becomes \"OnJump\" method. // If you're not interested in the value from the control that triggers the action, use a method without arguments. public void OnJump() { // your Jump code here } // If you are interested in the value from the control that triggers an action, you can declare a parameter of type InputValue. public void OnMove(InputValue value) { // Read value from control. The type depends on what type of controls. // the action is bound to. var v = value.Get<Vector2>(); // IMPORTANT: // The given InputValue is only valid for the duration of the callback. Storing the InputValue references somewhere and calling Get<T>() later does not work correctly. } } The component must be on the same GameObject if you are using Send Messages, or on the same or any child GameObject if you are using Broadcast Messages. When using Invoke Unity Events When the notification behavior of PlayerInput is set to Invoke Unity Events, each Action has to be routed to a target method. The methods have the same format as the started, performed, and canceled callbacks on InputAction. public class MyPlayerScript : MonoBehaviour { public void OnFire(InputAction.CallbackContext context) { } public void OnMove(InputAction.CallbackContext context) { var value = context.ReadValue<Vector2>(); } } Notification behaviors You can use the Behavior property in the Inspector to determine how a PlayerInput component notifies game code when something related to the player has occurred. The following options are available: Behavior Description Send Messages Uses GameObject.SendMessage on the GameObject that the PlayerInput component belongs to. Broadcast Messages Uses GameObject.BroadcastMessage on the GameObject that the PlayerInput component belongs to. This broadcasts the message down the GameObject hierarchy. Invoke Unity Events Uses a separate UnityEvent for each individual type of message. When this is selected, the events available on the PlayerInput are accessible from the Events foldout. The argument received by events triggered for Actions is the same as the one received by started, performed, and canceled callbacks. Invoke CSharp Events Similar to Invoke Unity Events, except that the events are plain C# events available on the PlayerInput API. You cannot configure these from the Inspector. Instead, you have to register callbacks for the events in your scripts. The following events are available: onActionTriggered (collective event for all actions on the player) onDeviceLost onDeviceRegained In addition to per-action notifications, PlayerInput sends the following general notifications: Notification Description DeviceLostMessage The player has lost one of the Devices assigned to it. This can happen, for example, if a wireless device runs out of battery. DeviceRegainedMessage Notification that triggers when the player recovers from Device loss and is good to go again. Device assignments Each PlayerInput can have one or more Devices assigned to it. By default, no two PlayerInput components are assigned the same Devices, but you can force this; to do so, manually assign Devices to a player when calling PlayerInput.Instantiate, or call InputUser.PerformPairingWithDevice on the InputUser of a PlayerInput. If the PlayerInput component has any Devices assigned, it matches these to the Control Schemes in the associated Action Asset, and only enables Control Schemes which match its Input Devices. UI input The PlayerInput component can work together with an InputSystemUIInputModule to drive the UI system. To set this up, assign a reference to a InputSystemUIInputModule component in the UI Input Module field of the PlayerInput component. The PlayerInput and InputSystemUIInputModule components should be configured to work with the same InputActionAsset for this to work. Once you've completed this setup, when the PlayerInput component configures the Actions for a specific player, it assigns the same Action configuration to the InputSystemUIInputModule. In other words, the same Action and Device configuration that controls the player now also controls the UI. If you use MultiplayerEventSystem components to dispatch UI events, you can also use this setup to simultaneously have multiple UI instances on the screen, each controlled by a separate player."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/PlayerInputManager.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/PlayerInputManager.html",
    "title": "The Player Input Manager component | FSM Unity Framework",
    "keywords": "The Player Input Manager component NOTE: The Input System package comes with a sample called Simple Multiplayer which you can install from the package manager UI in the Unity editor. The sample demonstrates how to use PlayerInputManager to set up a simple local multiplayer scenario. The Player Input system facilitates setting up local multiplayer games, where multiple players share a single device with a single screen and multiple controllers. Set this up using the PlayerInputManager component, which automatically manages the creation and lifetime of PlayerInput instances as players join and leave the game. Property Description Notification Behavior How the PlayerInputManager component notifies game code about changes to the connected players. This works the same way as for the PlayerInput component. Join Behavior Determines the mechanism by which players can join when joining is enabled. See documentation on join behaviors. Player Prefab A prefab that represents a player in the game. The PlayerInputManager component creates an instance of this prefab whenever a new player joins. This prefab must have one PlayerInput component in its hierarchy. Joining Enabled By Default While this is enabled, new players can join via the mechanism determined by Join Behavior. Limit Number of Players Enable this if you want to limit the number of players who can join the game. Max Player Count(Only shown when Limit number of Players is enabled.) The maximum number of players allowed to join the game. Enable Split-Screen If enabled, each player is automatically assigned a portion of the available screen area. See documentation on split-screen multiplayer. Join behaviors You can use the Join Behavior property in the Inspector to determine how a PlayerInputManager component decides when to add new players to the game. The following options are available to choose the specific mechanism that PlayerInputManager employs. Behavior Description Join Players When Button IsPressed Listen for button presses on Devices that are not paired to any player. If a player presses a button and joining is allowed, join the new player using the Device they pressed the button on. Join Players When Join Action Is Triggered Similar to Join Players When Button IsPressed, but this only joins a player if the control they triggered matches a specific action you define. For example, you can set up players to join when pressing a specific gamepad button. Join Players Manually Don't join players automatically. Call JoinPlayer explicitly to join new players. Alternatively, create GameObjects with PlayerInput components directly and the Input System will automatically join them. Split-screen If you enable the Split-Screen option, the PlayerInputManager automatically splits the available screen space between the active players. For this to work, you must set the Camera property on the PlayerInput prefab. The PlayerInputManager then automatically resizes and repositions each camera instance to let each player have their own part of the screen. If you enable the Split-Screen option, you can configure the following additional properties in the Inspector: Property Description Maintain Aspect Ratio A false value enables the game to produce screen areas that have an aspect ratio different from the screen resolution when subdividing the screen. Set Fixed Number If this value is greater than zero, the PlayerInputManager always splits the screen into a fixed number of rectangles, regardless of the actual number of players. Screen Rectangle The normalized screen rectangle available for allocating player split-screens into. By default, any player in the game can interact with any UI elements. However, in split-screen setups, your game can have screen-space UIs that are restricted to just one specific camera. See the UI Input section on the Player Input component page on how to set this up using the Player Input component, InputSystemUIInputModule and MultiplayerEventSystem components. PlayerInputManager notifications PlayerInputManager sends notifications when something notable happens with the current player setup. These notifications are delivered according to the Notification Behavior property, in the same way as for PlayerInput. Your game can listen to the following notifications: Notification Description PlayerJoinedMessage A new player joined the game. Passes the [PlayerInput](PlayerInput.mdPlayerInputManager sends a Player Joined notification for each of these. PlayerLeftMessage A player left the game. Passes the PlayerInput instance of the player who left."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Pointers.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Pointers.html",
    "title": "Pointers | FSM Unity Framework",
    "keywords": "Pointers Pointer Devices are defined as InputDevices that track positions on a 2D surface. The Input System supports three types of pointers: Touch Mouse Pen Controls Each of these types implements a common set of Controls. For a more detailed descriptions of these Controls, refer to their scripting reference. Control Type Description position Vector2Control The current pointer coordinates in window space. delta Vector2Control Provides motion delta in pixels accumulated (summed) over the duration of the current frame/update. Resets to (0,0) each frame. Note that the resolution of deltas depends on the specific hardware and/or platform. press ButtonControl Whether the pointer or its primary button is pressed down. pressure AxisControl The pressure applied with the pointer while in contact with the pointer surface. This value is normalized. This is only relevant for pressure-sensitive devices, such as tablets and some touch screens. radius Vector2Control The size of the area where the finger touches the surface. This is only relevant for touch input. Window space The coordinates within Player code are in the coordinate space of the Player window. Within Editor code, the coordinates are in the coordinate space of the current EditorWindow. If you query Pointer.current.position in UnityEditor.EditorWindow.OnGUI, for example, the returned 2D vector will be in the coordinate space of your local GUI (same as UnityEngine.Event.mousePosition)."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Processors.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Processors.html",
    "title": "Processors | FSM Unity Framework",
    "keywords": "Processors An Input Processor takes a value and returns a processed result for it. The received value and result value must be of the same type. For example, you can use a clamp Processor to clamp values from a control to a certain range. Note: To convert received input values into different types, see composite Bindings. Using Processors Processors on Bindings Processors on Actions Processors on Controls Predefined Processors Clamp Invert Invert Vector 2 Invert Vector 3 Normalize Normalize Vector 2 Normalize Vector 3 Scale Scale Vector 2 Scale Vector 3 Axis deadzone Stick deadzone Writing custom Processors Using Processors You can install Processors on bindings, actions or on controls. Each Processor is registered using a unique name. To replace an existing Processor, register your own Processor under an existing name. Processors can have parameters which can be booleans, integers, or floating-point numbers. When created in data such as bindings, processors are described as strings that look like function calls: // This references the processor registered as \"scale\" and sets its \"factor\" // parameter (a floating-point value) to a value of 2.5. \"scale(factor=2.5)\" // Multiple processors can be chained together. They are processed // from left to right. // // Example: First invert the value, then normalize [0..10] values to [0..1]. \"invert,normalize(min=0,max=10)\" Processors on Bindings When you create Bindings for your actions, you can choose to add Processors to the Bindings. These process the values from the controls they bind to, before the system applies them to the Action value. For instance, you might want to invert the Vector2 values from the controls along the Y axis before passing these values to the Action that drives the input logic for your application. To do this, you can add an Invert Vector2 Processor to your Binding. If you're using Input Action Assets, you can add any Processor to your Bindings in the Input Action editor. Select the Binding you want to add Processors to so that the right pane of the window displays the properties for that Binding. Select the Add (+) icon on the Processors foldout to open a list of all available Processors that match your control type, then choose a Processor type to add a Processor instance of that type. The Processor now appears under the Processors foldout. If the Processor has any parameters, you can edit them in the Processors foldout. To remove a Processor, click the Remove (-) icon next to it. You can also use the up and down arrows to change the order of Processors. This affects the order in which the system processes values. If you create your Bindings in code, you can add Processors like this: var action = new InputAction(); action.AddBinding(\"<Gamepad>/leftStick\") .WithProcessor(\"invertVector2(invertX=false)\"); Processors on Actions Processors on Actions work in the same way as Processors on Bindings, but they affect all controls bound to an Action, rather than just the controls from a specific Binding. If there are Processors on both the Binding and the Action, the system processes the ones from the Binding first. You can add and edit Processors on Actions in Input Action Assets the same way as you would for Bindings: select an Action to edit, then add one or more Processors in the right window pane. If you create your Actions in code, you can add Processors like this: var action = new InputAction(processors: \"invertVector2(invertX=false)\"); Processors on Controls You can have any number of Processors directly on an InputControl, which then process the values read from the Control. Whenever you call ReadValue on a Control, all Processors on that Control process the value before it gets returned to you. You can use ReadUnprocessedValue on a Control to bypass the Processors. The Input System adds Processors to a Control during device creation, if they're specified in the Control's layout. You can't add Processors to existing Controls after they've been created, so you can only add Processors to Controls when you're creating custom devices. The devices that the Input System supports out of the box already have some useful Processors added on their Controls. For instance, sticks on gamepads have a Stick Deadzone Processor. If you're using a layout generated by the Input System from a state struct using InputControlAttributes, you can specify the Processors you want to use via the processors property of the attribute, like this: public struct MyDeviceState : IInputStateTypeInfo { public FourCC format => return new FourCC('M', 'Y', 'D', 'V'); // Add an axis deadzone to the Control to ignore values // smaller then 0.2, as our Control does not have a stable // resting position. [InputControl(layout = \"Axis\", processors = \"AxisDeadzone(min=0.2)\")] public short axis; } If you create a layout from JSON, you can specify Processors on your Controls like this: { \"name\" : \"MyDevice\", \"extend\" : \"Gamepad\", // Or some other thing \"controls\" : [ { \"name\" : \"axis\", \"layout\" : \"Axis\", \"offset\" : 4, \"format\" : \"FLT\", \"processors\" : \"AxisDeadzone(min=0.2)\" } ] } Predefined Processors The Input System package comes with a set of useful Processors you can use. Clamp Name Clamp Operand Type float Parameters float min float max Clamps input values to the [min..max] range. Invert Name Invert Operand Type float Inverts the values from a Control (that is, multiplies the values by -1). Invert Vector 2 Name InvertVector2 Operand Type Vector2 Parameters bool invertX bool invertY Inverts the values from a Control (that is, multiplies the values by -1). Inverts the x axis of the vector if invertX is true, and the y axis if invertY is true. Invert Vector 3 Name Invert Vector 3 Operand Type Vector3 Parameters bool invertX bool invertY bool invertZ Inverts the values from a Control (that is, multiplies the values by -1). Inverts the x axis of the vector if invertX is true, the y axis if invertY is true, and the z axis if invertZ is true. Normalize Name Normalize Operand Type float Parameters float min float max float zero Normalizes input values in the range [min..max] to unsigned normalized form [0..1] if min is >= zero, and to signed normalized form [-1..1] if min < zero. Normalize Vector 2 Name NormalizeVector2 Operand Type Vector2 Normalizes input vectors to be of unit length (1). This is the same as calling Vector2.normalized. Normalize Vector 3 Name NormalizeVector3 Operand Type Vector3 Normalizes input vectors to be of unit length (1). This is the same as calling Vector3.normalized. Scale Name Scale Operand Type float Parameters float factor Multiplies all input values by factor. Scale Vector 2 Name ScaleVector2 Operand Type Vector2 Parameters float x float y Multiplies all input values by x along the X axis and by y along the Y axis. Scale Vector 3 Name ScaleVector3 Operand Type Vector3 Parameters float x float y float x Multiplies all input values by x along the X axis, by y along the Y axis, and by z along the Z axis. Axis deadzone Name AxisDeadzone Operand Type float Parameters float min float max An axis deadzone Processor scales the values of a Control so that any value with an absolute value smaller than min is 0, and any value with an absolute value larger than max is 1 or -1. Many Controls don't have a precise resting point (that is, they don't always report exactly 0 when the Control is in the center). Using the min value on a deadzone Processor avoids unintentional input from such Controls. Also, some Controls don't consistently report their maximum values when moving the axis all the way. Using the max value on a deadzone Processor ensures that you always get the maximum value in such cases. Stick deadzone Name StickDeadzone Operand Type Vector2 Parameters float min float max A stick deadzone Processor scales the values of a Vector2 Control, such as a stick, so that any input vector with a magnitude smaller than min results in (0,0), and any input vector with a magnitude greater than max is normalized to length 1. Many Controls don't have a precise resting point (that is, they don't always report exactly 0,0 when the Control is in the center). Using the min value on a deadzone Processor avoids unintentional input from such Controls. Also, some Controls don't consistently report their maximum values when moving the axis all the way. Using the max value on a deadzone Processor ensures that you always get the maximum value in such cases. Writing custom Processors You can also write custom Processors to use in your Project. Custom Processors are available in the UI and code in the same way as the built-in Processors. Add a class derived from InputProcessor<TValue>, and implement the Process method: IMPORTANT: Processors must be stateless. This means you cannot store local state in a processor that will change depending on the input being processed. The reason for this is because processors are not part of the input state that the Input System keeps. public class MyValueShiftProcessor : InputProcessor<float> { [Tooltip(\"Number to add to incoming values.\")] public float valueShift = 0; public override float Process(float value, InputControl control) { return value + valueShift; } } Now, you need to tell the Input System about your Processor. Call InputSystem.RegisterProcessor in your initialization code. You can do so locally within the Processor class like this: #if UNITY_EDITOR [InitializeOnLoad] #endif public class MyValueShiftProcessor : InputProcessor<float> { #if UNITY_EDITOR static MyValueShiftProcessor() { Initialize(); } #endif [RuntimeInitializeOnLoadMethod(RuntimeInitializeLoadType.BeforeSceneLoad)] static void Initialize() { InputSystem.RegisterProcessor<MyValueShiftProcessor>(); } //... } Your new Processor is now available in the Input Action Asset Editor window, and you can also add it in code like this: var action = new InputAction(processors: \"myvalueshift(valueShift=2.3)\"); If you want to customize the UI for editing your Processor, create a custom InputParameterEditor class for it: // No registration is necessary for an InputParameterEditor. // The system will automatically find subclasses based on the // <..> type parameter. #if UNITY_EDITOR public class MyValueShiftProcessorEditor : InputParameterEditor<MyValueShiftProcessor> { private GUIContent m_SliderLabel = new GUIContent(\"Shift By\"); public override void OnEnable() { // Put initialization code here. Use 'target' to refer // to the instance of MyValueShiftProcessor that is being // edited. } public override void OnGUI() { // Define your custom UI here using EditorGUILayout. target.valueShift = EditorGUILayout.Slider(m_SliderLabel, target.valueShift, 0, 10); } } #endif"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Sensors.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Sensors.html",
    "title": "Sensor support | FSM Unity Framework",
    "keywords": "Sensor support Sampling Frequency Accelerometer Gyroscope GravitySensor AttitudeSensor LinearAccelerationSensor MagneticFieldSensor LightSensor PressureSensor ProximitySensor HumiditySensor AmbientTemperatureSensor StepCounter Sensors are InputDevices that measure environmental characteristics of the device that the content is running on. Unity currently supports sensors on iOS and Android. Android supports a wider range of sensors than iOS. Note: To test your app on iOS or Android in the editor with sensor input from your mobile device, you can use the Unity Remote as described here. This currently supports Accelerometer, Gyroscope, GravitySensor, AttitudeSensor, and LinearAccelerationSensor. To determine whether a particular sensor is present, you can use its .current getter. // Determine if a Gyroscope sensor device is present. if (Gyroscope.current != null) Debug.Log(\"Gyroscope present\"); Unlike other devices, sensors are disabled by default. To enable a sensor, call InputSystem.EnableDevice()). InputSystem.EnableDevice(Gyroscope.current); To disable a sensor, call InputSystem.DisableDevice(). InputSystem.DisableDevice(Gyroscope.current); To check whether a sensor is currently enabled, use InputDevice.enabled. if (Gyroscope.current.enabled) Debug.Log(\"Gyroscope is enabled\"); Each sensor Device implements a single Control which represents the data read by the sensor. The following sensors are available: Device Android iOS WebGL Control Type Accelerometer Yes Yes Yes(1) acceleration Vector3Control Gyroscope Yes Yes Yes(1) angularVelocity Vector3Control GravitySensor Yes Yes Yes(1) gravity Vector3Control AttitudeSensor Yes Yes Yes(1) attitude QuaternionControl LinearAccelerationSensor Yes Yes Yes(1) acceleration Vector3Control MagneticFieldSensor Yes No No magneticField Vector3Control LightSensor Yes No No lightLevel AxisControl PressureSensor Yes No No atmosphericPressure AxisControl ProximitySensor Yes No No distance AxisControl HumiditySensor Yes No No relativeHumidity AxisControl AmbientTemperatureSensor Yes No No ambientTemperature AxisControl StepCounter Yes Yes No stepCounter IntegerControl Notes: Sensor support for WebGL on Android and iOS devices is available in Unity 2021.2 Sampling frequency Sensors sample continuously at a set interval. You can set or query the sampling frequency for each sensor using the samplingFrequency property. The frequency is expressed in Hertz (number of samples per second). // Get sampling frequency of gyro. var frequency = Gyroscope.current.samplingFrequency; // Set sampling frequency of gyro to sample 16 times per second. Gyroscope.current.samplingFrequency = 16; Accelerometer Use the accelerometer to measure the acceleration of a device. This is useful to control content by moving a device around. It reports the acceleration measured on a device both due to moving the device around, and due to gravity pulling the device down. You can use GravitySensor and LinearAccelerationSensor to get separate values for these. Values are affected by the Compensate Orientation setting. Gyroscope Use the gyroscope to measure the angular velocity of a device. This is useful to control content by rotating a device. Values are affected by the Compensate Orientation setting. GravitySensor Use the gravity sensor to determine the direction of the gravity vector relative to a device. This is useful to control content by device orientation. This is usually derived from a hardware Accelerometer, by subtracting the effect of linear acceleration (see LinearAccelerationSensor). Values are affected by the Compensate Orientation setting. AttitudeSensor Use the attitude sensor to determine the orientation of a device. This is useful to control content by rotating a device. Values are affected by the Compensate Orientation setting. LinearAccelerationSensor Use the accelerometer to measure the acceleration of a device. This is useful to control content by moving a device around. Linear acceleration is the acceleration of a device unaffected by gravity. This is usually derived from a hardware Accelerometer, by subtracting the effect of gravity (see GravitySensor). Values are affected by the Compensate Orientation setting. MagneticFieldSensor This Input Device represents the magnetic field that affects the device which is running the content. Values are in micro-Tesla (μT) and measure the ambient magnetic field in the X, Y, and Z axis. LightSensor This Input Device represents the ambient light measured by the device which is running the content. Value is in SI lux units. PressureSensor This Input Device represents the atmospheric pressure measured by the device which is running the content. Value is in in hPa (millibar). ProximitySensor This Input Device measures how close the device which is running the content is to the user. Phones typically use the proximity sensor to determine if the user is holding the phone to their ear or not. Values represent distance measured in centimeters. NOTE: The Samsung devices' proximity sensor is only enabled during calls and not when using speakerphone or Bluetooth earphones. This means the lock screen function won't work, allowing the user to use the display during the call. It is important to note that the proximity sensor only works during non-speakerphone or non-Bluetooth calls, as it is designed to prevent accidental touches during calls. However, the proximity sensor can work slightly differently on different Samsung phones. HumiditySensor This Input Device represents the ambient air humidity measured by the device which is running the content. Values represent the relative ambient air humidity in percent. AmbientTemperatureSensor This Input Device represents the ambient air temperature measured by the device which is running the content. Values represent temperature in Celsius degrees. StepCounter This Input Device represents the user's footstep count as measured by the device which is running the content. NOTE: To access the pedometer on iOS/tvOS devices, you need to enable the Motion Usage setting in the Input Settings."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Settings.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Settings.html",
    "title": "Input settings | FSM Unity Framework",
    "keywords": "Input settings Update Mode Background Behavior Filter Noise on .current Compensate orientation Default value properties Supported Devices Platform-specific Settings Play Mode Input Behavior To configure the Input System individually for each project, go to Edit > Project Settings… > Input System Package from Unity's main menu. The Input System stores input settings in Assets. If your Project doesn't contain an input settings Asset, click Create settings asset in the Settings window to create one. If your Project contains multiple settings Assets, use the gear menu in the top-right corner of the window to choose which one to use. You can also use this menu to create additional settings Assets. Note: Unity saves changes to these settings when you save the Project. This page describes each input setting in detail. Update Mode This is a fundamental setting that determines when the Input System processes input. The Input System processes input in one of three distinct ways: Type Description Process Events In Dynamic Update The Input System processes events at irregular intervals determined by the current framerate. Process Events In Fixed Update The Input System processes events at fixed-length intervals. This corresponds to how MonoBehaviour.FixedUpdate operates. The length of each interval is determined by Time.fixedDeltaTime. Process Events Manually The Input System does not process events automatically. Instead, it processes them whenever you call InputSystem.Update(). Note: The system performs two additional types of updates in the form of InputUpdateType.BeforeRender (late update for XR tracking Devices) and InputUpdateType.Editor (for EditorWindows). Neither of these update types change how the application consumes input. Background Behavior Determines how application focus is handled. That is, what happens when focus is lost or gained and how input behaves while the application is not in the foreground. This setting is only relevant when \"Run In Background\" is enabled in the Player Settings for the project. This setting is only supported on some platforms. On platforms such as Android and iOS, the game will not be run while the application is not in the foreground. Note that in the editor, \"Run In Background\" is considered to always be enabled as the player loop will be kept running regardless of whether a Game View is focused or not. Also, in development players on desktop platforms, the setting is force-enabled during the build process. Note: In the editor, Background Behavior is further influenced by Play Mode Input Behavior. See Background and Focus Change Behavior for a detailed breakdown. In particular, which devices are considered as canRunInBackground partly depends on the Play Mode Input Behavior setting. Setting Description Reset And Disable Non Background Devices When focus is lost, perform a soft reset on all Devices that are not marked as canRunInBackground and also subsequently disable them. Does not affect Devices marked as being able to run in the background. When focus is regained, re-enable any Device that has been disabled and also issue a sync request on these Devices in order to update their current state. If a Device is issued a sync request and does not respond to it, soft-reset the Device. This is the default setting. Reset And Disable All Devices When focus is lost, perform a soft reset on all Devices and also subsequently disable them. When focus is regained, re-enable all Devices and also issue a sync request on each Device in order to update it to its current state. If a device does not respond to the sync request, soft-reset it. Ignore Focus Do nothing when focus is lost. When focus is regained, issue a sync request on all Devices. Focus behavior has implications for how Actions behave on focus changes. When a Device is reset, Actions bound to Controls on the device will be cancelled. This ensures, for example, that a user-controlled character in your game doesn't continue to move when focus is lost while the user is pressing one of the W, A, S or D keys. The cancellation happens in such a way that Actions are guaranteed to not trigger. That is, even if an Action is set to trigger on button release, it will not get triggered when a button is down and gets reset by a Device reset. Filter Noise On Current This setting is disabled by default, and it's only relevant for apps that use the .current properties (such as Gamepad.current) in the API. If your app doesn't use these properties, leave this setting disabled. Otherwise, it adds needless overhead. Whenever there is input on a Device, the system make the respective Device .current. For example, if a Gamepad receives new input, Gamepad.current is assigned to that gamepad. Some Devices have noise in their input, and receive input even if nothing is interacting with them. For example, the PS4 DualShock controller generates a constant stream of input because of its built-in gyro. This means that if both an Xbox and a PS4 controller are connected, and the user is using the Xbox controller, the PS4 controller still pushes itself to the front continuously and makes itself current. To counteract this, enable noise filtering. When this setting is enabled and your application receives input, the system determines whether the input comes from a Device that has noisy Controls (InputControl.noisy). If it does, the system also determines whether the given input contains any state changes on a Control that isn't flagged as noisy. If so, that Device becomes current. Otherwise, your application still consumes the input, which is also visible on the Device, but the Device doesn't become current. Note: The system doesn't currently detect most forms of noise, but does detect those on gamepad sticks. This means that if the sticks wiggle a small amount but are still within deadzone limits, the Device still becomes current. This doesn't require actuating the sticks themselves. On most gamepads, there's a small tolerance within which the sticks move when the entire device moves. Compensate Orientation If this setting is enabled, rotation values reported by sensors are rotated around the Z axis as follows: Screen orientation Effect on rotation values ScreenOrientation.Portrait Values remain unchanged ScreenOrientation.PortraitUpsideDown Values rotate by 180 degrees. ScreenOrientation.LandscapeLeft Values rotate by 90 degrees. ScreenOrientation.LandscapeRight Values rotate by 270 degrees. This setting affects the following sensors: Gyroscope GravitySensor AttitudeSensor Accelerometer LinearAccelerationSensor Default value properties Property Description Default Deadzone Min The default minimum value for Stick Deadzone or Axis Deadzone processors when no min value is explicitly set on the processor. Default Deadzone Max The default maximum value for Stick Deadzone or Axis Deadzone processors when no max value is explicitly set on the processor. Default Button Press Point The default press point for Button Controls, and for various Interactions. For button Controls which have analog physics inputs (such as triggers on a gamepad), this configures how far they need to be held down for the system to consider them pressed. Default Tap Time Default duration for Tap and MultiTap Interactions. Also used by by touchscreen Devices to distinguish taps from to new touches. Default Slow Tap Time Default duration for SlowTap Interactions. Default Hold Time Default duration for Hold Interactions. Tap Radius Maximum distance between two finger taps on a touchscreen Device for the system to consider this a tap of the same touch (as opposed to a new touch). Multi Tap Delay Time Default delay between taps for MultiTap Interactions. Also used by touchscreen Devices to count multi-taps (See TouchControl.tapCount). Supported Devices A Project usually supports a known set of input methods. For example, a mobile app might support only touch, and a console application might support only gamepads. A cross-platform application might support gamepads, mouse, and keyboard, but might not require XR Device support. To narrow the options that the Editor UI presents to you, and to avoid creating input Devices and consuming input that your application won't use, you can restrict the set of supported Devices on a per-project basis. If Supported Devices is empty, no restrictions apply, which means that the Input System adds any Device that Unity recognizes and processes input for it. However, if Support Devices contains one or more entries, the Input System only adds Devices that are of one of the listed types. Note: When the Support Devices list changes, the system removes or re-adds Devices as needed. The system always keeps information about what Devices are available for potential, which means that no Device is permanently lost as long as it stays connected. To add Devices to the list, click the Add (+) icon and choose a Device from the menu that appears. Abstract Devices contains common Device abstractions such as \"Keyboard\" and \"Mouse\". Specific Devices contains specific hardware products. Override in Editor In the Editor, you might want to use input Devices that the application doesn't support. For example, you might want to use a tablet in the Editor even if your application only supports gamepads. To force the Editor to add all locally available Devices, even if they're not in the list of Supported Devices, open the Input Debugger (menu: Window > Analysis > Input Debugger), and select Options > Add Devices Not Listed in 'Supported Devices'. This setting is stored as a user setting (that is, other users who open the same Project can't see the setting). Platform-specific settings iOS/tvOS Motion Usage Governs access to the pedometer on the device. If enabled, the Description string supplied in the settings will be added to the application's Info.plist Editor Play Mode Input Behavior Determines how input is handled in the Editor when in play mode. Unlike in players, in the Editor Unity (and thus its input backends) will keep running for as long as the Editor is active regardless of whether a Game View is focused or not. This setting determines how input should behave when focus is not on any Game View – and thus Application.isFocused is false and the player considered to be running in the background. Setting Description Pointers And Keyboards Respect Game View Focus Only Pointer and Keyboard Devices require the Game View to be focused. Other Devices will route their input into the application regardless of Game View focus. This setting essentially routes any input into the game that is, by default, not used to operate the Editor UI. So, Devices such as gamepads will go to the application at all times when in play mode whereas keyboard input, for example, will require explicitly giving focus to a Game View window. This setting is the default. All Devices Respect Game View Focus Focus on a Game View is required for all Devices. When no Game View window is focused, all input goes to the editor and not to the application. This allows other EditorWindows to receive these inputs (from gamepads, for example). All Device Input Always Goes To Game View All editor input is disabled and input is considered to be exclusive to Game Views. Also, Background Behavior is to be taken literally and executed like in players. Meaning, if in a certain situation, a Device is disabled in the player, it will get disabled in the editor as well. This setting most closely aligns player behavior with editor behavior. Be aware, however, that no EditorWindows will be able to see input from Devices (this does not effect IMGUI and UITK input in the Editor in general as they do not consume input from the Input System)."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/SupportedDevices.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/SupportedDevices.html",
    "title": "Supported Input Devices | FSM Unity Framework",
    "keywords": "Supported Input Devices This page lists Input Device types and products that the Input System package supports, and the platforms they're supported on. Generic Support for the following Devices doesn't require specialized support of particular products. Device Windows Mac Linux UWP Android iOS tvOS Xbox(3) PS4(3) Switch(3) WebGL Mouse Yes Yes Yes Yes Yes No No Yes Yes No Yes Keyboard Yes Yes Yes Yes Yes No No Yes Yes No Yes Pen Yes No (1) No Yes Yes Yes No No No No No Touchscreen Yes No No Yes Yes Yes Yes(4) No No No Yes Sensors No No No No Yes Yes No No No No Yes(5) Joystick (2) Yes Yes Yes Yes Yes No No No No No Yes Notes: Tablet support for macOS is coming in Unity 2020.1. Joysticks are supported as generic HIDs (See Other gamepads, joysticks, and racing wheels below). Consoles are supported using separate packages. You need to install these packages in your Project to enable console support. Indirect touches are received from Siri Remote. Sensor support for WebGL on Android and iOS devices is available in Unity 2021.2 Gamepads Device Windows Mac Linux UWP(13) Android iOS(6) tvOS(6) Xbox(7) PS4/PS5(7) Switch(7) WebGL Xbox 360 (4) Yes Yes (3) Yes Yes No No No Yes No No Sometimes (2) Xbox One Yes (1) Yes (3) Yes (1) Yes Yes (1) Yes (6) Yes (6) Yes No No Sometimes (2) PS3/PS4 Yes (5) Yes (5) Yes (5) Yes (5) Yes (5, 8) Yes (5, 6) Yes (5, 6) No Yes No Sometimes (2) PS5 Yes (11) Yes (11) No (11) Yes (11) Yes (9, 11) No (11) No (11) No Yes No Sometimes (2) Switch Yes (10) Yes (10) Yes Yes No No No No No Yes Sometimes (2) MFi (such as SteelSeries) No Sometimes (12) No No No Yes Yes No No No No Notes: The trigger motors on the Xbox One controller are only supported on UWP and Xbox. WebGL support varies between browsers, Devices, and operating systems. XInput controllers on Mac currently require the installation of the Xbox Controller Driver for macOS. This driver only supports only USB connections, and doesn't support wireless dongles. However, the latest generation of Xbox One controllers natively support Bluetooth, and are natively supported on Macs as HIDs without any additional drivers when connected via Bluetooth. This includes any XInput-compatible Device. Unity doesn't support motor rumble and lightbar color over Bluetooth. Unity doesn't support the gyro or accelerometer on PS4/PS5 controllers on platforms other than the PlayStation consoles. Unity also doesn't support the DualShock 4 USB Wireless Adaptor. On UWP only USB connection is supported, motor rumble and lightbar are not working correctly. Unity supports Made for iOS (Mfi) certified controllers on iOS. Xbox One and PS4 controllers are only supported on iOS 13 or higher. Consoles are supported using separate packages. You need to install these packages in your Project to enable console support. Unity supports PS4 controllers on Android devices running Android 10 or higher. Unity supports PS5 controllers on Android devices running Android 12 or higher. Switch Joy-Cons are not currently supported on Windows and Mac. Some of official accessories are supported on Windows and Mac: \"Hori Co HORIPAD for Nintendo Switch\", \"HORI Pokken Tournament DX Pro Pad\", \"HORI Wireless Switch Pad\", \"HORI Real Arcade Pro V Hayabusa in Switch Mode\", \"PowerA NSW Fusion Wired FightPad\", \"PowerA NSW Fusion Pro Controller (USB only)\", \"PDP Wired Fight Pad Pro: Mario\", \"PDP Faceoff Wired Pro Controller for Nintendo Switch\", \"PDP Faceoff Deluxe Wired Pro Controller for Nintendo Switch\", \"PDP Afterglow Wireless Switch Controller\", \"PDP Rockcandy Wired Controller\". PS5 DualSense is supported on Windows and macOS via USB HID, though setting motor rumble and lightbar color when connected over Bluetooth is currently not supported. SteelSeries Nimbus+ supported via HID on macOS. On UWP only USB connection is supported, motor rumble and lightbar are not working correctly. On Android it's expected to be working from Android 12. On iOS/tvOS it's currently recognized as a generic gamepad and most controls do work. To ensure all controller types are detected on UWP, enable the HumanInterfaceDevice setting in UWP Player Settings. WebGL The Input System supports the Standard Gamepad mapping as specified in the W3C Gamepad Specification. It also supports gamepads and joysticks that the browser surfaces without a mapping, but this support is generally limited to detecting the axes and buttons which are present, without any context as to what they mean. This means gamepads and joysticks are generally only useful when the user manually remaps them. The Input System reports these Devices as generic Joysticks. Support varies between browsers, Devices, and operating systems, and further differs for different browser versions, so it's not feasible to provide an up-to-date compatibility list. At the time of this publication (September 2019), Safari, Chrome, Edge, and Firefox all support the gamepad API, but only Chrome reliably maps common gamepads (Xbox and PlayStation controllers) to the W3C Standard Gamepad mapping, which allows the Input System to correctly identify and map controls. Note: WebGL currently doesn't support rumble. Other gamepads, joysticks, and racing wheels The Input System supports any Device which implements the USB HID specification. However, for Devices which don't have specific layouts implemented in the Input System, the system can only surface the information available from the HID descriptor of the Device, which limits how precisely it can describe a control. These Devices often work best when allowing the user to manually remap the controls. If you need to support a specific Device, you can also add your own mapping for it. See documentation on HID for more information."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "Introduction Installation Concepts Workflows Workflow - Direct Workflow - Embedded Actions Workflow - Actions Asset Workflow - PlayerInput Component Using the Input System Actions Input Action Assets Input Bindings Interactions Devices Controls Processors Events Layouts Player Input Component Player Input Manager Component Input settings User Management Supported Input Devices Pointers Touch support Mouse support Pen, tablet, and stylus support Keyboard support Gamepad support Joystick support Sensor support HID support UI support On-screen Controls Editor Features Using Input in the Editor Debugging Input testing How do I...? Architecture Migrating from the old input system Contributing Known Limitations"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Testing.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Testing.html",
    "title": "Input testing | FSM Unity Framework",
    "keywords": "Input testing The Input System has built-in support for writing automated input tests. You can drive input entirely from code, without any dependencies on platform backends and physical hardware devices. The automated input tests you write consider the generated input to be the same as input generated at runtime by actual platform code. Setting up test assemblies To set up a test assembly that uses the Input System's automation framework, follow these steps: In the Packages/manifest.json file of your project, com.unity.inputsystem must be listed in testables. This is necessary for test code that comes with the package to be included with test builds of your project. You can, for example, add this after the dependencies property like so: }, \"testables\" : [ \"com.unity.inputsystem\" ] Create a new assembly definition (menu: Create > Assembly Definition) or go to an assembly definition for a test assembly that you have already created. Add references to nunit.framework.dll, UnityEngine.TestRunner, and UnityEditor.TestRunner (as described in How to create a new test assembly), as well as Unity.InputSystem and Unity.InputSystem.TestFramework for the Input System. Setting up test fixtures Use InputTestFixture to create an isolated version of the Input System for tests. The fixture sets up a blank, default-initialized version of the Input System for each test, and restores the Input System to its original state after the test completes. The default-initialized version has all built-in registrations (such as layout and processors), but doesn't have any pre-existing Input Devices. NOTE: InputTestFixture will not have custom registrations performed from Unity startup code such as [InitializeOnLoad] or [RuntimeInitializeOnLoadMethod]. Layouts needed during tests have to be manually registered as part of the test setup. You can use the fixture as a base class for your own fixture: class MyTests : InputTestFixture { [Test] public void CanPressButtonOnGamepad() { var gamepad = InputSystem.AddDevice<Gamepad>(); Press(gamepad.buttonSouth); } // If you need custom setup and tear-down logic, override the methods inherited // from InputTestFixture. // IMPORTANT: If you use NUnit's [Setup] and [TearDown] attributes on methods in your // test fixture, this will *override* the methods inherited from // InputTestFixture and thus cause them to not get executed. Either // override the methods as illustrated here or call the Setup() and // TearDown() methods of InputTestFixture explicitly. public override void Setup() { base.Setup(); // Add setup code here. } public override void TearDown() { // Add teardown code here. base.TearDown(); } } IMPORTANT: If you do this, do not add a [SetUp] or [TearDown] method. Doing so will cause the methods in InputTestFixture to not be called, thus leading to the test fixture not properly initializing or shutting down. Instead, override the Setup and/or TearDown method inherited from InputTestFixture. Alternatively, you can instantiate it in your fixture: [TestFixture] class MyTestFixture { private InputTestFixture input = new InputTestFixture(); // NOTE: You have to manually call Setup() and TearDown() in this scenario. [SetUp] void Setup() { input.Setup(); } [TearDown] void TearDown() { input.TearDown(); } } This is especially useful when creating a larger setup for game testing using PrebuiltSetup. [PrebuildSetup(\"GameTestPrebuildSetup\")] public class GameTestFixture { public Game game { get; set; } public InputTestFixture input { get; set; } public Mouse mouse { get; set; } public Keyboard keyboard { get; set; } public Touchscreen touchscreen { get; set; } public Gamepad gamepad { get; set; } //... } #if UNITY_EDITOR public class GameTestPrebuildSetup : IPrebuildSetup { public void Setup() { UnityEditor.EditorBuildSettings.scenes = new[] { new UnityEditor.EditorBuildSettingsScene(\"Assets/Scenes/Main.unity\", true) }; } } #endif Note that you do not generally need to clean up any input-related data you set up. This includes devices you add, layouts you registered, InputSettings you modify, and any other alteration to the state of InputSystem. InputTestFixture will automatically throw away the current state of the Input System and restore the state from before the test was started. Writing tests When writing a test, use InputSystem.AddDevice<T>() to add new Devices. [Test] public void PlayerInput_CanInstantiatePlayer_WithSpecificControlScheme() { InputSystem.AddDevice<Gamepad>(); var keyboard = InputSystem.AddDevice<Keyboard>(); var mouse = InputSystem.AddDevice<Mouse>(); var prefab = new GameObject(); prefab.SetActive(false); var prefabPlayerInput = prefab.AddComponent<PlayerInput>(); prefabPlayerInput.actions = InputActionAsset.FromJson(kActions); var player = PlayerInput.Instantiate(prefab, controlScheme: \"Keyboard&Mouse\"); Assert.That(player.devices, Is.EquivalentTo(new InputDevice[] { keyboard, mouse })); Assert.That(player.controlScheme, Is.EqualTo(\"Keyboard&Mouse\")); } To feed input, the easiest way is to use the Press(button), Release(button), PressAndRelease(button), Set(control,value), and Trigger(action) helper methods provided by InputTestFixture. [Test] public void Actions_WhenDisabled_CancelAllStartedInteractions() { var gamepad = InputSystem.AddDevice<Gamepad>(); var action1 = new InputAction(\"action1\", binding: \"<Gamepad>/buttonSouth\", interactions: \"Hold\"); var action2 = new InputAction(\"action2\", binding: \"<Gamepad>/leftStick\"); action1.Enable(); action2.Enable(); Press(gamepad.buttonSouth); Set(gamepad.leftStick, new Vector2(0.123f, 0.234f)); using (var trace = new InputActionTrace()) { trace.SubscribeTo(action1); trace.SubscribeTo(action2); runtime.currentTime = 0.234f; runtime.advanceTimeEachDynamicUpdate = 0; action1.Disable(); action2.Disable(); var actions = trace.ToArray(); Assert.That(actions.Length, Is.EqualTo(2)); //... } } Alternatively, you can use code to feed arbitrary input events into the system, and run arbitrary input updates: [Test] public void PlayerInput_JoiningPlayerThroughButtonPress_WillFailIfDeviceIsNotUsableWithPlayerActions() { var playerPrefab = new GameObject(); playerPrefab.SetActive(false); playerPrefab.AddComponent<PlayerInput>(); playerPrefab.GetComponent<PlayerInput>().actions = InputActionAsset.FromJson(kActions); var manager = new GameObject(); var listener = manager.AddComponent<MessageListener>(); var managerComponent = manager.AddComponent<PlayerInputManager>(); managerComponent.joinBehavior = PlayerJoinBehavior.JoinPlayersWhenButtonIsPressed; managerComponent.playerPrefab = playerPrefab; // Create a Device based on the HID layout with a single button control. const string kLayout = @\" { \"\"name\"\" : \"\"TestDevice\"\", \"\"extend\"\" : \"\"HID\"\", \"\"controls\"\" : [ { \"\"name\"\" : \"\"button\"\", \"\"layout\"\" : \"\"Button\"\" } ] } \"; InputSystem.RegisterLayout(kLayout); var device = InputSystem.AddDevice(\"TestDevice\"); using (StateEvent.From(device, out var eventPtr)) { ((ButtonControl)device[\"button\"]).WriteValueIntoEvent(1f, eventPtr); InputSystem.QueueEvent(eventPtr); InputSystem.Update(); } Assert.That(listener.messages, Is.Empty); Assert.That(PlayerInput.all, Is.Empty); } Note: For reference, you can find the tests for the Input System itself in its GitHub repository."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Touch.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Touch.html",
    "title": "Touch support | FSM Unity Framework",
    "keywords": "Touch support Touch support Touchscreen Device Controls Using touch with Actions EnhancedTouch.Touch Class Touch Simulation Reading all touches Touch support is divided into: low-level support implemented in the Touchscreen class. high-level support implemented in the EnhancedTouch.Touch class. Note: You should not use Touchscreen for polling. If you want to read out touches similar to UnityEngine.Input.touches, see EnhancedTouch. If you read out touch state from Touchscreen directly inside of the Update or FixedUpdate methods, your app will miss changes in touch state. Touch input is supported on Android, iOS, Windows, and the Universal Windows Platform (UWP). Note: To test your app on iOS or Android in the editor with touch input from your mobile device, you can use the Unity Remote as described here. Touchscreen Device At the lowest level, a touch screen is represented by an InputSystem.Touchscreen Device which captures the touch screen's raw state. Touch screens are based on the Pointer layout. To query the touch screen that was last used or last added, use Touchscreen.current. Controls Additional to the Controls inherited from Pointer, touch screen Devices implement the following Controls: Control Type Description primaryTouch TouchControl A touch Control that represents the primary touch of the screen. The primary touch drives the Pointer representation on the Device. touches ReadOnlyArray<TouchControl> An array of touch Controls that represents all the touches on the Device. A touch screen Device consists of multiple TouchControls. Each of these represents a potential finger touching the Device. The primaryTouch Control represents the touch which is currently driving the Pointer representation, and which should be used to interact with the UI. This is usually the first finger that touches the screen. primaryTouch is always identical to one of the entries in the touches array. The touches array contains all the touches that the system can track. This array has a fixed size, regardless of how many touches are currently active. If you need an API that only represents active touches, see the higher-level EnhancedTouch.Touch class. Each TouchControl on the Device, including primaryTouch, is made up of the following child Controls: Control Type Description position Vector2Control Absolute position on the touch surface. delta Vector2Control The difference in position since the last frame. startPosition Vector2Control The position where the finger first touched the surface. startTime DoubleControl The time when the finger first touched the surface. press ButtonControl Whether the finger is pressed down. pressure AxisControl Normalized pressure with which the finger is currently pressed while in contact with the pointer surface. radius Vector2Control The size of the area where the finger touches the surface. touchId IntegerControl The ID of the touch. This allows you to distinguish individual touches. phase TouchPhaseControl A Control that reports the current TouchPhase of the touch. tap ButtonControl A button Control that reports whether the OS recognizes a tap gesture from this touch. tapCount IntegerControl Reports the number of consecutive tap reports from the OS. You can use this to detect double- and multi-tap gestures. Using touch with Actions You can use touch input with Actions, like any other Pointer Device. To do this, bind to the pointer Controls, like <Pointer>/press or <Pointer>/delta. This gets input from the primary touch, and any other non-touch pointer Devices. However, if you want to get input from multiple touches in your Action, you can bind to individual touches by using Bindings like <Touchscreen>/touch3/press. Alternatively, use a wildcard Binding to bind one Action to all touches: <Touchscreen>/touch*/press. If you bind a single Action to input from multiple touches, you should set the Action type to pass-through so the Action gets callbacks for each touch, instead of just one. EnhancedTouch.Touch Class The EnhancedTouch.Touch class provides a polling API for touches similar to UnityEngine.Input.touches. You can use it to query touches on a frame-by-frame basis. Because the API comes with a certain overhead due to having to record touches as they happen, you must explicitly enable it. To do this, call EnhancedTouchSupport.Enable(): using UnityEngine.InputSystem.EnhancedTouch; // ... // Can be called from MonoBehaviour.Awake(), for example. Also from any // RuntimeInitializeOnLoadMethod code. EnhancedTouchSupport.Enable(); Note: Touchscreen does not require EnhancedTouchSupport to be enabled. You only need to call EnhancedTouchSupport.Enable() if you want to use the EnhancedTouch.Touch API. The EnhancedTouch.Touch API is designed to provide access to touch information along two dimensions: By finger: Each finger is defined as the Nth contact source on a Touchscreen. You can use Touch.activeFingers to get an array of all currently active fingers. By touch: Each touch is a single finger contact with at least a beginning point (PointerPhase.Began) and an endpoint (PointerPhase.Ended or PointerPhase.Cancelled). Between those two points, an arbitrary number of PointerPhase.Moved and/or PointerPhase.Stationary records exist. All records in a touch have the same touchId. You can use Touch.activeTouches to get an array of all currently active touches. This lets you track how a specific touch moves over the screen, which is useful if you want to implement recognition of specific gestures. See EnhancedTouch.Touch API documentation for more details. Note: The Touch and Finger APIs don't generate GC garbage. The bulk of the data is stored in unmanaged memory that is indexed by wrapper structs. All arrays are pre-allocated. Touch Simulation Touch input can be simulated from input on other kinds of Pointer devices such as Mouse and Pen devices. To enable this, you can either add the TouchSimulation MonoBehaviour to a GameObject in your scene or simply call TouchSimulation.Enable somewhere in your startup code. void OnEnable() { TouchSimulation.Enable(); } In the editor, you can also enable touch simulation by toggling \"Simulate Touch Input From Mouse or Pen\" on in the \"Options\" dropdown of the Input Debugger. TouchSimulation will add a Touchscreen device and automatically mirror input on any Pointer device to the virtual touchscreen device. Reading all touches To get all current touches from the touchscreen, use EnhancedTouch.Touch.activeTouches, as in this example: using Touch = UnityEngine.InputSystem.EnhancedTouch.Touch; public void Update() { foreach (var touch in Touch.activeTouches) Debug.Log($\"{touch.touchId}: {touch.screenPosition},{touch.phase}\"); } Note: You must first enable enhanced touch support by calling InputSystem.EnhancedTouch.Enable(). You can also use the lower-level Touchscreen.current.touches API."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UISupport.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UISupport.html",
    "title": "UI support | FSM Unity Framework",
    "keywords": "UI support Setting up UI Input How the bindings work Pointer-type input Navigation-type input Tracked-type input Multiplayer UIs Virtual mouse cursor control UI and game input UI Toolkit support You can use the Input System package to control any in-game UI created with the Unity UI package. The integration between the Input System and the UI system is handled by the InputSystemUIInputModule component. Note The Input System package does not support IMGUI. If you have OnGUI methods in your player code (Editor code is unaffected), Unity does not receive any input events in those methods when the Active Input Handling Player Setting is set to Input System Package. To restore functionality you can change the setting to Both, but this means that Unity processes the input twice. Setting up UI input The InputSystemUIInputModule component acts as a drop-in replacement for the StandaloneInputModule component that the Unity UI package. InputSystemUIInputModule provides the same functionality as StandaloneInputModule, but it uses the Input System instead of the legacy Input Manager to drive UI input. If you have a StandaloneInputModule component on a GameObject, and the Input System is installed, Unity shows a button in the Inspector offering to automatically replace it with a InputSystemUIInputModule for you. The InputSystemUIInputModule is pre-configured to use default Input Actions to drive the UI, but you can override that configuration to suit your needs. You can use the following properties to configure InputSystemUIInputModule: Property Description Move Repeat Delay The initial delay (in seconds) between generating an initial IMoveHandler.OnMove navigation event and generating repeated navigation events when the Move Action stays actuated. Move Repeat Rate The interval (in seconds) between generating repeat navigation events when the Move Action stays actuated. Note that this is capped by the frame rate; there will not be more than one move repeat event each frame so if the frame rate dips below the repeat rate, the effective repeat rate will be lower than this setting. Actions Asset An Input Action Asset containing all the Actions to control the UI. You can choose which Actions in the Asset correspond to which UI inputs using the following properties. By default, this references a built-in Asset named DefaultInputActions, which contains common default Actions for driving UI. If you want to set up your own Actions, create a custom Input Action Asset and assign it here. When you assign a new Asset reference to this field in the Inspector, the Editor attempts to automatically map Actions to UI inputs based on common naming conventions. Deselect on Background Click By default, when the pointer is clicked and does not hit any GameObject, the current selection is cleared. This, however, can get in the way of keyboard and gamepad navigation which will want to work off the currently selected object. To prevent automatic deselection, set this property to false. Pointer Behavior How to deal with multiple pointers feeding input into the UI. See pointer-type input. Cursor Lock Behavior Controls the origin point of UI raycasts when the cursor is locked. You can use the following properties to map Actions from the chosen Actions Asset to UI input Actions. In the Inspector, these appear as foldout lists that contain all the Actions in the Asset: Property Description Point An Action that delivers a 2D screen position. Use as a cursor for pointing at UI elements to implement mouse-style UI interactions. See pointer-type input. Set to PassThrough Action type and Vector2 value type. Left Click An Action that maps to the primary cursor button used to interact with UI. See pointer-type input. Set to PassThrough Action type and Button value type. Middle Click An Action that maps to the middle cursor button used to interact with UI. See pointer-type input. Set to PassThrough Action type and Button value type. Right Click An Action that maps to the secondary cursor button used to interact with UI. See pointer-type input. Set to PassThrough Action type and Button value type. Scroll Wheel An Action that delivers gesture input to allow scrolling in the UI. See pointer-type input. Set to PassThrough Action type and Vector2 value type. Move An Action that delivers a 2D vector used to select the currently active UI selectable. This allows a gamepad or arrow-key style navigation of the UI. See navigation-type input. Set to PassThrough Action type and Vector2 value type. Submit An Action to engage with or \"click\" the currently selected UI selectable. See navigation-type input. Set to Button Action type. Cancel An Action to exit any interaction with the currently selected UI selectable. See navigation-type input. Set to Button Action type. Tracked Device Position An Action that delivers a 3D position of one or multiple spatial tracking devices, such as XR hand controllers. In combination with Tracked Device Orientation, this allows XR-style UI interactions by pointing at UI selectables in space. See tracked-type input. Set to PassThrough Action type and Vector3 value type. Tracked Device Orientation An Action that delivers a Quaternion representing the rotation of one or multiple spatial tracking devices, such as XR hand controllers. In combination with Tracked Device Position, this allows XR-style UI interactions by pointing at UI selectables in space. See tracked-type input. Set to PassThrough Action type and Quaternion value type. How the bindings work The UI input module can deal with three different types of input: pointer-type input, navigation-type input, and tracked-type input. For each of these types of input, input is sourced and combined from a specific set of Actions as detailed below. Pointer-type input To the UI, a pointer is a position from which clicks and scrolls can be triggered to interact with UI elements at the pointer's position. Pointer-type input is sourced from point, leftClick, rightClick, middleClick, and scrollWheel. Note The UI input module does not have an association between pointers and cursors. In general, the UI is oblivious to whether a cursor exists for a particular pointer. However, for mouse and pen input, the UI input module will respect Cusor.lockState and pin the pointer position at (-1,-1) whenever the cursor is locked. This behavior can be changed through the Cursor Lock Behavior property of the InputSystemUIInputModule. Multiple pointer Devices may feed input into a single UI input module. Also, in the case of Touchscreen, a single Device can have the ability to have multiple concurrent pointers (each finger contact is one pointer). Important Because multiple pointer Devices can feed into the same set of Actions, it is important to set the action type to PassThrough. This ensures that no filtering is applied to input on these actions and that instead every input is relayed as is. From the perspective of InputSystemUIInputModule, each InputDevice that has one or more controls bound to one of the pointer-type actions is considered a unique pointer. Also, for each Touchscreen devices, each separate TouchControl that has one or more of its controls bound to the those actions is considered its own unique pointer as well. Each pointer receives a unique pointerId which generally corresponds to the deviceId of the pointer. However, for touch, this will be a combination of deviceId and touchId. Use ExtendedPointerEventData.touchId to find the ID for a touch event. You can influence how the input module deals with concurrent input from multiple pointers using the Pointer Behavior setting. Pointer Behavior Description Single Mouse or Pen But Multi Touch And Track Behaves like Single Unified Pointer for all input that is not classified as touch or tracked input, and behaves like All Pointers As Is for tracked and touch input. If concurrent input is received on a Mouse and Pen, for example, the input of both is fed into the same UI pointer instance. The position input of one will overwrite the position of the other. Note that when input is received from touch or tracked devices, the single unified pointer for mice and pens is removed including IPointerExit events being sent in case the mouse/pen cursor is currently hovering over objects. This is the default behavior. Single Unified Pointer All pointer input is unified such that there is only ever a single pointer. This includes touch and tracked input. This means, for example, that regardless how many devices feed input into Point, only the last such input in a frame will take effect and become the current UI pointer's position. All Pointers As Is The UI input module will not unify any pointer input. Any device, including touch and tracked devices that feed input pointer-type actions, will be its own pointer (or multiple pointers for touch input). Note: This might mean that there will be an arbitrary number of pointers in the UI, and several objects might be pointed at concurrently. Note If you bind a device to a pointer-type action such as Left Click without also binding it to Point, the UI input module will recognize the device as not being able to point and try to route its input into that of another pointer. For example, if you bind Left Click to the Space key and Point to the position of the mouse, then pressing the space bar will result in a left click at the current position of the mouse. For pointer-type input (as well as for [tracked-type input](#tracked-type input)), InputSystemUIInputModule will send ExtendedPointerEventData instances which are an extended version of the base PointerEventData. These events contain additional data such as the device and pointer type which the event has been generated from. Navigation-type input Navigation-type input controls the current selection based on motion read from the move action. Additionally, input from submit will trigger ISubmitHandler on the currently selected object and cancel will trigger ICancelHandler on it. Unlike with [pointer-type](#pointer-type input), where multiple pointer inputs may exist concurrently (think two touches or left- and right-hand tracked input), navigation-type input does not have multiple concurrent instances. In other words, only a single move vector and a single submit and cancel input will be processed by the UI module each frame. However, these inputs need not necessarily come from one single Device always. Arbitrary many inputs can be bound to the respective actions. Important While, move should be set to PassThrough Action type, it is important that submit and cancel be set to the Button Action type. Navigation input is non-positional, that is, unlike with pointer-type input, there is no screen position associcated with these actions. Rather, navigation actions always operate on the current selection. Tracked-type input Input from tracked devices such as XR controllers and HMDs essentially behaves like pointer-type input. The main difference is that the world-space device position and orientation sourced from trackedDevicePosition and trackedDeviceOrientation is translated into a screen-space position via raycasting. Important Because multiple tracked Devices can feed into the same set of Actions, it is important to set the action type to PassThrough. This ensures that no filtering is applied to input on these actions and that instead every input is relayed as is. For this raycasting to work, you need to add TrackedDeviceRaycaster to the GameObject that has the UI's Canvas component. This GameObject will usually have a GraphicRaycaster component which, however, only works for 2D screen-space raycasting. You can put TrackedDeviceRaycaster alongside GraphicRaycaster and both can be enabled at the same time without advserse effect. Clicks on tracked devices do not differ from other pointer-type input. Therefore, actions such as Left Click work for tracked devices just like they work for other pointers. Multiplayer UIs The Input System can also handle multiple separate UI instances on the screen controlled separately from different input Bindings. This is useful if you want to have multiple local players share a single screen with different controllers, so that every player can control their own UI instance. To allow this, you need to replace the EventSystem component from Unity with the Input System's MultiplayerEventSystem component. Unlike the EventSystem component, you can have multiple MultiplayerEventSystems active in the Scene at the same time. That way, you can have multiple players, each with their own InputSystemUIInputModule and MultiplayerEventSystem components, and each player can have their own set of Actions driving their own UI instance. If you are using the PlayerInput component, you can also set up PlayerInput to automatically configure the player's InputSystemUIInputModule to use the player's Actions. See the documentation on PlayerInput to learn how. The properties of the MultiplayerEventSystem component are identical with those from the Event System. Additionally, the MultiplayerEventSystem component adds a playerRoot property, which you can set to a GameObject that contains all the UI selectables this event system should handle in its hierarchy. Mouse input that this event system processes then ignores any UI selectables which are not on any GameObject in the Hierarchy under Player Root. Virtual mouse cursor control Note While pointer input generated from a VirtualMouseInput component is received in UI Toolkit, the VirtualMouseInput component is not officially supported for use with UI Toolkit. At the moment, it only works in combination with the Unity UI system. If your application uses gamepads and joysticks as an input, you can use the navigation Actions to operate the UI. However, it usually involves extra work to make the UI work well with navigation. An alternative way to operate the UI is to allow gamepads and joysticks to drive the cursor from a \"virtual mouse cursor\". Tip To see an example of a VirtualMouseInput setup, see the Gamepad Mouse Cursor sample included with the Input System package. To set this up, follow these steps: Create a UI GameObject with an Image component. This represents a software mouse cursor. Then, add it as a child of the Canvas that the cursor should operate on. Set the anchor position of the GameObject's RectTransform to the bottom left. Make it the last child of the Canvas so that the cursor draws on top of everything else. Add a VirtualMouseInput component to the GameObject. Then, link the Image component to the Cursor Graphic property, and the RectTransform of the cursor GameObject to the Cursor Transform property. If you want the virtual mouse to control the system mouse cursor, set Cursor Mode to Hardware Cursor If Available. In this mode, the Cursor Graphic is hidden when a system Mouse is present and you use Mouse.WarpCursorPosition to move the system mouse cursor instead of the software cursor. The transform linked through Cursor Transform is not updated in that case. To configure the input to drive the virtual mouse, either add bindings on the various actions (such as Stick Action), or enable Use Reference and link existing actions from an .inputactions asset. Important Make sure that the InputSystemUIInputModule on the UI's EventSystem does not receive navigation input from the same devices that feed into VirtualMouseInput. If, for example, VirtualMouseInput is set up to receive input from gamepads, and Move, Submit, and Cancel on InputSystemUIInputModule are also linked to the gamepad, then the UI receives input from the gamepad on two channels. At runtime, the component adds a virtual Mouse device which the InputSystemUIInputModule component picks up. The controls of the Mouse are fed input based on the actions configured on the VirtualMouseInput component. Note that the resulting Mouse input is visible in all code that picks up input from the mouse device. You can therefore use the component for mouse simulation elsewhere, not just with InputSystemUIInputModule. Note Do not set up gamepads and joysticks for navigation input while using VirtualMouseInput. If both VirtualMouseInput and navigation are configured, input is triggered twice: once via the pointer input path, and once via the navigation input path. If you encounter problems such as where buttons are pressed twice, this is likely the problem. UI and game input Note A sample called UI vs Game Input is provided with the package and can be installed from the Unity Package Manager UI in the editor. The sample demonstrates how to deal with a situation where ambiguities arise between inputs for UI and inputs for the game. UI in Unity consumes input through the same mechanisms as game/player code. Right now, there is no mechanism that implicitly ensures that if a certain input – such as a click – is consumed by the UI, it is not also \"consumed\" by the game. This can create ambiguities between, for example, code that responds to UI.Button.onClick and code that responds to InputAction.performed of an Action bound to <Mouse>/leftButton. Whether such ambiguities exist depends on how UIs are used. In the following scenarios, ambiguities are avoided: All interaction is performed through UI elements. A 2D/3D scene is rendered in the background but all interaction is performed through UI events (including those such as 'background' clicks on the Canvas). UI is overlaid over a 2D/3D scene but the UI elements cannot be interacted with directly. UI is overlaid over a 2D/3D scene but there is a clear \"mode\" switch that determines if interaction is picked up by UI or by the game. For example, a first-person game on desktop may employ a cursor lock and direct input to the game while it is engaged whereas it may leave all interaction to the UI while the lock is not engaged. When ambiguities arise, they do so differently for pointer-type and navigation-type. Handling ambiguities for pointer-type input Note Calling EventSystem.IsPointerOverGameObject from within InputAction callbacks such as InputAction.performed will lead to a warning. The UI updates separately after input processing and UI state thus corresponds to that of the last frame/update while input is being processed. Input from pointers (mice, touchscreens, pens) can be ambiguous depending on whether or not the pointer is over a UI element when initiating an interaction. For example, if there is a button on screen, then clicking on the button may lead to a different outcome than clicking outside of the button and within the game scene. If all pointer input is handled via UI events, no ambiguities arise as the UI will implicitly route input to the respective receiver. If, however, input within the UI is handled via UI events and input in the game is handled via Actions, pointer input will by default lead to both being triggered. The easiest way to resolve such ambiguities is to respond to in-game actions by polling from inside MonoBehaviour.Update methods and using EventSystem.IsPointerOverGameObject to find out whether the pointer is over UI or not. Another way is to use EventSystem.RaycastAll to determine if the pointer is currently over UI. Handling ambiguities for navigation-type input Ambiguities for navigation-type Devices such as gamepads and joysticks (but also keyboards) cannot arise the same way that it does for pointers. Instead, your application has to decide explicitly whether to use input for the UI's Move, Submit, and Cancel inputs or for the game. This can be done by either splitting control on a Device or by having an explicit mode switch. Splitting input on a Device is done by simply using certain controls for operating the UI while using others to operate the game. For example, you could use the d-pad on gamepads to operate UI selection while using the sticks for in-game character control. This setup requires adjusting the bindings used by the UI Actions accordingly. An explicit mode switch is implemented by temporarily switching to UI control while suspending in-game Actions. For example, the left trigger on the gamepad could bring up an item selection wheel which then puts the game in a mode where the sticks are controlling UI selection, the A button confirms the selection, and the B button closes the item selection wheel. No ambiguities arise as in-game actions will not respond while the UI is in the \"foreground\". UI Toolkit support As of Unity 2021.2, UI Toolkit is supported as an alternative to the Unity UI system for implementing UIs in players. Input support for both Unity UI and UI Toolkit is based on the same EventSystem and BaseInputModule subsystem. In other words, the same input setup based on InputSystemUIInputModule supports input in either UI solution and nothing extra needs to be done. Internally, UI Toolkit installs an event listener in the form of the PanelEventHandler component which intercepts events that InputSystemUIInputModule sends and translates them into UI Toolkit-specific events that are then routed into the visual tree. If you employ EventSystem.SetUITookitEventSystemOverride, this default mechanism is bypassed. Note XR (tracked-type input) is not yet supported in combination with UI Toolkit. This means that you cannot use devices such as VR controllers to operate interfaces created with UI Toolkit. There are some additional things worth noting: UI Toolkit handles raycasting internally. No separate raycaster component is needed like for uGUI. This means that TrackedDeviceRaycaster does not work together with UI Toolkit. A pointer click and a gamepad submit action are distinct at the event level in UI Toolkit. This means that if you, for example, do button.RegisterCallback<ClickEvent>(_ => ButtonWasClicked()); the handler is not invoked when the button is \"clicked\" with the gamepad (a NavigationSubmitEvent and not a ClickEvent). If, however, you do button.clicked += () => ButtonWasClicked(); the handle is invoked in both cases."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UseInEditor.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UseInEditor.html",
    "title": "Using Input in the Editor | FSM Unity Framework",
    "keywords": "Using Input in the Editor Unlike Unity's old Input Manager, you can use the new Input System from within EditorWindow code as well. For example, you can gain access to pen pressure information like this: class MyEditorWindow : EditorWindow { public void OnGUI() { var pen = Pen.current; if (pen != null) { var position = pen.position.ReadValue(); var pressure = pen.pressure.ReadValue(); //... } } } This encompasses all code called from OnGUI() methods, which means that you can also use the Input System in property drawers, Inspectors, and other similar places. Note: Unity doesn't support Actions in Edit mode. Coordinate System The coordinate system differs between EditorWindow code and UnityEngine.Screen. EditorWindow code has its origin in the upper-left corner, with Y down. UnityEngine.Screen has it in the bottom-left corner, with Y up. The Input System compensates for that by automatically converting coordinates depending on whether you call it from your application or from Editor code. In other words, calling Mouse.current.position.ReadValue() from inside EditorWindow code returns mouse coordinates in Editor UI coordinates (Y down), and reading the position elsewhere returns it in application screen coordinates (Y up). Internally, an editor-specific Processor called AutoWindowSpace handles this translation."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UserManagement.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/UserManagement.html",
    "title": "User Management | FSM Unity Framework",
    "keywords": "User Management The Input System supports multi-user management through the InputUser class. This comprises both user account management features on platforms that have these capabilities built into them (such as Xbox and PS4), as well as features to manage Device allocations to one or more local users. Note: The user management API is quite low-level in nature. The stock functionality of Player Input Manager component (see Player Input Manager) provides an easier way to set up user management. The API described here is useful when you want more control over user management. In the Input System, each InputUser represents a human interacting with the application. For example, you can have multiple users playing a game together on a single computer or device (local multiplayer), where each user has one or more paired Input Devices. The PlayerInputManager class uses InputUser internally to handle users. Note: In the editor, all InputUser instances are automatically removed when exiting play mode thus also removing any device pairings. In essence, InputUser is considered a player-only API. Device pairing You can use the InputUser.PerformPairingWithDevice method to create a new InputUser instance and pair it with an InputDevice. You can also optionally pass in an existing InputUser instance to pair it with the Device, if you don't want to create a new user instance. To query the Devices paired to a specific InputUser, use InputUser.pairedDevices. To remove the pairing, use InputUser.UnpairDevice or InputUser.UnpairDevices. Initial engagement After you create a user, you can use InputUser.AssociateActionsWithUser to associate Input Actions to it, and use InputUser.ActivateControlScheme to associate and activate a Control Scheme. You can use InputControlScheme.FindControlSchemeForDevice to pick a control scheme that matches the selected Actions and Device: var scheme = InputControlScheme.FindControlSchemeForDevice(user.pairedDevices[0], user.actions.controlsSchemes); if (scheme != null) user.ActivateControlScheme(scheme); When you activate a Control Scheme, the Input System automatically switches the active Binding mask for the user's Actions to that Control Scheme. Loss of Device If paired Input Devices disconnect during the session, the system notifies the InputUser class. It still keeps track of the Device, and automatically re-pairs the Device if it becomes available again. To get notifications about these changes, subscribe to the InputUser.onChange event. Debugging Check the debugger documentation to learn how to debug active users."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-ActionsAsset.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-ActionsAsset.html",
    "title": "Workflow Overview - Using an Actions Asset | FSM Unity Framework",
    "keywords": "Workflow Overview - Using an Actions Asset The Actions Asset provides a way to define, group, and manage sets of actions as data stored in an asset, instead of defining them directly in your code. Using an Actions Asset allows you to keep the data that defines your actions separate from the GameObjects which should respond to the actions. This provides a further level of abstraction and organization, compared with embedding action definitions directly in your code. The Actions Asset is useful because it means all your action definitions are stored as a single asset file, separate from your scripts and prefabs. This can make it simpler to manage conceptually, and can help in practical situations where different people in your team might work on different parts of the project at the same time. Action Assets also provide other benefits over embedded actions in scripts, such as the ability to group actions into Action Maps and Control Schemes. Action Maps are a way to group related actions together, where each map relates to a different situation. For example, your game might involve driving vehicles and navigating on foot, and may have in-game menus. In this example, it would make sense to have three different action maps for each of these three situations, and your game code would switch between them as appropriate. The actions grouped into the \"driving\" action map might be called \"steer\", \"accelerate\", \"brake\", \"handbrake\", etc, whereas the actions grouped into the \"on foot\" action map might be \"move\", \"jump\", \"crouch\", \"use\", etc. The Control Schemes, also defined in an Action Asset, allows you to specify which bindings belong to the control schemes you define. You might have one control scheme which is \"Joypad\", and another control scheme which is \"Keyboard and Mouse\". This allows you to determine which control scheme the user is currently using, so your game can respond to the user accordingly. This feature is often used to adapt the in-game UI to show the correct keys or buttons in on-screen prompts. Accessing your Actions Asset from code When you use an Actions Asset, there are two distinct ways to access it from your code. You can either: Use an inspector reference to the Actions Asset, or Generate a C# class that wraps your Actions Asset. Your choice affects how you access your actions from code. With an inspector reference to your Actions Asset, you must read the actions by name using strings. If you use the Generate C# class feature, Unity generates an accompanying class as a new .cs script asset, which acts as a wrapper for your actions. You can then create an instance of the generated wrapper class in your code and directly use its API members which are named after the names of the actions that you configured. Both workflow options are described below using the same example, so you can see the difference. Referencing the Actions Asset in the inspector To use your Actions Asset through an inspector reference: Create a public InputActionsAsset field in your script. Assign the reference in the inspector. Access the Actions in your script by name, using strings. This string-based access is demonstrated in the example below. using UnityEngine; using UnityEngine.InputSystem; public class ExampleScript : MonoBehaviour { // assign the actions asset to this field in the inspector: public InputActionAsset actions; // private field to store move action reference private InputAction moveAction; void Awake() { // find the \"move\" action, and keep the reference to it, for use in Update moveAction = actions.FindActionMap(\"gameplay\").FindAction(\"move\"); // for the \"jump\" action, we add a callback method for when it is performed actions.FindActionMap(\"gameplay\").FindAction(\"jump\").performed += OnJump; } void Update() { // our update loop polls the \"move\" action value each frame Vector2 moveVector = moveAction.ReadValue<Vector2>(); } private void OnJump(InputAction.CallbackContext context) { // this is the \"jump\" action callback method Debug.Log(\"Jump!\"); } void OnEnable() { actions.FindActionMap(\"gameplay\").Enable(); } void OnDisable() { actions.FindActionMap(\"gameplay\").Disable(); } } Note: In the example above the reference to the \"move\" action is stored in a variable after it is found, to avoid accessing it by string every frame, which would be bad for performance. Referencing the Actions Asset through a C# wrapper To use your Actions Asset through a C# wrapper: Select your Actions Asset in the project window In the Inspector, enable Generate C# Class and select Apply. You should see a C# asset with the same name as your Actions Asset in your project window. Create an instance of your Actions C# class in your script. Access the Actions in your script by using the API of your Actions C# class. For example: using UnityEngine; using UnityEngine.InputSystem; public class DocsExampleActionsAssetCsWrapper : MonoBehaviour { // this field will contain the actions wrapper instance ExampleActions actions; void Awake() { // instantiate the actions wrapper class actions = new ExampleActions(); // for the \"jump\" action, we add a callback method for when it is performed actions.gameplay.jump.performed += OnJump; } void Update() { // our update loop polls the \"move\" action value each frame Vector2 moveVector = actions.gameplay.move.ReadValue<Vector2>(); } private void OnJump(InputAction.CallbackContext context) { // this is the \"jump\" action callback method Debug.Log(\"Jump!\"); } void OnEnable() { actions.gameplay.Enable(); } void OnDisable() { actions.gameplay.Disable(); } } Whether you use the C# wrapper option, or the inspector reference option, using an Action Asset like this gives you the ability to organize and edit your actions in the Actions Window. It is more flexible than using embedded actions, and directly reading device states, and is generally a good solution for many projects. However, you can also add one more step of abstraction using the Player Input component, to set up calls to methods based on your Action definitions. Note Because Action Assets can be used in these two different ways (by reference, or by C# wrapper), the code samples used throughout this documentation also vary in which way they are written. Some code samples might use a reference and strings to identify actions, and others might use the C# wrapper method. See also: Using Action Assets Generating a C# Class that wraps your actions"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-Direct.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-Direct.html",
    "title": "Workflow Overview - Directly Reading Device States | FSM Unity Framework",
    "keywords": "Workflow Overview - Directly Reading Device States This is the simplest and most direct workflow, but the least flexible. It’s useful if you want a quick implementation with one type of device. It might not be the best choice if you want to provide your users with multiple types of input or if you want to target multiple platforms. You can directly read the values from connected devices by referring to the device’s controls and reading the values they are currently generating, using code like this: using UnityEngine; using UnityEngine.InputSystem; public class MyPlayerScript : MonoBehaviour { void Update() { var gamepad = Gamepad.current; if (gamepad == null) { return; // No gamepad connected. } if (gamepad.rightTrigger.wasPressedThisFrame) { // 'Use' code here } Vector2 move = gamepad.leftStick.ReadValue(); { // 'Move' code here } } } The example above reads values directly from the right trigger, and the left stick, of the currently connected gamepad. It does not use the input system’s \"Action\" class, and instead the conceptual actions in your game or app, such as \"move\" and \"use\", are implicitly defined by what your code does in response to the input. You can use the same approach for other Device types such as the keyboard or mouse. This is often the fastest way to set up some code which responds to input, but it is the least flexible because there is no abstraction between your code and the values generated by a specific device. If you choose to use this technique: You won’t benefit from Unity’s management of actions and interactions. It is harder to make your game or app work with multiple types of input device. Your input bindings are hard-coded in your script, so any changes to bindings require changes to the code. It is harder to allow the user to remap their own controls to different actions at run time. You can find an example of this workflow in the sample projects included with the input system package. To find it, in the Project window, look in Assets > Samples > SimpleDemo and open the scene: SimpleDemo_UsingState. See Supported Devices for more information about devices supported by the input system, and the API to read their states. For more a more flexible workflow, you should use embedded actions or define your actions in an action asset, explained in the following pages."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-Embedded.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-Embedded.html",
    "title": "Workflow Overview - Using Embedded Actions | FSM Unity Framework",
    "keywords": "Workflow Overview - Using Embedded Actions You can use the InputAction class in your script to define actions in your script. This adds a layer of abstraction between your actual action code or methods, and the bindings to specific device controls. This means that instead of directly reading device states, you do not specify explicitly which controls (such as a gamepad trigger or stick) should do what in your code. Instead you create Actions, bind them to controls, and respond to the states or values from your Actions in your code. When you make a public InputAction field in a MonoBehaviour script, it displays in the inspector as a configurable field. The configurable field UI allows you to create a binding for the action. For example, here are two Actions defined using the InputAction class in a script: using UnityEngine; using UnityEngine.InputSystem; public class ExampleScript : MonoBehaviour { public InputAction move; public InputAction jump; } In the image below, you can see the actions displayed in the inspector. In this example they have been configured so they are bound to Gamepad controls. The InputAction class provides a way to bind interactions from a device’s controls to named actions in the inspector. When you bind actions to controls from a device in the inspector, you can then design your script to respond when the actions are performed without hard-coding references to specific devices in your script. This layer of abstraction provides you with the flexibility to modify or add multiple bindings without needing to change your code. To read values from your Actions, you must first enable the action, and then either repeatedly poll the action in your game loop, or add event handlers to the action. You must also disable the action when you no longer want the input to trigger event handlers. So, use actions such as those shown above in the small code sample, you would use a script like this: using UnityEngine; using UnityEngine.InputSystem; // Using embedded actions with callbacks or reading values each frame. public class ExampleScript : MonoBehaviour { // these embedded actions are configurable in the inspector: public InputAction moveAction; public InputAction jumpAction; public void Awake() { // assign a callback for the \"jump\" action. jumpAction.performed += ctx => { OnJump(ctx); }; } public void Update() { // read the value for the \"move\" action each frame. Vector2 moveAmount = moveAction.ReadValue<Vector2>(); } public void OnJump(InputAction.CallbackContext context) { // jump code goes here. } // the actions must be enabled and disabled // when the GameObject is enabled or disabled public void OnEnable() { moveAction.Enable(); jumpAction.Enable(); } public void OnDisable() { moveAction.Disable(); jumpAction.Disable(); } } See Actions for more information about both these techniques. You can find an example of this workflow in the sample projects included with the input system package. To find it, in the Project window, look in Assets > Samples > SimpleDemo and open the scene: SimpleDemo_UsingActions. Using InputActions also makes it easier to implement a system to allow the user to remap their own controls at run time. Using embedded actions like this is more flexible than directly reading device states, but less flexible than using an actions asset."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-PlayerInput.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflow-PlayerInput.html",
    "title": "Workflow Overview - Using an Actions Asset and PlayerInput Component | FSM Unity Framework",
    "keywords": "Workflow Overview - Using an Actions Asset and PlayerInput Component The highest level of abstraction provided by the Input System is when you use the Actions Asset and the Player Input component together. The Player Input component takes a reference to an Actions Asset, and provides a way to make connections between the Actions defined in that asset, and C# methods in your own MonoBehaviour scripts, so that your desired C# methods are called when the user performs an input action. It allows you to set up these connections using a UI in the inspector, instead of requiring you to write code to make those connections (as shown in the previous workflow example), as well as letting you choose how those methods are called. You would typically add the PlayerInput component to the same GameObject as your own MonoBehaviour script which contains the methods that should handle the response to the actions. In the above example image, you can see the PlayerInput component set up to map the \"move\", \"jump\" actions to OnMove and OnJump methods in a script, via Unity Events. This is an example of the script which would provide an implementation of these methods using UnityEngine; using UnityEngine.InputSystem; // This script is designed to have the OnMove and // OnJump methods called by a PlayerInput component public class ExampleScript : MonoBehaviour { Vector2 moveAmount; public void OnMove(InputAction.CallbackContext context) { // read the value for the \"move\" action each event call moveAmount = context.ReadValue<Vector2>(); } public void OnJump(InputAction.CallbackContext context) { // your jump code goes here. } public void Update() { // to use the Vector2 value from the \"move\" action each // frame, use the \"moveAmount\" variable here. } } This workflow has pros and cons when compared to the previous workflow which uses an Action Asset without a PlayerInput component. You can see compared with the previous workflow code example that this method requires less code, because you do not have to reference the Actions Asset or set up the event handler methods in your own script. However it does require more set-up in the Editor, and could make debugging more difficult because the connections between your actions and code are not hard-coded. As with the other workflows described in this section, there is a trade-off between flexibility, simplicity, and speed of implementation. Using the Player Input component provides the flexibility of being able to connect any action to any public method on a GameObject’s component without writing code, and allows you to make modifications to these connections without modifying code. However, although requiring less code, you may find that coding the connections in your own script is simpler and faster than setting up and keeping track of these connections in a PlayerInput component on a GameObject. To get started using this workflow, see the documentation for the Player Input component."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflows.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/Workflows.html",
    "title": "Input System Workflows | FSM Unity Framework",
    "keywords": "Input System Workflows There are multiple ways to use the Input System, and the workflow that’s right for you depends on how quickly you want to get up and running, how flexible you want your input code to be, and whether you prefer to set things up in the Unity Editor, or in code. To understand the different workflows so that you can choose between them, it’s important to first understand the concepts and terms used to describe them. Each of the four main workflows described below offers different levels of flexibility and abstraction. They are listed in an order of abstraction from least to most, where each adds a layer of abstraction, and therefore flexibility, to the previous. Directly Reading Device States Your script explicitly refers to device controls and reads the values directly. Can be the fastest way to set up input for one device, but it is the least flexible workflow. Read more Using Embedded Actions Your script uses the InputAction class directly. The actions display in your script’s inspector, and allow you to configure them in the editor. Read more Using an Actions Asset Your script does not define actions directly. Instead your script references an Input Actions asset which defines your actions. The Input Actions window provides a UI to define, configure, and organize all your Actions into useful groupings. Read more Using an Actions Asset and a PlayerInput component In addition to using an Actions Asset, the PlayerInput component provides a UI in the inspector to connect actions to event handlers in your script, removing the need for any intermediary code between the Input System and your Action Methods. Read more Note Because the Input System has multiple workflows, the code samples used throughout this documentation also vary, demonstrating techniques using various workflows. For example, some code samples may use embedded actions, and others might use an action asset."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Documentation~/index.html",
    "title": "Input System | FSM Unity Framework",
    "keywords": "Input System The Input System allows your users to control your game or app using a device, touch, or gestures. Introduction Unity supports input through two separate systems, one older, and one newer. The older system, which is built-in to the editor, is called the Input Manager. The Input Manager is part of the core Unity platform and is the default, if you do not install this Input System Package. This Input System package is a newer, more flexible system, which allows you to use any kind of Input Device to control your Unity content. It's intended to be a replacement for Unity's classic Input Manager. It iss referred to as \"The Input System Package\", or just \"The Input System\". To use it, you must install it into your project using the Package Manager. During the installation process for the Input System package, the installer offers to automatically deactivate the older built-in system. (Read more) To get started, see the Installation and Workflows sections. For a demo project, see the Warriors demo on GitHub. An example of a set of typical Input Actions for a game, configured in the Editor."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Input System © 2023 Unity Technologies Licensed under the Unity Companion License for Unity-dependent projects (see https://unity3d.com/legal/licenses/unity_companion_license). Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "A new Input System for Unity. Check out the Input System documentation for more info."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomComposite/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomComposite/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "This sample shows how to implement and register a custom InputBindingComposite."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomDevice/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomDevice/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "This sample demonstrates how to add author a custom device that plugs into the input system."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomDeviceUsages/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/CustomDeviceUsages/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "This sample shows how to tag devices with custom \"usages\" and how to bind actions specifically to devices with only those usages. This is useful if you have the same type of device that appears in multiple different roles that you want to distinguish when binding to the device. For example, when a device may appear in both the left and the right hand or may appear held in different orientations (say, horizontal vs vertical)."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/GamepadMouseCursor/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/GamepadMouseCursor/README.html",
    "title": "Licenses | FSM Unity Framework",
    "keywords": "Rather than adapting UIs for gamepad navigation/use, an oft-used alternative is to instead keep having UIs operated by pointer input but to drive the pointer from gamepad input. This sample demonstrates how to set this up with the input system. It uses a custom actions file for feeding input to the UI as the default actions are set up for gamepad navigation – something we don't want here as it would conflict with gamepad input being used for virtual cursor navigation. Note how InputSystemUIInputModule on the EventSystem GameObject is set up to reference actions from that file. The key component to take a look at is VirtualMouseInput on Canvas >> Cursor. The component is set up to receive input from the gamepad and translates it into motion on the RectTransform it is given. When going into play mode, you should also see a Virtual Mouse being added to the devices by the component. Note how the anchor position on the RectTransform is set to bottom left. This way the coordinate system responds to how mouse screen space operates. Note how Cursor is the last child of Canvas so that it draws on top of everything else. Note that Raycast Target on the Image component of the cursor is turned off to avoid raycasts from the mouse cursor hitting the cursor itself. Note that Cursor Mode on the VirtualMouseInput component is set to Hardware Cursor If Available. This will cause the component to look for a system mouse. If present, the system mouse is disabled and the system mouse cursor is warped to the virtual mouse position using Mouse.WarpCursorPosition. If no system mouse is present, Cursor Graphic will be used as a software mouse cursor. Licenses The cursor used in the example is from game-icons.net and made by Delapuite and released under the CC BY 3.0 license. It is used without modifications."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/InputRecorder/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/InputRecorder/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "This sample is both a demonstration for how to use InputEventTrace as well as a useful tool by itself in the form of the InputRecorder reusable MonoBehaviour component. One possible way in which you can use this facility, for example, is to record input, save it to disk, and then replay the same input in automation (e.g. in tests or when recording short video snippets of preset gameplay sequences for manual proofing)."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/RebindingUI/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/RebindingUI/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "This sample demonstrates how to use the Input System APIs to set up a rebinding UI. The main file is RebindActionUI which, aside from serving as an example, contains a reusable MonoBehaviour component for composing rebinding UIs. The RebindUIPrefab contains a ready-made prefab that can be used as a simple drop-in setup for rebinding an individual action. To demonstrate how to use images instead of textual display strings, take a look at GamepadIconsExample. Finally, the RebindSaveLoad script demonstrates how to persist user rebinds in PlayerPrefs and how to restore them from there. The icons used in the sample are taken from Free Prompts Pack v4.0 created by, and made available to public domain by Nicolae Berbece. Icons are licensed under Creative Commons CC0."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/SimpleDemo/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/SimpleDemo/README.html",
    "title": "SimpleDemo_UsingState | FSM Unity Framework",
    "keywords": "This sample shows how to set up a simple character controller using the input system. As there is more than one way to do it, the sample illustrates several ways. Each demonstration is set up as a separate scene. The basic functionality in all the scenes is the same. You can move and look around and fire projectiles (colored cubes) into the scene. In some scenes, only gamepads are supported but the more involved demonstrations support several different inputs concurrently. SimpleDemo_UsingState Source This starts off at the lowest level by demonstrating how to wire up input by polling input state directly in a MonoBehaviour.Update function. For simplicity's sake it only deals with gamepads but the same mechanism works in equivalent ways for other types of input devices (e.g. using Mouse.current and Keyboard.current). The key APIs demonstrated here are Gamepad.current and InputControl.ReadValue. public class SimpleController_UsingState : MonoBehaviour { //... public void Update() { var gamepad = Gamepad.current; if (gamepad == null) return; var move = Gamepad.leftStick.ReadValue(); //... } } SimpleDemo_UsingActions Source This moves one level higher and moves input over to \"input actions\". These are input abstractions that allow you to bind to input sources indirectly. In this scene, the actions are embedded directly into the character controller component. This allows setting up the bindings for the actions directly in the inspector. To see the actions and their bindings, select the Player object in the hierarchy and look at the SimpleController_UsingActions component in the inspector. The key APIs demonstrated here are InputAction and its Enable/Disable methods and its ReadValue method. public class SimpleController_UsingActions : MonoBehaviour { public InputAction moveAction; //... public void OnEnable() { moveAction.Enable(); //... } public void OnDisable() { moveAction.Disable(); //... } public void Update() { var move = moveAction.ReadValue<Vector2>(); //... } } The sample also demonstrates how to use a Tap and a SlowTap interaction on the fire action to implement a charged shooting mechanism. Note that in this case, we run the firing logic right from within the action using the action's started, performed, and canceled callbacks. fireAction.performed += ctx => { if (ctx.interaction is SlowTapInteraction) { StartCoroutine(BurstFire((int)(ctx.duration * burstSpeed))); } else { Fire(); } m_Charging = false; }; fireAction.started += ctx => { if (ctx.interaction is SlowTapInteraction) m_Charging = true; }; fireAction.canceled += ctx => { m_Charging = false; }; SimpleDemo_UsingActionAsset Source As more and more actions are added, it can become quite tedious to manually set up and Enable and Disable all the actions. We could use an InputActionMap in the component like so public class SimpleController : MonoBehaviour { public InputActionMap actions; public void OnEnable() { actions.Enable(); } public void OnDisable() { actions.Disable(); } } but then we would have to look up all the actions manually in the action map. A simpler approach is to put all our actions in a separate asset and generate a C# wrapper class that automatically performs the lookup for us. To create such an .inputactions asset, right-click in the Project Browser and click Create >> Input Actions. To edit the actions, double-click the .inputactions asset and a separate window will come up. The asset we use in this example is SimpleControls.inputactions. When you select the asset, note that Generate C# Class is ticked in the import settings. This triggers the generation of SimpleControls.cs based on the .inputactions file. Regarding the SimpleController_UsingActionAsset script, there are some notable differences. public class SimpleController_UsingActionAsset { // This replaces the InputAction instances we had before with // the generated C# class. private SimpleControls m_Controls; //... public void Awake() { // To use the controls, we need to instantiate them. // This can be done arbitrary many times. E.g. there // can be multiple players each with its own SimpleControls // instance. m_Controls = new SimpleControls(); // The generated C# class exposes all the action map // and actions in the asset by name. Here, we reference // the `fire` action in the `gameplay` action map, for // example. m_Controls.gameplay.fire.performed += //... } //... public void Update() { // Same here, we can just look the actions up by name. var look = m_Controls.gameplay.look.ReadValue<Vector2>(); var move = m_Controls.gameplay.move.ReadValue<Vector2>(); //... } } Just for kicks, this sample also adds keyboard and mouse control to the game. SimpleDemo_UsingPlayerInput Source Finally, we reached the highest level of the input system. While scripting input like in the examples above can be quick and easy, it becomes hard to manage when there can be multiple devices and/or multiple players in the game. This is where PlayerInput comes in. PlayerInput automatically manages per-player device assignments and can also automatically handle control scheme switching in single player (e.g. when the player switches between a gamepad and mouse&keyboard). In our case, we're not getting too much out of it since we don't have control schemes or multiple players but still, let's have a look. The first thing you'll probably notice is that now there are two script components on the Player object, one being the usual SimpleController and the other being PlayerInput. The latter is what now refers to SimpleControls.inputactions. It also has gameplay set as the Default Action Map so that the gameplay actions will get enabled right away when PlayerInput itself is enabled. For getting callbacks, we have chosen Invoke Unity Events as the Behavior. If you expand the Events foldout in the inspector, you can see that OnFire, OnMove, and OnLook are added to the respective events. Each callback method here looks like the started, performed, and canceled callbacks we've already seen on fireAction before. public class SimpleController_UsingPlayerInput : MonoBehaviour { private Vector2 m_Move; //... public void OnMove(InputAction.CallbackContext context) { m_Move = context.ReadValue<Vector2>(); } //... }"
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/SimpleMultiplayer/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/SimpleMultiplayer/README.html",
    "title": "Simple Multiplayer Demo | FSM Unity Framework",
    "keywords": "Simple Multiplayer Demo This demo shows a simple local multiplayer setup. Players can join by pressing buttons on the supported devices. As players join, the screen is subdivided in split-screen fashion. Joining is handled by the PlayerManager GameObject in the scene which has the PlayerInputManager component added to it. The component references Player.prefab which is instantiated for each player that joins the game. The prefab contains a GameObject that has a PlayerInput component added to it. The component references the actions available to each player which, by means of the control schemes defined in the asset, also determine the devices (and combinations of devices) supported by the game. The actions available to each player are intentionally kept simple for this demonstration in order to not add irrelevant details. The only action available to players is Teleport which players can trigger through a button on their device. When trigger, they will be teleported to a random position within the game area. This serves to demonstrate that player inputs are indeed separate. Note that each PlayerInput also references a Camera which is specific to each player. This is used by PlayerInputManager to configure the split-screen setup."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/UIvsGameInput/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/UIvsGameInput/README.html",
    "title": "UI vs Game Input | FSM Unity Framework",
    "keywords": "UI vs Game Input NOTE: More information related to ambiguities between UI and game input may be found here in the documentation. When interactive UI elements are overlaid over a game view, ambiguity may arise for inputs. If, for example, there is a UI.Button on screen that can be clicked/tapped, while clicking/tapping on the scene itself also has associated functionality, clicking on the UI button should not also trigger the corresponding action on the scene. This sample demonstrates how to handle input in such a situation. The Sample Scene The sample scene has a UI button in each of the corners of the screen. The camera in the scene can be rotated and projectiles can be fired while at the same time the buttons in the UI can be clicked. There are two ways to control the game: \"Pointer\", i.e. mouse (optionally combined with keyboard) or touch input, and \"Navigation\", i.e. gamepad input. Mouse/Touch/Keyboard Input When clicking any of the buttons, the \"status bar\" text along the bottom edge of the screen changes. Left-click-dragging with the mouse or finger-dragging with touch rotates the camera (note: only when starting the drag on the background). Alternatively, when using mouse&keyboard, holding the the left control key will engage camera control. Right-clicking with the mouse or tapping the second finger while rotating the camera shoots a projectile. Double-clicking/tapping on the scene resets the camera orientation. Pressing Escape will bring up the game menu. With touch input, an extra button is shown in the game UI to do that. Gamepad Input The right stick rotates the camera and the right trigger fires a projectile. Double-pressing the A button will reset the camera to its initial orientation. Holding the left trigger switch to UI focus. UI selection is now active and can be changed with the d-pad or the sticks. The A button performs a button press. Pressing B while in game brings up the main menu. How It Works Pointer Input For the most part, input processing is done in Update() such that actions are processed on a per-frame basis. Responses to actions that may conflict with UI input use IsPointerOverGameObject to determine whether the pointer is currently over UI. Since this is called from Update() and thus outside of input processing (i.e. not from within an InputAction callback), the method can be safely called and will return an accurate result. There are two implementations of handling the Fire action. One uses the same approach just mentioned where the action's response is dealt with once per frame. The second one, however, immediately creates a projectile within the callback and thus operates at sub-frame accuracy. For a low-frequency input such as the Fire action here, this is not generally a useful thing to do but it is done here for the sake of demonstration. We cannot call IsPointerOverGameObject from the action callback and thus need to use the UI's public raycasting interface to determine \"over UI?\" state manually for the current pointer position. Navigation Input Navigation input employs an explicit mode switch to go from gameplay to UI input. This is handled by OnUIEngage."
  },
  "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/UnityRemote/README.html": {
    "href": "Library/PackageCache/com.unity.inputsystem@1.7.0/Samples~/UnityRemote/README.html",
    "title": "Unity Remote Sample | FSM Unity Framework",
    "keywords": "Unity Remote Sample This sample is just a simple scene that lets you see try out the Unity Remote app. The app is useful for quickly testing your project in the Unity Editor without having to build and deploy to the device. This is supported for iOS and Android. More detailed information about the Unity Remote can be found in the Unity manual. Instructions: Install the Unity Remote app on your Android or iOS device. Connect the device to your computer via USB. On Android, make sure you have the Android SDK installed and configured appropriately in Unity. Also, USB debugging needs to be enabled. In Unity, go to Edit > Project Settings > Editor and select the device in the Unity Remote section. Open the Unity Remote app on the device. Open the UnityRemoteTest.unity scene from this sample. Enter play mode. After a short delay, the Unity Remote app on your device should switch to display the scene and you should be able to interact with the scene in the editor using the device."
  },
  "Library/PackageCache/com.unity.mathematics@1.2.6/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.mathematics@1.2.6/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.2.6] - 2022-02-11 Added Changed Made Il2CppEagerStaticClassConstructionAttribute internal to avoid conflicts with other definitions outside of the package. Deprecated Removed Fixed [1.2.5] - 2021-11-01 Added Changed Deprecated Removed Fixed Fixed property drawing when manually drawing a property that was hidden with [HideInInspector]. [1.2.4] - 2021-09-22 Added Added [Il2CppEagerStaticClassConstruction] to Unity.Mathematics types to run static constructors at startup. This improves IL2CPP performance slightly for types that have static constructors. Changed License file updated to satisfy Unity's package validation tests. Changed noise documentation in comments to xmldoc comments. Deprecated Removed Fixed Fixed Equals(object) override which did not check type before casting. This could cause exceptions to be thrown when the object did not match the expected type. Fixed incorrect math.tzcnt documentation which mentioned leading zero counts instead of trailing zero counts. Fixed float2x2.Rotate documentation to mention radians instead of degrees. Fixed documentation for methods and properties that were previously undocumented. [1.2.1] - 2020-08-06 Added Changed Deprecated Removed Fixed Fixed warnings for meta files existing even though the files they represent did not exist. Internal (Not ready for production) [1.2.0] - 2020-08-03 Added Added [MethodImpl(MethodImplOptions.AggressiveInlining)] to many static functions to improve IL2CPP performance. Added compress() that accepts a float4 and uint4. Added math.project() and math.projectsafe() for vector projection. Added math.EPSILON, math.INFINITY, math.NAN and their double counterparts. Added [Serializable] to RigidTransform. Added math.ceillog2(). Added math.floorlog2(). Added math.down(), math.forward(), etc for Cartesian coordinate axes that match UnityEngine Vector3 equivalents. Added math.ispow2(). Added half.MinValueAsHalf and half.MaxValueAsHalf to avoid having to explicitly convert from float. Added a float3x3 constructor which takes a float4x4 as input. Added [Serializable] to half types. Added some performance tests which can be run from the Unity test project. Added Random.CreateFromIndex() to assist in creating Random instances from loop indices. Changed Deprecated Removed Fixed Fixed documentation bug where quaternion.RotateX/Y/Z referred to a float4x4 instead of quaternion. Fixed code generation bugs which could cause Windows and Mac to generate different test code. Fixed some test asserts which used NaNs and signed zeros which failed in IL2CPP builds. Updated documentation for math.countbits() to include equivalent names on Intel and ARM architectures to aid in discoverability. Internal (Not ready for production) Added Unity.Mathematics.Geometry.Plane to represent planes in 3D space. Added more MinMaxAABB functionality from Unity.Physics.Aabb. Added Unity.Mathematics.Geometry.Math to hold static functions like AABB transformations. Added MinMaxAABB. [1.1.0] - 2019-07-08 Release stable version [1.1.0-preview.1] - 2019-06-27 Add new math.bitmask to return a bit mask from a bool4 [1.0.1] - 2019-04-15 Release stable version Modify all math constants (e.g math.PI) to provide float constant by default instead of double. Use for example math.PI_DBL to get the previous double constant. [1.0.0-preview.1] - 2019-02-28 Fixed bug where modifications on prefabs could not be reverted for vector properties when using context menu in Inspector. Fixed structure of the package for internal validation"
  },
  "Library/PackageCache/com.unity.mathematics@1.2.6/Documentation~/mathematics.html": {
    "href": "Library/PackageCache/com.unity.mathematics@1.2.6/Documentation~/mathematics.html",
    "title": "Unity.Mathematics | FSM Unity Framework",
    "keywords": "Unity.Mathematics A C# math library providing vector types and math functions with a shader like syntax. Used by the Burst compiler to compile C#/IL to highly efficient native code. The main goal of this library is to provide a friendly Math API familiar to SIMD and graphic/shaders developers, using the well known float4, float3 types...etc. with all intrinsics functions provided by a static class math that can be imported easily into your C# program with using static Unity.Mathematics.math. In addition to this, the Burst compiler is able to recognize these types and provide the optimized SIMD type for the running CPU on all supported platforms (x64, ARMv7a...etc.) NOTICE: The API is a work in progress and we may introduce breaking changes (API and underlying behavior) Usage You can use this library in your Unity game by using the Package Manager and referencing the package com.unity.mathematics. See the forum Welcome page for more details. using static Unity.Mathematics.math; namespace MyNamespace { using Unity.Mathematics; ... var v1 = float3(1,2,3); var v2 = float3(4,5,6); v1 = normalize(v1); v2 = normalize(v2); var v3 = dot(v1, v2); ... } Building Open the src\\Unity.Mathematics.sln under Visual Studio 2015 or MonoDevelop and compile in Debug\\Release. Contributing We don't yet accept PR on this repository. See the FAQ below. The project is using editorconfig to keep files correctly formatted for EOL and spaces. We assume that your IDE has support for editorconfig, you can download the following extensions if your IDE is listed: VS2015/VS2017 EditorConfig extension Visual Studio Code EditorConfig extension SublimeText EditorConfig extension Frequently Asked Question Why developing another Math library instead of using existing Unity Vector3...etc.? After years of feedback and experience with the previous API, we believe that providing an API that is closer to the way graphics developers have been using math libraries should better help its adoption and the ease of its usage. HLSL / GLSL math library is a very well designed, well understood math library leading to greater consistency. Why not using System.Numerics.Vectors? Mainly for the reason mentioned above, System.Numerics.Vectors is in many ways similar to our previous Vector library (more object oriented than graphics programming oriented). Also the fact that our Burst compiler is able to recognize a lot more patterns for SIMD types and math intrinsics makes it easier to work with a dedicated API that reflects this ability. Naming convention In C# int and float are considered builtin types. Burst extends this set of bultin types to also include vectors, matrices and quaternions. These types are bultin in the sense that Burst knows about them and is be able to generate better code using these types than what would be possible with equivalent code using custom types. To signify that these types are bultin their type names are in all lower case. The operators on these bultin types found in Unity.Mathematics.math are considered intrinsics and are thus always in lower case. There are no plans to extend the set of intrinsic types beyond the current set of vectors (typeN), matrices (typeNxN) and quaternions (quaternion). This convention has the added benefit of making the library highly compatible with shader code and makes porting or sharing code between the two almost frictionless. Why can't we send a PR yet? We are working on providing a Contributor License Agreement (CLA) with a sign-over functionality and our UCL License doesn't cover this yet. Licensing Unity Companion License (“License”) Software Copyright © 2019 Unity Technologies ApS For licensing details see LICENSE.md"
  },
  "Library/PackageCache/com.unity.mathematics@1.2.6/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.mathematics@1.2.6/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.mathematics copyright © 2019 Unity Technologies Licensed under the Unity Companion License for Unity-dependent projects (see https://unity3d.com/legal/licenses/unity_companion_license). Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [1.2.2] - 2023-01-30 Fixed Fixed exception thrown when removing marker. [1.2.1] - 2023-01-02 Changed Updated What's New page [1.2.0] - 2023-01-02 Added Added support for removing vsync time, so we can see actual CPU duration over multiple frames. A drop down has been added to 'remove' a marker from the analysis and has entries for \"FPS Wait\", \"Present Wait\" and \"Custom\" where you can select any marker from the table using the right click context menu to \"Remove Marker\". Added optional standard deviation (SD) column into marker table (on single view) for sorting data based on variation of marker timings in over the frames. Added export of the comparison table as CSV. Changed Improved profile analysis performance. Updated minimal suppourted version to Unity 2020.3. Fixed Fixed bug when depth filter and marker vsync removal both applied. Fixed commas in CSV export file. [1.1.1] - 2021-10-04 Fixed Fixed pulling frame data from the Profiler to exclude first and last frames if their main thread profiler data is incomplete, so that they don't skewer the analysis results (case 1359686). Fixed IndexOutOfRangeException thrown when using Profiler or selecting Profiler frames in Profile Analyzer. (1366931) [1.1.0] - 2021-07-23 Fixed Fixed x axis display on frame time graph when capture doesn't match Unity Profiler contents. Fixed selected marker name to be updated even when Profiler fails to sync selection. [1.1.0-pre.2] - 2021-04-16 Changed Ensured forward compilation compatibility of ProfilerWindow API usage. Fixed Fixed Frame View Dropdown not showing time in Microseconds when Microseconds are set as display unit type. Fixed ArgurmentOutOfRange exception on 'Clear Selection' usage in Frame Control. Improved loading and analysis progressbar progression to be monotonously incremental and reflect actual stage correctly. Fixed the marked frame time overlay so that it works on loaded data (case 1327888). Fixed keyboard frame controls so that they do not play an error sound (case 1327931). Fixed RangeSettings.IsEqual so that it doesn't throw an exception when there is a different dataView.selectedIndices. [1.1.0-pre.1] - 2021-02-22 Changed Used the new EditorWindow.docked API on 2020.1 and later to replace reflection usage. Made the Thread Selection window a utility window. Added a progress bar, which shows data loading progress. All no longer collapses in the thread view because it was superfluous and held no discernible purpose. Added information to the frame summary tooltip to show the selected ranges. Selecting the Frame, Thread, and Marker Summary title labels now also toggles their foldout. Timings in tooltips now display up to seven digits with a unit type change if below 0.01. This ensures the value is readable and correct. Frame Time Graph selection modifiers (Next, Previous, Grow, Shrink etc.) now work when multiple regions are selected. Added new (hidden) columns to the comparison tab for Percentage Diff for each of the diff types The Depth Slice drop-down is searchable and scrollable for Unity versions 2019.1 or newer. Improved tooltips on the top 10 marker duration time. Fixed The Thread Summary and Marker Summary views now correctly clip their contents when scrolling on 2018.4 and below. Marker detail bars are no longer drawn on top of the list's headers when scrolling. Fixed an issue with saving when there is no data loaded, which previously caused a null reference exception. Add to and Remove from name filter is no longer case sensitive. Optimized frame selection with Show Filtered Threads option enabled and All Threads turned on. Fixed an issue where the Upper Quartile of frame time option did not fit in the Graph scale dropdown in Thread Summary. Improved the error message when the Profile Analyzer fails to load a .data file. Fixed an issue where changing the depth slice and right-clicking on a marker caused an error in 2018.4 and earlier. Fixed the automatic increase of an unsaved file counter in Single view. Improved the Marker Column mode so that it is separate for Single and Compare view. The tooltip of the frame graph's scale control is now clearer and matches the documentation. Fixed an issue where a frame of '0' was incorrectly shown in the Marker Summary when the marker had a duration of 0ms. Fixed an issue with Selection Synchronization between Profiler Window and Profile Analyzer in Unity 2021.1 or newer. Added a log message when the loading or analysis fails due to domain reload. Fixed an issue with the table context menu, where it was not disabled during data processing. Fixed an issue where exporting a marker table that contained markers with commas or quotation marks to the .csv format would break the format. [1.0.3] - 2020-07-31 Fixes Fixed warnings in code so package will not break Unity projects that have warnings as errors enabled. [1.0.2] - 2020-06-30 Fixes Fixed issue where second profiler instance could appear after entering play mode when profile analyzer is open Improved performance of pulling data from Unity profiler Pull Data button now enabled when profiler is recording. This will stop the recording for the duration of the action of pulling of data. Fixed Median sorting in Comparison mode when value missing from one data set, Missing values always sorted before 0 values. [1.0.1] - 2020-06-16 Fixes Fixed \"median marker time (in currently selected frames)\" in tooltip for 'Top 10 markers'. Fixed profile analyzer 5.6 support Fixed minor visual artefact when no marker selection Fixed issue where frame time graph tooltips were not always appearing when hovering Marker Summary - Count Values are now correctly sorted in descending order Hiding selected marker in comparison view if lacking 2 valid data sets Corrected bucketing of histogram data for counts. Display was fractionally incorrect Fixed sorting of frames (by time/count) to be more stable by providing a secondary sort by frame index Marker table export now orders by descending median time (to match the default UI sort option). [1.0.0] - 2020-06-02 Changes Export window updates. Includes tooltips, better on screen positioning. If opened and then new data set loaded. New data set now correctly exported. Frame Time graph now shows border when selected Keys 1 and 2 select the first or second frame time graph to take keyboard focus Frame Time graph now shows highlighted when all frames selected, un highlighted when no frames selected. Fixes Improved histogram display when 0 range. Widened frame index buttons when frame value large 'total number of markers' value in Compare tab now corrected to use unfiltered count Improved error messages when jumpping to frame when profiler data doesn't match profile analyzer data [0.7.0-preview.4] - 2020-05-29 Changes Thread selection window Now only applys thread selection changes when clicking 'Apply'. Closing the window no longer applies the changes automatically. Now contains buttons to Reset to previous thread selection, Select just \"Main Thread\" or \"Common\" set selection (Main, Render and Jobs). Apply button is now greyed out while analysis is running. Group column enabled and used to split off group from thread name, default to by group first Re sorted when selection changed (so states sort correctly when sorting by state) Sort order preserved when buttons pressed and re-opening Fixed sort order for threads (10 after 9 rather than between 1 and 2) Marker sorting Marker table column filtering now preserved when clicking Analyse or Compare or tab switching. Compare table bars to now sort by bar size (=delta value) rather than the left/right value. Marker table bars now start sorted by descending size which is more common usage. Added depths columns into the comparison table and a 'Depth' option in the Marker Columns dropdown Added thread count into thread summary area Added threads column into marker tables (threads the marker occurs on) Thread summary now contains thread count, unique count per data set and selection count Frame time area now allows adding to the selection by holding CTRL (or COMMAND on Mac) Frame range value now includes tooltip to give more detail about the selection Improved text string for depth filter status on the top marker display Fixes Detected and ignored invalid frame markers (duration < 0) Fixed/Removed warnings on loading data in compare view when selection active. Disabled \"Analyze\" button while analysis is already running Fixed Marker Summary 'Top' dropdown selection text width Fixed thread selection window sort order Fixed tooltip on 'count' bars to be scalar value (not a time unit) Clamped selected region shown when zoomed in on frame time graph Frame Control value no longer overlaps with drop-down list when Units are set to Microseconds Fixed thread count text when analysis completes after swapping single/compare tabs during analysis Fixed Frame View's tooltip \"total time\" when rapidly changing frames Fixed overlapping text when selecting single frame when zoomed in Histogram frame count now lists \"1 frame\" when single frame rather than \"1 frames\" Mode: text in top bar now same size as rest of text. Clear Selection in marker right click context menu now only shown if a selection has been made Cut/Paste now supported in the include/exclude marker name filter Frame Time graph frame selection grow/shrink now keeps current selection in paired graph Fixed frame start/end display for \"Select Frames that contain this marker (within whole data set)\" Corrected bucketing of histogram data. Display was fractionally incorrect Histogram now shows non zero height bar if an item in the bucket Fixed right click \"Remove from Included Filters\" when using (quoted) markers with spaces in names Fixed data auto-load from Single to Compare tab when capturing after entering playmode Fixed auto right calculation to use most common difference Fixed auto right display to show + or - and not both at the same time Fixed infinite analysis loop when auto right calculation was clamping to max depth in the other data set Auto right now clamps to min/max depth rather than reverting to 'all' Fixed thread count bug when using comparison mode and loading data on top of existing data Selected marker refocused in marker list when table is regenerated (e.g. when selecting all frames containing the marker). Column by which the Marker Details is sorted by is now maintained when entering Play Mode Updated frame time graph context menu to make hotkeys more consistent with out Unity UI layouts Percent symbol no longer cut off in Mean frame contribution when the value is 100% Frame time graph now scales up when zooming when comparing data with different frame amounts Changing 'Auto Right' tick box no longer causes re-analysis if depth settings unchanged Most tooltips now show values without rounding, so its more obvious when low values give non zero deltas. Fixed individual max tooltip Disabled 'Pull data' button when Unity profiler is still capturing data (as the pull would not complete). Fixed right frame index in exported comparison CSV file. Frame selection no longer changed when moving the cursor in the Filter text box with arrow keys Help text now continues to be shown after entering play mode, while no data set loaded/pulled Fixed frame index button heights when alternative (Verdana) font used. Fixed cache styles to be updated when changing theme (to fix text colour in filters area) Single click (without drag) now always single frame rather than the group of frames in the pixel wide area. Split frame range into start and end rows to display more information when values large Marker Summary comparison times are now limited to 5 digits to prevent text clipping off Enhancements Optimised filter processing for more responsive scrolling when a large marker filter list is supplied. Optimised thread name summary display [0.6.0-preview.1] - 2020-02-04 Fixed a crash with the thread selection API in Unity 2020.1 Fixed marker and thread scroll bars Added extra documentation for public API elements [0.5.0-preview.2] - 2019-09-19 Minor documentation update to fix the changelog formatting [0.5.0-preview.1] - 2019-09-18 Features Added self time option to display 'exclusive' time of markers excluding time in children. Added ability to filter to a parent marker to reduce the marker list to a part of the tree. Added option to filter the column list to set groups (including custom). Added column for total marker time over the frame Added copy to clipboard on table entries Added export option for marker table Enhancements Improved Top N Markers graph to make it clearer this is for the median frames of each data set. Added thread count display (next to marker count). Added frame index to more tooltips to improve clarity of the data values (marker table, frame and marker summary). Added additional visual bars for total and count diffs. Added abs count column. Improved performance of adding to include/exclude filter via the right click menu by only refreshing the table (and no longer rerunning full analysis) Improved performance for scrolling by caching strings for profile table and comparison table. Added unaccounted time into the Top N Markers graph when total is less than the median frame time Added grow/shrink selection hot keys and menu options Added tooltip info for frame time duration for selection range Fixes Fixed issue with combined marker count when data sets have some unique markers. Fixed bars less than 1 pixel wide to clamp to min width of 1. Fixed help text for new editor skin in 2019.3 Fixed bug with calculation of the auto right depth offset (see with 2017.4/2018.4 comparisons) Improved the frame offset times in the frame time and comparison frame time exports Fixed bug with missing first frame of data / frame offset incorrect when reloading .pdata [0.4.0-preview.5] - 2019-04-02 Updated package.json file to indicate this package is valid for all unity versions [0.4.0-preview.4] - 2019-04-02 Fixed issue in 2017.4 with unsupported analytics API and a GUI style. [0.4.0-preview.3] - 2019-04-01 First public release of Profile Analyzer. [0.1.0-preview] - 2018-12-07 This is the first beta release of Profile Analyzer The profile analyzer tool augments the standard Unity Profiler. It provides multi frame analysis of the profiling data."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Profile Analyzer What's new Upgrade guide Profile Analyzer window Single view Compare view Frame Control pane Filters pane Frame Summary Thread Summary Marker Summary Statistics in the Profile Analyzer Workflows Collecting and viewing data Comparing frames from the same data set Comparing frames from different data sets Ordering frames by length"
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/analyzing-mulitple-datasets.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/analyzing-mulitple-datasets.html",
    "title": "Comparing frames from different data sets | FSM Unity Framework",
    "keywords": "Comparing frames from different data sets This workflow explains how to compare two frames from different data sets. In this example, it shows how to compare the median frames from each data set. Comparing the median frames helps you understand what might be happening in a frame that is central to the distribution for each data set. Step 1: Collect performance data to analyze Open the Profile Analyzer window (menu: Window > Analysis > Profile Analyzer). Click the Compare button in the toolbar to enter the Compare view. Collect some profiling data. To pull data from an active profiling session, click the Pull Data button. This pulls in the current set of available frames from the Profiler. If you don't have an active profile session, click the Open Profiler Window button, then load or record some data. For more information on how to collect data, see the workflow documentation on Collecting and viewing data. Pull a different data set that you want to analyze into each graph in the Frame Control pane. Step 2: Select frames of interest Enable Pair Graph Selection. Right-click on one of the graphs in the Frame Control pane and then choose Select Median Frame from the context menu. The Profile Analyzer then analyzes the two median frames of the data sets like this: You can then look further and compare the differences between the median frames of each data set. For further information, see the Ordering frames by length workflow, which extends the selected range and number of frames used from the middle of the frame distribution."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/collecting-and-viewing-data.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/collecting-and-viewing-data.html",
    "title": "Collecting and viewing data | FSM Unity Framework",
    "keywords": "Collecting and viewing data This workflow explains how to populate the Unity Profiler and Profile Analyzer with data. Note that the Profile Analyzer only analyzes CPU data from the Profiler. Step 1: Open the Unity Profiler Go to the menu: Window > Analysis > Profiler or press Ctrl+7 (Command+7 on macOS). If you have the Profile Analyzer window open, you can click the Open Profiler Window button. Step 2: Populate the Profiler with data To use the Profile Analyzer, you must populate it with data from the Profiler. To add data to the Profiler, you can either record some new data, or load a Profiler capture file in the .data file format. a) Record new data Click the Attach to Player dropdown at the top of the window (next to the Record button) and select a player to profile. By default this is set to Playmode. Click the Record button to start recording data. If you enabled Autoconnect to Profiler in the Build Settings, the Profiler automatically collects data when you start a built player. For more information on how to record data in the Profiler, see the documentation on Profiling your application in the Unity User Manual. b) Load data To load a saved .data file, in the top right of the Profiler window, select the Load button. Step 3: Pull the data into the Profile Analyzer window Open the Profile Analyzer window (menu: Window > Analysis > Profile Analyzer) and then select the Pull Data button in the Frame Control pane. The Profile Analyzer then pulls in the data that is loaded in the Profiler window. Tip The Profiler window and the Profile Analyzer window require a lot of screen real-estate. Docking the two windows together in a single tabbed window lets you navigate between the two views quickly. The Profile Analyzer in the Compare mode docked next to the Profiler in one window Step 4: Load and save Profile Analyzer data To save the data from the Profile Analyzer, click the Save button in any view. Select where you would like to save your data, and then Unity saves the data in the .pdata format. To load this data, click the Load button in any view. Note When you load data into the Profile Analyzer, the data must be in the Profile Analyzer .pdata format. If you have data from the Profiler in the .data file format, open it in the Profiler first, and in the Profile Analyzer select the Pull Data button."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/compare-view.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/compare-view.html",
    "title": "Compare view | FSM Unity Framework",
    "keywords": "Compare view In the Compare view you can load two data sets, which the Profiler Analyzer displays in two different colors. It displays the information from both data sets in a similar way to the Single view and has the same panes and panels. For information on navigating the window, see the Profile Analyzer window navigation documentation. The Compare view with two data sets loaded. Loading data To load data into the Compare view, select the Pull Data button in the frame control pane, and the Profile Analyzer pulls in any data in the Profiler window. Alternatively, select the Load button to load Profile Analyzer (.pdata) data you have saved from a previous session. Note If you select the Load option, the data must be in the Profile Analyzer .pdata format. If you have data from the Profiler in the .data file format, open it in the Profiler first, and then select the Pull Data button in the Profile Analyzer. For more information on how to pull data into the Profile Analyzer, see the workflow documentation on Collecting and viewing data. Marker Comparison list The Marker Comparison pane contains a sortable list of markers with a number of useful statistics, including the difference between the two sets. The proportional graphs with the < and > labels visualize the values of each marker, so you can see the difference between the timings of both samples. If you select a marker in the list, the Marker Summary panel displays in depth information on the marker. Each marker in the list is an aggregation of all the instances of that marker, across all filtered threads and in all ranged frames. You can filter the columns in the Marker Comparison list to a more relevant set. This is particularly useful if you want to filter out irrelevant data when you look for Time or Count values. To filter the columns, select the Marker columns dropdown from the Filters pane. For more information on how to filter data, see the Filters documentation. Marker Comparison columns and groups By default, the Marker columns dropdown in the Filters pane has six preset column layouts that you can use to adjust the layout of the Marker Comparison pane. They are: Time and count: Displays information on the average timings and number of times the markers were called. Time: Displays information on the average timings of the markers. Totals: Displays information about the total amount of time the markers took on the whole data set. Time with totals: Displays information about both the average and total times of the markers. Count totals: Displays information about the total number of times the markers were called. Count per frame: Displays information on the average total per frame the markers were called. Depths: Displays information on where the markers are in the Hierarchy. For more information, see the documentation on Depth Slices in Filters pane. Threads: Displays the name of the thread that the markers appear on. For more information, see the documentation on the Thread window in Filters pane. You can also use the Custom column layout to select your own custom mix of columns to add to the layout. To do this, right click on the header of any column, and manually enable or disable any of the columns as necessary. Note In this pane, the Left label refers to the first data set loaded into the Frame Control pane, which is colored blue. The Right label refers to the second data set, which is colored orange. The following table shows the columns that the Profile Analyzer displays when you select that layout. Time and count Time Totals Time with totals Count totals Count per frame Depths Threads Marker Name ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Left Median Right Median ✓ ✓ ✓ < > ✓ ✓ ✓ Diff ✓ ✓ Diff Percent Abs Diff ✓ ✓ ✓ Count Left Count Right ✓ ✓ < Count > Count ✓ Count Delta ✓ ✓ Count Delta Percent Abs Count ✓ Count Left Frame Count Right Frame ✓ < Frame Count > Frame Count ✓ Count Delta Frame ✓ Count Delta Percent Frame Abs Frame Count ✓ Total Left Total Right ✓ ✓ < Total > Total ✓ ✓ Total Delta ✓ Total Delta Percent Abs Total ✓ ✓ Depth Left Depth Right ✓ Depth Diff ✓ Threads Left Threads Right ✓ The following table explains what each column does: Column Description Marker Name Displays the name of the marker. Left Median Right Median The sum of activity for the marker. Left Median displays the first data set loaded into the Frame Control pane, colored blue. Right Median displays the second data set loaded into the Frame Control pane, colored orange. < > A visual representation of the Left Median (<) and Right Median (>) data. Diff The difference between the summed values in each data set. Negative values mean that the left (blue) set of data is bigger, positive means the right (orange) set of data is bigger. Diff Percent The difference relative to the first data set. Abs Diff The absolute difference between the summed values in each data set. Count Left Count Right The number of times the marker started or stopped. Count Left displays the first data set loaded into the Frame Control pane, colored blue. Count Right displays the second data set loaded into the Frame Control pane, colored orange. < Count > Count A visual representation of the Count Left and Count Right data. Count Delta The difference between the Count values in each data set. Negative values mean that the left (blue) set of data is bigger, positive means the right (orange) set of data is bigger. Count Delta Percent The difference in count relative to the first data set. Abs Count The absolute difference between the Count values for the selected frames. Negative values mean that the left (blue) set of data is bigger, positive means the right (orange) set of data is bigger. Count Left Frame Count Right Frame The average count of the marker over all non-zero frames. Count Left Frame displays the first data set loaded into the Frame Control pane, colored blue. Count Right Frame displays the second data set loaded into the Frame Control pane, colored orange. < Frame Count > Frame Count A visual representation of the Count Left Frame and Count Right Frame data. Count Delta Frame The difference between the Count Left Frame and Count Right Frame values. Negative values mean that the left (blue) set of data is bigger, positive means the right (orange) set of data is bigger. Count Delta Percent Frame The difference in average count relative to the first data set. Abs Frame Count The absolute difference between the number of times the marker started or stopped in each data set. Total Left Total Right The total time for the marker over the selected frames. Total Left displays the first data set loaded into the Frame Control pane, colored blue. Total Right displays the second data set loaded into the Frame Control pane, colored orange. < Total > Total A visual representation of the Total Left and Total Right data. Total Delta The difference between the total times over the selected frames in each data set. Negative values mean that the left (blue) set of data is bigger, positive means the right (orange) set of data is bigger. Total Delta Percent The difference in total time relative to the first data set. Abs Total The absolute difference between the total times over all of the selected frames in each data set. Depth Left Depth Right The level, or depth, that the marker appears at. The marker might appear on multiple depth levels. Depth Left displays the first data set loaded into the Frame Control pane, colored blue. Depth Right displays the second data set loaded into the Frame Control pane, colored orange. Depth Diff The difference between the Depth Left and Depth Right values. Threads Left Threads Right The name of the thread that the marker appears on. Threads Left displays the first data set loaded into the Frame Control pane, colored blue. Threads Right displays the second data set loaded into the Frame Control pane, colored orange. Marker Comparison context menu commands If you right-click on a marker in the Marker Comparison list you can control the filter and list even further. Command Function Select Frames that contain this marker (within whole data set) Select all the frames from the entire data set that contain an instance of this marker. Select Frames that contain this marker (within current selection) Select all the frames from a selected range of data that contain an instance of this marker. Select All Selects the entire data set, if you have a range of data selected. Add to / Remove From Include Filter Add or remove the selected marker to the Include filter. This filters the marker list to only markers that match. Add to Exclude Filter Add the selected marker to the Exclude filter. This removes the marker from the marker list. This is useful if you want to remove markers that are using up resources and skewing the markers that you are interested in. Set as Parent Marker Filter Limit the analysis to this marker and markers included below it on the callstack. For more information, see the Parent Marker documentation on the Filters page. Clear Parent Marker Filter Select this to clear the marker as a parent marker filter. Copy To Clipboard Copies the selected value to the clipboard. Analyzing data in Compare view For further information on how to analyze data in Compare view, see the workflow documentation on Comparing frames from different data sets and Comparing frames from the same data set."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/comparing-frames-same-dataset.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/comparing-frames-same-dataset.html",
    "title": "Comparing frames from the same data set | FSM Unity Framework",
    "keywords": "Comparing frames from the same data set This workflow explains how to compare two frames from the same data set. In this example, it explains how to compare the median and longest frames in a data set. Comparing the median and longest frames is useful to help understand what is happening in the longest frame that is not happening in an average frame, or what is taking longer than average to complete. Step 1: Collect performance data to analyze Open the Profile Analyzer window (menu: Window > Analysis > Profile Analyzer) and collect some profiling data. To pull data from an active profiling session, click the Pull Data button. This pulls in the current set of available frames from the Profiler. If you don't have an active profile session, click the Open Profiler Window button, then load or record some data. For more information on how to collect data, see the workflow documentation on Collecting and viewing data. Step 2: Open Compare view Click the Compare button in the toolbar to switch to the Compare View. Step 3: Select the median and longest frames In the Frame Control pane, right click on the top graph and choose Select Median Frame from the context menu. Next, right click on the lower graph and choose Select Longest Frame in the context menu. The Profile Analyzer then analyzes the two frames and displays the data for the median and longest frames: The Profile Analyzer window with the median and longest frames of the same data set selected"
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/filtering-system.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/filtering-system.html",
    "title": "Filters pane | FSM Unity Framework",
    "keywords": "Filters pane You can use the Filters pane in the Single and Compare view to reduce and filter the working data set. You can limit the data set by filters such as partial marker name match, a specific thread, or a specific stack level. As well as filtering by match, you can set the filter to exclude any markers by name. This is particularly useful to exclude markers the might distort the view, or aren't statistically relevant. Single view Filters pane (above) and Compare view Filters pane (below). Note the addition of further Depth Slice controls in the Compare view. The pane has the following controls: Control Function Name Filter Enter the name of a marker you would like to filter the data set by. Once you start typing, the Profile Analyzer automatically filters the Marker details pane to display the matching markers. You can also right click on a marker in the Marker details pane and select Add to Include Filter to add it to the filter. To include a marker name with a space, surround it in quotation marks Choose Any from the dropdown to match any of the terms to the markers in the list. Select All from the dropdown to match all of the listed terms to the markers in the list. Exclude Names Enter the name of a marker you would like to exclude from the data set. Once you start typing, the Profile Analyzer automatically filters the Marker details pane to display the matching markers. You can also right click on a marker in the Marker details pane and select Add to Exclude Filter to exclude it from the filter. Select Any from the dropdown to match any of the terms to the markers in the list. Select All from the dropdown to match all of the listed terms to the markers in the list. Thread Select which thread or threads to filter the data set by. Click the Select button to open the Thread window and filter the data set further. For more information, see the Thread window section of this page. Depth Slice Set the depth level of the Hierarchy to display. The Marker Details/Comparison view then displays all the markers on this level only. In the Compare view, the Auto Right checkbox is enabled by default, which automatically aligns the depth of the two data sets. For more information, see the Depth Slice section of this page. Parent Marker Displays the parent marker the data set is filtered by. To filter by a parent marker, right click on it in the Marker Details/Comparison list and then select Set As Parent Marker Filter. For more information, see the Parent Marker section of this page. Analyze (Single view) Compare (Compare view) Contains details of the number of markers and threads in the data set. Note: The button is redundant, because the Profile Analyzer automatically updates the marker pane with the filters you set. However, you can hover on the button to see timings of the analysis or comparison. Analysis type Select what times the Profile Analyzer includes in its analysis, from either Total or self. The Total option sets the marker times to inclusive, which means the time spent in the marker and its children is included in the filter. The Self option sets the marker times to exclusive, which means that the the time in the marker's children is excluded from the filter. Units Select the unit of measurement of time to display in the Profile Analyzer. Choose between Milliseconds (default) or Microseconds. Note: If you select Microseconds, the Profile Analyzer displays larger timings in milliseconds. Marker Columns Select a layout for the Marker Details/Comparison pane. For more information on the columns in this pane, see the documentation on Single view Marker Details list and Compare view Marker Comparison list. Thread window By default, the Profile Analyzer displays the markers on the main thread in the Marker Details/Comparison pane. To analyze the markers on other threads, under Threads, click the Select button, and the Threads window opens. The Thread window To add more threads to the analysis, enable the checkboxes next to their names, then click the Apply button. The Profile Analyzer then updates the data set. There are two pre-defined sets of threads: Main Only and Common Set. Main Only selects just the Main Thread, and the Common Set selects the Main Thread, plus the Render and Jobs threads. Click the Clear button to clear all of the threads you've selected, and Reset to reset the selection back to the previous thread set. Depth Slice When you select a Depth Slice level in the dropdown, it corresponds to the level of the marker within the hierarchy of the thread. You can visualize this in the Profiler window as follows: The Profile Analyzer in the Single view with a Depth Slice of 3 selected in the Filters pane. The Profiler window with the CPU Usage module selected, in Timeline view. Note that the markers in the Marker Details list correspond to the markers in the third level of the Main Thread's hierarchy. In the Compare view, the Profile Analyzer automatically aligns the depth of the two data sets so that the top level markers are aligned correctly. The value of the offset is displayed in brackets after the Auto Right checkbox. To override the automatic depth alignment, disable the Auto Right checkbox and then manually set the depth levels for the left (blue) and right (orange) sets. This is useful if you're comparing data sets from different versions of Unity. Parent Marker To filter the dataset by a specific marker and its children, right click on the marker in the Marker Details/Comparison pane and then select Set As Parent Marker Filter. You can visualize this in the Profiler window as follows: The Profile Analyzer in the Single view with the data filtered by the parent marker of BehaviourUpdate. The same frame opened in the Profiler window, with this thread highlighted. Note that the markers in the Timeline view correspond to those filtered in the Profile Analyzer."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/frame-range-selection.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/frame-range-selection.html",
    "title": "Frame Control pane | FSM Unity Framework",
    "keywords": "Frame Control pane You can use the Frame Control pane in both the Single and Compare views to select a range of frames to reduce the working set. The Frame Control pane is laid out as follows: The Frame Control in the Single view (top), and in the Compare view (bottom) Control Function A: Pull Data / Load / Save Click the Pull Data button to pull any data that is loaded in the Profiler window. Click the Save button to save the data as a .pdata file. Click the Load button to load a .pdata file. Note: You can only load a .pdata file. If you have data from the Profiler in the .data file format, open it in the Profiler first, and then select the Pull Data button in the Profile Analyzer. B: Frame control scale You can adjust the scale of the y-axis of the Frame Control graph by selecting this dropdown. By default, it scales to the maximum value of the data set. You can also choose from traditional frame boundaries. 16.67ms is equivalent to 60Hz/FPS, 33.33ms is equivalent to 30Hz/FPS, and 66.67ms is equivalent to 15Hz/FPS. C: Selected thread This displays the name of the marker which is selected in the Marker Details pane. When you select a thread from this list, the Profile Analyzer highlights its corresponding timings on the Frame Control graph in a green-blue color. D: Pair Graph Selection (Compare view only) When you enable this checkbox, the Profile Analyzer reflects any changes you make in the range selection of a data set in both data sets. This is important to ensure you compare the exact same number of frames in both data sets and get an accurate comparison. Selecting a frame range The Profile Analyzer uses all the frames in the data sets unless you select a sub-range. When you select a sub-range, it limits the analysis to just those frames which lets you focus on a specific frame or set of frames. To select a range of frames, click and drag on the Frame Control graph. To clear the selection, right click on the Frame Control graph and select Clear Selection. To help visualize which frames are in the current selection, the start and end frame number, plus the frame count in square brackets appears on the x-axis of the graph. The Frame Control in the Single view (top), with 187 frames selected, starting on frame 37 and ending on frame 224. The Compare view (bottom) has Pair Graph Selection enabled, with the same 237 frames selected on both graphs, starting on frame 135 and ending on frame 371. Frame range controls You can control the selection of data in both the Single and Compare views by using the following shortcuts, or by right-clicking and selecting an option from the context menu. Note In Compare view, make sure you enable the Pair Graph Selection checkbox to carry out the following commands on both graphs at the same time. Shortcuts Shortcut Function Shift+click Hold down the Shift key while clicking on the selection on the Frame Control graph to move the selection around freely. Left/Right arrow Move the selection forward or backwards by one frame. Ctrl + click (Command + click on macOS) Selects multiple parts of the data set. Hold down the Ctrl key (Command key on macOS) while making a selection and then click, and optionally drag, on different sections of the chart to select multiple parts of the data set. Equals (=) Extend the selection by one frame on each end of the selection. Alt+Equals (⌥ + Equals on macOS) Reduce the selection by one frame on each end of the selection. Shift+Equals Extend the selection by 10 frames on each end of the selection. Hyphen (-) Reduce the selection by one frame on each end of the selection. Alt+Hyphen (⌥ + Hyphen on macOS) Extend the selection by one frame on each end of the selection. Shift+Hyphen Reduce the selection by 10 frames on each end of the selection. Comma (,) Extend the start of the selection by one frame. Alt+Comma (⌥ + Comma) on macOS Reduce the start of the selection by one frame. Shift+Comma Extend the start of the selection by 10 frames. Period (.) Extend the end of the selection by one frame. Alt+Period (⌥ + Period on macOS) Reduce the end of the selection by one frame. Shift+Period Extend the end of the selection by 10 frames. 1 2 Compare view only In Compare view, with Pair Graph Selection disabled, use the 1 or 2 key on your keyboard to switch between frames. 1 selects the top data, and 2 selects the bottom data. Context menu commands Right click on the Frame Control graph to bring up the context menu. Menu item Function Clear Selection Clears the selected range. The Profile Analyzer then performs the analysis on the whole data set. Invert Selection Inverts the selected range. Select Shortest Frame Selects the frame with the shortest time. Select Longest Frame Selects the frame with the longest time. Select Median Frame Selects the frame with the Median time. Move selection left / right Move the whole selection one frame backwards, or one frame forwards. Grow selection Extend the selection by one frame on each end of the selection. Select the (fast) operation to extend the selection by 10 frames at each end. Shrink selection Reduce the selection by one frame on each end of the selection. Select the (fast) operation to reduce the selection by 10 frames at each end. Grow selection left / right Extend the start or the end of the selection by one frame. Select the (fast) operation to extend the start or the end of the selection by 10 frames. Shrink selection left /right Reduce the selection by one frame at the beginning or the end of the selection. Select the (fast) operation to reduce the start or the end of the selection by 10 frames. Zoom Selection Zoom the Frame Control graph to display the selected range only. Zoom All Zoom out to show all frames. The current selection range is highlighted. Show Selected Marker Enable this setting to highlight the selected marker's time on the Frame Control graph. By default, this setting is enabled and the Profile Analyzer highlights the marker's timings in green on the graph. Show Filtered Threads Enable this setting to highlight the current filtered thread times on the Frame Control graph. The Profile Analyzer highlights the timings in purple. This setting is disabled by default. Show Frame Lines Enable this setting to display the common frame boundaries as a horizontal line on the Frame Control graph. This setting is enabled by default. Order By Frame Duration Enable this setting to display the order of the frames by their duration from smallest to largest on the Frame Control graph, rather than by frame index. By default, this setting is disabled. This setting is particularly useful to group similar performant frames together."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/frame-summary.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/frame-summary.html",
    "title": "Frame Summary | FSM Unity Framework",
    "keywords": "Frame Summary The Frame Summary pane displays a summary of the frame times for the range of data selected. This pane provides useful information about the frames in the data selection, including the maximum, minimum, upper and lower quartile, mean, and median timings. The Frame Summary pane in Single view (left,) and Compare view (right) The Frame Summary pane is a good way for you to see at an overview what frames might be an outlier and how evenly distributed the timings are in the data set. For example, in the above screenshot, in the Compare view, while the median values are fairly similar, with little difference, the maximum frame value differs greatly, which suggests that the Right data set had more spikes in performance, that you could focus investigating further. Statistics Statistic Description Frame Count The number of frames selected in the Frame Control pane. In Compare view, the Left count corresponds to the first data set loaded into the Profile Analyzer, colored blue, and the Right count corresponds to the second data set loaded into the Profile Analyzer, colored orange. The Diff count is the difference in frame count between the Right and Left values. When this number is negative, it indicates that the Left frame count is larger than the Right frame count. When the Diff is a positive number, it means that the Right frame count is larger than the Left frame count. Start The frame number that the data selection starts on. In Single view, you can click the button next to this number to jump to the relevant frame in the Profiler window. End The frame number that the data selection ends on. In Single view, you can click the button next to this number to jump to the relevant frame in the Profiler window. Max The largest (maximum) frame time in the data selection. In Compare view, the Diff column shows the difference between the Right and Left timings. Upper Quartile Displays the upper quartile of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Median Displays the median value of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Mean Displays the mean value of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Lower Quartile Displays the lower quartile of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Min The smallest (minimum) frame time in the data selection. In Compare view, the Diff column shows the difference between the Right and Left timings. Underneath the statistics, the Profile Analyzer displays the timings as a histogram and box and whisker plot. For further information on the statistics available and how to analyze them, see the documentation on Statistics in the Profile Analyzer."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/index.html",
    "title": "About the Profile Analyzer package | FSM Unity Framework",
    "keywords": "About the Profile Analyzer package The Profile Analyzer aggregates and visualizes frame and marker data from a set of Unity Profiler frames to help you understand their behavior. You can use the Profile Analyzer to compare two sets of data side-by-side, which complements the single frame analysis already available in the Unity Profiler. For information on how to use the Profile Analyzer, see the documentation on the Profile Analyzer window The Profile Analyzer window, with two sets of data loaded Installing the Profile Analyzer To install this package into versions of Unity that support the package manager follow the instructions in the Package Manager documentation. When using any Unity version newer than 2021.2.0a5, you can just click this link to install it by name. Earlier versions of Unity For earlier versions, follow this link to the Profile Analyzer download and place the contents into your Project's Assets folder. Requirements This version of the Profile Analyzer is compatible with the following versions of the Unity Editor: 5.6 and later Known limitations The Profile Analyzer has the following known limitations: The original Profile data is not saved in the Profile Analyzer .pdata file. Therefore, you should store both the Unity Profiler .data file (or the .raw file exported from a stand alone player) along with the .pdata file. When you click on a marker, the Profile Analyzer attempts to jump to the same marker in the Unity Profiler if the same data is loaded. You must make a selection in the Unity Profiler beforehand for this to work. In the Unity Profiler in the UI view, the vertical height is not correct. On Unity 5.6 and 2017.4, the Thread Select Window allows you to collapse the 'All' threads item, which has no purpose."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/marker-summary.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/marker-summary.html",
    "title": "Marker Summary | FSM Unity Framework",
    "keywords": "Marker Summary The Marker Summary pane summarizes the marker data of the current selected item in the Marker Details list. You can use this pane to get an overview of the markers that contribute the most amount of time to the data set. For more information on the statistics in this pane, see the Statistics documentation. The Marker Summary pane, in Single View (left), and Compare View (right) Statistics Statistic Description Marker name Displays the name of the selected marker. Mean frame contribution Visualization of the marker's mean frame contribution as a percentage of the data set's total time. First frame Link to the frame that the marker first appeared in. Click the button to jump to the relevant frame in the Profiler window. Top by frame costs Displays the longest occurrences of the marker in the data set. Use the dropdown to display up to 10 entries in the list. Max The largest (maximum) frame time in the data selection. In Compare view, the Diff column shows the difference between the Right and Left timings. Upper Quartile Displays the upper quartile of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Median Displays the median value of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Mean Displays the mean value of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Lower Quartile Displays the lower quartile of the data set. In Compare view, the Diff column shows the difference between the Right and Left timings. Min The smallest (minimum) frame time in the data selection. In Compare view, the Diff column shows the difference between the Right and Left timings. Individual Max The maximum value of an individual marker instance. Individual Min The minimum value of an individual marker instance."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/ordering-frames-by-cost.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/ordering-frames-by-cost.html",
    "title": "Ordering frames by length | FSM Unity Framework",
    "keywords": "Ordering frames by length This workflow explains how to order the frame data in the Profile Analyzer by frame length, shortest to longest, which results in an s-curve style graph. The Profile Analyzer groups frames with similar performance together, which enables you to create sub-ranges over a set of similar performing frames without including any outliers. This is particularly useful to normalize the data and to focus on only the shortest, longest, or average frames in the data set. This workflow applies for both Single view and Compare view. Step 1: Collect performance data to analyze Open the Profile Analyzer window (menu: Window > Analysis > Profile Analyzer) and collect some profiling data. To pull data from an active profiling session, click the Pull Data button. This pulls in the current set of available frames from the Profiler. If you don't have an active profile session, click the Open Profiler Window button, then load or record some data. For more information on how to collect data, see the workflow documentation on Collecting and viewing data. Step 2: Order the data set If you are in the Compare view, enable the Pair Graph Selection checkbox. By default, the data set is ordered by frame number. To order the frames by their length, right click on a graph in the Frame Control pane, and select Order by Frame Duration. The Profile Analyzer then orders the graph from shortest to longest frame. Step 3: Select frames of interest Click and drag on the graph to select a range of frames with similar performance from the middle of the distribution. Top, the Frame Control pane in Single view, with the graph ordered by frame duration, and a range of frames selected. Bottom, the Frame Control pane in Compare view, with the graph ordered by frame duration, and a range of frames selected. You can now use the Profile Analyzer to analyze data from frames that have a similar performance."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/profile-analyzer-window.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/profile-analyzer-window.html",
    "title": "Profile Analyzer window | FSM Unity Framework",
    "keywords": "Profile Analyzer window The Profile Analyzer window visualizes frame, thread, and marker data from the Unity Profiler window. You can use it to analyze the performance of specific markers, threads, and frames. To open the Profile Analyzer window, in the Unity Editor go to menu: Window > Analysis > Profile Analyzer. The Profile Analyzer window on start up, with no data loaded. The Profile Analyzer has two views: Single: The Single view displays one set of Profiler data. You can use this view to analyze specific events on one set of profiling data. Compare: In the Compare view, you can load two sets of Profiler data to compare. You can use the Compare view to compare two different sets of profiling data, or to compare different events in the same data set. In both views, the Profile Analyzer displays min, max, median, mean, and lower/upper quartile values of the selected frame range. It also displays information on the distribution of each profiler marker in histograms and box and whisker plots. Pulling and analyzing performance data The Profile Analyzer only analyzes CPU data. It can either analyze data from the active set of frames loaded into the Profiler, or from a saved Profile Analyzer .pdata file. The Profile Analyzer aggregates the activity of each marker, and generates useful statistics to help you analyze the marker costs over a number of threads and frames. This includes summing the runtime and counts of each occurrence of every marker for all active threads and frames the Profile captured. Pulling data from the Profiler window To pull data from an active profiling session, click the Pull Data button. This pulls in the current set of available frames from the Profiler and visualizes the analyzed results. If you don't have an active profile session, click the Open Profiler Window button, then load or record some data. Loading and saving a data set You can save or reload any data that the Profile Analyzer analyzes at any point in time. This means you can share your analysis with others and load the results into the Single or Compare view at any time. To save the data from the Profile Analyzer, click the Save button in any view. This saves the data in the .pdata format. To load this data, click the Load button in any view. Note If you select the Load option, the data must be in the Profile Analyzer .pdata format. If you have data from the Profiler in the .data file format, open it in the Profiler first, and then select the Pull Data button in the Profile Analyzer. For more information on how to pull data into the Profile Analyzer, see the workflow documentation on Collecting and viewing data. Window navigation The controls and navigation for the Profile Analyzer window are very similar in both the Single and Compare view. The Profile Analyzer window has the following controls across both views, which you can access at the top of the window: Control Function Single Enter the Single view mode. This is the default view mode. Compare Enter the Compare view mode. You can compare two sets of profiling data in this mode. Export Export the data into a .csv format. When you click this button, a dialog box opens and you can choose from: Marker table: Exports the marker data visible in Single view Single Frame Times: Exports the frame data visible in the Single view Comparison Frame Times: Exports both sets of data visible in the Compare view. This button is disabled if you haven't imported any data into the Profile Analyzer window. Open Profiler Window Click this button to open the Profiler window. When the Profiler window is open, this button changes to Close Profiler Window. When you load some data into the Profile Analyzer window, the window populates with the profiling data, and is laid out as follows: *The Profile Analyzer window in Single view Pane Description A: Frame control Displays individual frame timings, ordered by frame index or frame duration. You can also use this pane to select a range of frames for the Profile Analyzer to analyze. For more information on this pane, see the Frame control documentation. In Compare View, to select the same range on both sets of data, enable Pair Graph Selection underneath the charts and then click and drag a selection on either of the charts. For more information on this pane, see the Frame control documentation. B: Filters Use the filter pane to limit the data that the Profile Analyzer displays. You can filter by partial match name, thread, or depth slice. You can also exclude markers to remove any markers that aren't relevant from the filtered view. For more information on how to use the filter pane, see the Filter documentation. C: Top 10 markers on median frame(s) Visualizes the ten highest duration markers that contributed to the frame. In Compare view, this shows the ten highest markers for both sets of data. You can select any of the markers in this pane to see more information on them. The Profile Analyzer reflects changes you make to the Depth filter in this pane. In Compare view, to adjust how the Profile Analyzer draws the markers against each other, use the Ratio dropdown. Normalised displays the two data sets relative to their own time at the selected depth. Longest displays the absolute difference between the two sets at the selected depth. D: Marker details for currently selected range (Single view) Marker Comparison for currently selected range (Compare view) A sortable list of markers, with detailed information on their timings. For more information on the particular statistics available in these panes, see the documentation on Single view marker details list and Compare view Marker Comparison list. E: Frame summary Displays a summary of frame times. For more information on this pane, see the documentation on Frame Summary. F: Thread summary Displays information about the threads in the data set. For more information, see the documentation on the Thread Summary pane. G: Marker summary Summarizes the marker data of the current selected item in the Marker Details list. For more information, see the Marker summary documentation. Frame buttons If the data you analyze in the Profile Analyzer is also loaded into the Profiler window, you can click on the frame buttons in the Profile Analyzer window to jump to the relevant frames in the Profiler. The Frame Summary pane with the frame buttons highlighted. Frame range selection You can limit the analysis to a subset or selection of frames. To do this, you can click and drag on the chart in the Frame Control pane at the top of the Single and Compare views and select a range, or use the context menu commands to select a range. For more information on using the frame range pane, see the Frame Control pane documentation. Filtering You can also filter the data to limit the number of markers the Profile Analyzer displays. This includes filtering by thread, call depth and name substrings. You can order the remaining markers by any of the available metrics in the marker list control. For more information on how to use the filter system, see the Filters pane documentation."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/single-view.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/single-view.html",
    "title": "Single view | FSM Unity Framework",
    "keywords": "Single view The Single view displays information about a single set of profiling data. You can use it to analyze how markers perform across frames. The view is divided into several frames, which contain information on frame timings, as well as min, max, median, mean, and lower / upper quartile values for frames, threads, and markers. For information on navigating the window, see the Profile Analyzer window navigation documentation. The Single view in the Profile Analyzer window, with some data loaded Loading data To load data into the Single view, select the Pull Data button in the frame control pane, and the Profile Analyzer pulls in any data in the Profiler window. Alternatively, select the Load button to load Profile Analyzer (.pdata) data you have saved from a previous session. Note If you select the Load option, the data must be in the Profile Analyzer .pdata format. If you have data from the Profiler in the .data file format, open it in the Profiler first, and then select the Pull Data button in the Profile Analyzer. For more information on how to pull data into the Profile Analyzer, see the workflow documentation on Collecting and viewing data. Marker details list The Marker details pane contains a sortable list of markers with a number of useful statistics. If you select a marker in the list, the Marker Summary panel displays in depth information on the marker. Each marker in the list is an aggregation of all the instances of that marker, across all filtered threads and in all ranged frames. You can filter the columns in the Marker details to a more relevant set. This is particularly useful if you want to filter out irrelevant data when you look for Time or Count values. To filter the columns, select the Marker columns dropdown from the Filters pane. For more information on how to filter data, see the Filters documentation. Marker details columns and groups By default, the Marker columns dropdown in the Filters pane has six preset column layouts that you can use to adjust the layout of the Marker details pane. They are: Time and count: Displays information on the average timings and number of times the markers were called. Time: Displays information on the average timings of the markers. Totals: Displays information about the total amount of time the markers took on the whole data set. Time with totals: Displays information about both the average and total times of the markers. Count totals: Displays information about the total number of times the markers were called. Count per frame: Displays information on the average total per frame the markers were called. Depths: Displays information on where the markers are in the Hierarchy. For more information, see the documentation on Depth Slices in Filters pane. Threads: Displays the name of the thread that the markers appear on. For more information, see the documentation on the Thread window in Filters pane. You can also use the Custom column layout, to select your own custom mix of columns to add to the layout. To do this, right-click on the header of any column, and manually enable or disable any of the columns as necessary. The following table shows the columns that the Profile Analyzer displays when you select that layout. Time and count Time Totals Time with totals Count totals Count per frame Depths Threads Custom only Marker Name ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Depth ✓ ✓ ✓ ✓ ✓ ✓ ✓ Median ✓ ✓ ✓ Median Bar ✓ ✓ ✓ Mean ✓ SD ✓ Min ✓ ✓ ✓ Max ✓ ✓ ✓ Range ✓ ✓ ✓ Count ✓ ✓ Count Bar ✓ Count Frame ✓ ✓ Count Frame Bar ✓ Count SD ✓ 1st ✓ At Median Frame ✓ ✓ ✓ Total ✓ ✓ Total Bar ✓ ✓ Threads ✓ The following table explains what each column does: Column Description Marker Name Displays the name of the marker. Depth The depth in the hierarchy that the marker appears on. Median The median value of the marker's time distribution. Median Bar A visualization of the Median value. Mean The average value of the marker's time distribution. SD The standard deviation of the marker's time distribution. Min The minimum value of the marker's time distribution. Max The maximum value of the marker's time distribution. Range The difference between the marker's Min and Max timings. Count The number of times the marker was pushed / popped. Count Bar A visualization of the Count value. Count Frame The average number of times per frame that the marker was pushed/popped. Count Frame Bar A visualization of the Count Frame value. Count SD The standard deviation of Count value. 1st The frame number in which the marker first was pushed / popped. At Median Frame The sum of activity for the marker in the median frame. Total The total time spent for this marker in all selected frames. Total Bar A visualization of the Total value. Threads The name of the thread that the marker appears on. Marker Details context menu commands If you right click on a marker in the Marker Details list you can control the filter and list even further. Command Function Select Frames that contain this marker (within whole data set) Select all the frames from the entire data set that contain an instance of this marker. Select Frames that contain this marker (within current selection) Select all the frames from a selected range of data that contain an instance of this marker. Select All Selects the entire data set, if you have a range of data selected. Add to / Remove From Include Filter Add or remove the selected marker to the Include filter. This filters the marker list to only markers that match. Add to Exclude Filter Add the selected marker to the Exclude filter. This removes the marker from the marker list. This is useful if you want to remove markers that are using up resources and skewing the markers that you are interested in. Set as Parent Marker Filter Limit the analysis to this marker and markers included below it on the callstack. For more information, see the Parent Marker documentation on the Filters page. Clear Parent Marker Filter Select this to clear the marker as a parent marker filter. Copy To Clipboard Copies the selected value to the clipboard."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/statistics.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/statistics.html",
    "title": "Statistics in the Profile Analyzer | FSM Unity Framework",
    "keywords": "Statistics in the Profile Analyzer The Profile Analyzer displays a number of statistics about the data you're analyzing in the Frame, Thread, and Marker Summary panes.. Available statistics Statistic Description Min Min represents the lowest (minimum) value for the marker or frame time. Max Max represents the largest (maximum) value for the marker or frame time. Median Median is the middle value of a data set, and separates the higher half from the lower half of a data set. Mean Mean is the average value in a data set. It represents the sum of all of the values in the data set divided by the number of values in the data set. Lower and Upper Quartiles The lower quartile is the middle number between the smallest number and the median of the data set. The upper quartile is the middle value between the median and the highest value of the data set. Interquartile Range The interquartile range shows the range of values in the central 50% of the data. The range is equal to the difference between the upper and lower quartile values. How the statistics are represented The statistics are displayed in several ways in the Profile Analyzer. In the Frame, Thread, and Marker summary panes, the statistics are displayed as raw numbers as well as in histograms and box and whisker plots to give a visual representation of time distribution. The following section gives some examples of some common distributions you might find in your analysis. Single view Even distribution In this example, the graphs display a distribution of marker calls that range from 16.75ms to 17.26ms. The histogram on the left shows that a lot of the buckets are being hit at a fairly even amount. This is also evident in the box and whisker plot on the right where the box is large and is towards the middle of the upper and lower bounds. Outlier In this example, the graphs display a distribution of marker calls that range from 0.67ms to 5.32ms. The histogram on the left shows that the lower end buckets are used the most and only some of the more expensive buckets are hit. This is also reflected in the box and whisker plot, where the box appears towards the bottom of the range but the whisker, or upper bound of the range, is high up. Compare view Similar distribution In this example, there are two distributions that are similar, and both the histogram and box and whisker plots show a very similar pattern. This shows that the marker activity in both sets is similar. Different distribution In this example, there are two distributions that are different; both the histogram and the box and whisker plots show that the marker in the left (blue) data set ran for longer. The histogram shows that the blue data set used more expensive buckets, and the box and whisker plot is drawn higher up on its range. This means that the marker activity in left (blue) data set is more costly and should be investigated further. Overlapping distributions In this example, there are two distributions that are similar. Both data sets have the same lower bound and have some overlap in the middle of the range, but the right (orange) dataset uses some of the more expensive buckets and has a higher upper bound. This means that the activity in right (orange) data set is more costly or is being called more times and should be investigated further."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/thread-summary.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/thread-summary.html",
    "title": "Thread Summary | FSM Unity Framework",
    "keywords": "Thread Summary The Thread Summary pane displays information about the threads in the data set. You can use the information in this pane to compare and understand the number of threads used in the data set. By default, the Profile Analyzer only displays information about the Main Thread. To add more threads to the analysis, use the Thread button in the Filters pane. For more information, see the documentation on the Filters pane. The Thread Summary pane in Single View (left), and Compare View (right) Statistics Statistic Description Total Count The total number of threads in the data set, or data selection. The Compare view also has a Total column, which displays the total count for all threads across both data sets. Unique Count (Compare view only) The number of unique threads in each data set. A unique thread is one that is not in the other data set. The Total column displays the total of unique threads across both data sets. Selected The number of threads selected in the data set. To add more threads use the Thread button in the Filters pane. Graph Scale Select a scale for the plot. You can choose from: Median frame time Upper quartile of frame time Max frame time. At the bottom of the pane, there is a summary of the median run time of the current filtered threads with a box and whisker plot of them. You can use these graphs to analyze which threads Unity spends the most time on."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/upgrade-guide.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/upgrade-guide.html",
    "title": "Profile Analyzer upgrade guide | FSM Unity Framework",
    "keywords": "Profile Analyzer upgrade guide You do not need to take any actions to upgrade your project when you update this package."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/whats-new.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/Documentation~/whats-new.html",
    "title": "What's new in 1.2.2 | FSM Unity Framework",
    "keywords": "What's new in 1.2.2 Summary of changes in Profile Analyzer version 1.2.2. The main updates in this release include: Added Added support for removing vsync time, so we can see actual CPU duration over multiple frames. A drop down has been added to 'remove' a marker from the analysis and has entries for \"FPS Wait\", \"Present Wait\" and \"Custom\" where you can select any marker from the table using the right click context menu to \"Remove Marker\". Added optional standard deviation (SD) column into marker table (on single view) for sorting data based on variation of marker timings in over the frames. Added export of the comparison table as CSV. For a full list of changes and updates in this version, see the Profile Analyzer package changelog."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Profile Analyzer copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/README.html": {
    "href": "Library/PackageCache/com.unity.performance.profile-analyzer@1.2.2/README.html",
    "title": "Profile Analyzer | FSM Unity Framework",
    "keywords": "Profile Analyzer What Is This The Profile Analyzer aggregates and visualises frame and marker data from a set of Unity Profiler frames to help you reason about their behaviour over a number of frames, complementing the single frame analysis already available in the Unity Profiler. Features: Multi frame analysis of a single scan Each marker is shown with its median, min, max values over all frames, including histogram and box and whisker plots to view the distribution Various filtering options are available to limit the markers displayed by thread, call depth and name substrings. Data can be sorted for each of the displayed values. Comparison of two multi frame profile scans Each marker is shown with the difference in values between the scans, including with a visualisation to help quickly identify the key differences. Supports comparison of scans from two different Unity versions, or before and after an optimization is applied. How To Run Add the Profile Analyzer folder to your Unity project or install as a package. The 'Profile Analyzer' tool is opened via the menu item below the 'Window/Analysis' Menu in the Unity menu bar (or just in the 'Window' menu prior to 2018.1). Capturing Data Use the standard Unity Profiler to record profiling data from your application. In the Profile Analyzer pull the profiler data from the Unity Profiler. The profile data will be analyzed and appear in the single view along with both sides of the Compare View. This capture can be saved as a .pdata file for later comparision or sharing with others. Comparing Two Data Sets Pull data or load a previous .pdata file into the Left and Right slots to compare the two sets, comparison results will instantly appear. More Information For more information on the UI and common workflows please see the full documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Addons/readme.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Addons/readme.html",
    "title": "Enabling FBX Integration | FSM Unity Framework",
    "keywords": "Enabling FBX Integration Install FBX Exporter 2.0.1 or newer via Package Manager. Add the following dependencies to Unity.ProBuilder.AddOns.Editor.asmdef in the \"references\" array: \"Autodesk.Fbx\" \"Unity.Formats.Fbx.Editor\" Open Project Settings and add PROBUILDER_FBX_2_0_1_OR_NEWER to the Scripting Define Symbols field."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [5.2.2] - 2023-11-27 Fixed [PB-88] Fixed a bug that prevented the ProBuilder window to update its content after changing its docking position. [5.2.1] - 2023-11-06 Fixed Fixed a bug that prevented editing meshes or enabling view tools after the new shortcut improvements were added to the Editor. [5.2.0] - 2023-10-02 Fixed [PBLD-75] Fixed a bug where duplicating a GameObject with ProBuilderMesh children would not make unique Mesh for the duplicates. [PBLD-87] Fixed a bug where an infinite recursion loop could crash the editor when using the Boolean tool. Internal Updated analytics API for 2023.2 and after. Features Significantly improved performance of \"ProBuilder Window.\" [5.1.1] - 2023-08-01 Fixed Fixed an exception in CombineMeshes.AccumulateMeshesInfo when a ProBuilderMesh contains an empty Material array. [case: PBLD-78] Fixed a regression where it was no longer possible to set the extrusion point with the Poly Shape tool. [case: UUM-43960] Fixed warning on project import caused by merge conflicts in a meta file. [5.1.0] - 2023-06-01 Fixed [case: PBLD-70] Fixed a bug where unused materials were not removed from the mesh renderer when deleting faces. [case: PBLD-52] Fixed a bug where unused materials were not removed from the mesh renderer when detaching faces. [case: STO-2878] Fixed ProBuilderMesh renaming Mesh asset any time the component instance ID was modified. [case: STO-2878] In Unity Editor 2023.1 and newer, fixed an issue where reverting a prefab instance containing a ProBuilderMesh component would leak a Mesh asset in the scene. [case: PBLD-61] Fixed a bug where drawing a shape of size zero was causing errors and incorrect values in ProBuilderShape. [case: PBLD-65] Fixed stair shape \"Inner Radius\" parameter not being correctly applied when placing new stair shapes in scene. [case: PBLD-63] \"Strip ProBuilder Scripts\" now creates a mesh asset, fixing an issue where prefabs would no longer contain a valid mesh reference after stripping ProBuilder scripts. [case: PBLD-64] Fixed \"Strip ProBuilder Scripts\" not correctly appyling changes when used in a prefab stage. [case: PBLD-54] Fixed a possible error when inserting edge loop. Arch shape now allows 2 sides. Changes Moved cut tool cursor icon from runtime to editor resources. Added ProBuilderMesh.MakeUnique to ensure Mesh asset is distinct. Internal Fixed \"Detach Faces\" not creating unique mesh asset for duplicate objects. [5.0.7] - 2023-04-04 Fixed [case: PBLD-48] Fixed a bug where the minimum size of a shape did not take into account snap and grid sizes. [case: PBLD-34] Fixed a bug where Experimental Features Enabled was not activating when using Dedicated Server platform. [case: PBLD-41] Fixed an issue where UV distribution metric would not recalculate after the mesh optimization step. [case: PBLD-32] Fixed New Shape start location being incorrect after using right mouse button. [case: PBLD-19] Fixed shape creation when the camera perspective is set to Top. Made minor performance improvements and reduced allocations when querying for components. [case: PBLD-38] Fixed an issue where exported assets would incorrectly use the UInt32 mesh index format. [case: PBLD-43] Fixed an issue where activating the Edit Shape tool would prevent the Editor tool context from switching. [case: PBLD-57] Fixed error when building player with EntityBehaviour applied to prefabs. Changes Updated Object.FindObjectsOfType to use the new Objects.FindObjectsByType in Unity 2023.1. [5.0.6] - 2022-06-30 Bug Fixes [case: 1407518] Fixed issue where 'Detach Faces' action would not undo correctly. [case: 1393809] Fixed move tool when working with small scales. [case: 1395936] Fixed Editor crash when opening a EditorWindow dropdown (MacOS). [case: 1389642] Fixed Grid snapping not working properly with EditShape Tool. [case: 1368465] Fixed issue where extruding an element orthogonally to their normal would result in some additional extrusion along the normal. [case: 1369443] Fixed SerializedObjectNotCreatableException errors in the console with Shape Tool. [case: 1348463] Fixed issue where instantiating a prefab would not build UV2s. [case: 1348434] Added more detailed instructions in the missing UV2 warning log. Fix some styling issues with Overlays in 21.2 [case: 1350635] Fixed crash when using CSG operations. [case: 1405226] Fixed tooltips inconsistently showing and hiding. [case: 1407039] Fixed stair creation tool missing the inner radius parameter. [case: PBLD-3] Fixed vertex manipulation tools locking grid snapping settings on activation. [case: PBLD-9] Fixed issue where adding and then removing Collider or Trigger behaviours would cause meshes to not render in builds. [case: PBLD-7] Fixed ProBuilderize action not handling redo operation correctly. [case: PBLD-6] Fixed unnecessary reimport of all project textures on package install. [case: PBLD-11] Fixed Poly Shape Tool missing an undo step after setting height. [case: PBLD-13] Fixed a bug where a newly created shape would not be redrawn with Redo. [case: PBLD-15] Fixed a bug with URP that prevented some items from being selectable in the Game view. Changes Updates to API documentation Cylinder shape is allowed to have 3 sides, and can have an odd number of sides. [5.0.4] - 2021-06-08 Bug Fixes [case: 1334017] Fixed errors while exporting a PBShape using FBX Exporter and cleaning export. [case: 1332226] Fixed issue where some Gizmos menu items would be missing in projects that have ProBuilder package installed. [case: 1324374] Fixed incorrect vertex/edge/face rect selection when mesh's parent is rotated and/or scaled. [5.0.3] - 2021-04-01 Bug Fixes [case: 1324399] Fixing errors when building with prefabs in scene. [case: 1323666] Preventing to assign an empty mesh as MeshCollider. [case: 1322032] Fixing wrong ProBuilderMesh colors on domain reload when null. [case: 1322150] Fixing torus and sphere UV generation when creating New Shape. [case: 1320936] Fixing 'ProBuilderize' action creating empty submeshes. Changes Property Experimental Feature can now be reset with the rest of ProBuilder preferences. Add GameObject/ProBuilder menu to create primitives with default dimensions. [5.0.2] - 2021-03-11 Bug Fixes Fixed Draw Shape tool showing incorrect UVs when setting shape bounding box. Fixed Draw Shape tool not clearing selection when changing the active shape type. Fixed Cut Tool error when pressing Backspace with no vertices placed. Fixed Cut Tool error when finishing a cut segment with less than 3 vertices. Fixed Draw Shape tool truncating shape property fields in the Scene View Overlay. Changes Moved contents of warning box in Draw Shape tool to tooltips. Updated manual documentation. [5.0.1] - 2021-03-09 Bug Fixes Disable unstable test on Linux. [5.0.0] - 2021-03-08 Features Redesigned shape creation workflow. Shapes are now interactively drawn in the Scene View, and remain configurable after the point of creation via the ShapeComponent Inspector. Default shapes can still be created through the GameObject/ProBuilder menu. Added Point to Point Cut tool. Added a selection preview when using the Select Path tool. Added Selection X Ray option to highlight occluded element selections with a muted color. Default shortcut is Alt + Shift + X (modifiable in Shortcut Manager). Added Analytics for Actions and Menu Shortcuts Bug Fixes [case: 1304442] Update package description for SRPs : warning to users to add samples projects. [case: 1300329] Fixing Undo completely reverting all actions. [case: 1299638] Fixed missing dependency on com.unity.modules.physics. [case: 1296104] Fixing freeze transform with negative scales. [case: 1296494] Fixing audio visualizer sample. [case: 1296428] Cleaning Polyshape tool when leaving the EditorTool. [case: 1296427] Removing NullRef Exception on Undo/redo with CutTool. [case: 1296422] Closing MenuToolToggles actions when closing ProBuilder window. [case: 1296520] Fixing New Poly Shape menu entry not working [case: 1254339] Correct offset when rendering UVs and correct export when UV Editor is docked. Fixed PolyShape in Prefab mode: PolyShapeMode was not serialized when exiting prefab mode. Update point insertion visualization. [case: 1259845] Fixed dimension overlay being hidden on playmode or reboot of the editor. [case: 1267383] Fixed Bezier Shape and Poly Shape component preventing build when Script Stripping was enabled. [case: 1256246] Ensuring edges subdivision is not creating hole and that arch shapes does not create degenerated triangles Scaled Auto UVs now remain in place when modifying geometry. [case: 1265296] Add tooltips to UV Actions window. [case: 1265623] Ensure that ProGrids snapping is enabled (not only active) when using snapping in ProBuilder [case: 1256154] Fixed StackOverflow Error when smoothing mesh in Probuilderize, also fixed an error in the display of edges when count > ushort.maxValue. [case: 1252668] Replaced mesh template tests for Connect Edges with more stable methods. [case: 1262236] Ensure PolyShape is not empty to avoid nullref while exporting. [case: 1184921] Add a custom preview for ProBuilderMesh prefabs. [case: 1276085] Fixed UV Actions window allowing resize events outside of the containing window. [case: 1277788] Take into account the default parent object in Unity 2021 and up. [case: 1276074] Fixed a case where Fit UVs action could result in NaN values. [case: 1281254] Fixed shader warning in URP for UNITY_PASS_FORWARDBASE macro redefinition. Fixed rect selection not working with Universal Render Pipeline. [case: 1283107] Fixed Bevel settings slider disappearing when values exceed 1. [case: 1283103] Fixed typo in Center Pivot tooltip. [case: 1283067] Fixed Export Prefab throwing an error when overwriting the root asset of an exported prefab instance. [case: 1284735] Fixed a possible exception when creating a shape, undoing, then redoing while the Smooth Group Editor window is open. [case: 1283111] Fixed Poly Shape tool not snapping placed vertices with grid snapping enabled. [case: 1284741] Fixed missing tooltips for some items in the Smooth Group Editor window. [case: 1283167] Fixed Mesh Collider mesh value not updating with modifications. [case: 1285651] Fixed tooltip going out of screen when screen display is scale up [case: 1285654] Fixed selected faces highlight for isometric camera mode in sceneview. [case: 1286045] Fixed selection cleaning problem after scene restart. [case: 1266769] Fixed tooltip window not rendering correctly on Linux. [case: 1281658] Fixed warning when modifying a PBMesh with particule effect using PBMesh as shape emitter. [case: 1317148] Fixed edge selection returning incorrect results with some Unity versions. [case: 1312537] Fixed script stripping on disabled objects when building. [case: 1311258] Fixed material reverting when subdividing edge. [case: 1317773] Fixed undo after shape creation. Changes Modified VertexManipulationTool to inherit from EditorTool. Adding a new MenuAction in Samples to merge faces from edges selection. Removing preprocessor directives for Unity 2018 and below for Probuilder 5.0. Modified the AppendVerticesToEdge to handle edges split for non-convex faces. Removed unused \"About\" images. Removed unused HDRP shader variants. MergeElements.Merge moved to public API. Upgraded PolyShape tool to EditorTool and correct some features in it. Updated documentation for ProBuilder 5.0.0. Add GameObject/ProBuilder menu to create primitives with default dimensions. Added com.unity.modules.physics and com.unity.modules.imgui modules as dependencies. Internal Remove backwards compatibility breaking API changes. Fix Material.SetInt deprecation warnings. Fix failing Undo tests on macOS. [4.4.0] - 2020-08-12 Features Added a Select Path tool. The default shortcut is Ctrl + Shift + Click or Cmd + Shift + Click. Added iterative selection on edges. Bug Fixes Ensure \"ProBuilderize\" action is enabled for current selection on opening main window. [case: 1258589] Fixed error in runtime sample examples. Fixed warning in ShapeEditor caused by duplicate using statements. [case: 1258421] Fixed an issue where GI UV streams would be lost at runtime due to ProBuilderMesh assigning a new Mesh to the statically combined MeshFilter. [case: 1251289] Fixed m_WireMaterial null reference exception when re-importing ProBuilder. Fixed TooltipEditor.Hide affecting performance linearly with scene size. [case: 1259506] Fixed shortcut not being saved in 2018.4. Fixed vertex colors not applying gamma-correct value when color space is Linear. [case: 1251574] Disable export options when no probuilder meshes are selected [case: 1248708] Fixed physx error when welding all vertices to a singularity. Fixed tooltips always clamping to the left of the screen on secondary monitors. Fixed EditorUtility.SetIconEnabled not respecting the enabled parameter. [case: 1241105] Fixed an issue where Select Edge Loop could overflow when encountering certain non-manifold geometry. [case: 1242263] Fixed UV Editor Move and Rotate tools throwing null reference exceptions as of Unity 2020.2.0a9. [case: 1251289] Fixed exception on script reloads in the EditorMeshHandles class. Fixed exception on script reloads in the EditorMeshHandles class. Reduced the amount of repaints in ProBuilder window. [case: 1249071] Fixed Shift selection being inconsistent. [case: 1249056] Fixed Ctrl selection being inconsistent. [case: 1247270] Fixed Null Reference Exception when entering Prefab Stage after merging multiple ProBuilderMesh. [case: 1252394] Fixed dragging selection with Ctrl problem (and key pressed problem in general). Fixed Cylinder shape clamping segments to 48. Changes Update to Settings Manager 1.0.3. Known Issues Prefab Stage does not work properly when ProBuilderMesh contains overrides that append geometry. Changelog dates pre-2.4.11-f.0+r4081 are incorrect. These releases were made from SVN, and the history was lost when converting to Git. [4.3.1] - 2020-06-01 Bug Fixes [case: 1251289] Fixed exception on script reloads in the EditorMeshHandles class. [4.3.0] - 2020-05-15 Features Added UI to reset Shape Editor parameters. Make public and document the Poly Shape component. Bug Fixes [case: 1242879] Fixed import error caused by UPM CI log file. [case: 1230949] Fixed EditorMeshHandles losing material references in some cases. [case: 1232389] Fixed Null Reference Exception thrown on applying Subdivide Object on complex New Poly Shape [case: 1238115] Fixed Exception thrown on selecting UV 2 mode with \"Lock the SceneView handle tools\" is enabled in the UV Editor Fixed New Shape menu item always creating a Cube instead of the last selected shape. [case: 1237636] Fixed exception being thrown when stripping pb components and smooth groups editor is open [case: 1230069] Fixed selection commands not being remapped to ProBuilder when in edge/face/vertex mode (Select All/Invert Selection/Deselect All) [case: 1225223] Fixed Mesh Collider component missing a reference to the mesh in builds. [case: 1225427] Fixed UV Editor exporting the UV template offset by 11 pixels when the editor window was dockable. Fix ProBuilderMesh.sharedTextureLookup throwing a null reference exception when accessed from runtime. [case: 1209522] Fixed Poly Shape component allowing incompatible Preset feature. [case: 1213742] Fixed bug where Delete menu item would incorrectly shows as available with no selection. [case: 1192479] Fixed an issue where translating UV positions in the UV Editor with a handle would not update the Inspector offset values. [case: 1176370] Fixed entering Play Mode with the Shape Editor open creating a new shape in the scene. [case: 1218413] Fixed Poly Shape input incorrectly snapping vertices when input plane is not on a grid. [case: 1223330] Fixed a crash when undoing the creation of a Poly Shape object. Fixed Shape Editor preferences not respecting \"Reset All Preferences\". [case: 1132509] Fixed Bevel sometimes resulting in non-conforming face normals. [case: 1158748] Fixed Export and Strip Scripts actions leaving Mesh Filter component hidden on resulting GameObjects. [case: 1224413] Fixed Shape Editor leaving behind a preview object if window is closed during play mode. [case: 1201746] The Move tool is now compatible with grid snapping (Unity 2019.3 and higher). [case: 1217930] Fixed duplicated objects shape reverts to original's mesh when entering play mode [case: 1214103] Fixed ProBuilder created meshes not rendering in project builds. [case: 1217024] Fixed inverted picking bias when cursor is not hovering selected object. [case: 1195261] Fixed an issue where the ProBuilderize action could cause the toolbar to emit GUILayout Group errors. [case: 1211721] Fixed tooltips in the ProBuilder toolbar not rendering on macOS. [case: 1167627] Fixed boolean operations not retaining material information. [case: 1210096] Fixed UV Editor rotation field allowing values outside of 360°. [case: 1214103] Fixed ProBuilder created meshes not appearing in built projects. [case: 1211169] Fixed Generate Shadow Object example throwing a Null Reference Exception when invoked. [case: 1209864] Fixed New Shape tool creating new GameObjects with the same name. [case: 1201858] Fixed Export action not showing a warning when attempting to export an empty selection. [case: 1205318] Fixed OBJ export not retaining material color information when Scriptable Render Pipeline is active. [case: 1194858] Fixed a Null Reference Exception in some cases when using the Bevel tool. [case: 1204731] Fixed marquee selection of mesh elements when using the High Definition Render Pipeline. [case: 1211096] Fixed tooltips rendering off screen in some cases. [case: 1203685] Fixed Poly Shape creation tool not accepting input when Scene View gizmos are disabled. [case: 1161998] Fixed an issue where scene objects could be selected through modal windows. [case: 1198568] Fixed MeshFilter and MeshCollider always showing properties as \"Overridden\" on Prefab instances. [case: 1204088] Fixed UV Editor actions window not using the mouse event, allowing inactive window properties to appear as interactable. [case: 1183101] Fixed broken help link in Smoothing Editor window. [case: 1173650] Fixed an issue that resulted in vertices and edges becoming unselectable on macOS in some cases. Fixed Edge pre-selection highlight not rendering on macOS when using Metal as a graphics backend. [case: 1198568] Fixed issues with prefab overrides not being applied to ProBuilderMesh components correctly. [case: 1198568] Fixed MeshFilter and ProBuilderMesh components incorrectly showing instance overrides on un-modified prefab instances. [case: 1194858] Fixed Bevel Edges throwing NullReferenceException in some cases. [case: 1208475] Fixed Set Pivot and Center Pivot actions throwing NullReferenceException if the selected mesh contains children. [case: 1183100] Fixed the Shape Editor Stair slider fields appearing too small to contain the text when Android is the selected build target. [case: 1206302] Fixed an issue with the Merge action that could result in invalid geometry. [case: 1161998] Fixed an issue where ProBuilder meshes could be selected through overlaying Editor windows. [case: 1198588] Fixed a rare case where ProBuilderMesh could throw errors due to an invalid internal state. [case: 1196134] Fixed an issue where resetting component data on a ProBuilderMesh through the Inspector window would not update the MeshFilter and scene gizmos. Fixed an issue where an invalid selection on ProBuilderMesh could prevent the selected mesh from being edited due to errors. Fixed case where the default material could fail to initialize when a Scriptable Render Pipeline is in use at runtime. Changes [case: 1203585] Removed Custom Shape option from the Shape Editor window. The ProBuilder Mesh component now has an icon in the Inspector. The MeshFilter component is now hidden by default on GameObjects created by ProBuilder, and the ProBuilderMesh component is renamed in the Inspector to ProBuilder MeshFilter. Known Issues Certain properties on ProBuilderMesh, MeshFilter, and MeshCollider always show as overridden on prefab instances. The ProBuilderMesh icon is always the Pro Skin version. The ProBuilderMesh icon is toggle-able in the Gizmos window (initial value is 'Off'). Marquee selection when the Universal Render Pipeline is enabled does not work (currently blocked due to URP missing required functionality). [4.2.1] - 2019-11-22 Features Added the Offset Elements action to quickly move selected mesh elements in world, local, or element space. Added the ability to set a custom range in the Subdivide Edges options window. Added the ability to set wireframe and edge line width on macOS when using Metal as the graphics backend. Bug Fixes Fixed Boolean Editor menu item not respecting the Experimental Features Enabled preference. Fixed Boolean Editor preview images not updating with selection changes. Fixed potential error when pressing the Object Mode shortcut without a ProBuilder Editor instance available. Fixed an issue where a currently editing text field would lose focus when a ProBuilder tooltip was shown. Fixed a case where Weld Vertices could leave the mesh selection in an invalid state. Fixed an issue where a ProBuilder tooltip could appear when hovering on a window on top of the toolbar. Fixed the ProBuilder toolbar background color applying to entire button (Unity 2019.3 only). Fixed Select by Material sometimes returning incorrect results. Fixed Merge Objects not retaining active GameObject components and properties. Added a dialog when New Poly Shape fails to find an unlocked Inspector window. Fixed case where New Poly Shape could create a mis-aligned object when used with ProGrids. Fixed edge case where the ProBuilder selection could become desynchronized with the Unity selection. Fixed UV Editor window minimum width not being sufficient to accommodate the toolbar. Fixed a case where duplicate ProBuilderMesh components could share a reference to the same Mesh. Fixed an issue that caused mesh element picking to stop working for some users. Fixed the Smooth Group window toolbar buttons not showing active state correctly. Fixed a case where selecting a previously edited ProBuilderMesh could append to the prior selection instead of replacing it. Fixed some cases where ProBuilderMesh would incorrectly enforce a max vertex count. Fixed an issue that caused errors to thrown when entering play mode with the Shape Tool window open. Fixed the Shape Tool destroying and recreating the preview GameObject when adjusting settings. Fixed an issue where the Poly Shape editor would destroy the GameObject if initialized with an invalid path. Fixed a potential error when undoing a ProBuilderize action. Fixed a warning when importing ProBuilder to macOS projects using Metal as the rendering backend. Changes [Samples] Rename LWRP to Universal Render Pipeline. Added support for holes when creating shapes from a polygon (API only). [Samples] Remove obsolete references from EventSystem GameObject. [Samples] Remove GUILayer components from Camera. Shape Tool settings are now persistent as user settings. [4.1.0] - 2019-07-03 Features Added the option to export assets either as prefabs, or just the mesh. Improved the naming of exported mesh assets. Added API examples as a sample package. Re-enabled FBX Exporter integration, adding support for quad export and automatic removal of ProBuilder components on export. Added ability to duplicate faces to a new game object or to new submesh. Added the option to toggle the dimensions overlay between object and element bounds. Added a shortcut to toggle the dimensions overlay between object, element, and off states. Added a Duplicate Face action. The UV Inspector is now resize-able from all sides and corners. Added the ability to set a Poly Shape height to 0 for a single face plane. Add support for HDRP and LWRP pipelines through Samples packages (available in the Package Manager UI for ProBuilder). Added the ability to copy UV transform values between manual and auto unwrapped faces. Added an explicit toggle to enable or disable rendering the background texture when exporting UV templates. Added additional methods for testing and fixing face topology (MeshValidation.ContainsNonContiguousTriangles and MeshValidation.EnsureFacesAreComposedOfContiguousTriangles). Bug Fixes Fixed new shapes not instantiating in prefab staging scenes. Fixed Unlit Color and Face Highlight shaders flickering in some cases. Fixed shortcuts not saving correctly in Unity 2018.3. Fixed a potential exception when an incompatible version of ProGrids is present in the project. Fixed an exception when maximizing and un-maximizing the ProBuilder editor window. Fixed the Boolean Editor incorrectly resizing it's contents. Fixed an exception when Connect Edges is executed on an edge with coplanar faces. Fixed a potential exception in the Normals class when vertex count exceeds ushort.max. Fixed an issue when upgrading ProBuilder from a version lower than 4.0.0 that would result in meshes with multiple materials being condensed to a single material. Fixed a potential exception when removing ProBuilderMesh components from code. Fixed an inconsistency in UV projection that could result in faces being unwrapped differently between Unity versions. Fixed obscured edges sometimes taking priority over visible edges when picking elements. Fixed Select Holes action incorrectly showing as disabled in some cases. Fixed compile errors when opened in Unity 2018.4. Fixed scene information view not showing the correct selected element counts. Fixed vertex dots rendering slightly offset from the vertex position with an orthographic camera. Fixed Poly Shape creation tool not recognizing terrain when adding the origin point. Fixed Select Faces with Color not selecting faces with no color. Fixed compile error in runtime samples on Unity 2019.3. Fixed Split Vertices not collecting coincident vertices when selected with a mouse click. Fixed case where Apply Material would not be registered for Undo. Fixed Export OBJ resulting in corrupted files when exporting multiple objects as a single model. Fixed Poly Shape tool incorrectly rendering a mesh preview before the shape is finalized. Fixed case where Poly Shape could leave the active tool in an invalid state. Fixed case where drag selecting a single vertex would enable \"Collapse Vertices\" in toolbar Fixed case where drag-and-dropping material onto selected faces applies material to all faces if \"Edit UVs in Scene\" enabled Fixed Box Project UVs not resetting UV coordinates to origin. Fixed naming conflict with UnityEngine.Snapping in Unity 2019.3. Fixed Apply Material not respecting the current face selection when editing UVs. Fixed the dimensions overlay graphics not updating when modifying vertices. Fixed bug where selecting an element with the shift key held would not make it the active selection. Fixed Poly Shape tool not clearing the mesh when an invalid path is created. Fixed Collapse Vertices action showing as available in some cases where the action was not applicable. Fixed an issue where setting the toolbar to use icon or text mode would not immediately refresh the UI. Fixed bug where exporting an OBJ could fail if a mesh did not have vertex colors. Fixed the shortcut for Copy UVs on macOS referencing Control instead of Command. Fixed an issue where the ProBuilder toolbar font size would initially be very small, then later return to the correct size (specific to Unity 2019.3). Fixed a bug that caused the Material Editor to not render the preview material on HDPI screens. Fixed a null reference error when attempting to subdivide a face with non-contiguous triangles. Changes Assembly definitions now have Auto Referenced enabled, meaning it is no longer required that a project use Assembly Definition files in order to access the ProBuilder API. Project layout restructured such that the git url may now be used as a version in the project manifest. Improved the default UV layout of Cone shape. Settings Manager is now a dependency instead of bundled with code. Changed the default value of Apply Transforms to false when exporting models. Changed the default value of As Group to true when exporting models. [4.0.4] - 2019-03-13 Bug Fixes Set the pivot of merged meshes to the active selection pivot instead of origin. Fixed Reset Auto UVs not breaking element and texture group associations. Fixed in-scene texture tool setting faces to manual when editing in face mode. Fixed undo not refreshing the UV editor graph. Fixed UV editor tools remaining disabled after a selection change. Fixed Auto Stitch UVs action not getting correct edge for adjacent faces. Fixed Boolean Tool expecting certain mesh attributes to be present, causing errors in some cases. Fixed texture groups not saving correctly in some cases. Fixed selected object outline not rendering in specific cases. Fixed typo in Mirror Objects tooltip. Fixed Export Asset breaking prefab instances. Vertex Color action tooltip no longer mentions deprecated paint mode. Fixed changelog dates for January 2019. Fixed Material Editor rendering incorrectly when an SRP material is assigned to the \"Quick Material\" field. Lessen an edge case where Math.PointInPolygon could return the wrong value. Fixed an exception when keyframing mesh properties via Timeline. Fixed project settings not being consistently being saved in certain cases. Fixed assertion when importing ProBuilder to an empty project. Fixed warning in tests caused by a left-over meta file from a temporary directory. Fixed creating a new PB shape with the Shape Tool does not dirty the scene. Fixed MaterialEditor not filtering out unused submesh index when applying new material. Fixed Vertices/Faces/Edges picking in Prefab mode. Fixed Action window in UVEditor not scrollable. Fixed deep selection on faces shouldn't trigger when multiple faces are selected. Fixed UV action panel scroll does not work if nothing or just one vertex selected. Fixed ProBuilderize action potentially creating un-editable geometry if \"Import Smoothing\" is enabled. Fixed change to one member of a UV group not replicating to the other member of the group. Fixed UV group not having their settings applied at the same time. Fixed sometimes needing to click twice on a face in the UV editor to the right actions. Fixed de-selecting vertices not working properly in certain situations. Fixed edge selection sometimes picking edge behind the mesh. Fixed UV panel shows incorrect info when zero UVs are selected. Fixed deep Selection should not trigger if no elements are selected. Fixed Element Selection lost after using Weld action. Fixed UVs converting to manual when merging 2 objects. Fixed Selection being discarded when connecting edges. Fixed trigger and collider types not always hiding properly. Fixed Box Project resulting in skewed UVs. Fixed in-scene Texture Move tool not translating UVs at a constant speed when scaled. Fixed Weld Vertices and Collapse Vertices sometimes leaving invalid edges in affected faces. Fixed ProBuilder window not updating when toggling handle orientation from shortcut key. Fixed Subdivide action resulting in horizontally flipped UV coordinates. Fixed Box Project action resulting in horizontally flipped UV coordinates. Fixed UV Editor preview texture sometimes not appearing. Fixed lag when setting select mode via shortcuts. Frame bounds focuses on element selection when in Object mode. Apply material incorrectly applies to face selection when in Object mode. Dimension overlay displays element bounds when in Object mode. Fixed handle position not updating when the element selection mode changed. Fixed Strip ProBuilder Scripts on Build not being respected. Fixed Export OBJ not sharing vertex data, resulting in split vertices in other DCCs. Changes Name newly merged GameObjects after the active selected GameObject. Changed the order of UV transform operation (rotation and scale are now called before applying the anchor). Changed picking to now prefers the object hovered over the selected object. Added \"Open ProBuilder\" button back to ProBuilderMesh component Editor. Increased the minimum size of Auto UV Editor window to not require vertical scrollbars. Significantly improve the accuracy of Convert to Auto UV action. Add support for auto UV unwrapped faces in the Auto Stitch UVS action. Use ShortcutManager API to manage single-key shortcuts. Added option to show current tool delta in the Scene View (\"Show Handle Info\".) [4.0.3] - 2019-01-25 Bug Fixes Fix an issue that caused version validation to run when entering playmode, resulting in large memory spikes and lag. Fix tooltips closing other popup dialogs. [4.0.2] - 2019-01-18 Bug Fixes Fix OBJ export failing due to missing materials. [4.0.1] - 2019-01-16 Bug Fixes Add missing [3.0.9] changelog entry. Update package metadata to meet current requirements. [4.0.0] - 2019-01-14 Features New public API. Project now distributed as source code, with assembly definition files. Add experimental pre-selection highlight for vertices and faces (enable in Preferences / ProBuilder / Experimental). Improve the behaviour of vertex and edge selection with hidden faces. Add ability to resize the UV settings window. Dimensions overlay now works with mesh element selections. Update FBX Exporter integration to use version 2.0.0. Improve performance of UV calculation methods. Improve the default UV mapping of sphere primitives. Support new 2018.3 prefab system. Redesigned Lightmap UV workflow now exposes settings on the ProBuilderMesh component, provides a modifiable default value, and is generally smarter about keeping Lightmap UV channels in sync with changes. Improve performance of toolbar rendering by caching some frequently accessed selection information. Redesigned settings interface, now supports search and resetting individual fields. Add support for Pivot and Center handle position toggle. Handles now support operating in selection space (Position: Pivot + Orientation: Normal). Texture scene tool now supports vertices and edges. Improve performance of mesh rebuild functions. Improve performance of vertex, edge, and face gizmos. Respect Unity pivot mode and pivot orientation (note that setting Orientation: Normal overrides the Unity pivot orientation). Add a preference to disable depth testing for mesh selection highlights. Add manual documentation. Bug Fixes Fix regression that broke dragging and dropping GameObjects onto ProBuilder meshes. Fix Poly Shape and Bezier Shape incorrectly resetting materials to default. Fix Export not generating UV2 in some cases. Fix Export functions not refreshing the Project view. Fix edge colors not matching preferences. Fix oversized vertex handle pre-selection billboard. Fix Collapse Vertices breaking mesh topology. Fix \"UV Overlap\" warnings on default shapes when baking GI. Fix mismatched plane width, height segment fields. Fix ProGrids \"Push to Grid\" affecting un-selected vertices. Fix Extrude incorrectly applying smoothing groups to extruded face sides. Fix Detach to GameObject sometimes including children in duplicated GameObject. Fix vertex handle pre-selection gizmo drawing 2x larger on scaled screens. Fix \"Detach to GameObject\" deleting the current face selection. Fix deprecated GUID check running on every domain reload. Fix ProBuilderize importing quad topologies with incorrect winding. Fix Extrude Edges sometimes splitting vertices when extruding as a group. Fix toolbar vertex actions showing as available in some cases where not applicable. Fix case where drag selecting mesh elements could clear the current selection. Fix element preselection highlight incorrectly showing when a GUI control has focus. Fix Create Poly Shape throwing errors in some cases. Fix Connect Edges action showing incorrect results in notifications. Fix incorrect use of object finalizers in some classes. Fix vertex drag selection with \"Select Hidden: Off\" omitting distinct but coincident vertices. Fix changes to MeshRenderer materials being incorrectly reset by ProBuilder. Fix Delete Faces tooltip not showing \"Backspace\" as the shortcut key on Windows. Fix Auto UV settings inspector not allowing certain properties to be edited with multiple selections. Fix face, edge, and vertex modes requiring user to first select an object before registering element selection when clicking. Fix bug where adjusting shape creation parameters would move the preview mesh. Fix bug where the mesh element gizmos would not respect the screen DPI on startup. Fix Pipe and Sphere shapes not setting a consistent pivot point. Fix possible null reference exception when deleting the Shape Editor preview mesh. Automatically destroy invalid Poly Shape objects when the selection is lost. Detach to GameObject now sets the detached object as a child of the donor mesh. Fix vertex billboards not rendering when the backing graphics API does not support geometry shaders. Fix new shapes not instantiating on the grid when Snap New Shape To Grid is enabled. Fix potential error when Poly Shape enters an invalid path. Fix Poly Shape not instantiating on grid when ProGrids is enabled. Fix toggling static flags not consistently rebuilding lightmap UVs when Auto Lightmap UVs is enabled. Fix Poly Shape not aligning with the ProGrids plane. Fix FBX Exporter incompatibilities breaking compiliation. Fix stair shape showing dimensions field twice. Fix incorrect wireframe overlay when editing a Poly Shape. Changes Tests and documentation are no longer imported with package, significantly improving initial import times. Face selection highlight is now rendered with both front and back faces. Adding custom actions to the ProBuilder toolbar is now done by registering an attribute. ProBuilder Debug Editor removed. Rename MenuAction::DoAlternativeAction to DoAlternateAction. Simplify assembly definition files, merging ProBuilder.Core & ProBuilder.MeshOperations to single assembly. Minor performance improvements to some common mesh editing actions. Remove \"Precise Element Selection\" preference. Project preferences are no longer saved in the Assets directory (now located at \"ProjectSettings/ProBuilderSettings.json\"). Improve performance of normal and tangent calculations. MeshSelection.Top() becomes MeshSelection.top property. Include third party dependencies as source code with assembly definitions instead of pre-compiled DLLs. Performance optimization for selection changes in editor. Make auto-resizing colliders opt-in instead of on by default. Use the last selected mesh element as the active selection pivot, matching object selection. Remove \"About\" window. Expose APIs necessary for mesh element picking at runtime. Changes since [4.0.0-preview.41] Testing new CI runner. [3.0.9] - 2018-05-30 Fix exporting to OBJ and PLY not refreshing assets when the destination directory is in the project. Fix bug that broke drag and dropping prefabs onto ProBuilder meshes. [3.0.8] - 2018-05-07 Bug Fixes Fix incompatibility with ProGrids 3.0.1. [3.0.7] - 2018-04-30 Features Add a material property to the Poly Shape component. Bug Fixes Poly Shape creation now respects the user-set default material. [3.0.6] - 2018-04-19 Bug Fixes Fix upgrade prompt failing to show on version changes. [3.0.5] - 2018-04-16 Bug Fixes Fix About Window opening on every script reload event. [3.0.4] - 2018-04-10 Bug Fixes Use the default material preference for shapes created through the Boolean Editor. [3.0.3] - 2018-04-05 Features API Examples are now published on Github. Expose poly shape creation methods. Add API example. Support drag and drop materials to ProBuilder meshes. Mesh handles now use Unity gizmo colors by default. New options to set unselected and selected edge colors. New option to set edge and wireframe line width (not available on Metal). Bug Fixes Fix scene info not updating with selection changes. Fix Apply Material only applying to parent gameobjects if children are also selected. Fix pb_Object.SetSelectedFaces setting duplicate vertex indices. Fix Alt+Num material shortcut throwing null if Material Editor has not been opened and no default palette is found. Fix bug where Undo on a Poly Shape would reset any mesh edits. Fix preferences interface not updating after resetting all preferences. Fix bug where the edge picker would prefer vertical lines over horizontal. Fix wireframe rendering with unselected edge color in certain cases. Fix edge selection preferring vertical edges. Expand preferences window contents to match size. Don't show \"shortcuts were cleared\" warning if no prior version is detected. Fix overexposed imgui controls in scene view with scene lighting disabled. Fix certain actions switching the current scene focus. Fix Create Material Palette failing to create asset. Export model files with culture invariant settings. Fix scene info display overlapping ProGrids toolbar in some cases. Fix local preferences not loading until restarting the ProBuilder editor. Fix Boolean Editor rendering a white texture filling the entire screen. Changes Remove update checking. Expose pb_MeshImporter class, making \"ProBuilderize\" action available at runtime. [3.0.1-f.0] - 2018-02-12 Features ProBuilder now runs as package manager module. Custom color palettes for Vertex Color Editor. Added a new window to display licenses for third party software used in ProBuilder. Debug symbols are now included. Support partial and complete drag selection for edge elements. New Set Trigger and Set Collider replace the deprecated pb_Entity component. Add pb_ShapeGenerator.CreateShape(pb_ShapeType shape) function to build default primitives without requiring parameters. Add preference to set the static flags for new ProBuilder objects. Bug Fixes Fix \"Select Hidden\" preference not matching the state shown in toolbar on launch. Fix Standard Vertex Color shader errors in 2018.1 and up. Catch edge cases of bad input for Poly Shape, preventing console spam. Match element drag rect visual to Unity's. Fix Poly Shape objects not updating graphics on undo when not currently editing the contour. Fix \"Connect Edges\" notification showing 2x the amount of edges as connected. Fix ProBuilder Editor not refreshing after a ProBuilderize action. Fix UV editor throwing null if shader has no _MainTex uniform. Fix potential error if a menu action is called without a pb_Editor instance. Improve performance of pb_Editor selection caching, addressing lag when selecting high vertex count objects. Fix scene toolbar sometimes not matching free/pro skin. Fix washed colors in gui textures with linear rendering. Remove unused preferences from the ProBuilder/Preferences window. Fix torus and icosphere not remembering last used settings in a session. Fix Poly Shape editor repainting the scene view more than necessary. Corrected tool-tips for \"Clear Smoothing\" and \"Break Smoothing\" buttons. Changes Move Dimensions Overlay into ProBuilder menu. Move ProBuilder2.Common and ProBuilder2.EditorCommon namespaces to ProBuilder.Core and ProBuilder.EditorCore. Move pb_Object to ProBuilder.Core namespace. Known Issues \"Library already loaded\" warning logs when importing Package Manager over Asset Store install. \"Cannot create menu item\" warnings when importing Package Manager over Asset Store install. Deleting an Asset Store package before importing a Package Manager version does not automatically pop up the Convert To Package Manager editor window. API Examples are not accessible from Package Manager. Changes from 3.0.0-f.1 Fix pb_Fbx errors when Unity FbxExporter is present in project. Update Package Manager description and display name. [2.9.8-f.2] - 2017-11-01 Bug Fixes Fix shader compile errors when targeting mobile platforms. Fix possible overflow in vector hashing functions. Fix ProBuilderize failing to import quads in some cases. Fix FBX export not including manually unwrapped UVs. Fix toolbar using old icons with basic skin. [2.9.7-f.5] - 2017-10-23 Features Unity 2017.3 beta support. New toolbar icons (Right/Context + Click in Toolbar -> Use Icon Mode). Significantly improved quad detection in ProBuilderize function. ProBuilderize now able to import smoothing groups. Support exporting quads to FBX format (requires Unity FbxExporter in project). Newly redesigned Smooth Groups Editor. New Select Face Loop and Select Face Ring actions. Bug Fixes Fix possible null reference when picking ProBuilder objects. Fix \"Select Hole\" disappearing instead of showing as disabled. Fix \"Extrude Face\" disabled icon not matching current mode. Fix Standard Vertex Color shader preventing builds on some platforms when fog is enabled. Changes Don't show pb_Lightmapping warnings by default. Smoothing groups may now extend beyond the 42 provided in the editor. Any smoothing group between 1 and 24, or greater than 42 is treated as a smooth face (currently only accessible in code). Set ProBuilder Standard Vertex Color shader fallback to \"Standard.\" [2.9.5-f.3] - 2017-08-30 Features Deep selection support when clicking faces. Bug Fixes Drag select with \"Select Hidden: Off\" now works consistently in Unity 5.6 and up. Fix automatic lightmapping attempting to update while ProBuilder is modifying geometry. Changes Unity 4.7 and 5.0 are no longer supported (2.9.4 will continue to be available for these version of Unity). [2.9.4-f.1] - 2017-08-03 Features When a lightmapping finishes baking show a warning if any ProBuilder objects marked as Detail were left out due to missing UV2s. Bug Fixes Fix single key shortcuts emitting system beep on Mac. Fix vertex movement not respecting snap settings when ProGrids is placed in Plugins folder. Don't delete UnityEngine meshes when unloading a scene in play mode. Fix occasional crashes when using Select Hole and Fill Hole actions. Fix hang on opening context menu with very large projects. Fix Vertex Color Palette not applying colors correctly in vertex mode. [2.9.3-f.0] - 2017-07-13 Features Support exporting quads in OBJ format. Add a context menu item to quickly create material palettes from the current Asset window selection. Bug Fixes Use additionalVertexStreams mesh attributes where possible when ProBuilderizing. Fix for Windows version sometimes endlessly creating folders when root folder moved from Assets/ProCore. [2.9.2-f.1] - 2017-06-22 Bug Fixes Add file stubs for deprecated repair actions to prevent compilation errors when updating. [2.9.1-f.0] - 2017-06-12 Features New PLY model exporter. OBJ export rewritten, now supports: Multiple texture maps Vertex colors (MeshLab format) Texture map offset / scale Local or world space mesh coordinates PBR maps (http://exocortex.com/blog/extending_wavefront_mtl_to_support_pbr) Improve Mesh Asset export dialog and options. Bug Fixes Fix ProBuilder preferences sometimes not loading. Fix deprecated method warnings in 2017.1 beta. Fix occasionally flipped UV axis on merged faces. Don't crash if ProBuilder folder has been renamed. [2.9.0-f.3] - 2017-05-22 Features Support for saving and loading custom Material Palettes. ProBuilder now able to store preferences per-project as well as globally. Significantly improve performance of Weld Vertices function. Bug Fixes Preferences Window now renders correctly in Unity 5.6 and up. Exit Bezier Shape editing when Esc is pressed. Quell unnecessary errors when ProGrids interface fails to load UnityEditor assembly. Fix potentially ambiguous reference to Axis enum in API examples. Fix mesh leak when exporting STL files. Changes Move pb_Constant into ProBuilder2.Common namespace. Move pb_Lightmapping class in EditorCommon namespace. Improve wording of warning when shortcut preferences are reset. [2.8.1-f.0] - 2017-04-17 Features Improve grid snapping when placing Poly Shapes. Add a callback when a mesh is rebuilt (pb_EditorUtility.AddOnMeshCompiledListener). Remove max width limitation on Material Editor window. Bug Fixes Fix incorrect UV render scaling on retina and other scaled screens. Fix deprecated warnings on Handles calls in Unity 5.6. Fix Icosphere API example deprecated function calls in Unity 5.6. Fix UnityObjectToViewPos warnings Unity 5.6. Fix Poly Shape not generating UV2 for mesh. Catch an occasional Null Reference when viewing UV2 channel. Fix Null Reference in Poly Tool undo callbacks. Fix errors in adding pb_Entity script during repair mesh references action. Improve consistency of Vector2/3/4 hashing functions. Fix particularly slow function in MergeFaces action. Fix preferences GUI layout. Changes Start Poly Shape height at 0. First Poly Shape click always sets pivot. [2.8.0-f.1] - 2017-03-29 Features New \"Poly Shape\" interactive shape. New \"Bezier Shape\" interactive shape. Unity 5.6 beta compatibility. Improve default UV layouts for new shapes. Add a shader for reference billboard planes (\"ProBuilder/Reference Unlit\"). Bug Fixes Fix material editor applying to child transforms of selection. Fix instantiated objects not getting a UV2 channel when \"Auto Generate UV2\" is enabled. Material Editor now works with a relative path. Fix incorrect handle rotations in Element mode. About Window now loads even when not in Assets/ProCore/ProBuilder. Address a rare NullReferenceException when ProBuilder Editor is initialized. Don't spam Console with errors if update check fails to connect (only affects WebPlayer target). Add a more descriptive message to update check if connection fails. Changes Automatically toggle Detail Entity Type object's lightmapping static flag, preventing broken lightmap atlases. [2.7.0-f.7] - 2017-02-24 Features New redesigned \"About\" window. New \"Check for Updates\" window and menu item. Add a repair script to apply materials when upgrading from Basic to Advanced. Include option to restrict \"Select by Vertex Color\" to current selection. Add \"Generate Shadow Volume\" API example and action. Add preference to enable experimental features (current feature: Bezier shape). Add option to smooth round sides of cylinder in Shape Tool. Add repair script to strip and rebuild pb_Object from Unity mesh. Add repair script for rebuilding shared index caches (addressed IndexOutOfRange errors in RefreshNormals function). Bug Fixes Fix inconsistent About Window behavior when importing updates. Make face highlight code snippet more robust in Runtime Editing example. Don't leave a progress bar behind if Probuilderize fails. Fix ProBuilder-ize adding pb_Entity multiple times. Apply Quick Offset adjustments to all selected ProBuilder objects. [2.6.9-f.3] - 2017-01-27 Bug Fixes Fix vertex handles appearing offset when using an orthographic scene view. Mark KDTree Triangle and pb_Stl as Any Platform in build targets. Fix compile errors in standard vertex color shader on Unity 5.5 on iOS Fix compile warnings in Unity 5.6.0b4 Use multi-Unity-version compatible shader in Vertex Colors API example. [2.6.8-f.1] - 2017-01-16 Features Add option to restrict Select by Material to the current selection. Add alternate method of specifying torus radius dimensions. Bug Fixes Fix regression that broke iOS and Android build targets. Fix arch preview shape and built shape sides not matching. Fix issue where UV Editor could be out of sync with scene UVs after planar or box projection. Don't fail face or vertex picking when the required materials aren't found. Whenever a prefab change is detected rebuild the mesh. Fix null reference when attempting to bevel open edges. Remove unused and buggy Debug UVs shader. [2.6.7-f.4] - 2017-01-09 Features New face extrusion options to extrude along face normal, vertex normal, or per-face. Bug Fixes Fix potential drag selection inconsistencies when picking vertices in Unity 5.5. Fix bug where Backspace key in UV Editor with a GUI control focused would incorrectly register as a shortcut. Fix auto uvs rotating around handle pivot when using the rotate gizmo. Add undo for auto uv changes made by gizmos. Fix uv tiling toolbar not applying with mixed selection. Move pb_Reflection to Editor so as not to be included in builds, fixing errors when targeting Windows Store. Remove unused ParseEnum function in pbUtil for same reason. Retain smoothing group information when extruding. When flipping a selection of face edges where faces are not quads be specific about the reason for failing. Fix misc. potential hangs when registering ProBuilder objects for Undo. Changes Make default height segments for cylinder 0. Move some common class files into the Core folder so they're included with the Core lib instead of MeshOps. When grow selection by angle is off force iterative to on. [2.6.6-f.0] - 2016-12-09 Features Unity 5.5 support. Improve readability of Unity version -> ProBuilder package chart. Improve performance of mesh rebuild functions. Add API example showing how to highlight faces based on distance to a point. Bug Fixes Don't update mouse edge when not in geometry mode. Fix face and edge previews only rendering a subset of elements on meshes with high vertex counts. ProBuilder Editor doesn't need to be open to export meshes. Fix weird triangulation on Door shape. Fix some incorrect calls to pbUndo.RecordObjects that could cause Unity to lock up with large meshes. Fix bug where \"Export Mesh Asset\" would always create an extra folder. Changes pb_Object.Refresh now accepts a bitmask to enable/disable different component refreshes. Move Create ProBuilder Cube menu item to \"GameObject/3D Object/ProBuilder Cube\". [2.6.5-f.0] - 2016-11-11 Features Add procedural mesh extrusion example. Bug Fixes Fix bug where face/vertex picking with hidden selection off and intersect rect would not work in deferred rendering path. Fix bug when \"Meshes are Assets\" is enabled where exiting play mode would clear the mesh cache. Fix bug when \"Meshes are Assets\" is enabled where entering play mode would invalidate the mesh cache. Fix bug when \"Meshes are Assets\" is enabled where deleting a selection of pb_Objects would leave orphaned cached meshes. Fix bug where UV2 channel of selected objects would be lost on scene save with \"Meshes are Assets\" enabled. Fix bug where stripping ProBuilder scripts with \"Meshes are Assets\" enabled would also delete the cached mesh asset. Fix incorrectly scaling slider control on retina display. [2.6.4-f.1] - 2016-10-19 Features Unity 5.5 Beta compatibility. Bug Fixes Fix bug where generating UV2 would incorrectly merge incompatible vertices in the optimization function due to vertex references not being unique when returning from GeneratePerTriangleMesh. Add icons for Drag Selection Mode. Add Toggle Drag Rect Mode icons. Fix selected vertex billboards not rendering in 5.5 beta. Fix crash when setting pivot with multiple objects selected. Fix setting pivot of a parent object moving children. Don't disable the object outline when Probuilder is open, but do disable wireframe. Fix prism shape height resulting in half-sized shapes. Add Triangulate Faces icon. Ensure icons are always imported with the correct settings. Fix \"Generate Scene UVs\" toggle not being respected in Generate UV2 action Fix bug where drag selecting with shift in subtraction mode with complete rect selection would always deselect the entire selection. [2.6.3-f.4] - 2016-09-23 Features New Vertex Positions Editor provides fine-grained control over vertex postions. Add Anchor setting for Auto UVs to align faces before user transforms. When drag selecting faces add option to select by intersection. Toggle with menu item Drag: Complete/Intersect. New icons for Select by {Material, Color}. Significantly improve editor performance when drag selecting faces and vertices with Select Hidden: Off. New alternate drag selection modes: Add, Subtract, and Difference affects how the Shift key modifies selection when drag selecting elements. Bug Fixes Fix bug where using Undo in face selection mode could potentially delete faces on prefabs. Fix import settings on icons. Fix bug where Fill Hole would not correctly align normals after operation, sometimes also reversing neighboring face normals. Fix bug where Fill Hole would sometimes leave the filled faces with invalid edge caches (resulting in incorrect normals on extruding from the filled face). Added miscellaneous missing actions to documentation. Fix bug where drag selecting faces would sometimes leave the picker rect visible. Disable Select By Material/Color when no elements are selected. Changes Use Unity's default UV2 unwrapping parameters since they seem to produce generally better results than the padded params currently in use. New ProBuilder objects instantiate with ShadowCastingMode.TwoSided (configurable in Preferences/ProBuilder). [2.6.2-f.0] - 2016-09-02 Features Add STL file export support. Standard Vertex Color shader now compatible with Unity 5.5 beta changes. Add API Example showing how to set custom default UV2 Unwrap Parameters. Bug Fixes Fix bug where exporting OBJ would sometimes insert \"AllFiles\" into the file path. Fix drag selecting edges then bridging sometimes using the default material instead of a neighboring one. Fix compile error when using Fog with Standard Vertex Color shader. Fix console warnings in Unity 5.5.0b1 Fix bug where Select Edge Loop would select too many edges. Don't show missing icon warnings unless in PB_DEBUG mode. [2.6.1-f.0] - 2016-08-26 Features Add Triangulate Face action. Add ability to view UV2/3/4 channels in UV Editor. Add ability to edit per-object UV2 generation parameters in the Generate UV2 options menu. Improve performance of \"Grow Selection\" when flood selecting with angle restriction. Improve performance of some selection actions when in face mode. Add RenameNewObjects script to API examples folder (shows use of OnProBuilderObjectCreated delegate). Add \"Select Faces with Material\" and \"Select Faces with Vertex Color\" to the Selection menu. New options icon in toolbar: gear instead of triple lines. Bug Fixes Fix \"About\" window showing every changelog ever instead of just the latest. Fix bug in pb_Math.Normal(pb_Face) overload that would potentially return normals facing the wrong direction if fed ngons. Fix UV Editor incompatibilities with retina display on macOS. Fix bold label text color in Debug Window when Pro skin is used. Increase max allowed vertex handle size to 3 to accomodate macOS retina display. Fix import settings for Center Elements disabled icon. In ProBuilder-ize function don't bother showing 'include children' dialog if the top selection already contains all valid meshfilters Fix Advanced icon in PB Basic rendering blurry in toolbar. When freezing transforms also apply rotation in world space. Fixes some issues when freezing hierarchies of objects. Fix bug where edge ring would include faces with odd number of edges. Added \"Fill Hole\" and \"Subdivide Edge\" to documentation. Fix occasionally flipped face normals when connecting edges or vertices on n-gons. When connecting edges weed out any edges that don't connect to anything, preventing accidental edge subdivisions. Changes Make default angle error for uv2 unwrap a little higher to avoid bad unwraps in some common cases. Move ProBuilder-ize function to menu actions. [2.6.0-f.1] - 2016-08-02 Features Add Bevel Edges action. Add Fill Hole action to quickly insert a face in a mesh hole (with option to fully select and fill hole or just the selected parts). Completely rewritten documentation: http://procore3d.github.io/probuilder2/ Add Select Hole menu action to quickly select the edges of any hole touching a selected vertex. Add a preference to disable \"Precise Element Selection.\" When disabled edge and vertex modes will always select an edge or vertex no matter how far from the element they are. Add \"Break Texture Groups\" button to UV editor. Add non-manifold edge extrusion pref to Extrude Edge settings window. Replace mesh optimation functions with faster and more accurate versions. Improve performance of topology query operations (Grow Selection, Shrink Selection, Edge Loop, Edge Ring). Center Pivot action now available in Basic. Add Generate UV2 toolbar entry when \"Disable Auto UV2 Generation\" is enabled. Add a delegate in pb_EditorUtility to notify subscribers when a new pb_Object has been initialized. New API example Tools > ProBuilder > API Examples > Log Callbacks Window demonstrates hooking into various editor delegates. Adds an experimental new option to store Mesh objects as Assets in the project so as not to clutter the Unity scene file. Use with a prefab for maximum scene lean-ness. Enable this feature in Preferences/ProBuilder/Experimental/Meshes Are Assets. Add support for local/common toggles in Edge Debug mode. Add Select Holes action to editor toolbar (selects all connected open edge paths). Connect {Edge, Vertices} re-factored for speed and more robust edge case handling. New \"Options\" button for toolbar icons. Improve performance of Delete Faces action. Improve performance of Subdivide action. Add Alt-S shortcut for Subdivide action. Add option to Mirror action to either duplicate or move the selection when mirroring. Bug Fixes Fix some instances where modifying a mesh would result in NaN warnings. Fix icosphere audio example scene in ProBuilder Basic. Add Center Pivot action to menu. Bypass sRGB sampling for icons, fixing dark appearance in pro skin. Fix regression where switching between icon mode and text mode in toolbar would sometimes not immediately reload the toolbar. Fix an issue where meshes would be discarded and rebuilt on every instance id change, which Unity does a lot. The result of constant mesh rebuilds being invalidating the lightmap, making getting a decent bake very difficult. Ignore API examples in any build target that hasn't been tested (which is all of them save for standalones). Fix edge extrusion leaving black geometry when extrusion fails. Add extrude settings button to edge extrude toolbar item. Add a single context-sensitive Extrude shortcut so that super+e works properly in both edge & face modes. Fix 'KeyNotFound' exception when centering pivot sometimes. Fix UV3/4 assignment and getter functions reading out of bounds channel index. Fix Delete key notification not showing. Fix editor toolbar \"leaking\" due to incorrect hideflags in Unity 4. Fix cases where user could provide bad input to Arch generator. Fix Weld Vertices not welding vertices in some cases. Set detail pb_Objects with ReflectionProbeStatic flags. Fix key shortcuts for hidden but enabled menu actions not working. Don't show hover tooltips if mouse is outside window bounds. Fix some edge cases in Conform Normals action. Fix Grow Selection itererative field incorrectly being disabled when \"Grow by Angle\" is off. Fix issue where n-gons with > 90 degree angles would not auto UV unwrap correctly. Fix some cases where subdivide would fail due to non-planar vertex positions on a face. Fix bug where extruding edges or faces would sometimes align the inserted face normals incorrectly. Hide geometry actions when in object mode. Fix edge selection when mouse is hovering an object not in the selection but a valid edge is within distance of mouse. Fix bug where subdividing a face with an adjacent concave n-gon would break the adjacent face. When generating the menu item text for shortcuts always use lower case, since Mac doesn't recognize upper case as shortcuts. Fixes an issue with shortcuts not working on OSX. Support cases where texture groups on pb_Object aren't in linear order. Clear debug lines when a selected object is deleted. Fix bug where Detach Faces to submesh would incorrectly split all the detached selection vertices. Put UV Editor in namespace, preventing errors where common function names would be confused with other assets. In pbUndo use each individual object vertex count when deciding whether to diff or store object state for undo. Fixes hang when performing actions with small selections on large objects. Lower UV toolbar buttons by 1px when not using the Command GUIStyle since Button style adds 1px padding. When building ProBuilder delete user generated folders so that upgrades don't overwrite them. Changes Menu toolbar re-arranged for consistency. Remove UV2 generation parameters from pb_Object. Add a public function for setting tangents on pb_Objects. Deprecate GenerateUV2 extension method since mesh optimization is now an intertwined process. Improve hashing function in IntVec3 and Edge. Suffix pb_Math.Approx functions to make implicit casting of vectors more difficult to do accidentally. Move \"World Space\" toggle up in the Auto UV editor In Auto UV mode rename the scale property Tiling. Detach Selection now behaves like toolbar option panel instead of popup. Remove unnecessary option to save duplicates of selected gameobjects when using Merge Objects action. In addition to changing the icon and text, also show a brief explanation of the current handle alignment mode in the tooltip. Move Mirror to object level, making it an action instead of panel popup. [2.5.0-f.0+r4241] - 2016-04-07 Features Toolbar redesign now adapts to both vertical and horizontal layouts (swap between Text and Icons by context clicking in the Inspector). New tooltips show inline documentation and keyboard shortcuts. Hold Shift to instantly view hovered tooltip, and turn off tooltips on hover in Preferences menu. New Subdivide Edges action inserts vertices along selected edges. GUI items are no longer stored in Resources. Changing the location of the ProBuilder directory is still supported. Add option to collapse vertices to the first selected vertex instead of always averaging. Mark the current mode and floating state in the toolbar context menu. Add preference toggle to disable Dimension Overlay lines. New Color Mask setting in Vertex Painter Editor enables painting only to specified component. Vector4 UV{3, 4} channels can now be stored in pb_Object (use pb.SetUVs(index, List )). Bug Fixes Fix shortcut editor modifier keys not being correctly stored. Fix Freeze Transforms moving objects when selection contains hierarchies of meshes. Entity visibility toggles no longer interferes with Collisions, as well as remembers all manually changed object visibility. Fix Element Toolbar placement in Scene view on Retina display Macs. Fix UVs all being set to {0,0} when using Weld in the UV Editor. When extruding an edge check that the new face winding order is equivalent to the face of the donor edge and flip if necessary. Fix shortcut editor not recognizing all keycode values (notably Alpha0-9). Changes Remove option to display Mode Toolbar in the Inspector window. ProBuilder2.Math namespace removed, pb_Math now belongs to ProBuilder2.Common. [2.4.11-f.0+r4081] - 2016-04-07 Bug Fixes Fix regression in 2.4.10f2 that broke assigning materials in ProBuilder Basic. API Add onEditLevelChanged delegate to pb_Editor to notify other classes of edit level changes (Polybrush compatibility). [2.4.10-f.2+r4027] - 2016-04-07 Features Shape and Material windows are now dockable (context click in window and select Window/Set {Floating, Dockable}). Add \"Snap to Face\" shortcut when dragging the move tool (hold 'C' while dragging to snap tool to the nearest face). New ShaderForge compatible Standard Vertex Color shader on ProBuilder default material. Bug Fixes Unity 5.4 compatibility. Workaround for Unity crash \"Check DisallowAllocation Error\" Fix most cases of meshes going completely black when modifying them in any way. Fix NullRef error when scaling a single selected edge. Changes Remove various Get{Vertices, UVs, Triangles} functions from pb_Object. Use pbUtil.ValuesWithIndices directly instead. Remove Instantiation API Example (there's nothing special about instantiating ProBuilder meshes anymore). [2.4.9-f.1+r3978] - 2016-04-07 Features New \"Flip Edge\" tool swaps the direction of a connecting edge in a quad. Bug Fixes Fix bug where Trigger or Collider entities could remain visible in play mode when using source. Fix slowdowns when inserting edge loops due to undo. Fix missing namespace errors in Unity 5.3. Increase the resolution with which vertex positions are compared to avoid incorrectly merging distinct vertices (often causing trouble when modeling at very small dimensions). [2.4.8-f.1+r3764] - 2016-04-07 Features Enable Set Pivot, Delete, and Vertex Painter in ProBuilder Basic. New Standard Shader with vertex color support (thanks to Unity Forum user @defaxer). Bug Fixes Add tooltips for every action in the ProBuilder toolbar. Fix consistent horizontal scrollbar showing in ProBuilder window. Smooth edges of curved stair sides, and align step UVs to match rotation. Use white text color in Dimensions Overlay when Unity Personal skin is used. Ensure DLLs retain GUID between releases, enabling simpler upgrades. Fix poor UV editor precision when working with small distances. Fix ~10px vertical offset image when rendering UV template in Unity 5. Fix slightly offset image when rendering UV template from docked UV Editor window. Changes Increase minimum allowed zoom in UV editor. Make warning shown when connecting edges or vertices fails a bit more descriptive. Don't show tangents and bitangents when Show Normals is enabled in the smoothing editor. Prototype becomes ProBuilder Basic. [2.4.7-f.0+r3664] - 2016-04-07 Changes New upgrade procedure skips complicated Upgrade Kit in favor of a slightly more manual but more reliable approach. See FAQ or ProBuilder Documentation for more information. Bug Fixes Fix possible null reference error when working with prefabs that have been duplicated. Additional error checking when stripping pb_Objects from scene added. When ProBuilder-izing objects, ask user whether or not to traverse children. [2.4.6-f.0+r3616] - 2016-04-07 Features Add preference to set default Entity type. Add preference to set dedicated keyboard shortcuts for entering Object, Face, Vertex, and Edge mode. New Curved Stair generator, and stairs now produce manifold geometry. Add \"Batch Upgrade\" menu items to Upgrade Kit, allowing users to run one action for an entire project. New GUI slider allows un-clamped input to the float field in Shape Tool. Bug Fixes Alt + E shortcut now works with only 2 vertices selected. Fix bug where colliders would be incorrect when instantiating trigger entities. Fix some cases where \"Connect\" would result in incorrect geometry. Fix UV editor not recognizing shortcuts sent from scene view. Fix occasional \"Non-finite value in mesh vertices\" error when extruding. Account for inconsistently sized vertex color arrays when ProBuilder-izing meshes. Fix null reference errors when Shape Creation Tool is open with preview enabled and a script reload takes place. Fix null reference sometimes caused by a Mirror action. Fix bug where merging objects would always add a MeshCollider, even if one already exists. Fix mesh bounds not refreshing when adjusting vertices with Quick Offset tool. Fix mis-calibrated drag selection wwhen first entering element mode after moving an object. Fix issue where duplicating GameObjects with child pb_Objects would leave references to original meshes intact, resulting in odd behavior when deleting objects. Fix bug where prefabs would not \"Apply\" changes to all children equally. Improve performance when editing scenes with many ProBuilder object prefabs. [2.4.5-p.0+r3531] - 2016-04-07 Bug Fixes Fix bugs in Copy UV Settings and Quick Apply Material shortcuts. [2.4.5-f.1+r3519] - 2016-04-07 Features New Torus shape. Greatly improve editor performance when working with medium to large meshes. New skin for scene info label, including more data about selection. Automatically batch vertices even if they don't belong to a smoothing group (actual mesh vertex is now shown in scene info box). Add mesh dimensions overlay (ProBuilder > Object Info > {Show, Hide} Dimensions Overlay). Make vertex colors button extend-able, with the option to set which color editor the shortcut should open (Palette or Painter). New option to show element/object mode toolbar in scene (default), including positioning parameters. Improve vertex painter performance with large meshes. Unity 5.2 compatibility. Bug Fixes Fix bug where reverting a prefab with non-prefab ProBuilder children would throw errors. Fix hangs when performing various actions. Merge now retains the GameObject properties of the first selected object. Axis snapping with ProGrids now translates correctly when object rotation is non-identity. Performance improvements in UV editor for large meshes. Fix bug where OBJ exporter wouldn't properly write submeshes. Fix one possible cause of \"Mesh.{uv, colors} is out of bounds\" errors. Catch null reference errors when creating wireframe overlays for meshes exceeding Unity's max vertex count. Fix issue where Merge objects would cause meshes to lose their graphics, requiring a refresh. Fix menu items showing as enabled when not applicable. Add pb_Entity in ProBuilderize if the RequireComponent attribute fails to do so. Fix bug where ProGrids wouldn't affect elements when PB is built to a DLL. Fix bug where the cube shortcut would ignore material preference. Fix vertices merging incorrectly in Optimize function when colors don't match. Recalculate mesh bounds after moving the pivot. Always refresh/rebuild meshes after making them assets, fixing issues with duplicate mesh references. Fix a few more causes of leaks in the mesh and line rendering systems. Fix bug where scaling a new object in the shape tool wouldn't take effect until after first refresh. Fix bug where Insert Edge Loop and ConnectEdges would sometimes select too many edges after application. Fix ProGrids over-zealously collapsing vertices when in axis snapping mode. Correctly set element toolbar position when toggling between scene and editor window placement. Re-enable user set vertex handle color preferences. Changes Move default textures out of resources folder. On pb_Object::Start, call ToMesh before Refresh since Refresh could try to set UVs or Colors to a mesh that has inconsistent vertex counts. Remove most functions accepting a pb_IntArray[] sharedIndex cache and replace with Dictionary versions. [2.4.4-p.1+r3425] - 2016-04-07 Features Add scale shortcut toolbar for Auto UVs. Add Control+Shift+Left-Click when UV editor is open to copy auto UV settings (including texture). Bug Fixes Fix errors when building a project with geometry containing null materials. Fix rare null reference error when switching scenes. [2.4.4-f.1+r3385] - 2016-04-07 Features Where possible* indices are now collapsed to share a single vertex. Add context menu to swap between dockable window modes in vertex painter. Unity 5.1 beta compatibility. New Icosphere shape. New API example shows a deformed icosphere with FFT spectrum. Grow Selection is now roughly one gajillion times faster. Grow with Angle can now optionally select all faces on a plane instead of just the ones near the perimeter. New vertex handle gizmos are now culled (and much faster). Add option to select only visible elements (Select All or Select Visible toggles this). New Repair/pb_RepairMeshReferences script fixes duplicate mesh references. Improve edge selection logic, making edge selection much easier. Add a preference to enable backface selection on meshes. Re-enable 'NoDraw' faces (now implemented as a shader replacement at compile time). Improve subdivide action performance. Improve performance when editing large numbers of vertices. Smooth Normals Window now displays vertex normals with culling, and much faster. Repair Missing Script references is now cancelable. Add option to extrude elements as a group or individual. Bug Fixes Improve Flip Normals shortcut selection context handling. Enable Subdivide shortcut in Top mode. Fix arch geometry that broke when subdividing caps. Fix bug where setting arch radius would also set the thickness to 0.01. Add option to toggle cap generation on/off in arch tool. Fix bug where extruding multiple adjacent faces with a shared center point would not correctly translate the shared center vertex. Fix bug where Smoothing Window would not repaint on selection change. Improve performance of MergeVertices function, helping to address lag after modifying large objects. Fix bug where selecting faces obscured by a culled face would sometimes not register. Remove obsolete preference entries. Add Undo support when a click drag changes the selection in the UV editor. Fix 'Quaternion Look Rotation is Zero' log spam when a face contains degenerate triangles. Fix most instances of mesh and material leaks in Editor. Fix bug where applying prefab changes to pb_Objects with the Editor closed would not propogate changes to instances. Hide some internal MonoBehaviours from the Scripts menu. Fix bug where deleting a face with 'delete' key shortcut would change static flags. Fix null ref when entering play mode with collider entities sporting boxcollider components. Fix bug where Connect Vertices would fail on thin isosceles triangles. Fix bug where Connect Edges would mangle adjacent long skinny faces. When adding colliders via pb_Entity toolbar, scan current collider components for isTrigger values and apply to new collider if found. Fix some instances where convexity and trigger for EntityType.Collider & EntityType.Trigger types would not be set on initialization. When detaching faces to a new object, make the detached object selected. Fix bug where exiting to Top or Plugin level would not clear the selection mesh. Copy userCollisions field when serializing pb_Object. Fix regression in Unity 5 that causes prefabs to lose instance modifications on save and entering playmode. When mirroring objects, make the mirrored results the new selection. Fix bug where setting entity type then undoing wouldn't catch changes to collider. Fix bug where duplicating multiple objects would leave pb_Object references pointing to same object. When probuilder-izing objects, perform the action in-place (and add undo support). Catch errors when repairing missing script references on objects with null materials. Changes Remove dependency on ProCore lib to communicate with ProGrids. Rename scripts to uniformly follow pb_ prefix and pascal case for runtime, underscore case for editor. API New Optimize() method calls CollapseSharedVertices and GenerateUV2. Replaces GenerateUV2() in most cases. Move most of remaining scripts into proper namespaces. ProBuilder2.GUI namespace become ProBuilder2.Interface to avoid conflicts with UnityEngine.GUI. Move Triangulation code into pbTriangleOps. Significantly improved performance of RefreshNormals() function. New VerifyMesh() function in pb_EditorUtility guarantees good mesh reference and geometry. Add ability to delete unfixable components in pb_MissingScriptEditor. New PointIsOccluded() check in pb_Handle_Utility tests if a point is visible in editor. Significantly improve performance of pbUtil.RemoveAt(). Significantly improve performance of many pbMeshOps methods. New pb_LineRenderer and pb_MeshRenderer provide fast gizmo drawing in the SceneView. Vertices must be smoothed, and have the same texture coordinate to qualify for weld. f1 Patch Notes Fix ProGrids not affecting vertices / faces / edges in Edit mode. Minor tweak to vertex handle color. [2.4.3-p.0+r3216] - 2016-04-07 Features Weld distance now adjustable in UV editor. Bug Fixes Fix weird arch geometry near caps, noticeable when inserting edge loops. Improve 'Flip Normals' shortcut context awareness. [2.4.3-f.0+r3202] - 2016-04-07 Features Add preference to enable back-face selection. Bug Fixes Remove 'here' console log. Fix regression in 2.4.0 that broke Undo when used with ProGrids. Fix 'Look Rotation is Zero' console logs when selecting a face with degenerate triangles. Fix bug where sometimes clicking a face would not register due to a culled face intercepting the raycast. [2.4.2-f.0+r3202] - 2016-04-07 Features New debug window visualizes mesh information in the sceneview. Bug Fixes Fix regression that broke prefab editing applying to instances. Fix latency in SceneView when selecting elements in the UV window. Fix bug where selecting elements in the UV window would not Undo correctly. Fix regression that caused UV handle to not update its position when right-click dragging. Fix bug where texture rotation handle in the scene view would not snap correctly on finishing a UV adjustment. Fix bug where drag selecting edges or faces could select elements behind the scene camera. [2.4.1-f.1+r3174] - 2016-04-07 Features New \"Export UV Template\" function saves a PNG of your UV maps. Add new preference to show object vertex, face, and triangle count in the scene view (Preferences/Show Scene Info). Bug Fixes Edge wireframe no longer renders the material preview wells. Fix performance issues when editing large objects in the UV editor with Auto UVs. Fix bug where 'Push to Grid' from ProGrids would not Undo correctly. Fix lagging wireframe when running \"Freeze Transforms\" action. Fix null ref when deleting multiple faces. [2.4.0-f.4+r3132] - 2016-04-07 Features Unity 5 support. New wireframe shader overrides Unity default when ProBuilder Editor is open. New 'Merge Faces' geometry action combines selected faces to a single face. Add Missing Script Reference repair item. Show color name in vertex tools instead of RGBA info. When creating an mesh Asset, also create a prefab with the new mesh and materials already wired up. Cull hidden edges when in Edge mode. Fix spotty face selection highlight rendering when using Deferred Rendering. Add preference to disable automatic UV2 generation while modeling (improves editor performance). When selecting a texture-grouped face in UV editor, show an indicator of all faces in group. Improve performance when modifying geometry & UVs in Unity 5. New dark background in UV editor for Unity light skin users. Improve performance when selecting objects with large vertex counts. Bug Fixes Fix crash when a face material is null (defaults to Unity Default-Diffuse). Fix incorrect results when extruding multiple faces sharing a single center vertex (usually seen on the top of a cylinder). Save vertex colors when ProBuilder-izing a mesh. Fix occasional null ref when continuing UVs. Support Undo in UV Editor Auto panel. Support Undo for Push to Grid events from ProGrids. Fix occasional Index out of range errors when subdividing, triangulating, and setting pivot. Fix crash when running Fix Missing Script References in Unity 5.0.0b18 (big thanks to Michael N!) Improve the performance of Planar Mapping manual UVs. Create Material data asset path if it doesn't exist (fixes errors when saving Material preferences). Fix bug where dragging UVs in Unity 5 would sometimes corrupt the mesh. Disable Continuous Baking when dragging elements or making continuous changes to the mesh, fixing corruption issues in Unity 5. Fix occasional erroneous error message when subdividing faces. Fix null ref error when Auto UV panel is open with nothing selected. Allow V key usage when not in Element mode. Fix regression where instantiated objects would not respect ProGrids alignment. Fix leaks when deleting or duplicated pb_Objects. Fix occasional null ref errors when welding or collapsing vertices. When double-clicking a texture grouped face in UV editor, select the entire group. Fix regression that caused performance spikes when deleting or instantiating objects. When detaching faces to a new object, copy all of that objects properties. Add Undo support to Shape creation tool. Fix bug where running Flip Normals from the Menu would not immediately update the mesh graphics. When serializing pb_Objects, save color and material information (materials are now loaded by name). Fix bug where the texture handle tool would sometimes (most times) move UVs in the wrong direction. When entering Texture Blending mode in Vertex Painter, set the color to a solid variant of one of the available textures. Fix bug where projected UVs in manual mode could potentially be placed very far from the current handle. Fix lag when drag selecting edges on objects with large vertex counts. Fix bug where setting entity type would not immediately refresh the mesh. Fix minor edge selection bug that would break edge highlighting when not directly hovering a mesh. Improve appearance of Grow and Extrude foldouts in editor window. Fix bug where clicking on a vertex could sometimes select the object behind it. Fix compile errors when building to WebGL target. When creating a mesh asset, ensure that the source object mesh is not referenced by the new mesh asset. Changes Remove 'NoDraw' feature (necessary for Unity 5 compatiblity). Beta Rudimentary Boolean tool added - this is very early in development. Fix issue where \"Repair Missing Script References\" script could get stuck on prefab instances. Silence cast exception error in pb_Object_Editor. [2.3.3-f.1+r2970] - 2016-04-07 Features Significantly improve performance of Subdivide action. Bug Fixes Fix incorrect language in Plane generator. Fix bug that resulted in mangled vertices when Welding. [2.3.2-f.2+r2947] - 2016-04-07 Features Add a toggle in pb_Entity to turn off automatic collision generation. Improve UV editor grid logic (now follows camera and resizes at far zoom levels). New PostProcessor automatically strips ProBuilder scripts when building executables (toggle-able in Preferences). Bug Fixes Respect ProGrids X key shortcut when translating faces. Fix build errors with Static Batching enabled. When applying Smooothing Groups, if no face is selected apply group to entire object. Static Flags now initialized with Occluder Static unchecked. Pressing 'F' while a single vertex is selected no longer frames the entire object. Vertex colors are now copied when stripping ProBuilder scripts. [2.3.1-f.1+r2900] - 2016-04-07 Features New Vertex Painter tool. New 'Triangulate ProBuilder Object' action for facetized poly-world look. Significantly improve UV editor performance when drawing many elements. ProBuilderize action now preserves UVs. Bug Fixes Fix error when opening Material Editor after assigning a Substance Material. Fix bug that caused pb_Editor to freeze when editing prefabs made from ProBuilder objects. Remove prefab dependency on ProBuilder.Instantiate or RebuildMeshOnWake. Fix mesh leak in Shape Tool. Enable Alt+NumKey material shortcut when in Object level. When shift-extruding faces in Edge mode, default to face extrusion over edge. Fix leaking mesh and material in Face editing mode. Fix install script bug that would incorrectly delete non-ProBuilder files. Changes Handle position is now calculated as the center of selection bounding box. API New 'HueCube.cs' API example script demonstrates changing single vertex colors. New 'pb_SerializableObject' class provides serializable storage for ProBuilder objects. Add 'pb_Object::InitWithSerializableObject' constructor. Beta Notes f1 Fix install script bug that would incorrectly delete non-ProBuilder files. Beta Notes f0 New Vertex Painter tool. Fix error when opening Material Editor after assigning a Substance Material. Handle position is now calculated as the center of selection bounding box. [2.3.0-f.14+r2861] - 2016-04-07 Features New UV Editor window. New 'Material Editor' window for quickly applying materials to ProBuilder objects. Completely redesigned ProBuilder EditorWindow. Dynamically displays only relevant action buttons. Editor: New 'Select Edge Loop' command (double click on an edge, or shift+double click to ring selection). Editor: New 'Detach to Object' action creates a new ProBuilder object from a face selection. Editor: New 'Shrink Selection' command. Editor: 'Invert Selection' command now works for Edges and Vertices in addition to Faces. Editor: Performance improvements when editing large meshes. Editor: 'Grow' settings allow for a user set maximum angle between adjacent faces to limit selection growth. Editor: New extendable GUI settings for 'Extrude' allow for user-set extrusion distance. Editor: Add 'Distance' setting to Weld tool. Editor: Remove requirement that all pb_Objects be scaled to (1,1,1). Editor: Add context menu to swap between floating / dockable windows. Editor: New 'Conform Normals' geometry operation. UV: New 'Continue UVs' action. With the UV Editor open, select a face then Ctrl + Click an adjacent face to seamlessly match UV coordinates. UV: Right click translation handle in UV editor to set a new pivot point (Ctrl snaps to grid, Shift key disables proximity snapping). UV: Merge Auto UV and Manual UV editors to a single ALL POWERFUL editor window. UV: New Box projection UV unwrapping. Bug Fixes Editor: Fix bug where toggling NoDraw would sometimes fail. Editor: Fix issue where Undo would sometimes cause actions immediately following to fail with 'Index Out of Range' exceptions. Editor: Fix Quick Apply Texture shortcut regression from last version. Editor: Fix bug that caused 'Undo Change Face Seletion' to delete faces. Editor: Fix bug where ProBuilder.Instantiate() would not properly traverse prefab hierarchy when initializing ProBuilder objects. Editor: Catch yet another 'Look Rotation is Zero' warning that would slow the editor to a crawl. Editor: Fix inconsistent Undo operations on Unity 4.3+ installs. Editor: Catch NullRef errors when dragging non-Material type objects into the SceneView. Editor: Fix NullRef errors on 'Connect Vertices' actions with multiple faces selected. Editor: Fix bug where handle rotation with multiple vertices and no faces selected would be incorrect, resulting in strange behavior. Editor: Fix bug in QuickStart script that would install Unity3.5 DLLs for Unity 4.3+ versions, breaking Undo operations. Editor: Fix ProBuilder SceneView toolbar positioning when Deferred Rendering is active. Editor: Improve Edge selection consistency. Editor: Fix incorrect zoom behavior with fewer than 2 vertices selected. Editor: Fix 'Set Pivot' moving selected pb_Objects all ova' the place. Editor: Fix regression that broke Lightmap channels on Prefab objects. Editor: Frame selection now takes all selected pb_Objects into calculations. Editor: Fix regression which broke instanced Prefab geometry when running \"Apply\". Editor: Implement 'Undo' when drag selecting elements. Editor: Window now implements a scroll bar when necessary. Editor: Switching to Rotate or Scale tool no longer resets the handle alignment to 'Local'. Editor: Fix bug where Subdivide / Connect Edges / Connect Vertices would not set the selection to match the newly created sub-objects. Editor: Fix Mirror Tool incorrectly placing mirrored object pivots. Editor: Fix bug where sub-object selection highlights would be left behind when modifying an object's transform via Inspector. Editor: Always refresh an object's materials when Undoing modifications to the SharedMaterial array. Editor: Face selection graphic now sits flush with faces. Editor: Fix bug where ProBuilder-ized meshes would instantiate disabled. Editor: Fix incorrect behavior when scaling multiple ProBuilder objects at once. Editor: When exiting AutoUV mode (formerly Texture Mode) remember the previous Edit Level, Selection Mode, and Handle aligment. Editor: Fix 'Weld Vertices' action failing to properly compare all vertices. Editor: Fix incorrect behavior when attempting to modify pb_Objects with children, or children of pb_Objects. Editor: Replace deprecated code for Unity 5. Editor: ProBuilder-ized objects now inherit donor mesh name. Editor: Catch null-ref when closing pb_Editor with Smoothing Window open. Editor: Register Undo when creating new objects (with Merge or Mirror actions). Editor: When Alt key is held, do not allow handles to capture mouse. Editor: Fix leaking mesh preview object when entering Playmode. Editor: Allow submeshes to reference Null materials. Editor: Fix z-fighting face highlight in Deferred Rendering path. Editor: Setting EntityType is now undo-able. Editor: Fix bug where extruding from faces would not inherit the correct winding order. UV: Retain UV modifications when Subdividing, Connecting, or otherwise noodling around with a face. Changes 'Top' and 'Geometry' modes become 'Object' and 'Element', respectively. Remove drill-down interface for pb_Object dimensions in favor of just always showing them. No longer show element highlights when generating shape previews. Smoothing editor now accessible via main Editor window and Menu items. Remove Lightmapping window shortcut button from Editor window. When setting a pb_Object to EntityType::Trigger, also toggle 'isConvex' on the collider. Texture Window becomes AutoUV Window, and no longer houses material placement tools (Material Editor window replaces this functionality). Smoothing Editor Normals is now a float field, allowing users to set the size of debugging mesh normal lines. API Move all menu and editor commands to pb_Menu_Commands class. Add ProBuilder::Instantiate(GameObject go) overload. Slightly improve pbMeshUtils::GetConnectedFaces() performance (still incredibly slow). New methods for caclulating point inclusion for complex polygons in pb_Math. pb_Handle_Utility is new and has some super cool stuff in it, and I think I forgot to mention it in the last changelogs. Add a Repair script to fix missing UV or Vertex caches. Improve performance of GetUniversalEdges by approximately 3x. Move ProjectionAxis to pb_Enum, and it's associated methods to pb_Math. Remove ProjectionAxis.AUTO, add entries for all other axes. New pb_Material_Editor window. New pb_Bounds2d class adds some functionality for AABB calculations. Improve frequency of expensive caching in pb_Editor. Removed most naughty words from the codebase. New pb_MenuCommands class synchronizes behavior between MenuItems and Editor buttons. Beta Notes: f14 Editor: New 'Conform Normals' geometry operation. Editor: Fix bug where extruding from faces would not inherit the correct winding order. API: Add new 'GetWindingOrder(pb_Face face)' extension for pb_Object. Beta Notes: f13 API: TranslateVertices now operates in local space. Add TranslateVertices_World for backwards compatibility. API: Extrude now optionally outs the appended faces. Beta Notes: f12 Change \"Ledge Height\" to \"Door Height\" in Door creation panel. Omit Entity information from instantiated pb_Object's name. Fix bug where duplicate faces could be selected when using Grow Selection without an Angle parameter set. Beta Notes: f11 Fix sometimes incorrect results when selecting UV islands. Show UV popups in UV Editor window. Beta Notes: f8 Fix bug where faces with flipped normals would extrude with incorrect winding order. Merge Entity and Visgroup toggles. 'J' key toggles UV editor open / closed. Visgroup status is now retained during playmode state changes. [2.2.5-f.5] - 2016-04-07 Features Add 'Arch' tool to Shape Creation Panel. New parameters for Door shape generator. New 'Selection / Select All Faces with Material' menu item. Add a Selection menu item to select all faces with current material. New live information update show face movement information. Fancy new install script automatically detects previous installations and forgoes the need for user interaction in most upgrade cases. Bug Fixes Fix inconsistent Undo for face selection on ProBuilder objects. Clean up Shader warnings on initial import. Fix ProBuilder.Instantiate() ignoring position and rotation parameters. Don't force rename objects when changing the Entity type. Fix face selection highlight being incorrectly affected by Fog. Fix bug where handle alignment preference would be lost occasionally. Fix bug where Grow Selection (non-planar) would allow duplicate faces to be selected. Fix bug where prefab objects would throw \"Shader wants normals\" warnings and sometimes not initialize in scene. Fix issue where UV2 channel would not correctly initialize on prefab objects. Fix bug where rapidly clicking to add faces would frequently result in the deselection of all faces. Fix bug where MirrorTool would incorrectly affect source object's transform. Fix duplicate and mirrored objects affecting the original mesh geometry. Fix null-ref error when using Edge Ring tool on a non-circuital ring. Fix bug where mesh colliders added via Entity component menu would incorrectly have the 'Convex' flag toggled. Fix bug that caused mirrored objects to lose the source object's entity type. Remove the ability to select non-ProBuilder objects when in Geometry or Texture mode (toggle-able via ProBuilder/Preferences). Lower distance threshold for mouse distance to line to be considered selectable. Fix bug that broke scaling objects when not in PB editor. Fix regression that broke deep copying objects when duplicating or copy/pasting. Clean up Shape Creation interface to consistently show build button at bottom of screen, and provide scroll bars when necessary for parameters. Duplicate entire GameObject (including attached components) when running 'ProBuilder-ize' action. Fixed bug with cone shape generation not using radius parameter. Enable NavMeshStatic and OffMeshLinkGen flags by default on new pb-Objects. Add pb_Object component check in addition to pb_Entity check in Repair / Validate Components. Fix install script breakage on Unity 4.3+ Fix Mirror Tool incorrectly modifying donor object's normals. Fix issue where applied changes to ProBuilder prefabs would not immediately update all other instances. Fix inconsistent extrusion with Edge and Vertex selections. Fix bug where TextureWindow would not initialize with current selection. Automatically clean up degenerate triangles caused by vertex merge/weld operations. Enable 'Push to Grid' support for ProGrids users with vertex, edge, and face selection. Fix bug that caused vertices behind the scene camera to be selected incorrectly in some cases. Fix object incorrectly instantiating off-grid with strange pivot placement. (Beta) Rename AboutWindow to avoid namespace conflicts. Changes Rename 6by7 root folder to ProCore. New ProCore.dll replaces SixBySeven.dll (shared classes between ProCore products). API pb_Object.SelectedTriangles is no longer guaranteed to contain only unique indices. Convert pb_Preferences_Internal::GetEnum<> to use ints instead of strings, modify pb_Editor to match. pb_Object.SelectedTriangles is no longer guaranteed to contain values corresponding to uniqueIndices array. Remove deprecated pb_Face::DeepCopy. Implement copy constructor. Move many of ProBuilder's classes to namespaces (ProBuilder2.Common, ProBuilder2.MeshOperations, etc). New ClassesEditing segment of Classes folder contains all non-essential files. This allows for a single ProBuilderCore.dll that can be redistributed with ProBuilder objects allowing users without ProBuilder to view and load ProBuilder objects. [2.2.4-f.0] - 2016-04-07 Bug Fixes Fix 'Null Reference Error' when editing objects at runtime. Fix crash at runtime when ProBuilder object is selected. [2.2.3-f.0] - 2016-04-07 Features New 'Grow Selection Plane' which expands the selected face to nearby faces on the same plane. Bug Fixes Fix regression where handle tool would not default to Top level editing in Geometry mode when no vertices were selected. Fix bug where colliders would be lost on upgrading PB install. Enable multi-object editing for pb_Entity inspectors. API Move and rename pb_Object::MeshWithMesh to pbMeshUtils.DeepCopyMesh. Fix PlaneNormal not returning a normalized vector (yikes!). [2.2.2-f.4] - 2016-04-07 Features New 'Texture Groups' UV setting. Select faces and group to project seamless UVs. New 'Make Asset' Action allows users to save ProBuilder objects as Mesh objects. New 'Subdivide' command. New 'Connect' command (edges, faces, vertices). New 'Insert Edge Loop' command. New 'Select Ring' command. New 'Grow Selection' command (Alt-G). Significant performance improvements when working with large objects. New preferences to set vertex handle colors and size. Improve performance when drag selecting edges. New 'Remove Degenerate Triangles' Repair menu item. New snap to nearest vertex feature. When moving vertices, hold 'V' to snap handle to nearest vertex. New 'Quick Offset' tool in pb_Object inspector window. Set a value and immediately move the selected vertices by that amount (thanks to Matt1988 for initially developing this feature). Bug Fixes Override Frame selection to focus on only selected vertices (thanks @nickgravelyn for this tip). Fix inconsistent keyboard shortcuts on Mac. Tool buttons are now respected by ProBuilder handle. Fix bug where ProBuilder GUISkin wouldn't correctly initialize when left open during a Unity restart. Fix bug where double clicking a pb_Object to select all would not properly select all Edges. Fix bug where ProBuilder would affect other EditorWindow GUI layouts. Fix bug where Mirror Tool would fail to correctly initialize objects with pb_Entity. Drag selection box now more closely matches Unity's default drag box. Update and improve ProBuilderize Action (now attempts to create faces instead of just triangles). Fix Rotation handle incorrectly updating to match selection when dragging (occasionally throwing Quaternion.LookRotation == Zero warnings). Fix Scale tool incorrectly using world coordinates when translating vertices. Fix weird Prism geometry. Fix bug where setting an object pivot with ProGrids enabled would sometimes move the object's vertices off grid. Edges may now be shift-deselected. Update Undo defines to check against Unity versions 4.1 -> 4.9. 'Use' events when shortcuts are detected. Seems to work about 60% of the time on Mac. Fix bug where pivot would instantiate offset from grid when used in conjunction with ProGrids. Fix bug that broke OBJ export when attempting to export more than one model per session. Changes Vertex Color shortcuts are now declared in ProBuilderMenuItems, allowing users to edit them without installing Source. Reorganized Menu structure. API Selection management at object level is now entirely set in pb_Object, using new SetSelected[Faces, Edges, Triangles]. New naming and placement guidelines for Menu items (see pb_Constant). New ShiftExtrude() method in pb_Editor removes duplicate code in Handle functions. New pb_Editor_Graphics class replaces calls to UnityEngine.Handles in pb_Editor. Move most MenuItems to ProBuilder2.Actions namespace (exceptions being Windowed items). New pbUndo class replaces #if > UNITY_4_3 junk. [2.2.0-f.4] - 2016-04-07 Features Update Undo code for Unity 4.3 compatibility (Install Interface will determine the correct package for your Unity version automatically). Add Rotate and Scale tool when editing faces or vertices (accessed by 'E' and 'R' shortcuts, respectively). Add EditLevel toolbar in sceneview for quickly viewing and setting EditLevel. @Genstein suggested improvement. New Edge selection mode. New 'Bridge Edges' action. Selected 2 edges to create a face bridging them. New 'Collapse Selected Vertices' action. Select any number of vertices and merge them to a single point. New 'Split Selected Vertices' action. Splits the selected vertices. New 'Weld Selected Vertices' action. Checks if any selected vertices share a point, and if so, merge them. New 'Invert Selection' action. (ProBuilder -> Edit -> Invert Selection). New 'Extrude' action (ProBuilder -> Edit -> Extrude). Works for single or multiple faces, as well as edges. Hold shift while moving a face to automatically extrude (works for translate, rotate, and scale). New Install / Upgrade interface provides options to install Release and Source versions, as well as older packages. Source code is now included as an installation option. New Door primitive type in Shape Generator. New Pipe primitive in Shape Generator. New Sprite primitive in Shape Generator. New Cone primitive in Shape Generator. Improved Runtime Example scene demonstrating face highlighting. New \"Default Material\" user preference. New \"Select Faces with Material\" tool. New API example scene showing object and primitive instantiation New GUI buttons for 'Flip Normals', 'Mirror Object', 'Set Pivot', 'Vertex Color Interface' and 'Extrude Face'. Add ability to select vertex by clicking on it. Add preference for turning off sceneview notifications (Preferences/ProBuilder). New preference item allows you to specify the 'Force Convex' field of a 'Mesh Collider' if it is set to default collider. New 'Reset Projection Axis UV' repair tool. Resets all UV settings to use the 'Auto' face projection. New 'Force Pivot to Vertex' and 'Force Pivot to Grid' preferences allow for easier grid snapping. New default material for ProBuilder objects. Bug Fixes Fix system beep on Mac OS when using keyboard shortcuts (this could be a headling feature). Fix bug where detaching or deleting a face wouldn't always reset the _uniqueIndices array, causing bugs in the handle selection code. Add undo functionality to DetachFace action. Fix bug where vertex color information would be lost on duplication, refresh, build, or just about any other action you can imagine. Fix bug where detaching a face could result in empty entries to the pb_Object->_sharedIndices member, throwing null-ref. Fix InvertFaceSelection not correctly updating the pb_Object->SelectedTriangles list. Don't show 'Nodraw Face' notification if in Top Level editing mode. 'G' key now exits Texture Mode. Texture window shortcuts now show notifications. Fix button sizing in pb_Editor window. Show notification when toggling Selection Mode from GUI button. Fix error in 'Detach Face' where occasionally a null shared index array would survive the rebuild. Fix compile errors in Editor code when exporting to Web. Fix bug where notification for Selection Mode handle would be incorrect. Fix bug where deleting a face, then undoing so would result in a NullReferenceError Fix bug where 'Fix GameObject Flags' would improperly exit on failing to find a pb_Entity component. Fix vertex selection mouse icon drawing when not in Vertex Editing mode. Fix vertex color interface losing user preferences across Unity launches. Fix issue where pb_Upgrade_Utility would break installation on failing to run. Fix bug where rotated UVs would not move in the proper direction when dragging with texture move tool. Enable z-testing for face selection graphic. Don't show notification post-installation of Static Flag fixes if no fixes were performed. Fix bug where texture handles sometimes wouldn't match the selected face's transform. Refactor shortcut code to differentiate between modal specific actions. Fixes bug where entity assignments would incorrectly be applied in Geometry level and not Top level. Fix incorrect skin colors in Unity Free on 4.3. Fix bug introduced in 2.1.4 that broke texture handle toggling (thanks, H. David). Fix bug where UV rotate tool would be incorrectly calculated on selection change. Change UV scale and rotation behavior to no longer operate in world coordinates. Fix bug where extruding would occasionally corrupt the pb_Object.uniqueIndices cache, resulting in 'NullRefError' in pb_Object::GetVertices. Adjust minSize of pb_Editor window to completely encompass buttons. Re-word toggle select mode and edit level notifications and make them consistent between the different access points. Fix bug where 'Axis Constraints' toggle in ProGrids would not be respected when translating faces. Fix bug where UV and Smoothing group changes would not immediately revert on Undo operations. Fix regression that broke Ctrl-Left click to copy UV settings to face. Fix bug where ProBuilder Editor skin settings would \"leak\" to other Editor windows. Fix bug where collisions would sometimes not respect user preference when creating new geometry. Fix bug where SceneView would sometimes not refresh on an Undo event. Fix bug where pressing 'W' key in the SceneView Fly mode would lock the camera to forward movement. Changes In pb_Entity, switch the 'Sphere Collider' option for 'Remove Collider'. Change verbage in Geometry shortcut description. Add tooltip for selection mode toggle button. Show HandleAlignment text when using shortcut to modify. Move DetachFace to Edit menu. StaticBatchingFlags.BatchingStatic is now set by default on Occluder and Detail entity objects, and toggled appropriately when NoDraw is detected. Move \"Create ProBuilder Cube\" to \"GameObject->Create Other\" menu Re-organize ProBuilder menu. New \"Fix GameObject Flags\" utility to address static batching issues. Users experiencing issues with missing ProBuilder objects at compile time should run this command once (per scene). Remove 'Faces' menu item, merge with 'Geometry' Move 'Mirror Tool' and 'Vertex Color Interface' to Editor Core. Repair scripts now live in their own folder. Tool scripts (any Action with an interface) now live in their own folder. Remove unused beta upgrade script from Install folder. Drag selecting faces now (optional; defaults to true) limits face searching to selected objects. Remove 'Seamless' mode. API / Internal Add get/set for pb_Obect->_sharedIndices. Use ProBuilder.Actions namespace for all non-window requiring functions. When initializing a pb_Object with a pb_Object, use the vertex cache instead of accessing the mesh. Remove per-vertex smoothing methods in pb_Object. Remove _smoothIndices member from pb_Object. Move pb_Profiler to ClassesCore, allowing usage at runtime. Add 'color' property to pb_Face. Used when setting Mesh.colors32. New pb_Edge class (not currently in use). New ProBuilder.Instantiate(GameObject go) method. Behaves exactly like UnityEngine.GameObject.Instantiate() and may be used with ProBuilder and non-ProBuilder objects. Move math methods from pbUtil to pb_Math. Added List<> overrides to many of the more commonly used pb_Object method calls. Clean up face selection graphic rendering code (small editor performance improvement). New FixDegenerateTriangles method (handy when merging vertices or faces). CombineObjects method re-built for faster combine operations. New ProBuilder2.Common, ProBuilder2.MeshOperations, and ProBuilder2.Math namespaces. Partially integrated. New pb_Editor_Enum class and namespace. Known issues With Unity 4.3 and up, undoing a Collapse Vertices operation is slow. Merging rotated objects does not account for UV rotation. OBJ export, something broken, etc. Unity inserts an additional Undo when selecting a new face on an already selected object. Can't shift-click to deselect edges. [2.1.4-f.0] - 2016-04-07 Features Notifications are now displayed when a shortcut is recognized. New preview feature in Geometry Interface. Interactively create and place shapes. Remove dependency on concave MeshCollider for face selection. New MenuItems for opening the Texture Window, and assorted editor commands. Changes Move GUI folder to Resources, allowing 6by7 root folder to be placed anywhere in Project hierarchy. Decouple collisions from ProBuilder API entirely. Bug Fixes Fix bug where Mesh.Colors32 property would be lost on duplication. Clamp values in Geometry Interface to sane values. Fix plane generation pivot location when segments < 0. Fix bug that caused Unity to no longer recognize numberical input. Fix regression in 2.1.3 that caused MeshColliders break on entering playmode. Fix bug where shortcut keys would sometimes not be recognized. When updating ProBuilder, the editor window is now force-reloaded. Editor window is now sized correctly for both dockable and non-dockable frames. Fix compile errors when building project in Unity 4.1.2+ Fix bug that caused merged objects to incorrectly snap vertex points while ProGrids window is present. Fix NullReferenceError when clicking Merge button with nothing selected. Fix GUISkin issues in Unity 3.5. Fix GUISkin modifications affecting pb_Geometry_Editor incorrectly. Fix 'Delete Face' notification incorrectly displaying on OSX. Fix merged objects losing collisions. API ProBuilder.Shortcut is now pb_Shortcut. Add pb_Upgrade_Utility as a base class for all updating operations. [2.1.3] - 2016-04-07 Features New Vertex Color Interface. New 'Detach Face' action. New 'Toggle Mover Visibility' button. Changes pb_Mesh_Extension renamed to pb_Object_Extensions. Transition default shader to Diffuse Vertex Color. Bug Fixes Fix pb_Object breakage when upgrading to 2.1.2+ from <= 2.1.1. Fix bug where switching to Geometry mode would not always correctly set Tool.current to Tools.None. Fix bug where calling the distinctIndices member of a pb_Face would sometimes throw an exception. Fix null reference errors when deleting object faces. Fix regression in 2.1.2 that caused non-cube type primitives to lose entity data and mesh information. Fix regression that caused Nodraw Visiblity Toggle to break. API Remove unnecessary calls to the mesh reference when accessing vertex information (most notably in UV mapping functions). Cache distinct indices in pb_Face, replacing pb_Face::DistinctIndices() with pb_Face.distinctIndices. Add pb_Edge class, and accompanying methods to retrieve all face edges and selectively perimeter edges. Add SetColors32(Color32[] colors) to pb_Object class. Add DetachFace(pb_Face face) to pb_Object class. Internal Update to SVN 1.7, small adjustments to build scripts. Add shell script to build distributable packages on OSX. [2.1.2] - 2016-04-07 Features New interface for pb_Entity class in Inspector. Scale transform now supported. Double click pb_Object face to select all faces. New ProBuilder/About window provides more build information. Full prefab support (removes \"Create Prefab\" button from ProBuilder editor). Changes Rewrite context tip for Lightmapping button to reflect it's new purpose. Automatically freeze scale transform when applying any change to vertices. Always ZTest for selection graphic in face mode. 'G' key now toggles between Edit Levels. Remove face vertex handle information from scene view. Remove install script from package. Bug Fixes Fix bug where user would be allowed to add multiple collision components to pb_Object. Fix bug where geometry would shift on Undo/Redo incorrectly. Fix leak when deleting pb_Objects. Fix regression in 2.1.1 that introduced a leak on switching pb_Objects while in ModeBased vertex editing. Fix bug where selection graphics would occasionally not update on undo, redo, or prefab apply / revert. Fix bug where setting EntityType would destroy transform parent/child connections. Fix incorrecty window sizing in pb_Editor. Fix rare error log when duplicating prefab objects. API Add OnVertexMovementFinished event to pb_Editor. Internal Implement SixBySeven shared library. [2.1.1] - 2016-04-07 Features Add MirrorTool action. Add Prism primitive. Add ProBuilderizer action (API example). Add Flip Winding Order action (flips face normals). Add dimensions parameter to Prism and Cube in Geometry Interface. Add ability to delete faces (select faces and press backspace) Changes \"Auto NoDraw\" becomes \"NoDraw Tool\", and features a vastly improved interface. Scroll bars added to ProBuilder Preferences panel, allowing for unlimited preference additions. Add undo support to Set Pivot action. No longer force rename pb_Objects post-initialization. Comment out menu item for Project Wide Nodraw utility, leaving action available for advanced users. Bug Fixes Fix bug where handles in Seamless editing mode would not draw. Fix bug where selected objects would disappear at runtime. Fix bug where drag selection would not be recognized in Seamless editing mode. Fix Unity crash when importing packages while ProBuilder window is open. Fix regression in 2.1 where a MeshCollider would always be assigned to pb_Object, regardless of Collider settings. Fix cylinder generation code to properly account for height divisions (now accepts 0 as a parameter). Fix bug where undoing texture modifications would not consistently refresh pb_Object to original state. Fix bug where pb_Objects would disappear at runtime with static batching enabled. Add overload to TranslateVertices that accepts bool forceDisableSnap. Fix bug in PivotTool that caused vertices to incorrectly be snapped when setting new pivot with snapping enabled. API Changes Add pb_Object::InitWithObject Add ProBuilder::CreateObjectWithObject Add pb_Object::GetName Add ProBuilder::CreatePrimitive(ProBuilder.Shape shape) Internal Add DrawAllFaceNormals to #DEBUG flagged pb_Editor. Update Sublime Extension to version 3. [2.1.0] - 2016-04-07 Features Add Smoothing Group support. New face selection graphic system respects depth order + speed boost. Add drag selection support for faces. UV2 channel generation now totally automated. New Lightmap Window exposes UnwrapParam properties per-object for fine-grained UV2 generation control. Add smart object naming, with the convention \"pb(Shape Type)([Entity Type])-(Object ID)\" - ex: pb-Cube[Detail]-1701) Add new \"Mover\" entity type, which is non-static and allows complete control at runtime. Add support for n-gon faces. Changes 'World' is now default handle alignment. Update default materials with dedicated textures. Update QuickStart window with more explicit options. Default values for Cylinder are now slightly more sane. Bug Fixes Fix ProceduralMaterials throwing errors in Texture Editor. Fix rare bug where incorrect vertex indices would be selected in an UpdateSelection() call, throwing a NullReferenceException. Fix bug where toggling selected faces would not correctly remove vertices from internal selection list. Fix bug where pivot would center at 0,0,0 on merging objects. Hide ACG property in Inspector window. Fix bug where merged objects would lose EntityType information. Fix bug where prefab creation would not account for pb_Group data. Fix bug where merged objects would lose normal data. Fix bug where exiting Texture Mode would not consistently set Edit Mode to Top. Fix bug where generating UV2 channel would incorrectly hide NoDraw faces, breaking synchronization with pb_Editor UI. Fix bug where ListenForTopLevelMovement would incorrectly fire, significantly slowing scene navigation. Fix bug where duplicating multiple objects would result in referenced pb_Objects. Fix bug in pb_Group where SetActive would incorrectly be called in Unity 3.5. Fix bug where collision meshes would not correctly update after an Undo / Redo event. Fix bug where drag selection would not exit properly if a function key is pressed mid drag. Fix bug where vertex handles would incorrectly be drawn in Top level editing mode. Fix bug where deleting a pb_Object would occasionally cause a NullReferenceError in UpdateSelection(). Fix bug where Occluder objects would not allow textures to be applied. Fix bug where box colliders would not properly inherit trigger boolean value. Fix bug where merging objects or creating groups would not snap pivot point to grid (this also introduces centered pivot points). Fix rare bug where get_localRotation would fail. Fix white flash in Texture Window preview. Fix bug where ProBuilder would not remember Handle Alignment setting. Fix bug where editor selection property would not correctly update on object deletion. Fix minor bug where vertex handles would sometimes not immediately draw on entering Geometry editing mode. Fix bug where closing Texture Window manually would not always exit EditLevel.Texture. Fix bug where an Undo/Redo event would sometimes cause pb_Editor to attempt to refresh every pb_Object in scene. Fix bug where exiting EditLevel.Texture to Geo Mode would not correctly remember the previous SelectionMode. Fix bug where cylinder object sometimes initialize with un-even side lengths. Fix bug where on deleting a pb_Object's MeshCollider, ProBuilder would not immediately re-initialize it (prevents common PEBKAC error). API Integrate Doxygen (Still a work in progress - feel free to drop by the forums with any questions). Add SharedTrianglesWithFacesExclusive for extracting shared triangle indices exclusive to passed faces. VerticesWithIndiceArray is now VerticesWithTriangleArray. Remove pb_Object::CreatePrimitive. Use pb_Shape for object creation, or pb_Object::CreateCube(float scale). Add OnVertexMovement EventHandler to pb_Object. pb_Object::CreateObjectWithPointsfaces is now pb_Object::CreateObjectWithVerticesFaces. Actions Update AutoNodraw to cast from all vertices + center point when determining hidden flag. In PivotTool.cs, snap pivot point to grid if no vertices are selected. Refactor EntityType.Brush to EntityType.Detail. Internal Add pb_Profiler class Add UVee window + ProBuilder specific modifications Add internal preference to force update preference when necessary (usually means adding shortcut items). Significant performace improvements in handle drawing."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/CONTRIBUTING.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/CONTRIBUTING.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing All contributions are subject to the Unity Contribution Agreement(UCA) By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions. Pull Requests Please include an entry to the changelog for any PR, along with a Fogbugz ticket number if applicable. New logs should be placed under the ## [Unreleased] header at the top of the changelog. Ex: # Changelog All notable changes to this project will be documented in this file. The format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/) and this project adheres to [Semantic Versioning](http://semver.org/spec/v2.0.0.html). ## [Unreleased] ## Features - Added some neat new feature. ## Changes - [case: 1203585] Removed `Custom Shape` option from the `Shape Editor` window. ## [4.3.0-preview.2] - 2020-01-15 etc."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Arch.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Arch.html",
    "title": "Arch | FSM Unity Framework",
    "keywords": "Arch The basic arch shape is a curved symmetrical shape (see A in the image below). You can customize the basic arch shape to create tunnels, pipes, roofs, and many other objects. (A) Roof: arch with two sides and increased depth (B) Wheel: arch with increased number of sides, thickness and degrees (and no end cap) (C) Basic arch shape (default values) (D) Tunnel: Arch with increased depth and thickness When you customize an arch shape, keep in mind that when you change the thickness and number of sides, it is relative to the overall size and circumference. For example, two arches with the same thickness but where one arch is twice as long as the other look very different. The shorter one appears much smoother than the other. You can customize the shape of an arch with these shape-specific properties: Property: Description: Thickness Set the thickness of the arch in meters. The larger the thickness, the smaller the opening becomes. The default value is 0.1. The minimum value is 0.01. Sides Count Set the number of sides for the arch. The more sides you use (relative to the size of the Radius), the smoother the arch becomes, so the closer you get to a semi-circle. Conversely, if you set this value to 2, the arch turns into a peaked roof, regardless of the radius. The default value is 5. Valid values range from 2 to 200. Arch Circumference Set the circumference of the arch in degrees. For example, an arch of 360 degrees is circular. The default value is 180. Valid values range from 1 to 360. End Caps Enable this option to create faces for the ends of the arch (default). Disable this option if you know the ends are not visible to the camera (this is an optimization strategy that reduces the number of polygons that Unity has to render). Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/CenterPivot.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/CenterPivot.html",
    "title": "Center Pivot | FSM Unity Framework",
    "keywords": "Center Pivot The Center Pivot action moves the pivot point for the Mesh to the center of the object’s bounds. For example, if you move a lot of vertices on one side of your Mesh, when you try to rotate the object, it rotates around a point outside of the Mesh. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Center Pivot). Note: If you have multiple objects selected, each object's new pivot point becomes the center of each object, regardless of the position of any other object."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cone.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cone.html",
    "title": "Cone | FSM Unity Framework",
    "keywords": "Cone A cone is a shape that tapers from a circular (or roughly circular) base to a point. You can adjust the smoothness. (A) Cone with wide radius (B) Basic cone shape (default values) (C) Cone with increased height (D) Cone with increased number of sides You can customize the shape of a cone with this shape-specific property: Property: Description: Sides Count Set the number of sides for the cone. The more sides you use, the smoother the sides of the cone become. The default value is 6. Valid values range from 3 to 64. Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cube.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cube.html",
    "title": "Cube | FSM Unity Framework",
    "keywords": "Cube A cube is the default shape in ProBuilder. It is a six-sided 3D square. The cube shape has no shape-specific properties."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cylinder.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Cylinder.html",
    "title": "Cylinder | FSM Unity Framework",
    "keywords": "Cylinder A cylinder is a shape with long straight sides and two circular (or roughly circular) ends, like a tube. They have a radius and a height. (A) Cylinder without a smoothing group (B) Cylinder with a high sides count (C) Cylinder with a lot of height cuts (D) Default cylinder shape You can customize the shape of a cylinder with these shape-specific properties: Property: Description: Sides Count Set the number of sides for the cylinder. The more sides you use, the smoother the sides of the cylinder become. The default value is 6. Valid values range from 4 to 64. Height Cuts Set the number of divisions to use for the height of the cylinder. For example, a value of 3 produces 4 faces on every side of the cylinder. The default value is 0. Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Door.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Door.html",
    "title": "Door | FSM Unity Framework",
    "keywords": "Door The ProBuilder door shape provides a convenient door Mesh that you can set into a level wall. There are no polygons where it meets the wall, so you can place it directly into the wall. (A) Length (from the bounding box) (B) Height (from the bounding box) (C) Pediment Height (D) Side Width (E) Width (from the bounding box) You can customize the shape of a door with these shape-specific properties: Property: Description: Pediment Height Set the height of the top of the door frame in meters. The default value is 0.5. The minimum value is 0.01. Side Width Set the width of the door frame on the sides in meters. The default value is 0.75. The minimum value is 0.01."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Bevel.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Bevel.html",
    "title": "Bevel (Edge) | FSM Unity Framework",
    "keywords": "Bevel (Edge) The Bevel Edge action splits the selected edge(s) into two edges, with a new face between. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Bevel Edges). Bevel Options To change the width of the bevel, change the Distance to move the newly created edge(s) from the position of the original edge(s). This becomes the width of the new face(s)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Bridge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Bridge.html",
    "title": "Bridge Edges | FSM Unity Framework",
    "keywords": "Bridge Edges The Bridge Edges action creates a new face between two selected edges. By default, this action can only bridge two open edges (that is, no face on the open or free side). However, you can override this; to do so, navigate to the Preferences and enable Allow non-manifold actions. Tip: You can also use this action with the Alt/Opt+B shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Bridge Edges)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Connect.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Connect.html",
    "title": "Connect Edges | FSM Unity Framework",
    "keywords": "Connect Edges The Connect Edges action inserts an edge that connects the centers of each selected edge. Tip: You can also use this action with the Alt/Opt+E shortcut, or from the ProBuilder menu, or from the ProBuilder menu (Tools > ProBuilder > Selection > Smart Connect). If you select more than two edges, ProBuilder creates as many new edges as possible without creating bad geometry. You can connect across several faces, as long as they share a selected edge."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Extrude.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Extrude.html",
    "title": "Extrude Edges | FSM Unity Framework",
    "keywords": "Extrude Edges The Extrude Edges action pushes a new edge out from each selected edge, connected by a new face for each edge. This action only works on open edges (that is, an edge that has no connected face on one side). However, you can override this restriction with the Allow non-manifold actions option. Each new face follows the direction of the normals of the face that is adjacent to the selected edge. You can invoke this action in either way: Select one or more edge(s) and click Extrude Edges. By default, the distance of the extrusion is 0.5, but you can change that with the Distance option. Tip: You can also use the Ctrl/Cmd+E shortcut instead of the button with this method, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Extrude). Select one or more edge(s) and then hold Shift while you move, rotate, or scale the selected edge(s). This method ignores the options but provides greater control, especially with the direction of the extrusion. Extrude Edges Options These options apply only if you use the Extrude Edges button or the Ctrl/Cmd+E shortcut. Property: Description: As Group Enable this option to keep the sides of extruded edges attached to each other if you select more than one edge to extrude. Distance Distance to extrude the edge(s). Both positive and negative values are valid."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_FillHole.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_FillHole.html",
    "title": "Fill Hole (Edges) | FSM Unity Framework",
    "keywords": "Fill Hole (Edges) The Fill Hole action creates a new face that fills any holes that touch the selected edges. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Fill Hole). Fill Hole Options Enable the Fill Entire Hole option to fill the entire Mesh opening. This is the default. If you disable this option, ProBuilder tries to build a Mesh between the selected open edges. For example, if you have a missing quad, you can select two adjacent edges in order to create a triangular polygon that covers half of the hole."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_InsertLoop.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_InsertLoop.html",
    "title": "Insert Edge Loop | FSM Unity Framework",
    "keywords": "Insert Edge Loop The Insert Edge Loop action adds a new edge loop from the selected edge(s). An edge loop is a series of edges that are directly connected. They often encircle a 3D object and connect back at the origin point. Note: Loops only continue through quads (four-sided polygons), not triangles. Tip: You can also use this action with the Alt/Opt+U shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Insert Edge Loop). This creates more geometry than Connect Edges or Connect Vertices, but it makes the geometry cleaner. For example, if you insert an edge on a single face of a cube you get a T-junction, but if you insert an edge loop instead, you get the same geometry all around the cube, if the loop is only passing through quads. You can use Insert Edge Loop while you edit your geometry, and then delete the extra unnecessary edges when you're finished to optimize the geometry."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_SetPivot.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_SetPivot.html",
    "title": "Set Pivot (Edges) | FSM Unity Framework",
    "keywords": "Set Pivot (Edges) Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected edges. Tip: You can also launch this action with the Ctrl/Cmd+J shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Set Pivot). In this example: Left panel: The pivot point of the door is at its bottom-left corner. Middle panel: Two edges are selected on the opposite side, so the Set Pivot action changes the pivot to the center of those top edges. Right panel: The pivot point is now at the top right, even when in Object editing mode."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Subdivide.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Edge_Subdivide.html",
    "title": "Subdivide Edges | FSM Unity Framework",
    "keywords": "Subdivide Edges The Subdivide Edges action divides the selected edge(s) into multiple edges. By default, ProBuilder splits the edge in two, but in the Options window, you can set your own number of Subdivisions. Tip: You can also use this action with the Alt/Opt+S shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Smart Subdivide). Subdivide Edges Options To change the number of new edges created, change the Subdivisions option. By default, the number of subdivisions is 1, which means that ProBuilder splits a single selected edge into two edges. If you change this value to 3, then a single edge becomes four edges. By default, the range of valid values is 1 to 32, but you can set your own range: Click the arrow next to expand the Subdivisions section. The Range property appears. Set the lower limit of the range on the left and the upper limit on the right."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Entity_Trigger.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Entity_Trigger.html",
    "title": "Entity type actions | FSM Unity Framework",
    "keywords": "Entity type actions ProBuilder provides some default \"entity\" behaviors. These are MonoBehaviours that provide some commonly used functionality. Note: These actions are not available from the The ProBuilder toolbar in icon mode. Set Trigger The Set Trigger action assigns the Trigger Behaviour script to selected objects, which does the following: If no collider is present, the Trigger Behaviour script adds a MeshCollider component. If the collider is a Mesh Collider, the Trigger Behaviour script enables its Convex property. The Trigger Behaviour script enables the collider's isTrigger property. The Trigger Behaviour script sets the Mesh Renderer Material to ProBuilder's Trigger Material. The Trigger Behaviour script automatically disables the Mesh Renderer when you enter Play Mode or build your project. Tip: You can also use the T shortcut to set the selected object(s) as a trigger, or from the ProBuilder menu (Tools > ProBuilder > Object > Set Trigger). Set Collider The Set Collider action assigns the Collider Behaviour script to selected objects, which does the following: If no collider is present, the Collider Behaviour script adds a MeshCollider component. The Collider Behaviour script sets the MeshRenderer Material to ProBuilder's Collider Material. The Collider Behaviour script automatically disables the MeshRenderer when you enter Play Mode or build your project. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Set Collider)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Bevel.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Bevel.html",
    "title": "Bevel (Faces) | FSM Unity Framework",
    "keywords": "Bevel (Faces) The Bevel action performs the Bevel Edge action on all the edges of the selected face(s). Bevel Options To change the width of the bevel, change the Distance to move the newly created edge(s) from the position of the original edge(s). This becomes the width of the new face(s)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_ConformNormals.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_ConformNormals.html",
    "title": "Conform Normals (Faces) | FSM Unity Framework",
    "keywords": "Conform Normals (Faces) The Conform Normals action sets all normals on the selected face(s) to the same relative direction. ProBuilder uses the direction that most of the selected faces on the object are already facing. In this example, all three selected faces on this cube end up pointing away from the Camera, as the Transform gizmo shows. This is because two out of three of the selected faces are pointing in the opposite direction, even though the majority of the normals are pointing towards the camera. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Conform Face Normals)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Delete.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Delete.html",
    "title": "Delete Faces | FSM Unity Framework",
    "keywords": "Delete Faces The Delete Faces replace deletes the selected face(s). Tip: You can also use this action with the Backspace shortcut (Delete key on macOS), or from the ProBuilder menu (Tools > ProBuilder > Geometry > Delete Faces). On Windows, the Delete key deletes the entire Mesh. You can use Undo to reverse it."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Detach.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Detach.html",
    "title": "Detach Faces | FSM Unity Framework",
    "keywords": "Detach Faces The Detach Faces action detaches the selected face(s) from the rest of the Mesh. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Detach Faces). Note: When you detach a face, the newly separated Mesh remains in place. This image shows the detached Mesh above only to illustrate that the Mesh is now detached from the the rest of the original Mesh. By default, ProBuilder creates a new sub-Mesh for the detached face(s). If you want to keep it as a sub-Mesh inside the same GameObject, you can change the Detach To option. Detach Face Options The Detach To drop-down menu allows you to choose whether you want the detached face(s) to become a separate Mesh (the default), or keep them inside the original Mesh as a new sub-Mesh. Choice: Result: Game Object ProBuilder detaches the face(s) to a new, separate Mesh object. Submesh ProBuilder detaches the face(s) to a sub-Mesh within the original Mesh object."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Duplicate.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Duplicate.html",
    "title": "Duplicate Faces | FSM Unity Framework",
    "keywords": "Duplicate Faces The Duplicate Faces action copies each selected face and either creates a new Mesh or leaves it in the same GameObject as a sub-Mesh, depending on the default options. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Duplicate Faces). Duplicate Options By default, when you duplicate a face, ProBuilder copies it into a new GameObject. However, you can choose the Submesh option from the Duplicate To drop-down menu to save it as a sub-Mesh in the same GameObject."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Extrude.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Extrude.html",
    "title": "Extrude Faces | FSM Unity Framework",
    "keywords": "Extrude Faces The Extrude Faces action creates a new face. To do this, it pulls out the currently selected face, and attaches sides to each edge. By default, each new face follows the direction of its vertex normals, but you can change this with the Extrude By option. You can invoke this action in either way: Select one or more face(s) and click Extrude Faces. By default, the distance of the extrusion is 0.5, but you can change that with the Distance option. Tip: You can also use this action with the Ctrl/Cmd+E shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Extrude). Select one or more face(s) and then hold Shift while moving, rotating, or scaling the selected face(s). This method ignores the options but provides greater control, especially with the direction of the extrusion. When you use this method with the scaling control, it creates an inset. Extrude Faces Options These options only apply if you are using the Extrude Faces button or the Ctrl/Cmd+E shortcut. Property: Description: Extrude By Direction for extruding each selected face. Face Normals Use the selected face's own surface direction. Adjacent faces remain connected. Vertex Normals Use the selected face's Vertex normals. Adjacent faces remain connected. This is the default. Individual Faces Use the selected face's own surface direction. However, adjacent faces do not remain connected. Distance Distance to extrude the faces(s). Both positive and negative values are valid."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_FlipNormals.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_FlipNormals.html",
    "title": "Flip Face Normals | FSM Unity Framework",
    "keywords": "Flip Face Normals The Flip Face Normals action flips the normals only on the selected face(s). This differs from the Flip Normals action, which flips the normals on every single face on the Mesh. Tip: You can also use this action with the Alt/Opt+N shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Flip Face Normals)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_FlipTri.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_FlipTri.html",
    "title": "Flip Face Edge | FSM Unity Framework",
    "keywords": "Flip Face Edge The Flip Face Edge action swaps the triangle orientation on the selected face(s) with four sides. This reverses the direction of the middle edge in a quad. Use this to smooth ridges in quads with varied height corners. Note: This only works on quads (four-sided polygons). This action is often called Turn Edges in other 3D modeling applications. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Flip Face Edge)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Inset.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Inset.html",
    "title": "Inset | FSM Unity Framework",
    "keywords": "Inset To create a new face set into the currently selected face, you can use the Extrude Faces action and scale the extruded face. Inset faces give you a lot of flexibility for building more sophisticated shapes. For example, you can extrude an inset face inwards to create interior walls, or extrude outwards for a more detailed exterior. To create an inset face: Activate the Face mode. Select the face you want to add the inset to. Activate the Scale action. Hold Shift and scale along either axis of the face, and let go when you are satisfied. For example, if you want to inset the top face of a cube, you can scale along either the x-axis or the z-axis. Scale along the other axis of the face to complete the inset. For example, if you extruded along the z-axis in step 4, then scale in the x-axis."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Merge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Merge.html",
    "title": "Merge Faces | FSM Unity Framework",
    "keywords": "Merge Faces The Merge Faces action merges selected faces into a single face, and removes any dividing edges. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Merge Faces). Caution: Be careful when you merge two unconnected faces, because this can produce unexpected results with any texture mapping. This action can sometimes create unusual geometry artifacts, such as vertices at T-junctions or floating (winged) vertices (that is, unused vertices sitting on an edge with no other edge passing through it). It is better to merge faces only when necessary."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_SetPivot.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_SetPivot.html",
    "title": "Set Pivot (Faces) | FSM Unity Framework",
    "keywords": "Set Pivot (Faces) Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected faces. Tip: You can also launch this action with the Ctrl/Cmd+J shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Set Pivot). In this example: Left panel: The pivot point of the block tower is at the center of the entire Mesh. Middle panel: The top faces are selected, so the Set Pivot action changes the pivot to the center of those top faces. Right panel: The pivot point is now at the top, even when in Object editing mode."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Subdivide.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Subdivide.html",
    "title": "Subdivide Face | FSM Unity Framework",
    "keywords": "Subdivide Face The Subdivide Face action splits each selected face. To do this, it adds a vertex at the center of each edge and connects them in the center. Tip: You can also use this action with the Alt/Opt+S shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Smart Subdivide). This allows you to add a lot more detail to your geometry. Alternatively, you can use the Cut tool to control the exact shape of the new faces."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Triangulate.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Face_Triangulate.html",
    "title": "Triangulate Faces | FSM Unity Framework",
    "keywords": "Triangulate Faces The Triangulate Faces action reduces selected faces to their base triangles. This creates a faceted, non-smooth appearance. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Triangulate). Tip: When you triangulate faces, you can smooth the hard edges with the Smooth Group Editor window."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Freeze_Transform.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Freeze_Transform.html",
    "title": "Freeze Transform | FSM Unity Framework",
    "keywords": "Freeze Transform The Freeze Transform action sets the selected object's position, rotation, and scale to world-relative origin ({0,0,0}) without changing any vertex positions. That means it resets the pivot location and clears all Transform values, but doesn't change the size, shape, or location of the object in the Scene. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Freeze Transform)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/HandleAlign.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/HandleAlign.html",
    "title": "Orientation | FSM Unity Framework",
    "keywords": "Orientation The Orientation action sets the orientation of scene handles when you select objects and elements. Click the Orientation button to switch between the three states: Global, Local, or Normal. Depending whether you are in text mode or icon mode, the button displays the following to indicate what state the action is in: Icon mode: Text mode: Description: Orientation: Global Similar to a compass, the handle orientation is always the same, regardless of local rotation. Orientation: Local Similar to left vs right, handle orientation is relative to the object's rotation. Orientation: Normal Special mode that aligns the handles to the exact normal direction of the selected face. This action is available in all edit modes. Tip: You can also use this action with the P shortcut, or from the ProBuilder menu (Tools > ProBuilder > Interaction > Toggle Handle Orientation)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_ConformNormals.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_ConformNormals.html",
    "title": "Conform Normals (Objects) | FSM Unity Framework",
    "keywords": "Conform Normals (Objects) The Conform Normals action sets all face normals on the selected object to the same relative direction. ProBuilder uses the direction that most of the faces on the object are already facing. In this example, the majority of the normals on this shape are pointing towards the Camera, so after applying this action, all normals point in the same direction. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Conform Object Normals)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Export.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Export.html",
    "title": "Export | FSM Unity Framework",
    "keywords": "Export The Export action exports the selected ProBuilder object(s) to a 3D Model file. Tip: You can also export to each format by selecting the specific format from the ProBuilder menu (Tools > ProBuilder > Export > Export <format>). For an overview of how to export ProBuilder objects and and re-import the exported files, see Exporting and re-importing. Export options Select the format you want to export to from the Export Format drop-down menu. The options available depend on which format you choose: OBJ-specific options STL-specific options PLY-specific options Asset-specific options Prefab-specific options The only option that applies to all file format types is the Include Children property, which you can enable to include not only selected Meshes, but also the child objects of selected objects in the exported Model. Asset-specific options The Asset format type also provides the Replace Source option. The Replace Source option determines whether ProBuilder updates the GameObject's MeshFilter.sharedMesh to point to the exported Mesh data (enabled) or to the source of the exported Mesh (disabled). STL-specific options The STL format type also provides the STL Format option, which lets you choose whether to use the ASCII or Binary representation of the STL file specification. PLY-specific options The PLY format type also provides the following options: Property: Description: Export As Group Enable this option to combine all selected objects and export them as a single Model file. Otherwise, ProBuilder exports each Mesh separately. Apply Transforms Enable this option to apply the GameObject transform to the Mesh attributes before ProBuilder exports them. With both this option and Export as Group enabled, you can export your whole Scene, edit it, then re-import it with everything exactly where you left it. Right Handed Enable this option to use right-handed coordinates. Unity's coordinate system is left-handed, but most major 3D modeling software applications use right-handed coordinates. Quads Enable this option to preserve quads where possible. OBJ-specific options The OBJ format type also provides the following options: Property: Description: Export As Group Enable this option to combine all selected objects and export them as a single Model file. Otherwise, ProBuilder exports each Mesh separately. Apply Transforms Enable this option to apply the GameObject transform to the Mesh attributes before exporting. With this option and Export as Group enabled, you can export your whole Scene, edit, then re-import it with everything exactly where you left it. Right Handed Enable this option to use right-handed coordinates. Unity's coordinate system is left-handed, but most major 3D modeling software applications use right-handed coordinates. Copy Textures Enable this option to copy texture maps to the file destination and reference them from local paths in the Material library. Disable it if you want the Material library to reference an absolute path to the Textures instead of copying them. See Re-importing an exported Mesh for more information. Vertex Colors Enable this option to write vertex colors with the MeshLab format. Some 3D modeling applications can import vertex colors from an unofficial extension to the OBJ format. Note: This can break import in some applications, so use it with caution. Texture Offset, Scale Enable this option to write import texture scale and offset parameters values to the exported mtlib file. Some 3D modeling applications import texture scale and offset parameters (Blender, for example). Note: This can break import in some applications, so use it with caution. Export Quads Enable this option to preserve quads where possible. Prefab-specific options The Prefab format type also provides the Replace Source option. The Replace Source option determines whether ProBuilder updates the GameObject's MeshFilter.sharedMesh to point to the exported Mesh data (enabled) or to the source of the exported Mesh (disabled)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_FlipNormals.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_FlipNormals.html",
    "title": "Flip Normals | FSM Unity Framework",
    "keywords": "Flip Normals The Flip Normals action flips the normals of all faces on the selected object(s). This is especially useful if you want to convert an exterior-modeled shape into an interior space. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Flip Object Normals)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_LightmapUVs.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_LightmapUVs.html",
    "title": "Lightmap UVs | FSM Unity Framework",
    "keywords": "Lightmap UVs Use the Lightmap UVs action to manually generate any missing lightmap UVs. This works on each Mesh in every open scene that is missing lightmap UVs. Tip: You can edit lightmap generation parameters for specific objects in the ProBuilderMesh component on ProBuilder objects. You can also modify the defaults on the ProBuilder Preferences window. There is one option for this action which you can access like any other toolbar option. You can also access it using the menu (Tools > ProBuilder > Editors > Open Lightmap UV Editor), because it appears on the Lightmap UV Editor window."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Merge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Merge.html",
    "title": "Merge Objects | FSM Unity Framework",
    "keywords": "Merge Objects The Merge Objects action merges two or more selected ProBuilder GameObjects into a single ProBuilder GameObject. Warning: If you merge two objects that intersect, the new object might have overlapping UVs. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Merge Objects)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Mirror.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Mirror.html",
    "title": "Mirror Objects | FSM Unity Framework",
    "keywords": "Mirror Objects The Mirror Objects action creates mirrored copies of objects. Mirroring is especially useful when you want to create symmetrical items. You can build one half, mirror it, and then Weld the two Meshes together for a perfectly symmetrical result. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Mirror Objects). Mirror Objects Options Property: Description: X, Y, Z Check each axis you want to mirror on. You can choose one axis only, or multiple axes. Duplicate Enable this option to create a duplicate object and mirror it, leaving the original unchanged. Set these properties, then click Mirror."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_ProBuilderize.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_ProBuilderize.html",
    "title": "ProBuilderize | FSM Unity Framework",
    "keywords": "ProBuilderize The ProBuilderize action converts the selected object(s) into ProBuilder-editable objects. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > ProBuilderize). ProBuilderize options Use the following ProBuilderize options to customize how your object is converted to a ProBuilder object. Property: Description: Import Quads Enable this option to keep Meshes quadrangulated when ProBuilder imports them. Disable it to import the Mesh as triangles. Import Smoothing Enable this option to use a smoothing angle value to calculate smoothing groups. Smoothing Threshold Set this value to decide which adjacent faces to add to a smoothing group. Use a value that is higher than the difference of any adjoining angle that is adjacent to the face(s) you want to add to a smoothing group. This setting is only available if Import Smoothing is enabled."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Subdivide.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Subdivide.html",
    "title": "Subdivide Object | FSM Unity Framework",
    "keywords": "Subdivide Object The Subdivide Object action divides every face on selected objects, allowing for greater levels of detail when modeling. To do this, for each face, it adds a vertex at the center of each edge and connects them in the center of the face. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Subdivide Object)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Triangulate.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Object_Triangulate.html",
    "title": "Triangulate (Objects) | FSM Unity Framework",
    "keywords": "Triangulate (Objects) The Triangulate action reduces all polygons to their base triangles. This creates a sharp, faceted appearance. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Object > Triangulate). Note: To smooth some of the hard edges, you can add and remove smoothing groups across the faces of the Mesh."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Offset_Elements.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Offset_Elements.html",
    "title": "Offset Elements | FSM Unity Framework",
    "keywords": "Offset Elements The Offset Elements action moves the selected element(s) according to the default values. You can change the default values with the Offset Settings. This tool is available in Vertex, Edge, and Face edit mode and appears as Offset Vertices, Offset Edges, and Offset Faces on the text buttons on the ProBuilder toolbar. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Offset Elements). Offset Elements options Using the Offset Settings lets you enter a precise value to move vertices, edges, and faces. Property: Description: Coordinate Space Select the relative space for moving the elements. World Move the element in world space. This is the default. Local Move the element relative to the GameObject. Element Move the element relative to the itself. Handle Moves the element relative to the handle. Translate Set positive or negative values to move for each axis. By default, X and Z are set to 0 and Y is set to 1."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Pipe.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Pipe.html",
    "title": "Pipe | FSM Unity Framework",
    "keywords": "Pipe The pipe shape is similar to the ProBuilder Cylinder, but hollow. It has similar properties, but you can also specify the thickness of the pipe wall. (A) Pipe with increased number of sides and thickness (B) Pipe with only three sides (C) Basic pipe shape (default values) (D) Pipe with increased number of height segments (faces per side) You can customize the shape of a pipe with these shape-specific properties: Property: Description: Thickness Set the thickness of the walls of the pipe in meters. The thicker the value, the smaller the hole becomes. The default value is 0.25. The minimum value is 0.01. Sides Count Set the number of sides for the pipe. The more sides you use, the smoother the sides of the pipe become. The default value is 6. Valid values range from 3 to 64. Height Cuts Set the number of divisions to use for the height of the pipe. For example, using a value of 3 produces four faces on every side of the pipe. The default value is 0. Valid values range from 0 to 31. Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Plane.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Plane.html",
    "title": "Plane | FSM Unity Framework",
    "keywords": "Plane A plane is a four-sided 2D shape. It can be a square or a rectangle, with two dimensions (width and height) in any orientation. (A) Square plane with 2 subdivisions along the width and 1 along the length (B) Square plane with 3 equal subdivisions along both the width and the length (C) Rectangular plane with no subdivisions You can customize the shape of a plane with these shape-specific properties: Property: Function: Width Cuts Set the number of divisions to use for the width of the plane. For example, if you use a value of 3, ProBuilder produces a plane with four \"columns\". If you specify 3 for both the Width Cuts and Height Cuts, ProBuilder builds a plane with 16 faces. The default value is 1. The minimum value is 0. Height Cuts Set the number of divisions to use for the length of the plane. For example, using a value of 3 produces four \"rows\". If you specify 2 for both the Width Segments and 1 for the Length Segments, ProBuilder builds a plane with six faces. The default value is 1. The minimum value is 0."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Prism.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Prism.html",
    "title": "Prism | FSM Unity Framework",
    "keywords": "Prism A prism is a shape with two identical ends and flat sides. The basic ProBuilder prism is like a three-dimensional triangle, stretched along the z-axis. The prism shape has no shape-specific properties."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ProBuilderMesh.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ProBuilderMesh.html",
    "title": "ProBuilder MeshFilter component | FSM Unity Framework",
    "keywords": "ProBuilder MeshFilter component This component appears on any ProBuilder Mesh GameObject. It displays the Object Size of the ProBuilder Mesh in X, Y, and Z. It also allows you to generate lightmap UVs for the Mesh, and customize how ProBuilder generates them. Lightmap UVs and their parameters To customize and generate lightmap UV parameters for this Mesh, enable the Lightmap Static option. A new section appears below the option. (A) The Unwrap Parameters section contains standard UV parameters for Generating Lightmap UVs on this ProBuilder Mesh. (B) Click the Apply button to save (or the Reset button to discard) the modifications you made to the Unwrap Parameters section. (C) If your Mesh is missing lightmap UVs, a warning message appears. Click the Generate Lightmap UVs button to regenerate the lightmap UVs for this Mesh. The message and the button disappears as soon as ProBuilder regenerates the UVs."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/SelectPath.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/SelectPath.html",
    "title": "Select Path | FSM Unity Framework",
    "keywords": "Select Path Use Select Path to show and select a path from the face underneath the cursor to the selected face on the surface of a Mesh. You can move your mouse to test different paths to the selected face and then click to select the path you want to select. To select a path of faces: In Face mode, select the face you want to build a path to. Hold both Ctrl/Cmd and Shift while you move your cursor around to see options for which faces ProBuilder might select. As you move, ProBuilder shows a preview of which faces you can select. When you see the pattern of faces that you want to select, click in place. ProBuilder grows the selection to include all the faces in that path. Note: This action is only available with the Ctrl/Cmd+Shift+click shortcut when the UV Editor window is closed. No menu or toolbar button activates this action, and you can't remap the shortcut."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Grow.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Grow.html",
    "title": "Grow Selection | FSM Unity Framework",
    "keywords": "Grow Selection The Grow Selection action expands the selection outward to adjacent faces, edges, or vertices. This action is available in the vertex, edge, and face modes. Tip: You can also use this action with the Alt/Opt+G shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Grow Selection). Grow Selection Options Property: Description: Restrict To Angle Enable this property to grow the selection only to those faces within a specified angle. Max Angle Set the maximum angle allowed when growing the selection. ProBuilder ignores this property and prevents you from editing it unless the Restrict to Angle property is enabled. Iterative Enable this property to grow the selection one adjacent face at a time, each time you press the Grow Selection button. This property is enabled automatically (and is not editable) if the Restrict to Angle property is disabled."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Loop_Edge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Loop_Edge.html",
    "title": "Select Edge Loop | FSM Unity Framework",
    "keywords": "Select Edge Loop The Select Edge Loop action selects an edge loop from each selected edge. An edge loop is a series of edges that are directly connected. Note: This action only works on quads (four-sided polygons), and is only available in edge mode. Tip: You can also use this action with the Alt/Opt+L shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Select Loop)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Loop_Face.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Loop_Face.html",
    "title": "Select Face Loop | FSM Unity Framework",
    "keywords": "Select Face Loop The Select Face Loop action selects a face loop from each selected face. ProBuilder only considers faces part of a loop if they are quads (contain exactly four sides). Face loops generally run along the Z-axis, while face rings generally run along the X-axis. This action is only available in face mode. Tip: You can also use this action with the Alt/Opt+L shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Select Loop)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Rect_Intersect.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Rect_Intersect.html",
    "title": "Rect | FSM Unity Framework",
    "keywords": "Rect Use the Rect action to choose whether drag selection should only select elements completely inside the drag rectangle, or also elements that are partially inside the the drag rectangle. This action is available only in the edge and face modes. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Interaction > Toggle Drag Rect Mode). Visual cues Depending whether you are in text mode or icon mode, the button displays the following to indicate what state the action is in: Icon mode: Text mode: Description: Rect: Complete Only select elements that are contained entirely within the drag rectangle. Rect: Intersect Select elements that are entirely or partially inside the drag-rectangle."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Ring_Edge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Ring_Edge.html",
    "title": "Select Edge Ring | FSM Unity Framework",
    "keywords": "Select Edge Ring The Select Edge Ring action selects a ring from each selected edge. An edge ring is a series of edges which share faces but are not directly connected. Note: This action only works on quads (four-sided polygons), and is only available in edge mode. Tip: You can also use this action with the Alt/Opt+R shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Select Ring)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Ring_Face.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Ring_Face.html",
    "title": "Select Face Ring | FSM Unity Framework",
    "keywords": "Select Face Ring Selects a face ring from each selected face. ProBuilder only considers faces part of a ring if they are quads (contain exactly four sides). Face rings generally run along the X-axis, whereas face loops generally run along the Z-axis. This action is available only in face mode. Tip: You can also use this tool with the Alt/Opt+R shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Select Ring)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectByMaterial.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectByMaterial.html",
    "title": "Select by Material | FSM Unity Framework",
    "keywords": "Select by Material The Select by Material action selects all faces on this object that have the same Material as the selected face(s). You can also extend the selection to other GameObjects if you disable the Current Selection option. This action is useful if you want to replace all Materials on a complex object. It is only available in face mode. Tip: You can also access this action from the ProBuilder menu (Tools > ProBuilder > Selection > Select Material). Select by Material Options Enable the Current Selection option to extend the selection to other faces on the currently selected GameObject(s) only. By default, this option is disabled. When disabled, ProBuilder selects every face that has a matching Material on any GameObject in the scene. This is particularly useful if you want to replace this Material with another on every GameObject in the scene at once."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectByVertexColor.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectByVertexColor.html",
    "title": "Select by Colors | FSM Unity Framework",
    "keywords": "Select by Colors The Select by Colors selects all faces on this object which have the same vertex color as the selected face. You can also extend the selection to other GameObjects if you disable the Current Selection option. Even if the vertex color isn't currently visible (for example, if it has a Material that doesn't show colors, like the checkerboard Material), the colored faces are still selected. This action is useful for grouping out sections of your Mesh with different vertex colors. It is available in the vertex, edge, and face modes. Tip: You can also access this action from the ProBuilder menu (Tools > ProBuilder > Selection > Select Vertex Color). Select by Colors Options Enable the Current Selection option to extend the selection to other faces on the currently selected GameObject(s). By default, this option is disabled. When disabled, ProBuilder selects every face with the currently selected vertex color on any GameObject in the scene."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectHidden.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectHidden.html",
    "title": "Select Hidden | FSM Unity Framework",
    "keywords": "Select Hidden Use the Select Hidden action to determine whether ProBuilder selects or ignores hidden elements when you perform a drag-selection. Clicking the Select Hidden button to switch between the two states: On or Off. Depending whether you are in text mode or icon mode, the button displays the following to indicate what state the action is in: Icon mode: Text mode: Description: Hidden: On Drag selection selects all elements, regardless of their visibility. Hidden: Off Drag selection ignores any elements that you can't currently see. This action is available in all edit modes. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Interaction > Toggle Select Back Faces)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectHole.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SelectHole.html",
    "title": "Select Holes | FSM Unity Framework",
    "keywords": "Select Holes The Select Holes action selects all elements along the selected open vertex or edge. A hole is like a removed face. This action is a useful shortcut for selecting all the edges around a missing face. If you have no elements selected, this action automatically selects all holes in the selected object. This action also tells you how many holes exist in the Mesh. It is only available in the vertex and edge modes. Tip: You can also access this action from the ProBuilder menu (Tools > ProBuilder > Selection > Select Hole)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Shift.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Shift.html",
    "title": "Shift | FSM Unity Framework",
    "keywords": "Shift Use the Shift action to change what happens to the selection when you Shift+Click an element or hold Shift while drag-selecting. Click the Shift button to switch between the three states: Add, Remove, or Difference. Depending whether you are in text mode or icon mode, the button displays the following to indicate what state the action is in: Icon mode: Text mode: Description: Shift: Add Shift+Click adds the selected elements to the current selection. Shift: Subtract Shift+Click removes the selected elements from the current selection. Shift: Difference Shift+Click toggles the selection: it adds unselected elements and removes selected elements. This action is available in the vertex, edge, and face modes. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Interaction > Toggle Drag Selection Mode)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Shrink.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_Shrink.html",
    "title": "Shrink Selection | FSM Unity Framework",
    "keywords": "Shrink Selection The Shrink Selection action removes the elements on the perimeter of the current selection. It performs the opposite action of the Grow Selection action. In this example: Left image: Five faces are selected. Right image: The Shrink Selection reduces the selection down to just the central square. This action is available in the vertex, edge, and face modes. Tip: You can also use this action with the Alt/Opt+Shift+G shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Shrink Selection)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SmoothingGroup.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Selection_SmoothingGroup.html",
    "title": "Select Smoothing Group | FSM Unity Framework",
    "keywords": "Select Smoothing Group Use the Select Smoothing Group action inside the Smooth Group Editor window to select faces matching the current smoothing group. To use this action: Open the Smooth Group Editor window. Click any face with a smoothing group defined. Note: The action doesn't shrink the current selection, so it's best to start with as small a selection as possible. Click the Select Smoothing Group button. Tip: You can also access this action from the ProBuilder menu (Tools > ProBuilder > Selection > Select Smoothing Group)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Sphere.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Sphere.html",
    "title": "Sphere | FSM Unity Framework",
    "keywords": "Sphere A sphere is a perfectly round 3D object. In ProBuilder, a sphere is actually an icosahedron with a minimum of 42 vertices (for 1 subdivision) shared across multiple triangles (faces). When you increase the subdivisions, the shape looks smoother, more sphere-like, but the geometry also becomes more complex. For example, with 2 subdivisions, the shape has 162 vertices; and for 5 subdivisions (the maximum), the shape has 2562 vertices. (A) Default sphere (three subdivisions). (B) Sphere with two subdivisions. (C) Sphere with four subdivisions. (D) Sphere shape with no subdivisions showing five triangles colored. A wireframe of a default sphere is superimposed on that shape (the same size but with one subdivision). (E) Sphere shape (one subdivision). You can customize the shape of a sphere with this shape-specific property: Property: Description: Subdivisions Set the number of times to subdivide each triangle. The default value is 3. Valid values range from 1 to 5. The more subdivisions you create, the smoother the sphere appears. However, remember that each subdivision increases the number of triangles exponentially, which means that it uses a lot more resources to render. Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Sprite.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Sprite.html",
    "title": "Sprite | FSM Unity Framework",
    "keywords": "Sprite In ProBuilder, a sprite shape is a plane with all values set to 1 unit. The sprite shape has no shape-specific properties."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Stair.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Stair.html",
    "title": "Stairs | FSM Unity Framework",
    "keywords": "Stairs You can create straight stairs, curved stairs, long stairs, stairs with a lot of steps, wide stairs, and stairs without side polygons. You can customize the shape of your stairs with these shape-specific properties: Property: Description: Steps Generation Select whether you want ProBuilder to build the same number of steps regardless of how the size of the stairs changes (the default) or make each step the same height and automatically adapt the number of steps to match the stairs size. Height Select this method if you want ProBuilder to generate a predictable height for each step in the staircase. This means that if you increase the height of the overall size of the staircase, the number of steps increases. Count Select this method if you want ProBuilder to generate a specific number of steps, regardless of any changes in the size of the staircase. This means that if you increase the height of the overall size of the stairs, each step becomes higher. This is the default value. Steps Height Set the fixed height of each step on the stairs. The default value is 0.2. This property is only available when the Steps Generation method is set to Height. Homogeneous Steps Enable this option to force every step to be the exactly the same height. This is enabled by default. If disabled, the height of the last step is smaller than the others depending on the remaining height. This property is only available when the Steps Generation method is set to Height. Steps Count Set the fixed number of steps that the stairs always has. The default value is 10. Valid values range from 1 to 256. This property is only available when the Steps Generation method is set to Count. Sides Enable this option to draw polygons on the sides of the stairs. This is enabled by default. You can disable this option if the sides of your stairs are not visible to the camera (for example, if your stairs are built into a wall). Circumference Set the degree of curvature on the stairs in degrees, where 0 makes straight stairs and 360 makes stairs in a complete circle. Remember that you might need to increase the number of stairs to compensate as you increase this value. The default value is 0. Valid values range from -360 (full turn to the left) to 360 (full turn to the right)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About ProBuilder What's new in version 5.0 Getting started Edit modes Tools vs. actions Installing ProBuilder Helpful links Interacting with ProBuilder The Edit mode toolbar The ProBuilder toolbar Tool panels Selection actions Object actions Vertex actions Edge actions Face actions The ProBuilder menu ProBuilder Window Editors Dimensions Overlay Selection Interaction Object Geometry Materials Vertex Colors Experimental Repair Debug Export Actions ProBuilder shortcuts Creating meshes Pre-defined shape Polygon shape Editing meshes Common editing tasks Modeling tips for novices Setting vertex colors Materials, Shaders, Textures, and UVs Creating and applying Materials Mapping Textures with UVs Smoothing hard edges Customizing ProBuilder Exporting and re-importing Action reference Bevel - Edges Bevel - Faces Bridge Edges Center Pivot Collapse Vertices Conform Normals - Faces Conform Normals - Objects Connect Edges Connect Vertices Delete Faces Detach Faces Duplicate Faces Export Extrude Edges Extrude Faces Fill Hole - Edges Fill Hole - Vertices Flip Face Edge Flip Face Normals Flip Normals - Objects Freeze Transform Grow Selection Insert Edge Loop Inset Lightmap UVs Merge Faces Merge Objects Mirror Objects Offset Elements Orientation ProBuilderize ProBuilder MeshFilter component Rect Select by Colors Select by Material Select Edge Loop Select Edge Ring Select Face Loop Select Face Ring Select Smoothing Group Select Hidden Select Holes Select Path Set Collider Set Pivot - Edges Set Pivot - Faces Set Pivot - Vertices Set Trigger Shift Shrink Selection Split Vertices Subdivide Edges Subdivide Faces Subdivide Object Triangulate Faces Triangulate - Objects Weld Vertices Tools reference Cut tool Poly Shape tool Shape tool Arch Cone Cube Cylinder Door Pipe Plane Prism Sphere Sprite Stairs Torus Editor window reference Lightmap UV Editor window Material Editor window Positions Editor window ProBuilder Preferences window Smooth Group Editor window UV Editor window UV Editor toolbar UV drop-down menu Actions Panel: Auto UV Mode Actions Panel: Manual UV Mode Vertex Colors window Troubleshooting Experimental features Bezier Shape tool Bezier Shape component Boolean operations Scripting API overview Glossary"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Torus.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Torus.html",
    "title": "Torus | FSM Unity Framework",
    "keywords": "Torus A torus is a 3D shape formed by a small circle that rotates around a bigger circle. It usually looks like a circular ring, or a donut. (A) Default torus (smooth) (B) Torus with 180-degree Horizontal Circumference (C) Torus with the Smooth property set to false (D) Torus with 180-degree Vertical Circumference You can customize the shape of a torus with these shape-specific properties: Property: Description: Rows Set the complexity of the Mesh, together with the Columns value. You can enter a value from 3 to 64. The higher the value, the smoother the shape, but at the cost of more polygons to calculate. The default value is 16. Columns Set the complexity of the Mesh, together with the Rows value. You can enter a value from 3 to 64. The higher the value, the smoother the shape, but at the cost of more polygons (and therefore more computation). The default value is 24. Tube Radius Set the radius of the tube itself in meters. The default value is 0.1. Horizontal Circumference Set the degree of the torus's circumference. For example, if you set this value to 180 (as the (B) image above demonstrates), it looks like someone cut half of the torus away (like leaving half of a donut). The default value is 360 (full torus). Vertical Circumference Set the degree of the tube's circumference. For example, if you set this value to 180 (as the (D) image demonstrates), it looks like a half-pipe. The default value is 360 (full tube). Smooth Enable this option to smooth the edges of the polygons. This property is enabled by default."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Collapse.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Collapse.html",
    "title": "Collapse Vertices | FSM Unity Framework",
    "keywords": "Collapse Vertices The Collapse Vertices action collapses all selected vertices to a single point, regardless of distance. Tip: You can also use this action with the Alt/Opt+C shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Collapse Vertices). ProBuilder uses the Collapse To First option to decide whether to collapse the vertices to a central point, or to the first selected vertex. Collapse Vertices Options The Collapse Vertices action determines where to locate the collapsed vertex in this way: If Collapse To First option is enabled, ProBuilder uses the location of the vertex that you selected first. Otherwise, ProBuilder calculates the center position between all selected vertices to use as the new location. By default, the Collapse To First option is disabled, so ProBuilder uses the center position as in the example image above."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Connect.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Connect.html",
    "title": "Connect Vertices | FSM Unity Framework",
    "keywords": "Connect Vertices The Connect Vertices action creates a new edge that connects the selected vertices. Tip: You can also use this action with the Alt/Opt+E shortcut, or from the ProBuilder menu (Tools > ProBuilder > Selection > Smart Connect). If you select more than two vertices, ProBuilder creates as many new edges as required, and adds extra vertices where necessary in order to keep the geometry valid. For example, if you connect three vertices around a quad, ProBuilder creates a new vertex in the middle to support the three new edges. You can connect across several faces as long as they share a selected vertex."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_FillHole.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_FillHole.html",
    "title": "Fill Hole (Vertices) | FSM Unity Framework",
    "keywords": "Fill Hole (Vertices) The Fill Hole action creates a new face that fills any holes that touch the selected vertices. Tip: You can also launch this action from the ProBuilder menu (Tools > ProBuilder > Geometry > Fill Hole). Fill Hole Options Enable the Fill Entire Hole option to fill the entire Mesh opening. This is enabled by default. If you disable this option, ProBuilder tries to build a Mesh between the selected open vertices. For example, if you have a missing quad, you can select any three adjacent vertices in order to create a triangular polygon that covers half of the hole."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_SetPivot.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_SetPivot.html",
    "title": "Set Pivot (Vertices) | FSM Unity Framework",
    "keywords": "Set Pivot (Vertices) Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected vertices. Tip: You can also launch this action with the Ctrl/Cmd+J shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Set Pivot). In this example: Left panel: The pivot point of the log is at the end corner of the cylinder. Middle panel: The vertices on the end are selected, so the Set Pivot action changes the pivot to the center of those end vertices. Right panel: The pivot point is now in the center of the cylinder end, even when in Object editing mode."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Split.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Split.html",
    "title": "Split Vertices | FSM Unity Framework",
    "keywords": "Split Vertices The Split Vertices action splits a vertex into individual vertices (one for each adjacent face) so that you can move the faces independently. Note: When a vertex splits, the newly separated vertices remain in place. This image only shows the vertices apart as a demonstration, to illustrate that the original vertex became four. Tip: You can also use this action with the Alt/Opt+X shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Split Vertices)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Weld.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/Vert_Weld.html",
    "title": "Weld Vertices | FSM Unity Framework",
    "keywords": "Weld Vertices The Weld Vertices action merges selected vertices within a specific distance of one another. You can set this distance with the Weld Distance option. Tip: You can also use this action with the Alt/Opt+V shortcut, or from the ProBuilder menu (Tools > ProBuilder > Geometry > Weld Vertices). Weld Vertices Options ProBuilder uses the Weld Distance value to determine whether to weld any two vertices together. For example, you can use a very low number (such as the default value, 0.001) to reduce vertices on a Mesh that are virtually occupying the same space. With higher numbers, more vertices fit the criterion of \"close enough to weld\". However, if you use a fairly low value, you can select more vertices than you intend to weld and still weld selected vertices that are close together. For example, in the image above, you could select all of the vertices and still achieve the same effect by using a Weld Distance of 0.25."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/_ReplaceSource.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/_ReplaceSource.html",
    "title": "| FSM Unity Framework",
    "keywords": "The Replace Source option determines whether ProBuilder updates the GameObject's MeshFilter.sharedMesh to point to the exported Mesh data (enabled) or to the source of the exported Mesh (disabled)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/api.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/api.html",
    "title": "About the ProBuilder Scripting API | FSM Unity Framework",
    "keywords": "About the ProBuilder Scripting API ProBuilder provides a Scripting API for C#, which you can use to extend the ProBuilder tools and windows. It includes these namespaces: UnityEditor.ProBuilder provides classes and enums for Unity editor integration. Use them to extend ProBuilder menus, windows, toolbars, and Mesh operations that are only available through the ProBuilder windows and tools. UnityEngine.ProBuilder provides classes, structs, and enums for compiling Meshes. Use them to access a lot of core ProBuilder functionality, such as creating Meshes, dealing with events, and some math functions. UnityEngine.ProBuilder.MeshOperations provides classes for Mesh editing. Use them to manipulate ProBuilder Meshes, including topology and I/0 operations. All Mesh creation and editing functionality is restricted to the UnityEngine.ProBuilder and UnityEngine.ProBuilder.MeshOperations libraries, which are both available at run time. ProBuilder stores Mesh data in a component (ProBuilderMesh) and compiles it to a UnityEngine.Mesh object as necessary. ProBuilderMesh stores the following Mesh information: Positions UVs Faces Triangles Material Smoothing group Auto/Manual UVs Note: ProBuilder can automatically UV unwrap triangles on a per-face basis. You can toggle this feature with the Face class. In addition, users can unwrap faces manually. Tangent (if user set) UV3/4 (if user set) Colors Shared indices (also called common vertices) Normals, tangents, collisions, and UVs are calculated as necessary. Create a Mesh This example demonstrates how to build a simple quad with the ProBuilder API (not with the ShapeGenerator class): // Create a new quad facing forward. ProBuilderMesh quad = ProBuilderMesh.Create( new Vector3[] { new Vector3(0f, 0f, 0f), new Vector3(1f, 0f, 0f), new Vector3(0f, 1f, 0f), new Vector3(1f, 1f, 0f) }, new Face[] { new Face(new int[] { 0, 1, 2, 1, 3, 2 } ) } ); Modify a Mesh Modifying a ProBuilder Mesh is different from modifying a Unity Mesh: instead of working with MeshFilter.sharedMesh you work with the ProBuilder representation of the Mesh: ProBuilderMesh. The basics are the same: set vertex positions, modify triangles (faces in ProBuilder), then rebuild the mesh. For example, to move the vertices up on that quad from the previous example: using UnityEngine; using UnityEngine.ProBuilder; #if UNITY_EDITOR using UnityEditor.ProBuilder; #endif public class MoveVertices : MonoBehaviour { void Start() { var cube = ShapeGenerator.CreateShape(ShapeType.Cube); // Move one face on the cube along the direction of its normal Vertex[] vertices = cube.GetVertices(); Face face = cube.faces[0]; Vector3 normal = Math.Normal(cube, face); // A face is a collection of triangles, stored in the indexes array. Because mesh geometry requires that seams // be inserted at points where normals, UVs, or other vertex attributes differ we use GetCoincidentVertices to // collect all vertices at a common position. // To see the difference, try replacing `cube.GetCoincidentVertices` with just `face.dinstinctIndexes`. foreach(var index in cube.GetCoincidentVertices(face.indexes)) vertices[index].position += normal; cube.SetVertices(vertices); // Rebuild the triangle and submesh arrays, and apply vertex positions and submeshes to `MeshFilter.sharedMesh`. cube.ToMesh(); // Recalculate UVs, Normals, Tangents, Collisions, then apply to Unity Mesh. cube.Refresh(); // If in Editor, generate UV2 and collapse duplicate vertices. #if UNITY_EDITOR EditorMeshUtility.Optimize(cube, true); #else // At runtime, `EditorMeshUtility` is not available. To collapse duplicate // vertices in runtime, modify the MeshFilter.sharedMesh directly. // Note that any subsequent changes to `quad` will overwrite the sharedMesh. var umesh = cube.GetComponent<MeshFilter>().sharedMesh; MeshUtility.CollapseSharedVertices(umesh); #endif } } Note that you should never directly modify the MeshFilter.sharedMesh. ProBuilder controls updating the Unity Mesh with ProBuilderMesh::ToMesh and ProBuilderMesh::Refresh functions."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/auto-uvs-actions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/auto-uvs-actions.html",
    "title": "Actions panel: Auto UV Mode | FSM Unity Framework",
    "keywords": "Actions panel: Auto UV Mode In Auto mode, the Actions panel displays the UV Mode: Auto label and the settings for automatic texture mapping. To access this panel, click the Convert to Auto button from the Actions panel in Manual UV Mode. The Auto mode provides the following features to help you with texture mapping: The Tiling & Alignment section defines the Texture's basic layout. The Transform section helps you set the position, orientation, and scaling in U and V. The Special section provides settings for changing UV direction. The Texture Groups section helps you manage tiling across multiple faces using Texture groups. Click the Reset UVs button when you want to clear any edits you made on the selected face(s). This can be very useful, especially if you are just learning how to use ProBuilder for texture mapping. To switch back to the Actions panel in Manual UV Mode and convert all selected faces to use manual UVs, click the Convert to Manual button. Tip: When you convert from Auto UVs to Manual there is no loss of resolution. However, if you modify vertices or edges individually in Manual mode, those changes don't transfer over to Auto mode. These settings are interconnected: if you change a Transform setting, it might affect something you previously set under the Tiling & Alignment section. For example, the Anchor refers to a location on the original UV face that has a Tiling value of 1. If you change the Tiling (scale) value, you might need to readjust the Offset to compensate. Tiling & Alignment Use the Fill Mode and Anchor properties to define how you want the image to appear across the selected face(s). These are basic guidelines for ProBuilder to be able to manage the Texture projection automatically. For more precise controls, use the Manual mode instead. Fill Mode The Fill Mode defines how to treat the UV image; whether to repeat it (to create a tile effect), stretch it, or fit it along the U or V axis. Value: Function: Tile Continuously tile (repeat) the UVs across the object. Fit Uniformly scale UVs to size. Stretch Expand the selection on all sides to fill space. Note: Certain configurations on the Transform and Special settings can neutralize the effect of the Fill Mode. For example, if you use the Stretch mode and you reduce the Tiling value (scale up the UV face), the UV might no longer show the entire image. Anchor Define where on the UV face the texture image appears. ProBuilder projects the image from the selected anchor point, so if you set it to Lower Left, the image projects up and to the right. By default this is set to None, but you can select any of the following points: (A) Upper Left (B) Upper Center (C) Upper Right (D) Middle Left (E) Middle Center (F) Middle Right (G) Lower Left (H) Lower Center (I) Lower Right Anchor points represent a point on the original UV face (before you scale or tile it). Transform The Transform section allows you to set precise values for the 2-dimensional size, rotation, and scale of the UV faces relative to the Texture image. When you modify these values, the changes appear in the UV Viewer. Alternatively, you can also manipulate the Offset, Rotation, and Tiling directly in the UV Viewer with the standard Unity Transform controls. Note: You can only change transform values for a face. If you try to switch to the vertex or edge UV editing modes, the UV Editor changes the Actions panel to Manual UV Mode. Offset Enter exact offset X and Y values, or drag the input fields to adjust. The offset value represents an offset from the Anchor position along the U or V axis. Rotation Enter exact rotation values here in degrees (0-360), or drag the slider to adjust. Don't forget that you are rotating the UV face, not the image. As you rotate the UV face to the right in the UV Viewer, the image projected on the Mesh face in the Scene view rotates to the left. Tiling There are three ways to change the size of a UV face in the UV Editor: Enter exact scale values in the X and Y properties for each axis. These values represent the size of the UV face relative to the Texture. For example, when you enter a value of 0.5 in Y, this halves the height of the UV face on the image, whereas a value of 2 in X doubles the width. Select the appropriate preset button to set how many times you want the Texture image to appear across the UV face. For example, the .5 preset only displays half the image: in the viewer, the UV face appears twice as large against the image, but in the Scene view, you can only see half of the image on the Mesh face. Conversely, using the 4 preset results in the image tiling four times across the Mesh face in the Scene view, so the UV face in the Viewer appears to be a quarter the size of the image. Select and drag on the scale gizmo to scale the UV face directly in the UV Viewer. In all three cases, ProBuilder updates the X and Y Tiling values to reflect any changes. Special These options help you control the Texture direction. World Space Enable this option to align UVs to the World. This ensures Textures on same-angle faces always line up, but UVs do not stay put when you move the object. Flip U Enable this option to flip the UVs horizontally. Flip V Enable this option to flip the UVs vertically. Swap U/V Enable this option to invert the horizontal and vertical UVs. Texture Groups Use Texture Groups to keep consistent tiling across several faces as if they were one. Each face in the group must share an edge with at least one other face in the group. Texture Group Number Displays the Texture Group ID of the currently selected face, or 0 if the face doesn't belong to a Texture Group. You can also assign the currently selected face(s) to an existing group if you already know the group's number. To do this, enter it in the Texture Group Number box. Group Selected Faces Click the Group Selected Faces button to create a new Texture Group from the selected face(s). Select Texture Group Click the Select Texture Group button to select all faces in the the group."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/bezier.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/bezier.html",
    "title": "Bezier Shape component | FSM Unity Framework",
    "keywords": "Bezier Shape component Warning: Bezier shapes are experimental, meaning that they are still under development, and might reduce ProBuilder's stability. Please use with caution. Use the Bezier Shape tool to create a more organic shape by defining a spline along which ProBuilder extrudes a 3-dimensional Mesh. When you activate the Bezier Shape tool to create a new shape, the Bezier Shape component provides properties that help you customize the new shape. After you activate the tool, the Bezier Shape is ready for editing: (A) Curve property values (B) Tangent modes (C) Shape property values Note: You can modify Mesh elements on the Bezier Shape Mesh with the standard ProBuilder editing tools and actions. However, each time you re-enter Bezier Shape editing mode, you lose any element changes. Editing a Bezier Shape When you are in Bezier Shape editing mode, the Bezier Shape component in the Inspector displays the Editing Bezier Shape button. If you are not in Bezier Shape editing mode, the Edit Bezier Shape button appears instead: To enter the Bezier Shape editing mode, click the Edit Bezier Shape button. To exit the Bezier Shape editing mode, select the button (Editing Bezier Shape) again. In Bezier Shape editing mode, you can modify the shape. To do this, perform the following tasks in the Scene view: Click and drag existing control points to move them around. When you select control points, translation and rotation handles appear for more precise control. Click existing control points to select them, then use Backspace (Windows) or Delete (macOS) to remove the points from the shape. Click along the bezier path line to add new control points. In the Inspector, you can also use the controls in the Bezier Shape component to: Enter Curve property values (position, rotation, tangent) directly. Click the Clear Points button to clear all control points on the Mesh. Click the Add Point button to add a new control point at the end of the Bezier path. Change the Tangent mode to use for the handles. Customize the overall look of the shape by setting the Shape property values. Curve property values Use these property values to set precise values for the curve's position, rotation, and tangent handles. Property: Description: Position Enter the local position of the selected control point. Tan. In Enter the local position of the selected control tangent in handle. Tan. Out Enter the local position of the selected control tangent out handle. Rotation Enter the additional rotation to be applied to the vertices of the extruded tube. Tangent modes Tangent modes change how much control you have over the tangent handles. Icon Tangent Mode Description Free Adjusting one tangent does not affect the other. Aligned Adjusting one tangent forces the other tangent's magnitude to match. Mirrored Locks tangent handles in a straight line. Shape property values Use these property values to set some basic characteristics of the Bezier Shape. Property: Description: CloseLoop Enable this option to loop the extruded path back around to the start point. Smooth Enable this option to use soft normals for the extruded pipe faces. Disable it for hard normals. Radius Enter the radius of the extruded pipe. Rows Enter the number of segments to insert between control points when extruding the pipe. Columns Enter the number of vertices that make up the ring around the radius of the pipe."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/boolean.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/boolean.html",
    "title": "Boolean operations | FSM Unity Framework",
    "keywords": "Boolean operations Warning: The Boolean feature is experimental, meaning that it is still under development, and might reduce ProBuilder's stability. Please use with caution. The experimental Boolean features provides an interface for creating new Meshes from boolean operations. Each type of operation takes two ProBuilder Mesh objects as the left and right inputs. ProBuilder combines the two input Meshes to create a new Mesh shape based on the operation you select. You can only access this feature from the menu, because it is experimental: To open the Boolean (Experimental) window, navigate to Unity's top menu and go to Tools > ProBuilder > Experimental > Boolean (CSG) Tool. Set references to the ProBuilder Meshes just under the preview windows on the left and the right side. Select one of the boolean operation types from the Operation drop-down menu: Intersection, Union, or Subtraction. Click the Apply button. ProBuilder performs the selected operation. Intersection The new Mesh matches the shape where the two original Meshes intersected in space. Union The new Mesh matches both of the original Meshes but as a single Mesh. Subtraction The new Mesh is like a union of the two original Meshes, minus the shape where the two shapes occupy the same space."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/customizing.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/customizing.html",
    "title": "Customizing ProBuilder | FSM Unity Framework",
    "keywords": "Customizing ProBuilder There are a number of ways to customize the way you work with ProBuilder. You can set preferences on the ProBuilder Preferences window: Dimensions Overlay: Customize which elements are used for the dimensions overlay. Experimental: Toggle ProBuilder beta features. General: This section contains a variety of settings, many of which control what information appears in the Scene view (see Interaction and display). Graphics: Mostly these settings control the look of Unity (see Interaction and display). Mesh Editing: Define behavior specific to editing Meshes. Mesh Settings: Define defaults for ProBuilder Meshes (such as default Material, collider type, and shadows). Snap Settings: Customize how snapping behaves with ProBuilder. Toolbar: Customize how the toolbars appear. See below for details. UV Editor: Change the size of the grid on the UV Editor window. In addition, you can limit the types of messages ProBuilder displays in the Console and whether to write them to a file. Logging You can choose what types of messages ProBuilder should log, and where the messages should appear. For example, you can set up logs to appear in your Unity console, to a log file locally, or both at the same time. You can also limit the log to only error messages, warning messages, debug messages, basic information, or any combination of these messages types. For details, see the documentation on the Debug menu. Interaction and display ProBuilder can display information about the Meshes in your Scene. For example, you can see the dimensions of your Mesh with Dimensions Overlay. You can also enable Show Scene Info to see how many vertices, edges, and faces are in the Scene and which are currently selected. ProBuilder can echo the name of the action you are performing if you enable the Show Action Notifications preference. You can use the Unity color scheme, or disable the Use Unity Colors preference to set your own colors for the ProBuilder geometry elements. When disabled, a number of other properties appear that allow you to set the colors for the Wireframe and the hover color (Preselection). You can also define distinct colors for vertices, edges, and faces in both selected and unselected states. You can also change the size of the points that appear for vertices (Vertex Size), and the lines that appear for edges or wireframe (Line Size). In addition, you can customize several things about any of the toolbars that ProBuilder provides: You can dock the toolbar or let it float over the Unity window. You can resize the toolbar both horizontally and vertically. You can also choose whether to display text or icons on the ProBuilder toolbar. Text vs Icon mode To define whether ProBuilder displays text or icons for each tool: Right-click on any empty space in the toolbar. Choose either Use Text Mode or Use Icon Mode from the context menu. Note: You can also use the Use Icon GUI option in the ProBuilder Preferences window. Floating vs Dockable To change whether the toolbar docks or floats in the Scene view: Right-click on any empty space in the toolbar. Choose either Open As Floating Window or Open As Dockable Window from the Window submenu of the context menu. Resizing the toolbar To resize the ProBuilder toolbar, click and drag the corners and sides of the toolbar until it is at the right size and shape. ProBuilder re-orders the icon or text contents to best fit the window size."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/cut-tool.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/cut-tool.html",
    "title": "Cut tool | FSM Unity Framework",
    "keywords": "Cut tool Use the Cut tool to subdivide Mesh faces with precision. To cut out a shape on a Mesh, you activate the Cut tool, which lets you define the cutout shape with points. The cutout shape becomes a new face on the Mesh. To create a new face on an existing Mesh face: Open the ProBuilder window (in Unity's top menu: Tools > ProBuilder window). The Edit Mode toolbar and the ProBuilder toolbar appear. Switch to one of the element editing modes. In the ProBuilder toolbar, click the Cut Tool button. The Cut Settings panel appears at the bottom of the Scene view with a Complete button and a Cancel button. If the ProBuilder toolbar is using Text mode, the button text background becomes red. To control snapping to nearby edges and vertices, use the options on the Cut Settings panel. Click on the Mesh face where you want the vertices for the new face to be. ProBuilder creates the cutout shape based on the edges you draw with these points. For example, you can specify three points on the Mesh to define a triangular shape and the fourth to close it: Tip: ProBuilder displays red edges as a visual warning if your points make an invalid edge or face. When this happens, undo adding each point until all the edges become blue again. As soon as you return to the first point and click it again (such as the fourth point in the triangle example), the cut is complete and the new face appears selected in the Scene view. Now you can either exit the tool or start another cut. **Note**: If nothing happened when you returned to the first point and clicked it again or clicked the **Complete** button, it is probably because the points you defined do not create a valid edge or face. For example, if the tool detected only one point, that does not make a valid edge. You can define more points, undo the previous points, or click the **Cancel** button to cancel the operation and start again. To start another cut, click the Start button. The Cut Settings panel displays a Complete button and a Cancel button again. To exit the Cut tool, you can: Click the Quit button on the Cut Settings panel. Select the Esc key. Click the Cut Tool button on the ProBuilder toolbar again. Tip: When you create a new face, the Cut tool creates extra edges in order to strengthen the geometry. Avoid merging the surrounding faces to remove the extra edges, because this could result in degenerated faces and broken geometry. Cut Settings panel When ProBuilder enters Cut mode, the following panel appears at the bottom of the Scene view: Enable the Snap to existing edges and vertices option to snap the points you draw on the target face to any nearby edges and vertices. This makes it easier to place points on the edges or vertices of the face. When snapping is enabled, the Snapping distance defines what happens as you approach a face border. By default, if you click within 0.1 units of an edge or vertex, the Cut tool adds your point on that border instead of adding a floating point directly on the face. When you activate the Cut tool, the Cut Settings panel displays a Complete button and a Cancel button. As soon as you complete the cut successfully, the Start button appears because the Cut tool is modal and you have the choice to either define another cutout or explicitly exit the tool: When you are ready to define points for a new cutout, click the Start button and draw more points on the Mesh. When you want to exit the tool, click the Quit button, select the Esc key, or click the Cut Tool button on the ProBuilder toolbar again."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/edge.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/edge.html",
    "title": "Edge actions | FSM Unity Framework",
    "keywords": "Edge actions This section of the ProBuilder toolbar provides access to ProBuilder editing actions that you can use in the Edge edit mode. Note: Some actions also have extra options or custom settings available. These action buttons have a special indicator in the top right corner. The documentation for each action includes information about these options. Bridge Edges Use the Bridge Edges action to create a new face between two selected edges. For more information, see the Bridge Edges action documentation. Bevel Use the Bevel action to split the selected edge(s) into two edges, with a new face between. For more information, see the Bevel action documentation. Connect Edges Use the Connect Edges action to insert a new edge that connects the centers of each existing selected edge. For more information, see the Connect Edges action documentation. Extrude Edges Use the Extrude Edges action to push a new edge out from each selected edge. For more information, see the Extrude Edges action documentation. Subdivide Edges Use the Subdivide Edges action to divide the selected edge(s) into multiple edges. For more information, see the Subdivide Edges action documentation. Fill Hole Use the Fill Hole action to create a new face that fills any holes that touch the selected edges. For more information, see the Fill Hole action documentation. Insert Edge Loop Use the Insert Edge Loop action to add a new edge loop from the selected edge(s). For more information, see the Insert Edge Loop action documentation. Cut Tool Use the Cut tool to create a new face on an existing Mesh. For more information, see the Cut tool documentation. Offset Edges Use the Offset Elements action in the Edge edit mode to move the selected edge(s) according to the settings. For more information, see the Offset Elements action documentation. Set Pivot Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected edges. For more information, see the Set Pivot action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/edit-mode-toolbar.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/edit-mode-toolbar.html",
    "title": "The Edit Mode toolbar | FSM Unity Framework",
    "keywords": "The Edit Mode toolbar Use the Edit Mode toolbar to switch between the four ProBuilder Edit modes: Object mode: the standard Unity mode for selecting and manipulating GameObjects. Vertex mode: the element mode for selecting and manipulating vertices (points) on a ProBuilder Mesh. Edge mode: the element mode for selecting and manipulating edges (lines) on a ProBuilder Mesh. Face mode: the element mode for selecting and manipulating faces (polygons) on a ProBuilder Mesh. The Vertex, Edge, and Face modes are also known collectively as the Element modes. Tip: By default, the Edit mode toolbar appears in the top middle of the Scene view, but you can reposition it with the Toolbar Location preference. Edit mode shortcuts In addition to the Scene view toolbar, keyboard shortcuts are also available: Escape switches to Object mode from any other mode. G toggles between the Object and Element modes. For example, if you are in Vertex mode, press G once to return to Object mode, and press it again to switch back to Vertex mode. H toggles between the Element modes (Vertex, Edge, and Face). You can use Unity's Shortcuts Manager to change the key assignments if you prefer."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/experimental.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/experimental.html",
    "title": "Experimental features | FSM Unity Framework",
    "keywords": "Experimental features ProBuilder has these experimental features: Bezier Shape tool Boolean operations These features are still under development; they are not fully tested, and might reduce ProBuilder's stability. Use these with caution. By default, experimental features are disabled. To enable them, enable the experimental features preference:"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/face.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/face.html",
    "title": "Face actions | FSM Unity Framework",
    "keywords": "Face actions This section of the ProBuilder toolbar provides access to ProBuilder editing actions that you can use in the Face edit mode. Note: Some actions also have extra options or custom settings available. These action buttons have a special indicator in the top right corner. The documentation for each action includes information about these options. Subdivide Faces Use the Subdivide Faces action to add a vertex at the center of each edge and connect them in the center. For more information, see the Subdivide Faces action documentation. Triangulate Faces Use the Triangulate Faces action to reduce selected faces to their base triangles. For more information, see the Triangulate Faces action documentation. Bevel Use the Bevel action to bevel every edge on the selected face(s). For more information, see the Bevel action documentation. Merge Faces Use the Merge Faces action to merge selected faces into a single face, and remove any dividing edges. For more information, see the Merge Faces action documentation. Conform Normals Use the Conform Normals action to set all selected face normals to the same relative direction. For more information, see the Conform Normals action documentation. Flip Face Edge Use the Flip Face Edge (Turn Edges) action to swap the triangle orientation on the selected face(s) with four sides. For more information, see the Flip Face Edge action documentation. Extrude Faces Use the Extrude Faces action to pull out the currently selected face and attach sides to each edge. For more information, see the Extrude Faces action documentation. Duplicate Faces Use the Duplicate Faces action to duplicate each selected face either as a new GameObject or in the same GameObject as a sub-Mesh. For more information, see the Duplicate Faces action documentation. Detach Faces Use the Detach Faces action to detach the selected face(s) from the rest of the Mesh. For more information, see the Detach Faces action documentation. Delete Faces Use the Delete Faces action to delete the selected face(s). For more information, see the Delete Faces action documentation. Cut Tool Use the Cut tool to create a new face on an existing Mesh. For more information, see the Cut tool documentation. Offset Faces Use the Offset Elements action in the Face edit mode to move the selected face(s) according to the settings. For more information, see the Offset Elements action documentation. Set Pivot Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected faces. For more information, see the Set Pivot action documentation. Flip Face Normals Use the Flip Face Normals action to flip the normals only on the selected face(s). For more information, see the Flip Face Normals action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/faq.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/faq.html",
    "title": "Troubleshooting | FSM Unity Framework",
    "keywords": "Troubleshooting This section covers the following issues: Faces not rendering (or appearing black) Pink Shaders Spotty Textures on Lightmapped GameObjects Missing FbxPrefab or assembly reference error Faces not rendering (black) If you are editing Mesh elements and suddenly notice that some of your faces have turned black, there are two common causes: There is a problem with auto-lightmapping. You have two edges or two vertices in a single Mesh, and they are sharing the same space. First, check the Auto Lightmap UVs setting on the Lightmap UV Editor window. Sometimes faces appear as black if auto-lightmapping hasn't rendered them yet. This usually happens if there are more rendering jobs than your resources can handle (particularly on older machines). To access the Auto Lightmap UVs setting: Open the Lightmap UV Editor window (Tools > ProBuilder > Editors > Open Lightmap UV Editor). Disable the Auto Lightmap UVs option. If you turn off the automatic lightmapping option and you still see faces that are not rendering, it may be that you have doubled vertices in your Mesh. To fix elements that are sharing the same place: Select the entire Mesh. Open the Options window for the Weld Vertices action and make sure that the Weld Distance value is very low (for example, 0.01 or 0.0001). This ensures the action doesn't weld vertices that are close but not doubled. Click the Weld Vertices button. Tip: You can enable the Show Scene Info property in the ProBuilder Preferences window while debugging. This displays information about the selected Mesh, including the total number of vertices, edges, and faces, and how many are currently selected in the top left corner of the Scene view. Pink Shaders If you are using the new Scriptable Rendering Pipeline (SRP), make sure you import the corresponding URP or HDRP Shaders needed to display vertex colors and textures correctly. For more information, see Support for scriptable render pipelines. Spotty Textures on Lightmapped GameObjects This happens when the GameObject does not have a UV2 channel. To build the UV2 channel: Select the affected GameObject(s). Run the Lightmap UVs action. Missing FbxPrefab or assembly reference error You might see one or more of these errors if you have imported the FBX Exporter package, then later removed it: <project-folder>/Addons/Fbx.cs(66,81): error CS0246: The type or namespace name `FbxNode' could not be found. Are you missing an assembly reference? <project-folder>/Addons/Fbx.cs(11,19): error CS0234: The type or namespace name `Formats' does not exist in the namespace `UnityEditor'. Are you missing an assembly reference? <project-folder>/Addons/Fbx.cs(8,7): error CS0246: The type or namespace name `Autodesk' could not be found. Are you missing an assembly reference? To resolve these errors, you can follow either of these fixes: Re-import the FBX Exporter package. Open PlayerSettings (from the top menu: Edit > Project Settings > Player) and remove the PROBUILDER_FBX_PLUGIN_ENABLED flag from Scripting Define Symbols."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/gloss.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/gloss.html",
    "title": "Glossary | FSM Unity Framework",
    "keywords": "Glossary -C- Coincident Coincident vertices share the same coordinate space, but are incident to two separate edges. -D- Degenerate triangles A degenerate triangle does not have any area. -I- Incident When an edge connects two vertices, those vertices are both incident to the edge. -M- Manifold and non-manifold geometry Manifold geometry is well-formed and could exist in the real world. Non-manifold geometry can cause errors and could de-stabilize your project. The following are examples of non-manifold geometry: Inner faces (faces enclosed inside another set of faces) Overlapping faces (occupying the same space) Faces that bisect another mesh Self-intersecting faces -Q- Quad (quadrangle) Most polygons are \"triangulated\", or divided into faces with three edges. However, ProBuilder supports four-sided faces, or \"quadrangles\" in many cases. \"Quad\" is the abbreviation for \"quadrangle\". -W- Winding order Winding order determines how a polygon's vertices are rendered (clockwise vs. counter-clockwise). A clockwise winding order is sometimes called \"right-handed\" and a counter-clockwise winding order is sometimes called \"left-handed\". In left-handed coordinate systems like Unity, where the y-axis is \"up\", counter-clockwise winding is front-facing. For right-handed systems, the rendering order is opposite . Winged edge When three or more faces meet at a boundary, the vertex where they meet is coincident with multiple faces and edges. The WingedEdge is a structure that holds information about a face, an edge, and a vertex at that boundary. It also provides a list of coincident edges so you can use the EdgeLookup to access information about them through scripting."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/hotkeys.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/hotkeys.html",
    "title": "ProBuilder keyboard shortcuts (hotkeys) | FSM Unity Framework",
    "keywords": "ProBuilder keyboard shortcuts (hotkeys) You can use keyboard shortcuts to access many of the actions that are available from the toolbar and the menu. The menu items that have associated shortcuts display the key combinations in the menu. You can also change the default shortcuts on a few items in Unity's Shortcut Manager. This page gives an overview of the default ProBuilder keyboard shortcuts. Where a command has Ctrl/Cmd as part of the keystroke, use the Control key on Windows and the Command key on macOS. Similarly, where a command has Alt/Opt as part of the keystroke, use the Alt key on Windows and either the Alt or Option key on macOS, depending on your keyboard. Note: There are a few exceptions where a shortcut uses the Control key for both Windows and macOS. These exceptions are clearly indicated on the action reference page and in the list below. Key combination: Action: Alt/Opt+Click(action button) Open options for any action on the toolbar. Alt/Opt+B Create a new face between two selected edges. Alt/Opt+C Collapse the selected vertices. Alt/Opt+E Create a new edge that connects either the selected vertices or the centers of each selected edge, depending on what editing mode you are in. Alt/Opt+G Increase (grow) the number of elements in the selection. Alt/Opt+L Select either the edge loop or face loop, depending on which edit mode is active. Alt/Opt+N Flip Face Normals. Alt/Opt+R Select either the edge ring or face ring, depending on which edit mode is active. Alt/Opt+S Subdivide the selected edges or faces. Alt/Opt+U Insert edge loop. Alt/Opt+V Weld selected vertices. Alt/Opt+X Split the selected vertex into individual vertices (one per adjacent face). Alt/Opt+# Apply a specific Material to the selected object(s) or face(s). Alt/Opt+Shift+G Reduce (shrink) the number of elements in the selection. Alt/Opt+Shift+X Toggle between displaying hidden selected elements in muted colors or not showing them at all. Alt/Opt+Shift+# Apply a vertex color to the selected object(s) or element(s). Backspace/Delete Delete the selected faces or Bezier shape points. Esc Enable the Object edit mode. G Toggle between the Object and Element (geometry) edit modes. H Cycle through Vertex, Edge, and Face edit modes. Ctrl+Click Align an adjacent face's UV coordinates to the current selection in the UV Editor window. This action is often called \"auto-stitching\". Note: You cannot remap this shortcut. It is only available while the UV Editor window is open, even though you can use it in the Scene view. Important: Unlike many other shortcut combinations involving the Ctrl key in Windows and the Cmd key in macOS, this action works with only the Ctrl key for both platforms. Ctrl+Shift+Click Copy one face's UVs to the other faces in the current selection. Note: You cannot remap this shortcut. It is only available while the UV Editor window is open, even though you can use it in the Scene view. Important: Unlike many other shortcut combinations involving the Ctrl key in Windows and the Cmd key in macOS, this action works with only the Ctrl key for both platforms. Ctrl/Cmd+Shift+Click Select all faces in a path between the current cursor position and the selected face(s). Note: You cannot remap this shortcut, and it is only available in the Scene view when the UV Editor window is closed. Ctrl/Cmd+Drag (while moving, rotating, or scaling) Snap to UV increments in the UV Editor window. Ctrl/Cmd+E Extrude edges and faces using the default options. Ctrl/Cmd+J Move the pivot to the center of the currently selected elements: - Vertices - Edges - Faces Ctrl/Cmd+K Create a new Mesh cube. Ctrl/Cmd+Shift+K Activate the Shape tool. P Toggle between the three orientation modes of the ProBuilder selection handle. Shift+Drag (while moving, rotating, or scaling) Extrude edges or faces. Shift+Hover Show tooltips when hovering over an action icon in the ProBuilder toolbar. 0 In the UV Editor, resets the canvas by centering the Texture in the UV viewer (the workspace area of the UV Editor)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/index.html",
    "title": "About ProBuilder | FSM Unity Framework",
    "keywords": "About ProBuilder You can build, edit, and texture custom geometry in Unity with the actions and tools available in the ProBuilder package. You can also use ProBuilder to help with in-scene level design, prototyping, collision Meshes, and play-testing. ProBuilder also comes with a Scripting API, so that you can write C# scripts to make your own tools and customizations. To add ProBuilder to Unity, refer to Installing ProBuilder. Some of the advanced features include: UV editing and texture mapping Applying Vertex Colors Creating parametric shapes In addition, the ProBuilder package includes a Model export feature, which you can use to tweak your levels in any 3D modeling software."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/installing.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/installing.html",
    "title": "Installing ProBuilder | FSM Unity Framework",
    "keywords": "Installing ProBuilder From version 3.0 and onwards, ProBuilder is only available from the Package Manager. To install this package, follow the instructions in the Package Manager documentation. Support for scriptable render pipelines If you are using either the Universal Render Pipeline (URP) or the High Definition Render Pipeline (HDRP), you also need to import the corresponding URP or HDRP Shaders needed to display vertex colors and textures correctly. To import these Shaders from the Samples section: Open the Package Manager (Window > Package Manager) inside the Unity Editor and select the ProBuilder package from the list of packages. Find the Samples section near the bottom of the package details. In some versions of the Unity Editor, the Package Manager collapses the Samples section by default, so you may have to expand it to see the support entries. Click the button next to the support entry that matches the render pipeline you are using. The Project view now displays the support folder containing the imported files under the Assets/Samples/ProBuilder/<version> folder of your project. Compatibility with Unity versions This version of ProBuilder is only compatible with Unity version 2019.4 and later."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/lightmap-uv.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/lightmap-uv.html",
    "title": "Lightmap UV Editor window | FSM Unity Framework",
    "keywords": "Lightmap UV Editor window The Lightmap UV Editor window contains the Auto Lightmap UVs setting. Use this to set whether or not you want ProBuilder to automatically rebuild lightmap UVs every time the Mesh changes for this Project. You can also toggle this setting on the ProBuilder Preferences window. To open this window, launch the toolbar option on the Lightmap UVs button ( ) from the ProBuilder toolbar. Tip: You can also access this window from the ProBuilder menu (Tools > ProBuilder > Editors > Open Lightmap UV Editor). This is a useful setting to enable, because it removes the process of manually building lightmap UVs. However, it can also be resource-intensive, especially for older or less powerful systems. If Auto Lightmap UVs has a significant impact on performance, it might be more efficient to disable Auto Lightmap UVs and just use the Lightmap UVs action ( ) when you need it. Tip: When one of the Meshes in any open scene is missing lightmap UVs, the Lightmap UV Settings window displays a warning message and the Build Missing Lightmap UVs button. When you click the button, ProBuilder builds the lightmap UVs and then hides the button and the message."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/links.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/links.html",
    "title": "Helpful links | FSM Unity Framework",
    "keywords": "Helpful links ProBuilder follows many of the same conventions as other 3D modeling software, so if you are an experienced 3D artist, you can read the ProBuilder toolbar section and begin using the ProBuilder tools right away. If you are new to 3D modeling, ProBuilder is a great way to learn. You can watch the tutorial videos on the Unity Youtube playlist to get started. Tutorials and documentation Support forum Twitter"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/manual-uvs-actions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/manual-uvs-actions.html",
    "title": "Actions panel: Manual UV Mode | FSM Unity Framework",
    "keywords": "Actions panel: Manual UV Mode In Manual mode, the Actions panel displays the UV Mode: Manual label and helpers for manual UV manipulation. The Manual mode provides the following functionality to help you with texture mapping: The Project UVs section provides different UV projection methods. The Selection section allows you to expand which UV elements are selected. The Edit section provides manipulation functionality. To access this panel, either click the Convert to Manual button from the Actions panel in Auto UV Mode, or click the UV Vertex or UV Edge edit mode buttons. Tip: If you get to the point where you have been making a lot of edits and something seems wrong but you're not sure how to fix it, you can clear all the edits. To do this, navigate to the UV Mode: Auto Actions panel and click the Reset UVs button. To switch back to the Actions panel in Auto UV Mode and convert all selected faces to Auto UVs, click the Convert to Auto button. Tip: Before you start to manipulate UVs manually, make sure you delete any faces that you don't need. For example, if you have a box that is sitting against the wall and doesn't move, delete the face pressed up against the wall. It is a waste of resources to render faces that aren't visible. Project UVs Select how you want ProBuilder to project the UVs: using the Planar or Box projection method. Planar Unwraps the selected face(s) using a Planar projection method. Planar projection draws the texture on an entire image as if it is projected from a single plane. That is, image travels perpendicularly from the virtual projection plane onto the surface. Box Unwraps the selected face(s) using a Box projection method. Box projection is like applying planar projection from all six planes at once. This type of projection is ideal for boxes and other 3-dimensional flat objects. Selection There are two selection helpers you can use to expand which UV elements are selected: Select Island and Select Face. Select Island With a UV element selected, click this to expand the selection so that it includes all other connected UV elements. Select Face With a Vertex or Edge selected, click to select all elements on the same face. Edit ProBuilder provides a number of manual manipulation features for working in the UV Editor: welding vertices; splitting and collapsing UVs; flipping UV elements horizontally and vertically; and resizing UV elements to match the UV space. Weld Weld collapses selected vertices together, but only if they are within a set distance. To adjust the distance modifier, click the + button on the right side of the Weld button. For example, it is good practice to use a low value, such as 0.01. Then you can select all of the UVs at the same time and use Weld to reduce duplicate UV vertices. This is an important step if you are planning on autostitching, because it requires faces to be adjacent, and duplicate edges produce undesired results. Collapse UVs Collapse UVs collapses all selected vertices to a single vertex, regardless of distance. Split UVs Split UVs breaks off the selected UV element(s) from any UV element(s), so that you can manipulate them independently. Flip Horizontal Flip Horizontal flips the selected UV element(s) in the horizontal direction. Flip Vertical Flip Vertical flips the selected UV element(s) in the vertical direction. Fit UVs Use Fit UVs to scale and move the selected UV element(s) to fit them exactly within the UV space. Autostitching Usually, to unwrap UV faces, you need to manually position each one individually and weld the UV vertices together. Autostitching performs this automatically. You can autostitch any two adjacent faces together. To do this, follow this procedure: Select a face in the Scene view. Open the UV Editor window and switch to UV Face editing mode. Select a face on the Mesh and then Ctrl+Click a face that shares an edge with the current selection. Important: Use the Ctrl key for both macOS and Windows. You can continue to Ctrl+Click one face at a time as long as it is adjacent to the selected face. Autostitching allows you to control how ProBuilder projects the Texture image across the Mesh. It is like building a UV quilt that uses planar projection. Copy UVs You can copy UVs from one face to another. For example, if you are working on a barrel and you want to copy the UVs from the top face to the bottom, follow these steps: Open the UV Editor. ProBuilder can only perform UV editing tasks when the UV Editor is open. In the Scene view, select the face you want to copy from. Ctrl+Shift+Click on the face you want to copy to. Important: Use the Ctrl key for both macOS and Windows. You can continue to Ctrl+Shift+Click each face you want to copy to."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/material-tools.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/material-tools.html",
    "title": "Material Editor window | FSM Unity Framework",
    "keywords": "Material Editor window Use the Material Editor to apply Materials to objects or faces. (A) Use the Quick Material section to set up a Material to apply with a button or a shortcut. (B) Use the Material Palette section to designate several Materials to use with buttons or shortcuts. You can also save and load new Palettes, and add more slots to use for Materials. To open this window, navigate to the ProBuilder toolbar and click the Material Editor button ( ). Tip: You can also access this window from the ProBuilder menu (Tools > ProBuilder > Editors > Open Material Editor). For an overview of working with Materials and texture mapping, see Materials, Shaders, Textures, and UVs. Quick Material The Quick Material section allows you to specify a Material to apply with either the shortcut combination or the Apply button. (A) You can set a reference to the Material to use as the \"quick\" Material. (B) ProBuilder shows a preview when you set a quick Material. Note: If you are working in Windows, this preview might not appear. (C) With a quick Material defined, any time you click Apply, ProBuilder applies the quick Material to the selected faces. Alternatively, you can hold down Ctrl/Cmd+Shift and click on a face to apply the quick Material to the clicked face. (D) Click Match Selection to change the quick Material to match the Material that is already on the selected face. Setting a quick Material To set a quick Material, set a reference to a Material Asset you want to use as the quick Material. You can drag in the Asset from the Project view or use the object picker next to the reference property. If there's a face on a GameObject in the scene that is already using the Material you want to set as quick, select that face and click the Match Selection button. Applying the quick Material To apply the quick Material: Select the object or face(s) you want to apply the quick Material to. Click the Apply button or use the Ctrl/Cmd+Shift+Click shortcut combination. Material Palette A Material Palette is a convenient way to keep several Materials ready to apply, either with the Material Editor or with shortcut combinations. You can set up to 10 different Materials on the Palette, or expand the slots in your Palette if you need more. You can also save your Palette as a custom Palette and load it in other Scenes or sessions to use it again. (A) Create a new Material Palette or select a saved Palette to load from the Palette drop-down menu. (B) Set a reference to the Material Palette you want to load. (C) Use one of the buttons to apply the associated Material to the selected face(s). Alternatively, you can use the shortcut displayed on the button (Alt/Opt+<number>) or select the preset from the ProBuilder menu (Tools > ProBuilder > Materials > Apply Material Preset <number>). (D) Set a reference to the Material you want to use in this slot of the Material Palette. (E) To delete an extra slot, click the red square on the right edge of the button. (F) Click Add to add a new slot to the Material Palette. Defining Materials on a Palette To set Materials on your Palette, set a reference to a Material Asset you want to use in a specific Material slot. You can drag in the Asset from the Project view or use the object picker next to the reference property. Applying Material from the Palette To apply a specific Material from the Palette: Select the object or face(s) you want to apply the Material to. Click the button to the left of the Material you want to apply. For example, if you want to apply the Material in the third slot, click the button that displays Alt + 3. Tip: You can find the number of the Material preset on each slot's button. Then you can use that number to apply its Material directly using either of these methods: Use the Alt/Opt+# shortcut. Select the Material preset from the ProBuilder menu (Tools > ProBuilder > Materials > Apply Material Preset #). Defining your own Material slots in your Palette By default, the Material Palette provides 10 slots (numbered 1 to 0) for your Materials. However, if you need to add more slots to your Palette, you can add any number of extra slots using this procedure: Click the Add button at the bottom of the Material Editor window. A new slot appears with an Apply button on the left and the Material picker on the right. Pick your new Material to use in the new slot. Note: You can't set up new shortcuts for the new slots but you can click their buttons in the Material Editor to apply them. To remove any extra slots from your Palette: Click the red square on the right side of the Apply button. Saving a custom Palette To save the current Palette to an Asset file: In the Material Palette drop-down menu, select New Material Palette. A new file with the default name of Material Palette.asset appears under the Assets folder in the Project view. (Optional) Change the filename to something that helps you identify it for future use. Warning: As soon as you make a change to your Material Palette, ProBuilder automatically saves it to the Palette. Loading a custom Palette All Material Palette Asset files in the current Project appear as menu items in the Material Palette drop-down menu in the Material Editor window. To load a saved Material Palette from file: Select the saved palette by name from the Material Palette drop-down menu."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-actions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-actions.html",
    "title": "Actions | FSM Unity Framework",
    "keywords": "Actions Use this sub-menu to access helper functions for stripping out ProBuilder scripts. Strip All ProBuilder Scripts in Scene Removes all ProBuilder scripts from all GameObjects in this scene, and only leaves the models. Strip ProBuilder Scripts in Selection Removes all ProBuilder scripts from selected GameObjects, and only leaves the models."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-debug.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-debug.html",
    "title": "Debug | FSM Unity Framework",
    "keywords": "Debug Use this sub-menu to access the Log Preferences window. Use the Log Preferences window to customize how ProBuilder logs messages. The darker color indicates that the setting is enabled (ON); the lighter color indicates that the setting is disabled (OFF). (A) Log Output defines where ProBuilder writes messages to. (B) Chatty-ness defines which kind of messages ProBuilder logs. Log Output Log Output defines where ProBuilder writes messages to: the Unity Console or to a log File: Click the Console button to enable writing to the Console. Click the File button to enable writing to a file. When the File output target is enabled, the Log Path widget and the open button are enabled. To set the Log Path, click the ... button and select a folder for the log file. This widget is only available if the File button is ON. Click the open button to open the saved ProBuilderLog.txt log file. This button is only available if the File button is ON. By default, ProBuilder writes messages only to the Console. Chatty-ness The Chatty-ness section defines which kind of messages ProBuilder logs: Toggle whether or not ProBuilder logs messages about the following: Errors, Warnings, General information, Debug messages, or everything. By default, ProBuilder logs everything, and these buttons are all ON. Click the Clear Log File button to reset the saved log file. This deletes all messages that ProBuilder previously logged."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-dimover.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-dimover.html",
    "title": "Dimensions Overlay | FSM Unity Framework",
    "keywords": "Dimensions Overlay Show or hide the dimensions for all three axes. This overlay appears on all Mesh objects, not just ProBuilder Meshes."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-editors.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-editors.html",
    "title": "Editors | FSM Unity Framework",
    "keywords": "Editors This sub-menu provides access to tools and editor windows. New Bezier Shape Use the Bezier Shape tool to define a Bezier curve around which ProBuilder extrudes a Mesh. Warning: This tool is experimental, meaning that it is still under development, and might reduce ProBuilder's stability. Use with caution. For more information, see the Bezier Shape tool documentation. New Poly Shape Use the Poly Shape tool to create custom ProBuilder Mesh shapes. For more information, see the Poly Shape tool documentation. Open Lightmap UV Editor Use the Lightmap UV Editor window to access the settings for generating light map UVs. For more information, see the Lightmap UV Editor window documentation. Open Material Editor Use the Material Editor window to apply materials to objects or faces. For more information, see the Material Editor window documentation. New Shape Use the New Shape tool to create new ProBuilder Mesh shapes such as cylinders, arches, and stairs. For more information, see the Shape tool documentation. Open Smoothing Editor Use the Smooth Group Editor window to create a smooth and rounded look, or sharp and hard cornered. For more information, see the Smooth Group Editor window documentation. Open UV Editor Use the UV Editor window to apply textures to objects or faces. You can also use it to automatically or manually wrap and unwrap textures. For more information, see the UV Editor window documentation. Open Vertex Color Editor Use the Vertex Colors window to apply or paint vertex colors onto Meshes. For more information, see the Vertex Colors window documentation. Open Vertex Position Editor Use the Positions Editor window to enter specific translation coordinates to modify vertex positions. For more information, see the Positions Editor window documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-experimental.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-experimental.html",
    "title": "Experimental | FSM Unity Framework",
    "keywords": "Experimental This sub-menu provides access to experimental features. Note: This sub-menu is only available when you enable the experimental features preference. Experimental features aren’t ready for public use, but are included for users to try out early, and report issues/feedback. Boolean (CSG) Tool The Boolean feature is the only option available on this sub-menu. It lets you create new Meshes from intersection, union, and subtraction boolean operations."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-export.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-export.html",
    "title": "Export | FSM Unity Framework",
    "keywords": "Export Use this sub-menu to export selected ProBuilder Meshes in various formats. Tip: When you perform an export using one of these menu items, Unity uses the default export options for the specific format. If you want to view or change the default export options, Alt/Opt+Click over the Export button on the ProBuilder toolbar. Export Asset Saves the selection as Unity Mesh .asset files. This format is only readable in Unity. Export Obj Exports the selected object(s) as .obj files (Wavefront OBJ format). This is a widely supported model format. It supports multiple Textures and Mesh groups. Export Ply Exports the selected object(s) as .ply files (Stanford PLY, or Polygon File Format). This format is generally supported and very extensible. It supports quads and vertex colors, but not multiple materials. Export Stl Ascii Exports the selected object(s) as ASCII .stl files (stereolithography, standard tessellation, or standard triangle format). This is a widely supported format, generally used in CAD software or 3D printing. It only supports Triangle geometry. Export Stl Binary Exports the selected object(s) as Binary .stl files (stereolithography, standard tessellation, or standard triangle format). This is a widely supported format, generally used in CAD software or 3D printing. It only supports Triangle geometry."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-geometry.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-geometry.html",
    "title": "Geometry | FSM Unity Framework",
    "keywords": "Geometry This sub-menu provides access to Vertex, Edge, and Face edit mode actions. Bevel Edges Use the Bevel action to bevel every edge on the selected face(s). For more information, see the Bevel action documentation. Bridge Edges Use the Bridge Edges action to create a new face between two selected edges. For more information, see the Bridge Edges action documentation. Collapse Vertices Use the Collapse Vertices action to colapse all selected vertices to a single point, regardless of distance. For more information, see the Collapse Vertices action documentation. Conform Face Normals Use the Conform Normals action to set all selected face normals to the same relative direction. For more information, see the Conform Normals action documentation. Delete Faces Use the Delete Faces action to delete the selected face(s). For more information, see the Delete Faces action documentation. Detach Faces Use the Detach Faces action to detach the selected face(s) from the rest of the Mesh. For more information, see the Detach Faces action documentation. Duplicate Faces Use the Duplicate Faces action to copy each selected face and move it to either a new Mesh or leave it as a sub-Mesh. For more information, see the Duplicate Faces action documentation. Extrude In Edge edit mode, use the Extrude Edges action to push a new edge out from each selected edge. In Face edit mode, use the Extrude Faces action to pull out the currently selected face and attach sides to each edge. For more information, see the documentation for the Extrude Edges and the Extrude Faces actions. Fill Hole In Vertex and Edge edit modes, use the Fill Hole action to create a new face that fills any holes that touch the selected vertices or edges. For more information, see the documentation for the Fill Hole (vertices) and Fill Hole (edges) actions. Flip Face Edge Use the Flip Face Edge (Turn Edges) action to swap the triangle orientation on the selected face(s) with four sides. For more information, see the Flip Face Edge action documentation. Flip Face Normals Use the Flip Face Normals action to flip the normals only on the selected face(s). For more information, see the Flip Face Normals action documentation. Insert Edge Loop Use the Insert Edge Loop action to add a new edge loop from the selected edge(s). For more information, see the Insert Edge Loop action documentation. Merge Faces Use the Merge Faces action to merge selected faces into a single face, and remove any dividing edges. For more information, see the Merge Faces action documentation. Offset Elements In Vertex edit mode, use the Offset Elements action to move the selected vertex or vertices. In Edge edit mode, use the Offset Elements action to move the selected edge(s). In Face edit mode, use the Offset Elements action to move the selected face(s). For more information, see the Offset Elements action documentation. Set Pivot To Selection Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected faces. For more information, see the Set Pivot action documentation. Smart Connect In Vertex edit mode, use the Connect Vertices action to create a new edge connecting the selected vertices. In Edge edit mode, use the Connect Edges action to insert an edge connecting the centers of each selected edge. For more information, see the documentation for the Connect Vertices and Connect Edges actions. Smart Subdivide In Edge edit mode, use the Subdivide Edges action to divide the selected edge(s) into multiple edges. In Face edit mode, use the Subdivide Faces action to add a vertex at the center of each edge and connect them in the center. For more information, see the documentation for the Subdivide Edges and Subdivide Faces actions. Split Vertices Use the Split Vertices action to split a single vertex into multiple vertices (one per adjacent face). For more information, see the Split Vertices action documentation. Triangulate Faces Use the Triangulate Faces action to reduce selected faces to their base triangles. For more information, see the Triangulate Faces action documentation. Weld Vertices Use the Weld Vertices action to merge selected vertices within a specific distance of one another. For more information, see the Weld Vertices action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-interaction.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-interaction.html",
    "title": "Interaction | FSM Unity Framework",
    "keywords": "Interaction This sub-menu provides access to selection interaction actions. Toggle Drag Rect Mode Use this to set the Rect action to Complete. This means that drag selection only selects elements that are fully inside the drag rectangle. For more information, see the Rect action documentation. Toggle Drag Selection Mode Use this to set the Rect action to Intersect. This means that drag selection selects any elements that intersect with the drag rectangle. For more information, see the Rect action documentation. Toggle Handle Orientation Use this to toggle between the three orientation states for Scene handles (Global, Local, or Normal). For more information, see the Orientation action documentation. Toggle Select Back Faces Use the Select Hidden action to define whether drag selection selects or ignores hidden elements. For more information, see the Select Hidden action documentation. Toggle X Ray Use this to switch between showing and hiding any selected hidden geometry. This menu item provides access to the Selection X-Ray preference."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-materials.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-materials.html",
    "title": "Materials | FSM Unity Framework",
    "keywords": "Materials Use this sub-menu to apply specific Material presets to the selection. Select the specific Material preset defined on the Material Editor window to set the associated Material on the selected object(s) or element(s)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-object.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-object.html",
    "title": "Object | FSM Unity Framework",
    "keywords": "Object This sub-menu provides access to Object edit mode actions. Center Pivot Use the Center Pivot action to move the pivot point for the Mesh to the center of the object’s bounds. For more information, see the Center Pivot action documentation. Conform Object Normals Use the Conform Normals action to set all face normals to the same relative direction. For more information, see the Conform Normals action documentation. Flip Object Normals Use the Flip Normals action to flip the normals of all faces on the selected object(s). For more information, see the Flip Normals action documentation. Freeze Transform Use the Freeze Transform action to set the selected object's position, rotation, and scale to world-relative origin. For more information, see the Freeze Transform action documentation. Merge Objects Use the Merge Objects action to merge two or more selected ProBuilder GameObjects. For more information, see the Merge Objects action documentation. Mirror Objects Use the Mirror Objects action to create mirrored copies of objects. For more information, see the Mirror Objects action documentation. Pro Builderize Use the ProBuilderize action to convert the selected object(s) into objects you can edit in ProBuilder. For more information, see the ProBuilderize action documentation. Set Collider Use the Set Collider action to assign the Collider Behaviour script to selected objects. For more information, see the Set Collider action documentation. Set Trigger Use the Set Trigger action to assign the Trigger Behaviour script to selected objects. For more information, see the Set Trigger action documentation. Subdivide Object Use the Subdivide Object action to divide every face on selected objects. For more information, see the Subdivide Object action documentation. Triangulate Object Use the Triangulate Object action to reduce all polygons to their base triangles. For more information, see the Triangulate action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-open.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-open.html",
    "title": "ProBuilder Window | FSM Unity Framework",
    "keywords": "ProBuilder Window This opens the ProBuilder toolbar and Edit mode toolbar together. By default, the Edit mode toolbar appears in the upper center of your Unity workspace, and the ProBuilder toolbar appears on the left side. See Customizing ProBuilder for information on how to customize the location and appearance of these toolbars and other ProBuilder windows."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-repair.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-repair.html",
    "title": "Repair | FSM Unity Framework",
    "keywords": "Repair Use this sub-menu to access several helper scripts that repair problems with ProBuilder Meshes in the scene. Rebuild All ProBuilder Objects Rebuilds Mesh representations from stored ProBuilder data for each GameObject in the scene. If you have a lot of GameObjects in a scene, this can take a while. Rebuild Shared Indexes Cache Discards all shared vertex position data and rebuilds based on proximity. Remove Degenerate Triangles Deletes triangles on a Mesh that are either taking up no space, or are duplicates. Check for Broken ProBuilder References Checks for and repairs any missing or broken ProBuilder references in the scene."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-selection.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-selection.html",
    "title": "Selection | FSM Unity Framework",
    "keywords": "Selection This sub-menu provides access to selection modifiers and actions. Grow Selection Use the Grow Selection action to expand the selection outward to adjacent faces, edges, or vertices. For more information, see the Grow Selection action documentation. Select Hole Use the Select Holes action to select all elements along the selected open vertex or edge. For more information, see the Select Holes action documentation. Select Loop In Edge editing mode, use the Select Edge Loop action to select an edge loop from each selected edge. In Face editing mode, use the Select Face Loop action to select a face loop from each selected face. Select Material Use the Select by Material action to select all faces which have the same Material as the selected face(s). For more information, see the Select by Material action documentation. Select Ring In Edge editing mode, use the Select Edge Ring action to select a ring from each selected edge. In Face editing mode, use the Select Face Ring action to select a face ring from each selected face. Select Smoothing Group Use the Select Smoothing Group action to select all faces which belong to the currently selected smoothing group. For more information, see the Select Smoothing Group action documentation. Select Vertex Color Use the Select by Colors action to select all faces on this object which have the same vertex color as the selected face. For more information, see the Select by Colors action documentation. Shrink Selection Use the Shrink Selection action to remove the elements on the perimeter of the current selection (Grow Selection in reverse). For more information, see the Shrink Selection action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-vertexcolors.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu-vertexcolors.html",
    "title": "Vertex Colors | FSM Unity Framework",
    "keywords": "Vertex Colors Use this sub-menu to apply specific Vertex Color presets to the selection. Select the specific vertex color preset defined on the Vertex Colors window to set the associated color on the selected object(s) or element(s)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/menu.html",
    "title": "The ProBuilder menu | FSM Unity Framework",
    "keywords": "The ProBuilder menu Use the ProBuilder menu to access most of the ProBuilder editors, actions, tools, and a few special features that are only available through this menu. For example, you can repair and debug ProBuilder, or access the experimental Boolean operations. The ProBuilder menu items include the following: ProBuilder Window Editors Dimensions Overlay Selection Interaction Object Geometry Materials Vertex Colors Experimental (only available when you enable the experimental features preference) Repair Export Debug Actions"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/modes.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/modes.html",
    "title": "Edit modes (Object vs Element) | FSM Unity Framework",
    "keywords": "Edit modes (Object vs Element) ProBuilder uses Edit modes to define what you are selecting and editing. The Object mode is the standard Unity edit mode: when you make a selection in Object mode, you are selecting the entire Mesh. The other three modes are collectively called the Element modes. These allow you to select and modify the individual elements of the geometry that make up a Mesh: Vertices, Edges, and Faces. A Vertex is a point where two or more angles meet (for example, on a cube, it's the corner point of each cube). An Edge is made up of two Vertices. A Face is composed of three or more Edges. An Object is the sum of all of these parts. To change Edit modes, click one of the mode buttons on the Edit mode toolbar, or use a mode shortcut key. Selecting and manipulating Use the ProBuilder Edit modes to access the individual elements of your Mesh, or the Mesh as a whole. Click the button that matches the object or element mode you'd like to edit in from the Edit mode toolbar. Icon Mode Description Select objects, modify the normals and the pivot, and merge objects together. For a complete list of actions you can perform in this mode, see Object actions. Select vertices and perform detailed editing such as vertex splitting and connecting. For a complete list of actions you can perform in this mode, see Vertex actions. Select edges and perform semi-complex geometry editing, and edge loop modeling techniques. For a complete list of actions you can perform in this mode, see Edge actions. Select faces on an object to perform basic tasks like moving, extruding, or even deleting them. For a complete list of actions you can perform in this mode, see Face actions. Click or drag to select the element you want to use. With the object(s) or element(s) selected, you can: Click one of the enabled buttons on the ProBuilder toolbar to apply an action to the selection or activate a tool. Select one of the enabled items on the ProBuilder menu to apply an action to the selection or activate a tool. Use one of the ProBuilder shortcuts to run one of the ProBuilder actions on the selection. Use any of the standard Unity Transform controls to move, rotate, or scale the selection. Shift+Drag while using any of the standard Unity Transform controls to extrude or inset the element(s)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/object-actions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/object-actions.html",
    "title": "Object actions | FSM Unity Framework",
    "keywords": "Object actions This section of the ProBuilder toolbar provides access to ProBuilder editing tools that you can use in the Object edit mode. Note: Some actions also have extra options or custom settings available. These action buttons have a special indicator in the top right corner. The documentation for each action includes information about these options. Conform Normals Use the Conform Normals action to set all face normals to the same relative direction. For more information, see the Conform Normals action documentation. Export Use the Export action to export the selected ProBuilder object(s) to a 3D Model file. For more information, see the Export action documentation. Lightmap UVs Use the Lightmap UVs action to generate any missing lightmap UVs for Meshes. For more information, see the Lightmap UVs action documentation. Triangulate Use the Triangulate action to reduce all polygons to their base triangles. For more information, see the Triangulate action documentation. Center Pivot Use the Center Pivot action to move the pivot point for the Mesh to the center of the object’s bounds. For more information, see the Center Pivot action documentation. ProBuilderize Use the ProBuilderize action to convert the selected object(s) into objects you can edit in ProBuilder. For more information, see the ProBuilderize action documentation. Subdivide Object Use the Subdivide Object action to divide every face on selected objects. For more information, see the Subdivide Object action documentation. Flip Normals Use the Flip Normals action to flip the normals of all faces on the selected object(s). For more information, see the Flip Normals action documentation. Mirror Objects Use the Mirror Objects action to create mirrored copies of objects. For more information, see the Mirror Objects action documentation. Merge Objects Use the Merge Objects action to merge two or more selected ProBuilder Meshes. For more information, see the Merge Objects action documentation. Freeze Transform Use the Freeze Transform action to set the selected object's position, rotation, and scale to world-relative origin. For more information, see the Freeze Transform action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/overview-ui.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/overview-ui.html",
    "title": "Interacting with ProBuilder | FSM Unity Framework",
    "keywords": "Interacting with ProBuilder ProBuilder provides several ways to interact with its tools and actions: (A) The Scene Information (B) The ProBuilder menu (C) ProBuilder shortcuts (D) Editor windows (E) The ProBuilder Preferences window (F) Transform tools for ProBuilder (G) ProBuilder component windows (H) The ProBuilder toolbar and the Edit mode toolbar (I) Options for ProBuilder actions (J) ProBuilder's custom Tool panel Scene Information Scene Information displays information about the Meshes in the scene, and which elements are selected. To toggle this on or off, use the Show Scene Info setting in the Preferences. Editor windows Editor windows provide features with extended functionality. For example, the UV Editor window (in the example image above) allows you to perform advanced texture manipulations, including texture mapping, UV unwrapping, and tiling. To access these windows, use the Probuilder menu, shortcuts, or the tool section of the ProBuilder toolbar. Component sections in the Inspector Most of these component sections appear in the Inspector when you create a Mesh with one of the creation tools: Shape Poly Shape Bezier shape (Experimental) When you first activate a creation tool, Unity adds these components to the new GameObject. They expose specific properties defined in the corresponding scripts which help define the topology. After you create the new Mesh, you can re-activate the tool for the same Mesh and change these properties to modify the Mesh's shape. Note: When you re-activate one of these tools, you lose any modifications you made to the Mesh through an action or through the Cut tool. For example, imagine you create a new Poly Shape with five points, and then extrude one of the faces. Next, you decide to remove one of the points, so you enter Poly Shape editing mode again. The extrusion disappears as soon as you re-enter Poly Shape editing mode. In addition to these creation tool components, every ProBuilder object has a ProBuilder MeshFilter component. It lets you customize lightmap UV parameters for each object. When you ProBuilderize a Mesh that was either exported into Unity or created as a regular primitive Unity Mesh, Unity adds this component to the Mesh. Transform tools in ProBuilder Most of the time, you interact with ProBuilder with translation, rotation, and scaling tools in much the same way that you interact with Unity. However, ProBuilder uses a combination of Edit modes and special key combinations to interact at a much deeper level with your Meshes. For example, you can use the Shift key with the scaling and translation tools in Face mode to create insets and extrusions. This allows you to build complex Meshes easily. For an overview of working with ProBuilder, see Creating Meshes, Editing Meshes, and Materials, Shaders, Textures, and UVs."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/overview.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/overview.html",
    "title": "Getting started | FSM Unity Framework",
    "keywords": "Getting started You can access all of ProBuilder's editing tools and actions from the ProBuilder toolbar, which dynamically adapts to your Edit mode and your current selection. Every toolbar button has a tooltip that shows a short summary about its use. Viewing tooltips is a great way to start learning about ProBuilder's functionality. Building and editing complex Meshes ProBuilder Meshes act like regular GameObjects in Unity. You can apply Transform values to them and add components to them. They respond to the physics system, and you can animate them and attach scripts to them in order to control them. However, standard Unity Meshes are not the same as ProBuilder Meshes: you can't edit them with ProBuilder actions until you convert them into ProBuilder objects. The most common way to build a ProBuilder Mesh is to create it and edit it entirely with ProBuilder tools and actions. Applying Materials and Vertex Colors You can apply any Material to ProBuilder Meshes using the Material Palette, either on selected faces in Element mode or across the entire GameObject. You can also apply Vertex Colors to your Mesh while you are still building the geometry. This can help easily identify parts of complex Meshes, such as the floor, or provide a little color while you are greyboxing. Editing UVs ProBuilder provides automatic UV unwrapping and a complete manual UV editor. Auto UV mode lets you tweak basics like offset, tiling, and rotation, while ProBuilder handles the complex UV work automatically. Manual UV mode gives you complete control of the UVs. With this workflow you can lay out your UV maps precisely. You can use whichever you prefer, or a mix of both, even on the same Mesh. For more information, see Materials, Shaders, Textures, and UVs Exporting to other formats You can also export your ProBuilder Meshes to several file formats, such as .asset, .obj, and .stl. Then you can reimport them into Unity and use them as Prefabs or spawn them. However, as soon as you convert a ProBuilder Mesh to the .asset format, it becomes a regular Unity GameObject, and you can't modify it with ProBuilder tools any more."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/polyshape.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/polyshape.html",
    "title": "Poly Shape tool | FSM Unity Framework",
    "keywords": "Poly Shape tool Use the Poly Shape tool to create a custom polygonal shape. When you activate the Poly Shape tool to create a new shape, the Poly Shape Tool panel appears in the bottom of the Scene view. After you initially create a Poly Shape, the Poly Shape is still active, but in editing mode: (A) You can toggle the Edit Poly Shape button on the Tools panel to toggle the Poly Shape editing mode on and off. (B) You can modify the Extrusion (height of the Mesh) and the Flip Normals properties on the Poly Shape component. (C) The Poly Shape Tool panel lets you the modify the Extrusion and the Flip Normals properties too. The Quit Editing button exits the Poly Shape editing mode. While the Poly Shape editing mode is active, you can also modify the base shape by adding, deleting, or moving any of the points that define the Poly Shape. Note: If you are in editing mode immediately after creating a new Poly Shape Mesh, you can also click New Poly Shape () on the ProBuilder toolbar to exit the Poly Shape tool. Editing a Poly Shape If you exited the Poly Shape tool, you can re-activate it to modify the Extrusion and the Flip Normals properties. To re-activate the Poly Shape editing mode: Select the Poly Shape you want to modify. The Tools panel displays the Edit Poly Shape button and the Poly Shape component appears in the Inspector with the Edit Poly Shape button. Click either button to activate the Poly Shape editing mode. Note: You can modify Mesh elements on the Poly Shape Mesh with the standard ProBuilder editing tools. However, each time you re-enter Poly Shape editing mode, you lose any element changes. To modify the shape in Poly Shape editing mode, perform the following tasks in the Scene view: Click and drag existing control points to move them around. Click existing control points to select them, then use Backspace (Windows) or Delete (macOS) to remove the points from the shape. Click along the perimeter line to add new control points. Click and drag the handle in the center of the Mesh to set the height. You can also use the controls in the Poly Shape component in the Inspector to: Enter a value to use for the height of the Mesh in the Extrusion property. Enable or disable the Flip Normals option to toggle whether the Camera displays the interior or exterior of the Mesh."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/preferences.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/preferences.html",
    "title": "ProBuilder Preferences window | FSM Unity Framework",
    "keywords": "ProBuilder Preferences window To define how you interact with ProBuilder, you can customize the user interface and how the tools work. ProBuilder provides the following preference sections, which you can change to suit your needs: Dimensions Overlay Experimental General Graphics Mesh Editing Mesh Settings Snap Settings Toolbar UV Editor Dimensions Overlay Use this setting to customize the Dimensions Overlay. By default, the overlay shows the dimensions of the selected Mesh elements only, but you can use the Bounds Display property to change this to show the dimensions of the select GameObjects, regardless of what elements (vertices, edges, or faces) are selected. Property: Description: Object Select this option to use the selected object(s) to calculate the dimensions of the world space bounds for the information that appears on the overlay. Element Select this option to use the selected element(s) to calculate the dimensions of the world space bounds for the information that appears on the overlay. This is the default. Experimental Use these settings to enable and disable ProBuilder experimental features. Warning: Experimental features are untested and might break your Project. Property: Description: Experimental Features Enabled Enable this option to access the New Bezier Shape experimental feature in the ProBuilder toolbar, and the Store Mesh as Asset option. Note: This setting has no affect on access to the Boolean (CSG) Tool, which is always available from the Experimental menu. Meshes Are Assets Enable this option to store Mesh information in the Project instead of in each separate Scene level. Note: When you toggle Experimental Features on or off, Unity has to recompile scripts because the changes add or remove functionality in Unity. This means that there is a delay before this option appears to change. General Use these properties to set some basic options for ProBuilder. Property: Description: Show Action Notifications Enable this option if you want ProBuilder to notify you when performing actions. Auto Lightmap UVs Enable this option to generate the UV2 channel after every geometry edit. This means you don't have to manually generate them every time the Mesh changes. UV2 channel generation for Meshes is necessary for lighting, but can be time-consuming. If you are editing objects with large numbers of vertices, disable this to save resources. Show Missing Lightmap UVs Warning Enable this option to show a warning in the console if ProBuilder shapes are missing a valid UV2 channel when Unity performs a lightmap bake. Show Handle Info Enable this option to display the information for moving, rotating, and scaling deltas in the bottom right of the Scene view. Note: If you have the Component Editor Tools panel open in the Scene view, it covers this information. Close the panel to display the information. Show Scene Info Enable this option to display the Mesh information overlay in the top left of the Scene view. These details include overall face, vertex and triangle counts, and the number of elements currently selected: Script Stripping Enable this option to automatically remove the extra data ProBuilder stores in a Scene. This includes all ProBuilder scripts, so if you are using the runtime API you should disable this feature. Graphics Use these settings to customize the size and color of Mesh elements. By default, the Use Unity Colors option is enabled. However, you can disable this option to set custom colors for a number of elements. Property: Description: Show Hover Highlight Enable this option to highlight the closest Mesh elements when your cursor moves towards them. Tip: You can also set the color to use for highlighting with the Preselection property. Selection X-Ray Enable this option to display any selected hidden geometry. Use Unity Colors Enable this property to use the standard Unity Color preferences. By default, this property is enabled. When you disable this option, a number of properties appear below. These allow you to specify your own colors to use instead of the Unity colors. For example, you can specify different colors for selected and unselected faces, edges, and vertices. Dither Face Overlay Enable this option to use dithering (dotted overlay) when you hover over or select items. If you disable this option, the overlay appears solid instead. This property is only available when Use Unity Colors is disabled. Wireframe Pick the color ProBuilder uses to display the Mesh's wireframe. This property is only available when Use Unity Colors is disabled. Preselection Pick the color ProBuilder uses to highlight the closest Mesh element. The Show Preselection Highlight property must be enabled in order to display highlights. This property is only available when Use Unity Colors is disabled. Selected Face Color Pick the color ProBuilder uses to display the selected face(s) in a ProBuilder Mesh. This property is only available when Use Unity Colors is disabled. Unselected Edge Color Pick the color ProBuilder uses to display the unselected edges in a ProBuilder Mesh. This property is only available when Use Unity Colors is disabled. Selected Edge Color Pick the color ProBuilder uses to display the selected edge(s) in a ProBuilder Mesh. This property is only available when Use Unity Colors is disabled. Unselected Vertex Color Pick the color ProBuilder uses to display the unselected vertices in a ProBuilder Mesh. This property is only available when Use Unity Colors is disabled. Selected Vertex Color Pick the color ProBuilder uses to display the selected vertex (or vertices) in a ProBuilder Mesh. This property is only available when Use Unity Colors is disabled. Vertex Size Set the size to render the vertex points on ProBuilder Meshes in the Scene view. Line Size Set the size to render the edges on ProBuilder Meshes in the Scene view. Note: On macOS, this property is only available if you use OpenGL instead of Metal. Wireframe Size Set the size to render the ProBuilder Mesh wireframe in the Scene view. Note: On macOS, this property is only available if you use OpenGL instead of Metal. Mesh Editing Use these settings to customize interacting with Meshes. Property: Description: Auto Resize Colliders Enable this option to automatically resize colliders according to Mesh bounds as you edit. Allow non-manifold actions Enable this option if you want to edit your Meshes with advanced techniques, such as bridging closed edges. Note that these complex actions can break your project unless you are familiar with their concepts and how to apply them. By default, this option is disabled. Mesh Settings Use these settings to establish default behavior for some ProBuilder options. Property: Description: Material Set a reference to the default Material you want to use for ProBuilder Meshes. By default, ProBuilder uses the ProBuilderDefault Material when creating new Meshes. Static Editor Flags Choose one of the Unity Static Settings as the default for new ProBuilder Meshes. The default value is Nothing. Mesh Collider is Convex Enable this option to set the default convex collider state for new ProBuilder objects. Pivot Location Choose the default pivot location for new ProBuilder objects. First Corner Use the \"first corner\" as the pivot point for the newly created Mesh. The first corner refers to where you first clicked in the Scene view to create it. Center Use the center of the newly instantiated object as the pivot point. Snap New Shape To Grid Enable this option to snap a newly instantiated object to the nearest grid point (as determined by ProGrids). Shadow Casting Mode Choose how new ProBuilder Meshes cast shadows. The default value is Two Sided. See the Cast Shadows property on the Mesh Renderer component for more information on this setting. Collider Type Set the default type of collision primitive to use for new ProBuilder objects. The default is Mesh Collider. None Do not use a collider. Box Collider Use a basic cube for the collider. Mesh Collider Use a custom shape collider to match the newly created Mesh. This is the default. Lightmap UVs Settings Set defaults for the standard Lightmap UVs parameters. To return to the default settings, click the Reset button. Snap Settings Use these properties to customize how snapping behaves with ProBuilder. Property: Description: Snap As Group Enable this option if you want each selected item to keep the same relative position to each other while snapping. This is the default. Disable this option to snap each selected item to the grid independently. Snap Axis Choose how vertices snap to the grid while moving. Active Axis Vertices snap only along the currently active axis. This is the default. All Axes Vertices snap to all axes simultaneously. Toolbar Use these properties to set default behavior for the ProBuilder toolbar. Property: Description: Shift Key Tooltips Enable this option to only show tooltips when the mouse cursor is hovering over a button and you are holding down Shift. By default, tooltips appear when the mouse cursor hovers over a button for more than a second. Icon GUI Enable this option to use toolbar buttons that display icons only. Disable this option to use toolbar buttons that display text only. Note: You can also use the context menu to switch between icons and text. Toolbar Location Choose the location where you want the Edit Mode toolbar to appear in the Scene view. Possible locations are: - Upper Center - Upper Left - Upper Right - Bottom Center - Bottom Left - Bottom Right UV Editor Use this setting to customize the UV Editor window. Property: Description: Grid Size Size of the grid in the UV Editor, for visual and functional purposes."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_actions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_actions.html",
    "title": "Action reference | FSM Unity Framework",
    "keywords": "Action reference This table lists all the actions available in ProBuilder, and indicates which Edit modes support it: Icon Text Bevel (Edges) x Bevel (Faces) x Bridge Edges x Center Pivot x Collapse Vertices x Conform Normals (Faces) x Conform Normals (Objects) x Connect Edges x Connect Vertices x Delete Faces x Detach Faces x Duplicate Faces x Export x Extrude Edges x Extrude Faces x Fill Hole (Edges) x Fill Hole (Vertices) x Flip Face Edge x Flip Face Normals x Flip Normals x Freeze Transform x Handle (see Orientation) x x x x Lightmap UVs x Grow Selection x x x Insert Edge Loop Inset x Merge Faces x Merge Objects x Mirror Objects x Offset Elements x x x Orientation x x x x ProBuilderize x Rect x x Select by Colors x x x Select by Material x Select Edge Loop x Select Edge Ring x Select Face Loop x Select Face Ring x Select Hidden x x x x Select Holes x x Select Path x Set Collider x Set Pivot (Edges) x Set Pivot (Faces) x Set Pivot (Vertices) x Set Trigger x Shift x x x Shrink Selection x x x Split Vertices x Subdivide Edges x Subdivide Faces x Subdivide Object x Triangulate Faces x Triangulate (Object) x Turn Edges (see Flip Face Edge) x Weld Vertices x"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_tools.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_tools.html",
    "title": "Tools reference | FSM Unity Framework",
    "keywords": "Tools reference Tools provide a modal environment where you can perform complex tasks, such as creating new Meshes or defining precise cuts on an existing Mesh. While a tool is active, you can't perform a different task without exiting the tool mode. For example, in the Scene view, if you click on a different object while using the Cut tool, the tool exits without performing the cut. Mesh creation tools add a specific scripting component to the GameObject which creates the initial Mesh. ProBuilder uses this component to manage any changes you make to its initial definition. For example, after you use the Shape tool to create a pipe-shaped Mesh, you can re-edit that Mesh to change the pipe primitive to an arch primitive, although you lose any refinements, such as extrusions or face merges and splits. Some tools are available from the Tools panel in the Scene view when the selection meets the tool's criterion. For example, if you select a Mesh you created with the Shape tool, the Edit Shape icon appears in the Tools panel, which you can click to activate the Shape tool and edit the bounding box or choose a different shape primitive. This table lists all the tools available in ProBuilder: Tool: Description: Shape tool Creates a new Mesh with the Shape component, which defines the Mesh's shape primitive. You define the same bounding box for all shape primitives. Tip: To modify the primitive or bounding box after creation, select the Mesh and then click Edit Shape on the Tools panel in the Scene view. Poly Shape tool Creates a new Mesh with the Poly Shape component. Tip: To modify the original after creation, select the Mesh and then click Edit Poly Shape on the Tools panel in the Scene view. Cut tool Cuts a sub-face into an existing Mesh face. You determine the shape of the new face by defining points on the Mesh to create the edges of the new face. You can use this tool on any face, regardless of whether you created the Mesh with the New Shape tool, the Poly Shape tool, or by probuilderizing a Unity Mesh. Note: For documentation on the Bezier Shape tool, refer to the Experimental features section of the documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_windows.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/ref_windows.html",
    "title": "Editor window reference | FSM Unity Framework",
    "keywords": "Editor window reference This section provides information on the following Editor windows that ProBuilder uses: Lightmap UV Editor Material Editor window Positions Editor window ProBuilder Preferences window Shape Tool window Smooth Group Editor window UV Editor window Vertex Colors window Note: For documentation on the Boolean (Experimental) window, see the Experimental features section of the documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/selection-tools.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/selection-tools.html",
    "title": "Selection actions | FSM Unity Framework",
    "keywords": "Selection actions This part of the ProBuilder toolbar provides access to selection modifiers and actions. Depending on whether you have the Toolbar display mode set to text mode or icon mode, the selection action button displays the following to indicate what state the action is in: In text mode, the button displays the label text followed by a colon and the name of the state. For example, the Select Hidden action displays either Select Hidden: On or Select Hidden: Off. In icon mode, the button displays a visual cue that reveals the action's current state. For example, the Orientation action uses these three icons to indicate the handle alignment states: (Local), (Global), and (Normal). The documentation for each action includes information about these visual indicators. Note: Some actions also have extra options or custom settings available. These action buttons have a special indicator in the top-right corner. The documentation for each action includes information about these options. Rect Use the Rect action to define whether drag selection should only select elements inside the drag-rect, or any intersected elements. For more information, see the Rect action documentation. Shift Use the Shift action to define how holding the Shift key affects selection. For more information, see the Shift action documentation. Orientation Use the Orientation action to set the orientation for Scene handles (Global, Local, or Normal). For more information, see the Orientation action documentation. Select Hidden Use the Select Hidden action to define whether hidden elements are selected or ignored when drag-selecting. For more information, see the Select Hidden action documentation. Select Edge Loop Use the Select Edge Loop action to select an edge loop from each selected edge. For more information, see the Select Edge Loop action documentation. Select Edge Ring Use the Select Edge Ring action to select a ring from each selected edge. For more information, see the Select Edge Ring action documentation. Select Face Loop Use the Select Face Loop action to select a face loop from each selected face. For more information, see the Select Face Loop action documentation. Select Face Ring Use the Select Face Ring action to select a face ring from each selected face. For more information, see the Select Face Ring action documentation. Select by Material Use the Select by Material action to select all faces which have the same Material. For more information, see the Select by Material action documentation. Select by Colors Use the Select by Colors action to select all faces on this object which have the same vertex color. For more information, see the Select by Colors action documentation. Shrink Selection Use the Shrink Selection action to remove the elements on the perimeter of the current selection (Grow Selection in reverse). For more information, see the Shrink Selection action documentation. Grow Selection Use the Grow Selection action to expand the selection outward to adjacent faces, edges, or vertices. For more information, see the Grow Selection action documentation. Select Holes Use the Select Holes action to select all elements along the selected open vertex or edge. For more information, see the Select Holes action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/shape-tool.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/shape-tool.html",
    "title": "Shape tool | FSM Unity Framework",
    "keywords": "Shape tool Use the Shape tool to create new ProBuilder Meshes based on common primitive shapes, such as cylinders, arches, and stairs. When you activate the Shape tool to create a new shape, the Create Shape panel appears in the bottom of the Scene view. You can either draw in the Scene view or set the properties directly in the Create Shape panel to define a bounding box that will hold the new shape. You can also customize any shape-specific settings, and then either exit the tool or create another new shape. Tip: After you create a new shape and exit the tool, you can redefine the bounding box, switch primitive shapes, and change the shape-specific settings when you select the Edit Shape icon from the Tools panel. If you made any modifications to the Mesh topology (such as splitting vertices, connecting edges, or extruding faces), you must reset the shape to remove all manual changes before you can proceed. (A) Shape icons let you choose the shape's primitive when you are drawing a new shape. (B) The Tools panel displays the Edit Shape icon when you have a Shape Mesh selected. You can toggle this icon to turn the editing mode on and off. (C) The Snapping option only appears when you use ProBuilder inside a version of Unity prior to 2021.1. It enables auto-snapping for the Shape tool. For more information, see Snapping. (D) The Shape Properties section lets you switch to a new shape primitive and define the size of the bounding box. When you are drawing a new shape, the Pivot property lets you specify whether to place the pivot at the corner or in the middle of the bounding box. For details, see Choosing the shape and size. (E) The <Shape> settings section lets you customize the new shape with properties that are specific to each shape. For example, you can change the number of sides (faces) for both cylinders and cones, but for cylinders you can also set the smoothing group and set segment heights for the sides. For details, see Customizing specific shapes. Choosing the shape and size The properties under the Shape Properties section let you choose a shape primitive and define the parameters of the bounding box in which ProBuilder draws the shape. Property: Description: Shape drop-down menu Select the shape primitive you want to create. For details, see Customizing specific shapes. Pivot Select whether you want ProBuilder to create the Mesh's pivot handle in the First Corner of the shape's bounding box or in its Center. The \"first corner\" is where you first clicked in the Scene view to define the bounding box. Note: This property is only available when you are creating the shape. X (Length) Y (Height) Z (Width) You can define the bounding box's size with these properties on every shape. When you select a different shape primitive or change any of these properties, ProBuilder automatically transforms the shape to fit the dimensions of the bounding box. Customizing specific shapes The following sections contain information about each shape primitive, including a list of any shape-specific properties available: Arch shape primitive Cone shape primitive Cube shape primitive Cylinder shape primitive Door shape primitive Pipe shape primitive Plane shape primitive Prism shape primitive Sphere shape primitive Sprite shape primitive Stairs shape primitive Torus shape primitive ProBuilder Meshes are similar to other GameObjects in Unity in terms of how they interact with other GameObjects and respond to Physics in the scene. However, you can use ProBuilder editing features to customize and deform the Mesh topology after you create them. Snapping To set a more accurate size, snap the bounding box to Unity's grid while you draw or move the preview shape in the Scene view. To do this, enable auto-snapping with the Shape tool by using one of the following methods that matches the version of the Editor you are using: Version 2021.1 and later: Activate Unity auto-snapping in the Editor. The Snapping option is not available because ProBuilder uses the Editor's auto-snapping setting. Versions 2019.4 through 2020.3: Enable the Snapping option. ProBuilder ignores Unity's auto-snapping setting in these versions of the Unity Editor. Note: Auto-snapping is only available when your gizmo handle is set to Global in any version of Unity. Regardless of whether auto-snapping is enabled, you can hold the Ctrl/Cmd key while drawing or positioning the new shape to move by increments. To set the size of these increments, use the Increment Snap property on the Grid and Snap window."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/smoothing-groups.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/smoothing-groups.html",
    "title": "Smooth Group Editor window | FSM Unity Framework",
    "keywords": "Smooth Group Editor window The Smooth Group Editor window allows you to add and remove smoothing groups across the faces of a ProBuilder Mesh. (A) The toggle buttons control the visibility of some areas of the window. (B) The Normals control is a visual aid that allows you to change the length of the normals displayed on the Mesh in the Scene view. (C) The help button opens up a quick reference panel for using this window. (D) The overlay controls panel allows you to customize how ProBuilder previews the smoothing group changes on the Mesh in the Scene view. (E) The main button panel allows you to define up to 23 sets of smoothing groups. Tip: You can also access this tool from the ProBuilder menu (Tools > ProBuilder > Editors > Open Smoothing Editor). Toggle Panel buttons The cluster of buttons in the top left corner of the window hide or display the extra panels: The Settings button shows or hides the overlay controls above the main button panel. The Preview button shows or hides the color cues for each smoothing group. The color for each defined group appears under the button in the main button panel, and all the faces in the smoothing group display that color on the Mesh in the Scene view. The Normals button shows or hides the Normals control. Normals control You can use the Normals slider control to change the length of the normals ProBuilder displays on the Mesh. Viewing the normals allows you to see which vertices are split, therefore affecting the smoothing. Move the slider position towards the left to shorten the length of the normals, or move them to the right to lengthen them. When the slider is all the way to the left, the normals are no longer visible. Quick reference panel The following hints about using the smoothing panels appear when you click the help button in the top right corner of the Smooth Group Editor window: To access the online documentation for Smoothing Groups, at the bottom of the panel, click the Open Documentation button. Overlay controls The overlay controls panel allows you to customize how ProBuilder previews the smoothing group changes on the Mesh in the Scene view. Property: Description: Preview Opacity Increase this value to show more of the color of the applied smoothing group and less of the Mesh's Material. The range of values is 0.001 (smoothing group colors are invisible) to 1 (only smoothing colors are visible). Preview Dither Enable this option to see how ProBuilder applies the smoothing to the Mesh with the dither overlay. Main button panel The main button panel allows you to define up to 23 sets of smoothing groups. Buttons These buttons in the top right corner help you manage smoothing groups: Click the Select Smoothing Group button to extend the face selection to all faces in the currently selected smoothing group. For best results, click a single face in the group to start and then click this button. ProBuilder doesn't shrink the current selection, so start with as small a selection as possible. Click the Clear Smoothing Group button to remove all faces from the currently selected smoothing group. Preview colors This panel is always visible, but the color under each button only appears in Preview mode. Whether or not you are in Preview mode, the button background changes color according to what faces and smoothing group you've selected: Button color: Description: No faces are in the selected Smoothing Group yet. None of the selected faces are in the selected Smoothing Group. All of the faces in the currently selected Smoothing Group are currently selected. The current selection includes faces that are in the currently selected Smoothing Group, but it also includes faces that are not in any Smoothing Group."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/tool-panels.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/tool-panels.html",
    "title": "Tool panels | FSM Unity Framework",
    "keywords": "Tool panels This portion of the ProBuilder toolbar provides access to most of the ProBuilder windows, tools, and actions. New Shape Use the New Shape tool to create new ProBuilder Mesh shapes such as cylinders, arches, and stairs. For more information, see Shape tool. New Bezier Shape Use the Bezier Shape tool to define a bezier curve around which ProBuilder extrudes a Mesh. Warning: Bezier shapes are experimental, meaning that they are still under development, and might reduce ProBuilder's stability. Please use with caution. Because this tool is experimental, the button only appears when you enable experimental features for ProBuilder. For more information about the tool, see Bezier Shape tool. New Poly Shape Use the Poly Shape tool to create a custom 2-dimensional shape and then extrude that shape to create a 3-dimensional Mesh. For more information, see Poly Shape tool. Smoothing Use the Smooth Group Editor window to create a smooth and rounded look. For more information, see the Smooth Group Editor window reference. Material Editor Use the Material Editor window to apply Materials to objects or faces. For more information, see the Material Editor window reference. UV Editor Use the UV Editor window to apply textures to objects or faces. You can also use it to automatically or manually wrap and unwrap textures. For more information, see the UV Editor window reference. Vertex Colors Use the Vertex Colors window to apply or paint vertex colors onto Meshes. For more information, see the Vertex Colors window reference."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/toolbar.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/toolbar.html",
    "title": "The ProBuilder toolbar | FSM Unity Framework",
    "keywords": "The ProBuilder toolbar You can open the ProBuilder toolbar from Unity’s top menu: Tools > ProBuilder > ProBuilder Window. When you open the main ProBuilder toolbar, the Edit mode toolbar also opens in the active Scene view, where you can see and change the current editing mode. Use the ProBuilder toolbar to access Editor windows, selection, creation, and editing tools for Meshes. Tool categories The ProBuilder toolbar is color-coded to help you quickly choose tools by type: Orange to access tools and windows Blue to access selection actions Green for Object actions Red for Element actions (on Vertex, Edge, and Face geometry) Dynamic layout The ProBuilder window dynamically changes to match your current edit mode and element selection: Buttons only appear if you can use them with your current edit mode. Buttons are disabled if you cannot use them on the current selection. Toolbar display modes The ProBuilder toolbar displays buttons as either text or icons. Tip: By default, the toolbar appears in Text mode, but you can switch to Icon mode instead. You can also resize the toolbar and switch between making it floating and dockable. See Customizing ProBuilder for details. Options for ProBuilder tools Some actions (such as Extrude, Collapse, and Grow Selection) also have extra options or custom settings, which can change how ProBuilder performs the action by default. If there are options available for an action, an indicator appears in the top-right corner of the button which allows you to access the Options window: In Icon Mode, the gear indicator appears next to an action button if there are options available. To open the Options window, Alt+Click anywhere on the button. In Text Mode, the + icon appears on the right side of an action button if there are options available. To open the Options window, click on the button's + icon. Caution: When you change one of these options, those changes become the default settings for that action in the scene until you change them again. For example, the Select by Material action allows you to specify whether ProBuilder limits the new selection to match only faces on the currently selected object or faces on any object in the scene. Note: The documentation for each action includes information about any options."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/tools.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/tools.html",
    "title": "Tools vs. actions | FSM Unity Framework",
    "keywords": "Tools vs. actions Tools provide a modal environment where you can perform complex tasks, such as creating new Meshes or defining precise cuts on an existing Mesh. For example, when you create Meshes, you can use the New Shape tool or the Poly Shape tool. Both of these are modal tools so that you can define dimensions and set any shape-specific properties available before ProBuilder builds the final Mesh. The Cut tool is also modal because you have to define several points on a Mesh where you want to create a new edge before ProBuilder creates the new face. Tip: When you activate a tool, its text button is highlighted on the ProBuilder toolbar. You can click the button again to exit the tool. Actions are immediate changes, such as selecting all faces with a specific color or splitting a single edge. As soon as you initiate an action, ProBuilder performs that action. For example, Grow Selection is an action that you initiate from the menu, toolbar button, or shortcut and it finishes immediately. You can modify its options to change the behavior of the action, but the options appear in a non-modal window. Actions are often only available in specific Edit modes, whereas tools are generally available in all modes. For example, you can launch the Cut tool in every mode except the Object mode and it behaves exactly the same, but most actions are specific to the selected element and behave differently, such as subdividing an element: Subdivide Edge divides the selected edge(s) into multiple edges. Subdivide Face adds a vertex at the center of each selected face and connects them in the center. Subdivide Object divides every face on the selected objects. For a list of tools available in this version of ProBuilder, see Tools reference. For a list of actions available in this version of ProBuilder, see Action reference."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv-editor-toolbar.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv-editor-toolbar.html",
    "title": "UV Editor toolbar | FSM Unity Framework",
    "keywords": "UV Editor toolbar (A) This first group of buttons contains shortcuts to the standard Unity manipulation tools. These have the same effect as the main Unity toolbar buttons. (B) The second button group contains shortcuts to ProBuilder's Element Edit modes. When you use Manual UV Editing, this allows you to select and manipulate UVs by Vertex, Edge, or Face. Note: When you use Auto UVs, you can only edit UVs by face. If you edit an Edge or Vertex, ProBuilder converts the selected UVs to Manual UVs. (C) The third group of buttons contain buttons to perform the following actions: Scope controls: Control how Unity's standard transform tools behave. Texture preview: Toggle whether the UV Viewer displays the face's Texture. Save UV image: Create UV Templates from the current UV mapping. Scope controls When Scope controls are on (), you can use Unity's standard Move, Rotate, and Scale tools to directly manipulate UVs in the scene, without affecting geometry. When Scope controls are off (), the Move, Rotate, and Scale tools return to normal geometry actions. When you use the Transform tools in the UV Viewer, you can snap to increments with the Ctrl (Windows) or Cmd (macOS) modifiers. To customize the size of these increment values, set the UV Snap Increment preference. Texture preview When Texture preview is on (), the selected face's Texture appears in the UV Viewer. This preview makes it easier to fit the UV to the Texture. When Texture preview is off (), the Texture does not appear in the UV Viewer. Save UV image Click the Camera icon () to open the Save UV Image window. You can use this window to render a UV Template for texture map painting, atlasing, and sprite sheets. Property: Function: Image Size Choose the total size for the rendered template. ProBuilder always renders the image as a square. Hide Grid Enable this option to prevent the grid from appearing in the rendered image. Line Color Choose the color that you want ProBuilder to use for rendering the UV lines. Transparent Background Enable this option to render a transparent background. Background Color Choose the color that you want ProBuilder to use for the background. This option is only available when the Transparent Background property is disabled. When you are satisfied with the options on this window, click the Save UV Template button. A file browser opens so you can choose the filename and location where you want to save the file."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv-editor.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv-editor.html",
    "title": "UV Editor window | FSM Unity Framework",
    "keywords": "UV Editor window Use the UV Editor window to manage texture mapping on the selected Mesh. To open this window, navigate to the ProBuilder toolbar and click the UV Editor button ( ). Tip: You can also access this window from the ProBuilder menu (Tools > ProBuilder > Editors > Open UV Editor). (A) The UV Editor toolbar contains general features for working with UVs. (B) The UV drop-down menu allows you to switch between UV channels: Choose UV to edit the UV mapping for shaders. Choose UV2 (read-only) to regenerate your baked or realtime lightmaps. (C) The Actions panel is a dynamic panel, similar to the ProBuilder toolbar: What kind of UVs you are editing (shader UVs or lightmap UV2s) determine which set of options appear in the panel. For shader UV options, you can switch between using manual UV editing or auto-texturing, and these each have their own specific options as well. For more information, see UV editing modes. Only actions compatible with the selected UV element type (vertex, edge, face) are available. If you are in object selection mode when you open the UV Editor, all options are disabled: you must select one or more faces, edges, or vertices to continue. (D) You can use the UV viewer to view and edit the selected object's UV elements directly. Tip: While working in the UV viewer's work area (canvas), sometimes the Texture becomes only partially visible (for example, if you zoomed in so you could see more detail on the Texture). To quickly re-center the Texture, use the 0 shortcut. You can also re-map this shortcut with the Shortcuts Manager. UV editing modes The UV Editor supports two modes when you edit texture mapping: Automatic : ProBuilder manages the texture mapping according to the settings in the Actions panel automatically, even when you resize the Mesh. This is the default. Use this mode for simple texturing work, especially architectural or hard-surface items. Tiling, Offset, Rotation, and other controls are available, while ProBuilder automatically manages the projection and updates it as you modify the Mesh geometry. For example, use automatic tiling to for a brick texture on a wall. This maintains the size and orientation of the brick even if you resize the wall. Textures like brick with a repeating pattern are ideal for automatic mode. Manual : Use the UV Editor to precisely unwrap and edit UVs, render UV Templates, project UVs, and more. To modify the texture mapping, you move, rotate, and resize the UV elements against the Texture in the viewer. ProBuilder provides two texture projection methods (Box and Planar), along with several helpers in the Actions panel to help you select and edit elements. For example, you can use a texture image for an element with details like a building where some areas contain windows or doors. Using the Manual mode in these cases is ideal, because you can position the UV elements precisely against the image. Each face supports only one mode at a time, but you can use a mixture of the two modes on the same Mesh. This is especially useful when some parts of a Model need to have tiling Textures, while others are unwrapped. If you select several faces at once, and some are in Automatic mode and others are in Manual, the Actions panel displays both buttons: Convert to Manual and Convert to Auto."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv_dropdown.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/uv_dropdown.html",
    "title": "UV drop-down menu | FSM Unity Framework",
    "keywords": "UV drop-down menu The UV drop-down menu allows you to switch between UV1 and UV2 modes (ProBuilder does not use UV3 and UV4). UV1 provides texture mapping functionality, and UV2 provides options for lightmap UV generation. UV1 When you select UV1 from the UV drop-down menu, the Actions panel displays the default UV mapping. If you select the object before you open the UV Editor window, the Actions panel is empty. To enable editing the UVs, choose one of these element Edit modes: Click the Face Edit Mode button and select one or more faces in the UV viewer to edit in either Auto or Manual UV mode. Click the Edge Edit Mode button and select one or more edges in the UV viewer to begin editing in Manual UV mode. Click the Vertex Edit Mode button and select one or more vertices in the UV viewer to begin editing in Manual UV mode. To switch between the Auto and Manual UV modes, click the Convert to Manual or Convert to Auto button at the top of the Actions panel. Both modes provide their own options in the Actions panel. Note: When using Auto UVs you may only edit UVs by face. Editing an edge or vertex automatically converts the selected UVs to Manual UVs. UV2 (read-only) When you select UV2 (read-only) from the UV drop-down menu, the Actions panel displays the Lightmap UV Settings and the Rebuild Selected UV2 button. Set up the options for lightmap UV generation for the selected UV2s, then click the Rebuild Selected UV2 button when you are finished. Note: This only affects the UV2s that are currently selected. If you want to rebuild the UV2s for the entire scene, use the Lightmap UV Editor window instead. UV3 (read-only) ProBuilder does not currently use this UV mode. UV4 (read-only) ProBuilder does not currently use this UV mode."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex-colors.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex-colors.html",
    "title": "Vertex Colors window | FSM Unity Framework",
    "keywords": "Vertex Colors window Use the Vertex Colors window to apply vertex colors onto an entire Mesh or only a portion. To open this window, click the Vertex Colors button ( ) from the ProBuilder toolbar. Tip: You can also access this window from the ProBuilder menu (Tools > ProBuilder > Editors > Open Vertex Color Editor). (A) Click the Reset button to return all the colors on the Color Palette to their default values. (B) You can import your own swatch library and use its colors instead of the default Unity colors. To do this, set a reference to the swatch library file. (C) Click the Apply button to set the associated color on the selected object(s) or element(s). (D) Use the standard Unity Color Picker to pick a custom color for the palette. (E) Use the eyedropper to match a custom color for the palette."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex-positions.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex-positions.html",
    "title": "Positions Editor window | FSM Unity Framework",
    "keywords": "Positions Editor window Use the Positions Editor window to enter specific translation coordinates to modify vertex positions. Tip: You can access this tool from the ProBuilder menu (Tools > ProBuilder > Editors > Open Vertex Position Editor). By default, Unity uses absolute coordinates to define positions, but you can click the World Space button in the top right corner to use relative coordinates instead (that is, relative to the Model Space). The index of each vertex appears on the left, followed by the X, Y, and Z values that define its position. You can use this window as an alternative to modifying vertex positions with the transform tools. For example, to view or modify the vertex positions on any selected ProBuilder Mesh: Click one of the element editing mode buttons on the Edit modes toolbar. Select the vertices, edges, or faces that you want to modify. Go to Tools > ProBuilder > Editors and select Open Vertex Position Editor. You can identify the vertices that appear in the Positions Editor by the index number that appears on the shape in the Scene view. Set new values for the coordinates you want to modify. ProBuilder immediately updates the positions."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/vertex.html",
    "title": "Vertex actions | FSM Unity Framework",
    "keywords": "Vertex actions This section of the ProBuilder toolbar provides access to ProBuilder editing actions that you can use in the Vertex edit mode. Note: Some actions also have extra options or custom settings available. These action buttons have a special indicator in the top right corner. The documentation for each action includes information about these options. Collapse Vertices Use the Collapse Vertices action to colapse all selected vertices to a single point, regardless of distance. For more information, see the Collapse Vertices action documentation. Weld Vertices Use the Weld Vertices action to merge selected vertices within a specific distance of one another. For more information, see the Weld Vertices action documentation. Connect Vertices Use the Connect Vertices action to create a new edge connecting the selected vertices. For more information, see the Connect Vertices action documentation. Fill Hole Use the Fill Hole action to create a new face filling any holes that touch the selected vertices. For more information, see the Fill Hole action documentation. Cut Tool Use the Cut tool to create a new face on an existing Mesh. For more information, see the Cut tool documentation. Offset Vertices Use the Offset Elements action in the Vertex edit mode to move the selected vertice(s) according to the settings. For more information, see the Offset Elements action documentation. Split Vertices Use the Split Vertices action to split a single vertex into multiple vertices (one per adjacent face). For more information, see the Split Vertices action documentation. Set Pivot Use the Set Pivot action to move the pivot point of this Mesh to the average center of the selected vertices. For more information, see the Set Pivot action documentation."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/whats-new.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/whats-new.html",
    "title": "What's new in version 5.0 | FSM Unity Framework",
    "keywords": "What's new in version 5.0 Summary of changes in ProBuilder package version 5.0. The main updates in this release include: Added ProBuilder now supports a special modal \"tool\" mode for some features (the new Cut tool, and the refactored Shape and Poly Shape tools). Because of these changes, the other features that have a more immediate effect haven been rebranded \"actions\". For more information, see Tools vs. actions. Added a new Cut tool that allows you to add points on an existing face to define a new sub-face. Added a new Selection X-Ray option to highlight hidden element selections with a muted color. The default shortcut is Ctrl/Alt+Shift+X (modifiable in the Shortcuts Manager), and you can also access it through the ProBuilder menu (Tools > ProBuilder > Interaction > Toggle X Ray). Added a new preview selection for the Select Path action. Updated The new Shape tool has been completely rebuilt as an EditorTool. The Poly Shape tool is now an EditorTool. Fixed Fixed Undo so that it only reverts the last action, instead of all actions performed. Selection, picking and highlight shaders have been updated to be compatible with SRPs as well as with orthographic cameras (there are several bug fixes that directly support these adjustments). For a full list of changes and updates in this version, see the ProBuilder package changelog."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-bezier.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-bezier.html",
    "title": "Bezier Shape tool | FSM Unity Framework",
    "keywords": "Bezier Shape tool The Bezier Shape tool uses a Bezier spline (curve) and extrudes along it to create a 3D version of the curve. You fine-tune the shape by using the tangent handles on the control points to bend the shape. To define a Mesh based on a bezier (curve) shape: Make sure the Experimental Features Enabled preference is enabled (go to Edit > Preferences in Windows or Unity > Preferences in macOS from the main menu in Unity, then select the ProBuilder category from the list). Open the ProBuilder window (in Unity's top menu: Tools > ProBuilder window). The Edit Mode toolbar and the ProBuilder toolbar appear. From the ProBuilder toolbar, click New Bezier Shape. Tip: You can also access this tool from the ProBuilder menu (Tools > ProBuilder > Editors > New Bezier Shape). The initial curve has two control points with tangent handles to control the curve's bend. By default, ProBuilder creates the curve in editing mode so you can continue to reshape it."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-polyshape.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-polyshape.html",
    "title": "Creating a Mesh with the Poly Shape tool | FSM Unity Framework",
    "keywords": "Creating a Mesh with the Poly Shape tool Use the Poly Shape tool to define a 2-dimensional custom shape (depth and width) and then extrude a 3-dimensional shape to define its height. Like the Shape tool, this is a modal tool that creates a new Mesh but also lets you re-activate the tool to edit the initial shape. Note: When you modify control points or settings, you lose any changes you made in editing the Mesh. To define a custom Mesh: Open the ProBuilder window (in Unity's top menu: Tools > ProBuilder window). The Edit Mode toolbar and the ProBuilder toolbar appear. From the ProBuilder toolbar, click New Poly Shape (). Tip: You can also access this tool from the ProBuilder menu (Tools > ProBuilder > Editors > New Poly Shape). In the Scene view, click to create control points. Control points form the outer bounds of your Mesh. You can click directly on another surface to create an outgrowth, even when that surface is on the y-axis. When you finish placing points, use either Enter (Windows), Return (macOS), or Spacebar to finalize the shape. Move the mouse up and down to set the Mesh height. This extrudes a 3D Mesh from the 2D shape you defined with your control points. Click to exit create mode. Your new Mesh is now in editing mode so you can continue to modify it."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-predefined.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create-predefined.html",
    "title": "Creating a pre-defined ProBuilder shape | FSM Unity Framework",
    "keywords": "Creating a pre-defined ProBuilder shape Use the Shape tool to create a ProBuilder Mesh from a predefined shape primitive. ProBuilder builds these Mesh shapes inside a bounding box that you define: The bounding box defines the size of each shape primitive using x, y, and z values relative to the \"first corner\" of the box: that is, the first click you make in the Scene view. If you switch between shape primitives when drawing or editing a shape, ProBuilder instantly adjusts the dimensions of the shape to fit within the bounding box. To define the bounding box, you can either draw it in the Scene view or set the dimensions on the Create Shape panel and then click in the Scene view to tell ProBuilder where you want to place the new shape. ProBuilder always builds the new shape on top of any existing Mesh in the Scene or, if there is no Mesh under your mouse, on the plane defined in the settings for Unity's Grid snapping. To create a Mesh from a predefined shape: Open the ProBuilder window (in Unity's top menu: Tools > ProBuilder window). The Edit Mode toolbar and the ProBuilder toolbar appear. From the ProBuilder toolbar, click the New Shape () tool. The Create Shape panel appears in the bottom of the Scene view and the New Shape text button is highlighted. Tip: You can also use the Ctrl/Cmd+Shift+K shortcut or use the menu (Tools > ProBuilder > Editors > New Shape) to activate the Shape tool. From the top of the Create Shape panel, choose the icon that matches the shape you'd like to create. Any shape-specific properties appear under the <Shape> Settings section at the bottom. Set the shape-specific options (width, height, radius, number of stairs) according to the type of shape. For example, the Stairs shape lets you customize the height of the steps, how curved to make them, and whether to create faces for the sides. While the Shape tool is active, you can't use the transform tools, but you can use the Camera to orbit around the new shape and zoom in and out. In the Scene view, click and hold the left mouse button to draw the base of the bounding box (along the x-axis and the z-axis) and then release the mouse button to validate the base. Move the mouse up (or down) the y-axis to draw the height, and then click to finalize the new Mesh. Tip: Alternatively, you can set all the properties on the Create Shape panel, including the bounding-box dimensions, and then hover over the Scene view while holding Shift to see a preview of the Mesh. When you are satisfied with how it looks and where it appears in the scene, click in the Scene view to finish creating the new Mesh. Note: If you draw the shape on an axis-aligned plane, enable auto-snapping to set a more accurate size on the defined shape than drawing freehand. Alternatively, you can move the shape by increments defined by the Increment Snap value instead. For more information, see Snapping. To modify the last shape you created, you can change the properties on the Create Shape panel. To create copies of the last shape you created, hold Shift to see a preview of the Mesh while moving in the Scene view, and then click to build the Mesh copy at that spot. You can do this as many times as you like until you exit the tool. To exit the tool, click the New Shape button again or select Esc. Note: If you decide later that you want to switch primitive shapes, change the size of the bounding box, or modify any of the shape-specific properties, select the Edit Shape icon from the Tools panel. After you create a Mesh shape, you can use any of the ProBuilder editing tools to fine-tune or customize that shape further. For example, you can build a plain cube and then use the Extrude Face and Delete Face tools to create windows and doorways to make a house."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-create.html",
    "title": "Creating Meshes | FSM Unity Framework",
    "keywords": "Creating Meshes ProBuilder provides several different tools for creating editable Meshes in Unity. The most common approach is to build a predefined shape with the Shape tool, which includes a library of shapes. These predefined shapes include standard geometric shapes, and some more complex shapes which correspond to objects that are common in level-building. For example, the Shape tool provides simple cubes, prisms, toruses, and other simple geometry that you can use to create buildings, vehicles, and other objects. It also provides some convenient predefined shapes that are typically found in buildings, such as stairs, arches and doors. To create predefined shapes, you can use either of the following methods with the Shape tool: Draw a bounding box in the Scene view. Then you can choose a primitive shape and customize it where possible with shape-specific parameters. Define the dimensions of the shape's bounding box on the Create Shape panel and customize it where possible with shape-specific parameters. Then Shift+click in the Scene view where you want ProBuilder to draw it. For example, the cube shape doesn't have any shape-specific parameters, but the stairs shape lets you set the curvature, the number of steps, and whether to build sides. If you need to make a Mesh shaped unlike any of the predefined shapes, you have several options: You can use the Poly Shape tool to create a custom 2-dimensional shape and then extrude that shape into a 3-dimensional Mesh. This is a good strategy for quickly building an irregular structure, like a medieval church or a star-shaped building. You can use the experimental Bezier tool to define a bezier curve around which ProBuilder extrudes a Mesh. For example, you can use this tool to create tunnels with lots of twists and turns. You can apply an experimental Boolean operation on two or more Mesh objects to create a new object. You can choose to create only the difference between the two (Intersection), or everything but the difference between the two (Subtraction), or one big Mesh that encompasses the two original Meshes plus the space in between them (Union). Warning: Bezier shapes and Boolean operations are experimental, meaning that they are still under development, and might reduce ProBuilder's stability. Use with caution. Whichever method you use to create your Mesh, you can edit it using any of the ProBuilder editing features, apply vertex colors, smooth its sharp edges, and apply Materials and Textures."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-smoothing.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-smoothing.html",
    "title": "Smoothing hard edges on Meshes | FSM Unity Framework",
    "keywords": "Smoothing hard edges on Meshes You can define Smoothing Groups to create a smooth and rounded look on part or all of your Mesh. If you include only a portion of your Mesh, the rest of the Mesh has more of a sharp and hard-cornered look. Smoothing does not subdivide the Mesh: it controls whether vertices are split for hard edges. This often works well for simpler Meshes, so it's great for anything that is isn't simple geometry. For example, you can use this on cylinders or more organic shapes, on curved walls, or on Mesh terrain chunks. Note: This produces a fairly subtle smoothing. If you need something that turns sharp edges into smooth curves, you need to either bevel those edges or subdivide the faces around them for greater control. Some of the tasks you can perform include: Defining a smoothing group Removing smoothing groups Selecting faces in the groups Defining a smoothing group You can define up to 23 groups per Mesh in order to control the degree of smoothness. To smooth a portion of your Mesh: Select the Face editing mode from the Edit Mode toolbar. Select the faces that you want to have smooth adjoining edges. Click the Smoothing action from the The ProBuilder toolbar. Click an unused smooth group number on the Smooth Group Editor window. Tip: Smooth groups already in use appear with a light blue highlight on the button. The selected faces now appear smoother. You can repeat these steps using different number buttons. Removing smoothing groups To clear selected face smoothing groups: Select the faces with the group defined. Click the Clear Smoothing Group button on the Smooth Group Editor window. Selecting faces in the groups To select all faces matching the current smoothing group index, click the Select Faces button on the Smooth Group Editor window."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-tasks.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-tasks.html",
    "title": "Common editing tasks | FSM Unity Framework",
    "keywords": "Common editing tasks This page describes some of the most common ways you can use the ProBuilder tools and actions to create and modify geometry. There any many more possibilities, but these should give you a good place to start developing your own workflow. General modeling tasks Selecting: Selecting elements is the first step you take when you want to change your Mesh. ProBuilder provides a whole range of selection actions that make it easier to target the elements you need to access. As a shortcut, when working in the Face editing mode, you can double-click any face to select all faces on the Mesh. Transforming: You can translate, rotate, and scale geometry elements to change the shape of a basic Mesh. For example, if you use the basic Stairs shape but you want to give it a crooked look, you could move some edges or vertices. Extruding: \"Extruding\" refers to dragging a face out from the Mesh. To do this, click the Face editing mode button on the Edit modes toolbar, select a face, hold Shift, and use the Transform controls. Alternatively, you can extrude edges (for example, if you want to build a ski slope, you could extrude an edge and then smooth it). Insetting: \"Insetting\" refers to dragging the edges of the face inside the Mesh. To do this, select a face, hold Shift, and use the Scale controls. Then drag in the opposite direction without the Shift modifier to complete the inset. Subdividing edges: If you have an irregular shape (for example, a wall with a peaked top) and you need to split it into four faces (for example, to insert windows), you can insert an edge loop to split the whole wall into two mirrored pieces, and then select only those edges along the front and back of the wall. Subdividing the two selected edges then creates a third vertex, which you can connect up with the vertices at the base of the peaked section to create four perfectly even sections on the front and the back. This approach is much easier than trying to subdivide a five-sided polygon. Cutting: This is similar to subdividing faces except that you control the shape of the new face, instead of letting ProBuilder split the face evenly. You click on the face to define the location of the edges and vertices using the modal Cut tool. Boolean operations: Some geometry is hard to create by moving faces, edges, and vertices. The experimental Boolean feature allows you to quickly combine two Meshes together to create a new Mesh. The final Mesh is either the addition of the two, the difference between the two, or only the common geometry between them, depending on the mode. Object-specific tasks Create a coffee mug: start with a cylinder, select all faces on the top and merge them. Next, create a slight inset on the merged top, and extrude it all the way down. Finally, create a half-torus, rotate it, move it next to the cylinder for the handle and merge the torus and cylinder together. Build a bed: start with a rectangular cube for the mattress. Create smaller rectangles for the legs of the bed, and then merge everything together to make one single Mesh. Make a bottle: start with a a cylinder. Merge the faces on the top end, then extrude the face up, scale it to the size of a neck, and extrude it up again. Building-specific tasks Make a hole (for a window or door): There are many methods you can use to do this, but some work better depending on what you are working on. For example, in an even and rectangular wall, such as on the first floor of a house, you could use this strategy: Select the two faces of the wall (back and front), then Shift+Scale to create an inset horizontally, and Scale vertically to make it an even border. Press Backspace to delete the insets. On the other hand, if you are working on an uneven or multi-sided wall, such as a castle or church wall, this strategy is preferable: Use the Insert Edge Loop action to create two vertical edges. Do the same on the horizontal plane (two for a window, one for a door). Adjust the loops so that the resulting hole is the right size and location for a window or a door. Select the face of the hole and press Backspace. If necessary, from the other side of the wall, delete the face on the other side. Whichever strategy you start with, you need to weld the newly exposed edges and vertices together: Select all the edges on one side only of the hole and use Shift+Translate until they connect up with the edge on the other side. Then weld the vertices together where the edges meet. Add a door: follow the \"Make a hole\" procedure, but to start, create a Door shape. Fit the hole to match, merge the two objects, and then weld the door to the frame you created. Make a tunnel with normals on both the inside and outside: duplicate the tunnel object and scale it slightly so that you bridge the edges. Next, flip the normals on the smaller one. Make a building with towers: start with a cube and inset on the top face, then extrude upward. Repeat this as many times as you like. You could also subdivide the top face to create multiple extrusions. Finding more inspiration Use the Unity ProBuilder channel to find videos that demonstrate how to use the tools effectively: ProBuilder Simple Objects - Crates and Barrels ProBuilder Building Structures with Interior and Exterior Prototyping a \"Medieval House\" in Unity with ProBuilder3D ProBuilder Greyboxing an Interior FPS Level Unity at GDC - Rapid worldbuilding with ProBuilder"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-tips.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit-tips.html",
    "title": "Modeling tips for beginners | FSM Unity Framework",
    "keywords": "Modeling tips for beginners To successfully create digital models, you need to find a balance between visual quality and resource requirements. Always remember these two statements: The less geometry you use in your Mesh, the faster your application runs. This is because every vertex, edge, or face requires computing resources. The more geometry you use in your Mesh, the more detailed and organic your GameObjects look. This is because more geometry means smaller polygons, which gives you more control over the shape. One strategy you can use to find a good balance is to start with major modifications, such as extruding and insetting, until your Mesh is roughly the right shape. During this phase, make sure that your geometry is minimal and well-structured: use quads (four-sided polygons) or triangles whenever possible. For example, use Insert Edge Loop instead of Connect Edges to keep the edges and faces as symmetrical as possible. After you have finished the rough modeling on your Mesh, you can begin to refine it by subdividing faces as needed. During this phase, you may decide that it is more important to achieve the correct shape quickly than to keep perfect uniformity. Finally, when you are satisfied with the look and shape of your Mesh, you can try to remove unnecessary geometry where possible, to simplify the Mesh. You can also remove any faces that are hidden behind Meshes or simply beyond the reach of the Camera. You can merge faces to reduce the number of edges Unity has to process, and collapse or weld extra vertices. Note: Make sure when you merge faces that the remaining geometry is still well-formed. For instance, if you use the Cut tool to create a sub-face in the middle of an existing face, don't remove the edges that connect the new sub-face to the original face's edges because this could result in degenerated faces and broken geometry. Ideal geometry When you create buildings and other architectural structures, you can use Unity's grid snapping to help you stick to right angles and perfect lines. It also makes you less likely to accidentally move vertices past an adjacent vertex, which would create impossible geometry. To make it easier to combine pieces of your buildings together later on, only use only 90-degree and 45-degree angles when building, and avoid unusual or unrealistic angles. For example, if you build the shell of a house as one Mesh and a staircase as a second Mesh, it is easier to fit the two together if they are aligned perfectly. Structures to avoid Try to avoid creating structures that contain vertices in irregular locations: T-Junctions Floating vertices T-Junctions If you can avoid vertices at T-junctions, that makes your geometry a lot cleaner: Note the edge that splits only the top into two faces and does not continue into the front (the purple face). ProBuilder created the vertices at either end of this edge during a Connect Edges action. To avoid this complexity, the Insert Edge Loop action creates edges all the way around the object, so that there are no T-junctions. Floating vertices Another structural issue to avoid is a floating (or winged) vertex: This vertex is only connected to one edge. ProBuilder left it behind after a Merge Faces operation. It adds to the complexity of the geometry, but it doesn't define any connections, so ProBuilder isn't using it. To fix a floating vertex, select the floating vertex and an adjacent vertex, then run the Collapse Vertices tool with the Collapse To First option enabled. Make sure you select the vertex you want to keep first."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-edit.html",
    "title": "Editing Meshes | FSM Unity Framework",
    "keywords": "Editing Meshes ProBuilder provides the following ways to edit ProBuilder Meshes: You can modify the elements of any ProBuilder Mesh to change its shape. When you move or extrude faces, edges, or vertices, you are distorting and deforming the Mesh itself. ProBuilder also lets you fill holes, split vertices, collapse edges, and many more actions, other than basic transformations. You can use these actions to build up an existing shape or combine it with other shapes, including merging or detaching Meshes. ProBuilder provides a modal Cut tool that lets you draw a custom sub-face onto an existing Mesh face. You can also modify any regular Unity GameObject with ProBuilder tools, if you Probuilderize it first. In addition to modifying Mesh elements, there are special editing modes for (predefined) Shapes, Poly Shapes, and Bezier Shapes that allow you to return to the shape you created or last defined: For Shapes based on shape primitives, you can change the size of the bounding box and even switch the shape primitive, after you finish creating it. For Poly Shapes, you can modify the base shape, the extrusion, or the normals after you finish creating it. For example, you can move the points that define the base or add new points to refine the base shape. You can also change the height and flip the normals. For Bezier Shapes, you can edit the underlying bezier curve; you can delete and move existing points, add new ones, close the loop, and smooth it. Warning: Bezier shapes are experimental, meaning that they are still under development, and might reduce ProBuilder's stability. Use with caution. Modifying objects and elements To edit objects and elements, you need to: Decide which actions can help you achieve the end results. There might be multiple solutions that can all produce the effect you want. This can be a challenging stage, particularly for new users who don't know what kind of tools and actions ProBuilder provides. Select the element(s) that you want to modify. Often, the editing tool or action impacts which elements you need to select and how you need to select them. Depending on which action you are using, set any options to help customize the outcome or change the default settings. If an action offers options, an indicator appears on the button in the ProBuilder toolbar: Alt/Opt+Click the gear indicator that appears in the top right of the button in Icon mode. Click the + icon that appears on the right side of the button in Text mode. Perform the action or activate the tool. Depending on what you are doing, this may be a simple matter of clicking a button. In some cases, you may be carrying out some intricate procedures. For example, you can click to extrude edges with the default settings, or you can use the Shift+Drag method to control exactly how and where to locate the extruded edge. ProBuilder tools and actions create, destroy, join, split, and transform objects and elements. Some actions modify the geometry of the Mesh without changing the overall shape, whereas some actions change both."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-exporting.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-exporting.html",
    "title": "Exporting and re-importing | FSM Unity Framework",
    "keywords": "Exporting and re-importing If you want to use a ProBuilder Mesh in another program, such as a 3D modeling application, you can use the Export action to save it to one of the supported formats. For example, you might be using a ProBuilder Mesh as a placeholder while greyboxing, but eventually want to create or enhance a complex Mesh. You can also export your ProBuilder Mesh to the Unity .asset format and re-import it to use as a Prefab or spawn it in the scene. Caution: When you export a ProBuilder Mesh to file, remember that if you want to re-import and use ProBuilder tools and actions on it, you need to ProBuilderize first. This section provides information on which formats are available, how to export ProBuilder objects to file, and also some tips for re-importing back into Unity. Supported formats ProBuilder allows you to export GameObjects to the following formats: Format: Description: OBJ Wavefront OBJ. This is a widely supported model format. It supports multiple Textures and Mesh groups. STL A widely supported format, generally used in CAD software or 3D printing. It only supports Triangle geometry. PLY Stanford PLY. Generally supported and very extensible. It supports quads and vertex colors, but not multiple Materials. Asset Unity Asset format, only readable in Unity. Exporting a ProBuilder Mesh To export one or more objects to one of the supported formats: Select the object(s) you want to export. Alt/Opt+Click over the Export button. The Export Options appear. Select the format you want to export to from the Export Format drop-down menu. The other Export options change depending on which format you selected. If you're exporting an OBJ to use in Unity, disable the Copy Textures option. Tip: When you re-import the Mesh, follow the instructions under Re-importing an exported Mesh. When you are finished setting the Export options, click Export. Use the file browser to save the exported 3D Model. Re-importing an exported Mesh When you import the OBJ format, select the 3D Model file from the Project view, and on the Material tab of the Model Import Settings window, set the following options: Choose Use Embedded Materials from the Location drop-down menu. Inside the Remapped Materials section, expand the On Demand Remap option group. Choose From Model's Material from the Naming drop-down menu. Choose Project-Wide from the Search drop-down menu."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-materials.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-materials.html",
    "title": "Creating and Applying Materials | FSM Unity Framework",
    "keywords": "Creating and Applying Materials Use the Material Editor to easily manage Materials you are using in the scene. You can carry out multiple editing tasks, including the following: Setting a quick Material and then using a shortcut to apply it to the selected face(s). Assigning up to 10 Materials to shortcuts and buttons in the editor so you can easily apply multiple Materials to faces without having to use the Project view to find them. Defining extra Material slots in your Palette so that you can manage extra Materials at once. Saving your Material arrangement in a new Material Palette so that you can find it easily and re-use it in other scenes and projects. Loading a previously saved Material Palette into the Material Editor. The Materials you use in the Material Editor are standard Unity Materials. If you want to create your own standard Unity Material, follow the instructions in Creating a material asset and assigning a Unity shader to it in the Unity manual. For Texture Materials, follow the instructions in Creating a Texture Material. Note: If you are using either the Universal Render Pipeline (URP) or the High Definition Render Pipeline (HDRP), you also need to import the corresponding URP or HDRP Shaders needed to display vertex colors and textures correctly. For more information, see Support for scriptable render pipelines. Creating a Texture Material Before you create the Material, make sure you have a 2-dimensional image file (BMP, JPG, PNG, TIF etc.). Then follow these instructions: Right-click in the Project view, in the folder you want to place the Material file, and select Create > Material from the context menu. Alternatively, you can use the main menu (Assets > Create > Material). Name the Material file. For example, you could describe the look or use of the Texture, such as \"BrickWall\" or \"CheckeredTile\". The Shader properties appear in the Inspector. Set a reference to the Texture file you want to use in the Albedo property. Now that you have a Texture Material, you can set it as the quick Material, add it to your Material Palette, and apply it to a whole object or only specific faces."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-texture-mapping.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-texture-mapping.html",
    "title": "Materials, Shaders, Textures, and UVs | FSM Unity Framework",
    "keywords": "Materials, Shaders, Textures, and UVs In Unity, materials allow you to specify which shader to use on a Mesh. Shaders perform a set of calculations that tell Unity how to render (draw) your Meshes based on properties specific to that shader. You can apply materials to make your floor look like it's made of tile, wood, stone, or anything else that you want. ProBuilder allows you to apply a material to the entire Mesh, or only on selected faces. This allows you to provide more realistic-looking surfaces during game play or while grey-boxing. For example, you might decide to use tiles on the floor, brick on some walls, and stone on others. Some materials use Textures, which are bitmap images (for example, Unity can import BMP, JPEG, PNG, and most standard 2-dimensional image formats). Unity projects these images on the surfaces of your Mesh to achieve a more realistic result. The Mesh stores the texture mapping data as UVs. These are 2D fold-outs of the actual 3D Mesh, as if you peeled back the skin of an object and laid it out flat, like the image below. UV coordinates (also sometimes called texture coordinates) are references to specific locations on the image. They only use two dimensions (u,v). Texture mapping is the list of 2D UV coordinates mapped to their 3D vertex counterparts on the surface in three dimensions (x,y,z). This mapping tells Unity exactly how and where to project the image on the Mesh. Note: For more in-depth information on these concepts and the relationship between rendering and GameObjects in Unity, see Materials, Shaders & Textures in the Unity User Manual. The Smooth Group editor lets you smooth the seams on portions of your Meshes and leaves others sharper. This can be very useful when creating organic Meshes, such as terrain objects. For Materials containing Textures, you can use ProBuilder's texture mapping to fine-tune the appearance. For example, if you apply a brick material to a wall, you can adjust the offset, rotation, and tiling of the object's UVs. This image shows the Textures before adjustments on the left and after adjustments on the right. On the right, the brick Texture is scaled down and tiled on the wall Mesh, and the wooden Texture is rotated to match the alignment of the signpost Mesh, so that the grain appears to be moving in a natural direction. The grass Texture on the right has been repositioned and smoothed."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-uvs.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-uvs.html",
    "title": "Mapping Textures with UVs | FSM Unity Framework",
    "keywords": "Mapping Textures with UVs To give your walls, doors, floors, or other objects a realistic look, use a Texture material, which is a special material with a 2-dimensional image defined on it. You can create it yourself or download one from the Asset Store. After you apply the Texture material, open the UV Editor to fine-tune the look of the Texture on your Mesh and adjust its appearance. The UV Editor window provides several texture mapping helpers for getting the position, orientation, and size correct. Tip: Before you start to manually edit UVs, delete any hidden faces. This helps simplify the UV unwrapping process, because it simplifies the complexity of the UV elements that appear in the UV Editor. Manipulating UV elements in the UV Editor You can manipulate elements with the ProBuilder edit modes in the UV Editor, but you are actually moving UI elements rather than geometry. Also, when the UV Editor window is open, you can still manipulate elements in the Scene view: if you turn on the Scope control, you are moving geometry; if you turn it off (), you are moving UVs. You can translate, rotate, and scale elements to move them into place: Translating allows you to align the UVs to the geometry. For example, if you have a wall with a window, you might need to move the Texture image so that the window image appears at the right height on your wall. Rotating turns the image being projected on the skin of the Mesh. For example, if you have an object that is made of wood, you need to line up the grain of the wood in the Texture image with the geometry in a way that makes it look natural. Scaling changes the size of the projection, so when you scale the UV elements down, a smaller portion of the image appears on the Mesh. For example, you might have a barrel image that contains the sides of the barrel on one half and the lid and bottom on the other half. Tip: If you hold down the Ctrl/Cmd key while translating in the UV Editor window, the UV element snaps to the grid. Auto UV mode features ProBuilder's Auto UV mode provides some basic settings and then uses that information to project the Texture image on the Mesh automatically. You can determine how ProBuilder should treat the image: repeat it, enlarge it, or distort it to fill the space. You can also use the transformation controls to translate, rotate, and scale UV elements, or use precise values with the Offset (Translate), Rotation, and Tiling (Scale) values. There are also a couple of shortcuts to flip the UVs. Tip: If something goes wrong with your UV mapping, and using Undo a few times doesn't fix the problem, you can always reset the UVs on a specific face and start again. To reset the UVs, select the face(s) you want to fix, navigate to the Auto UV mode Actions panel, and click the Reset UVs button. After you specify these guidelines, ProBuilder keeps the image projections consistent. Even if you change the geometry, you don't have to adjust or reposition the UVs. Continuous tiling When you work in Auto UV mode, you can designate several faces as a Texture Group. The faces inside a Texture Group behave as if they are one face for tiling. To make a Texture Group In the ProBuilder window, open the UV Editor. Select the faces you want to group, either from inside the UV Editor or in the Scene view. Click the Group Selected Faces button. In the UV Editor, the UV faces now move as one. To add a face to an existing group This procedure is the only way to add a face to an existing group without having to break the group and reform it from scratch. If you select the group and the new face and click Group Selected Faces, ProBuilder creates a new group containing the selection. Note the Texture Group ID under the Texture Groups section. Select the face(s) you want to add to the group. If this face already belongs to a group, a different number appears in the Texture Group text box. Otherwise, the default number (0) appears to indicate it is not assigned to any Texture Group. Enter the ID number of the desired group in the Texture Group ID text box. Note: This does not add the rest of the Texture Group to the selection. To select all the faces in a Texture Group Select any of the faces that belongs to the group. Click the Select Texture Group button. To remove a Texture Group This procedure removes any group that any of the selected face(s) belongs to. Select any of the faces that belongs to the group. Click the Break Selected Groups button. Manual UV mode features Any time you modify edges or vertices in the UV Editor, you enter the Manual UV mode. You can also select the face(s) you want to set as manual, and click the Convert to Manual button instead. The Actions panel provides a completely different set of options, and the UV Editor displays manual faces as yellow or orange in the UV Editor, whereas automatic faces appear turquoise or blue. To start manually editing UVs, choose between the box and planar projection modes. This difference between these is that planar projects the image from a single point across all faces, while the box method uses a planar projection for each face. When you've done this, you can use the transform tools to move, rotate, and scale UV elements, with the auto-snapping (Ctrl/Cmd+translate) shortcut combination where needed. Fit UVs shrinks or grows your UV face uniformly. You can also move faces into place, and then select either the vertices or edges and weld them together to make an island. If you need to change the shape of the UV face, move the vertices and edges directly. Alternatively, you can use the auto-stitch method to make ProBuilder build more islands. If you need to break them apart again, select each face you want to break off, then click the Split UVs button. Now you can move them around independently. You can temporarily change the pivot location; to do this, click and drag it around in the UV Editor. The pivot is the reference point for scaling and rotating. Changing the pivot location is only a temporary change: if you click something else and then re-select it, it snaps back to the center. To mirror the UV mapping on a face, you can flip the UVs either horizontally or vertically. You can also copy the UV mapping from one face to another. Tip: If something goes wrong with your UV mapping and using Undo a few times doesn't help, you can always reset the UVs on a specific face and start again. To reset the UVs, select the face(s) you want to fix, switch to Auto UV mode, navigate to the Auto UV mode Actions panel, and click the Reset UVs button. Common texture mapping tasks This section describes some of the most common ways you can use ProBuilder for texture-mapping. There any many more possibilities, but these should give you a good place to start developing your own workflow. Wooden beams: Select the UV face(s) and rotate them in Auto UV mode until the grain goes down the length of the beam. With the UV Editor open, you can select all faces at once in the Scene view by double-clicking on any face on the object. Stone or tile pathway: Depending on the length and crookedness of your pathway, you could: Autostitch the pieces together to create one large unwrapped UV map and then apply the stone or tile Material with a Planar projection. Select a face, grow the selection until all of the pathway is selected, and then apply the material. If needed, you can then tweak the UV alignment on the selected UV faces. You can also select a group of faces and tweak the placement in either mode to get the final look correct. Brick or stone walls: For objects that you want to make continuous, like walls, use the Auto UV mode. Start by selecting one of the faces on the side of the wall and grow the selection. Then you can scale the UV faces up so that the pattern tiles more (for smaller bricks), or down to tile less (bigger bricks). To tile across adjacent faces perfectly, group the faces together. Avoid including the corners, because they can create a stretch effect. If you prefer to use manual editing, you could use autostitching; start from the sides all around, and then finally add the tops. This makes the tiling around the corners look more natural. However, because this is the Manual UV mode, you have to re-position the UVs if you resize the geometry. Wooden barrels: You can use a Texture that uses different portions of the same image file to define the top, bottom, and sides. First, autostitch all the side faces together. This automatically changes the autostitched faces to use Manual UV mode. Next, you can move those faces by translating, and if you hold the Shift key while translating, the UVs snap to the grid. Scale up or down to fit the height exactly. You don't have to worry about the width, because the image wraps around the faces that are off the image. Finally, you can use the same methods to align and scale the barrel's top and bottom so that it appears exactly in the right place for the barrel image. Wooden crate: Add a heightmap to your Texture to give the illusion of extra geometry. You can set up one side and then copy those UV settings over to other faces. The Manual UV mode is best for objects like these. Finding more inspiration Use the Unity ProBuilder channel to find videos that demonstrate how to use these features effectively: ProBuilder Simple Objects - Crates and Barrels Prototyping a \"Medieval House\" in Unity with ProBuilder3D ProBuilder Simple Texturing with AutoUVs ProBuilder Advanced Texturing with Manual UVs ProBuilder Tutorial 5: Texturing Part I - Materials and Vertex Colors (v2.3, Unity) ProBuilder Tutorial 6: Texturing Part II - UV Editing and Unwrapping (v2.3, Unity) Creating a UV template ProBuilder can render out a UV template that allows you to open it in an image editing program and customize your Texture for the shape you need. Open the UV Editor. Click the Camera icon (). Save UV Image window pops up You can customize the appearance of the template, such as the color of the template lines or the size of the image file. Click Save UV Template and choose where you want it to save the file. ProBuilder creates a PNG file displaying a black background and lines etching the shape of the template. Now you can open it in an image editing program and create or refine the Texture image in exactly the right dimensions."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-vertexcolors.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-vertexcolors.html",
    "title": "Setting vertex colors | FSM Unity Framework",
    "keywords": "Setting vertex colors Applying vertex colors is a great way to colorize levels for prototyping, team layout, zones, and more. You can apply unique vertex colors to faces or objects in order to easily identify where they begin and end. You can also apply vertex colors to vertices and edges for visual effects. Warning: Not all shaders display vertex colors on a Mesh. However, you can see vertex colors on your Meshes as long as you use a Material that supports vertex colors (like the default ProBuilder Material). Editing modes Depending on what editing mode you are in and what you select on your Mesh, the vertex colors appear differently. For example, if you select a single vertex or edge, the color you apply is intense on that element and fades outward from it. However, if you select a face or the entire Mesh object, the color covers the face or Mesh evenly: Applying a color To apply a vertex color: Select the Editing mode you want from the Edit mode toolbar. Select the object(s) or element(s) that you want to apply a color to. Click the Vertex Colors button on the ProBuilder toolbar. The Vertex Colors window appears. Click the Apply button next to the color you want to apply. Alternatively, you can use the shortcut displayed on the button (Alt/Opt+Shift+<number>) or select the preset from the ProBuilder menu (Tools > ProBuilder > Vertex Colors > Set Selected Faces to Preset <number>). Note: To remove a vertex color, apply the white vertex color (#FFFFFF)."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-walkthroughs.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflow-walkthroughs.html",
    "title": "Level walkthroughs | FSM Unity Framework",
    "keywords": "Level walkthroughs Perform a walkthrough of your level frequently while you build it. This allows you to decide whether the rooms are an appropriate size for your character. If not, go back and edit your buildings or start again. For example, you should enter Play mode as soon as you put up the basic walls and ceiling and then walk around in your level. By seeing things from the perspective of your character, you can make a better game because you know what it feels like to play. ProBuilder offers many ways to do similar tasks: it is up to you to decide how you like to work. For example, you can either build structures from shapes like they are building blocks or you can make big cubes, reverse the normals and treat the inner space as the room inside: Four separate Meshes stitched together Large box with top face extruded down and then deleted Large box with flipped normals"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflows.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Documentation~/workflows.html",
    "title": "ProBuilder workflows | FSM Unity Framework",
    "keywords": "ProBuilder workflows"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/External/KdTree/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/External/KdTree/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "The MIT License (MIT) Copyright (c) 2013 codeandcats Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/External/KdTree/README.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/External/KdTree/README.html",
    "title": "KdTree | FSM Unity Framework",
    "keywords": "KdTree A fast, generic, multi-dimensional Binary Search Tree written in C# Forked from codeandcats KdTree. Changes from KdTree This branch is modified to compile for .NET Framework 3.5, and removes the non-MIT licensed GeoUtils class."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.probuilder © 2023 Unity Technologies Licensed under the Unity Companion License for Unity-dependent projects (see https://unity3d.com/legal/licenses/unity_companion_license). Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/README.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/README.html",
    "title": "| FSM Unity Framework",
    "keywords": "Table of Contents About Setup API Overview License Third Party Licenses Contributing About ProBuilder is a 3D modeling plugin for Unity. This readme provides a brief introduction for developers interested in working with the API. For more information, the following guides are available: See the Manual for information about working with ProBuilder in the Unity Editor. See the Scripting Reference for API documentation. Working code samples are also available from the package repository under the Samples~ subfolder, or from the Package Manager you can import them directly into your Unity project. Development Unity provides ProBuilder as a package, distributed with the Package Manager. To start working with ProBuilder source, clone the repository into your Packages directory. ~/Desktop/MyProject$ cd Packages/ ~/Desktop/MyProject/Packages$ git clone https://github.com/Unity-Technologies/com.unity.probuilder Then you can install the cloned package directly in the Package Manager using the \"local\" method. For more information on installing local packages in the Package Manager, see the Package Manager documentation. API There are 3 major namespaces. Namespace Function UnityEngine.ProBuilder Mesh types and functions to compile meshes to Unity compatible assets. UnityEngine.ProBuilder.MeshOperations Mesh editing. UnityEditor.ProBuilder Editor integration. Mesh data is stored in a component (ProBuilderMesh) and compiled to a UnityEngine.Mesh (referred to as UMesh from here on) as necessary. ProBuilderMesh retains the following mesh information: Positions UVs Faces Triangles Material Smoothing group Auto/Manual UVs* Tangent (if user set) UV3/4 (if user set) Colors Shared vertices (also referred to as \"common vertices\") Normals, tangents, collisions, and UVs are calculated as necessary. *ProBuilder can automatically UV unwrap triangles on a per-face basis. Face has a toggle to enable or disable this feature (users are free to unwrap faces by manually as well). Modifying a ProBuilder mesh is a bit different from a Unity mesh. Instead of working with the MeshFilter.sharedMesh you'll instead be operating on the ProBuilder representation of the mesh: ProBuilderMesh. A typical workflow looks like this: // Create a new cube primitive var mesh = ShapeGenerator.CreateShape(ShapeType.Cube); // Extrude the first available face along it's normal direction by 1 meter. mesh.Extrude(new Face[] { mesh.faces.First() }, ExtrudeMethod.FaceNormal, 1f); // Apply the changes back to the `MeshFilter.sharedMesh`. // 1. ToMesh cleans the UnityEngine.Mesh and assigns vertices and sub-meshes. // 2. Refresh rebuilds generated mesh data, ie UVs, Tangents, Normals, etc. // 3. (Optional, Editor only) Optimize merges coincident vertices, and rebuilds lightmap UVs. mesh.ToMesh(); mesh.Refresh(); mesh.Optimize(); License Unity Companion License Third Party Notices Third Party Licenses Contributing All contributions are subject to the Unity Contribution Agreement(UCA). By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions. Pull Requests Please include an entry to the changelog for any PR, along with a Fogbugz ticket number if applicable. New logs should be placed under the ## [Unreleased] header at the top of the changelog. See Contributing for more details."
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Samples~/Runtime/Runtime Editing/README.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Samples~/Runtime/Runtime Editing/README.html",
    "title": "Important | FSM Unity Framework",
    "keywords": "Important By default, ProBuilder scripts are removed during the build process. To enable runtime editing in Standalone builds: Open the Preferences window Select the ProBuilder category Under General, un-check Script Stripping"
  },
  "Library/PackageCache/com.unity.probuilder@5.2.2/Third Party Notices.html": {
    "href": "Library/PackageCache/com.unity.probuilder@5.2.2/Third Party Notices.html",
    "title": "| FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the license(s) indicated below: Poly2Tri Component Name: Poly2Tri License Type: 3- Clause BSD Poly2Tri Copyright (c) 2009-2010, Poly2Tri Contributors http://code.google.com/p/poly2tri/ Link to component: https://github.com/greenm01/poly2tri https://github.com/MaulingMonkey/poly2tri-cs https://github.com/procore3d/poly2tri-cs All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of Poly2Tri nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. csg.js Component Name: csg.js License Type: MIT Copyright (c) 2011 Evan Wallace (http://madebyevan.com/) Link to component: https://github.com/evanw/csg.js Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Component Name: csg.js License Type: MIT Copyright (c) 2015 Karl Henkel Link to component: https://github.com/karl-/pb_CSG Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. KdTree Component Name: KdTree License Type: MIT Copyright (c) 2013 codeandcats Link to component: https://github.com/codeandcats/KdTree Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. pb_Stl Component Name: pb_Stl License Type: MIT Copyright (c) 2016 Karl Henkel Link to component: https://github.com/karl-/pb_Stl Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Unity Utilities Component Name: Unity Utilities License Type: MIT Copyright (c) 2016 Shawn White Link to component: https://github.com/CapnRat/Unity-Utilities Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [Unreleased] Version Updated The version number for this package has increased due to a version update of a related graphics package. [14.0.8] - 2023-09-27 This version is compatible with Unity 2022.3.11f1. Added Added callbacks when RenderPipeline is created or disposed. ObjectID Render Request that provides a render texture with the ObjectId of each pixel. Fixed Fixed potentially broken rendering and errors after renaming a VolumeProfile asset. Fixed popup showing multiple time when trying to remove additional data while in multi selection.. Removed some unexpected SRP changed callback invocations. Fixed Rendering Debugger runtime UI getting occluded by user UI with sorting order larger than 0. Fixed console errors when debug actions are removed from Input Manager during play mode. When building for Built-in, shaders from any SRP are completely stripped. Fixed occasional ArgumentOutOfRangeException in StaticLightingSky. [14.0.7] - 2023-05-23 This version is compatible with Unity 2022.2.22f1. Fixed Fixed Decal Projector Editor fields so they are now saved when editing a prefab. Revert Property for animation curves on Volume Components Fixed an IES Importer issue producing incorrect results. Crash on keywords::LocalKeywordState::ResetWithSpace when shader contains Grab Pass. Fixed SRPs not being able to build using mode -nographics and -batchmode, since lens flare global texture prevents this from happening. [14.0.6] - 2023-03-24 This version is compatible with Unity 2022.2.13f1. Fixed Fixed a Render Graph bug where culled passes would be delegated to releasing a resource, resulting in unwanted leaking. [14.0.5] - 2022-12-12 This version is compatible with Unity 2022.2.4f1. Changed Allow VolumeComponent BoolParameter UI to display enabled/disabled dropdown instead of checkboxes. Fixed Fixed Light Editor didn't apply changes to SerializedObject. Added Local mode to fit Probe Volumes to scene. Fixed recalculating of apv probe positions. Fixed virtual offset pushing probes outside of geometry. [14.0.4] - 2022-11-04 This version is compatible with Unity 2022.2.2f1. Added Extension method to fetch the Render Pipeline assets from a BuildTarget. New XRSystem API to allow SRPs override the XR built-in stereo matrices. Changed Improved performance of APV baking. Allow setting order for panels on the rendering debugger. Fixed Fixed the reset of APV volume placement when using multi selection. Fixed an issue so that APV dilated data not being written back to disk. Fixed realtime subdivision so it culls empty cells. Hid the warning on the reflection probe if you disable APV. Fixed so that data isn't cleared for probes to be dilated into, to avoid bright colored splotches. Fixed probes so that the max distance between then are respected. Fixed uninitialized memory for virtual offset. Fixed NaN when you bake high intensity lights. Fixed the APV touchup volume test so it uses OBB instead of AABB. Fixed null reference when you enable the Camera in a project with multiple SRPs installed. Volume Component Editor Foldouts states are now stored by type instead of by position. Fixed SerializedObjectNotCreatableException on Volume Component Editors. Fixed null reference exception when settings null Render Pipeline Global settings on the Settings provider. Fixed swapping Volume Component in a Volume profile with mixed pipeline Volume Components. Default Volume Profile can now be recovered when it is being deleted from the project folder. Fixed editor drawer for Value tuples in the Rendering Debugger. Fixed an issue where FreeCamera would print an error when using old InputSystem. Fixed missing subdivision label when looking at APV realtime subdivision preview. Fixed shadow cascade editor so the snatches now appear and the gradient appearance is improved. Fixed global probe volumes not fitting to all objects. Fixed dropdowns for multiple editors. [14.0.3] - 2021-05-09 This version is compatible with Unity 2022.2.0b15. Fixed Added Shader Stripping Watcher so you get notifications when a Shader Variant is stripped. [14.0.2] - 2021-02-04 This version is compatible with Unity 2022.2.0a14. Added Added new extension TryRemoveElementsInRange to remove a range of elements from a IList. Added error on ResourceReloader when attempting to use [ReloadGroup] on ScriptableObject. Added Screen Coordinates Override shader utilities. Added API to blend between baking states for Probe Volumes. Aded explicit control over scenario blending factor and a debug mode for visualization. Fixed Fixed texture gather macros for GLCore and moved them from target 4.6 to target 4.5. Fixed cubemap array macros for GLCore. Fixed regression on ResourceReloader due to change for supporting built-in resources. Fixed issue with debug markers in Unity Profiler in deep profiler mode [14.0.1] - 2021-12-07 Added Linear version of function that sets FSR RCAS shader constants DebugUI.ObjectPopupField to render a list of UnityEngine.Objects as a popup on the Rendering Debugger. Add probe volume influence weight parameter Added support for multiple Baking States to Prove Volumes. Hidding Volume Components not available for the current pipeline on the Volume Profile Inspector. Changed Volume Component editor are now specified by CustomEditorAttribute instead of VolumeComponentEditorAttribute. Fixed The Volume Panel on the Rendering Debugger was not corretly showing cameras when they were added or deleted. Fixed issue in DynamicResolutionHandler when camera request was turned off at runtime, the ScalableBufferManager would leak state and not unset DRS state (case 1383093). Fixed undo in for DebugUI.EnumFields on the rendering debugger. (case 1386964) Fixed DebugUI.Enum fields collapsing their parent DebugUI.Foldout Fixed IES profile importer handling of overflow (outside 0-1 range) of attenutation splines values. Fixed issue with Probe Volume Baking window incorrectly displaying the icon for probe volumes in scenes that don't contain probe volumes. Fixed unnecessary memory allocation inside FSR's RCAS shader constants helper function. Fixed the issue with the special Turkish i, when looking for the m_IsGlobal property in VolumeEditor. (case 1276892) [14.0.0] - 2021-11-17 Added Context menu on Volume Parameters to restore them to their default values. Fixed Fixed XR support in CoreUtils.DrawFullscreen function. Changed Removed FSR_ENABLE_16BIT option from FSRCommon.hlsl. The 16-bit FSR implementation is now automatically enabled when supported by the target platform. [13.1.2] - 2021-11-05 Added Added function to allocate RTHandles using RenderTextureDescriptor. Added vrUsage support for RTHandles allocation. Fixed Fixed issue when changing volume profiles at runtime with a script (case 1364256). Fixed XR support in CoreUtils.DrawFullscreen function. Fixed an issue causing Render Graph execution errors after a random amount of time. [13.1.1] - 2021-10-04 Added Added support for high performant unsafe (uint only) Radix, Merge and Insertion sort algorithms on CoreUnsafeUtils. Added DebugFrameTiming class that can be used by render pipelines to display CPU/GPU frame timings and bottlenecks in Rendering Debugger. Added new DebugUI widget types: ProgressBarValue and ValueTuple Added common support code for FSR. Added new RenderPipelineGlobalSettingsProvider to help adding a settings panel for editing global settings. Added blending for curves in post processing volumes. New extension for Render Pipeline Global Settings for shader variants settings -> IShaderVariantsSettings. [13.1.0] - 2021-09-24 Added Debug Panels Framework See IDebugDisplaySettingsQuery. Fixed Fixed keyword and float property upgrading in SpeedTree8MaterialUpgrader [13.0.0] - 2021-09-01 Version Updated The version number for this package has increased due to a version update of a related graphics package. Added New IVolumeDebugSettings interface and VolumeDebugSettings<T> class that stores the information for the Volumes Debug Panel. Added AMD FidelityFX shaders which were originally in HDRP Added support for high performant unsafe (uint only) Radix, Merge and Insertion sort algorithms on CoreUnsafeUtils. Fixed Fixed black pixel issue in AMD FidelityFX RCAS implementation Fixed a critical issue on android devices & lens flares. Accidentally creating a 16 bit texture was causing gpus not supporting them to fail. Fixed serialization of DebugStateFlags, the internal Enum was not being serialized. [12.0.0] - 2021-01-11 Added Support for the PlayStation 5 platform has been added. Support for additional properties for Volume Components without custom editor Added VolumeComponentMenuForRenderPipelineAttribute to specify a volume component only for certain RenderPipelines. Calculating correct rtHandleScale by considering the possible pixel rounding when DRS is on Support for the PlayStation 5 platform has been added. Support for the XboxSeries platform has been added. Added Editor window that allow showing an icon to browse the documentation New method DrawHeaders for VolumeComponentsEditors Unification of Material Editor Headers Scopes New API functions with no side effects in DynamicResolutionHandler, to retrieve resolved drs scale and to apply DRS on a size. Added helper for Volumes (Enable All Overrides, Disable All Overrides, Remove All Overrides). Added a blitter utility class. Moved from HDRP to RP core. Added a realtime 2D texture atlas utility classes. Moved from HDRP to RP core. New methods on CoreEditorDrawers, to allow adding a label on a group before rendering the internal drawers Method to generate a Texture2D of 1x1 with a plain color Red, Green, Blue Texture2D on CoreEditorStyles New API in DynamicResolutionHandler to handle multicamera rendering for hardware mode. Changing cameras and resetting scaling per camera should be safe. Added SpeedTree8MaterialUpgrader, which provides utilities for upgrading and importing SpeedTree 8 assets to scriptable render pipelines. Adding documentation links to Light Sections Support for Lens Flare Data Driven (from images and Procedural shapes), on HDRP New SRPLensFlareData Asset Adding documentation links to Light Sections. Added sampling noise to probe volume sampling position to hide seams between subdivision levels. Added DebugUI.Foldout.isHeader property to allow creating full-width header foldouts in Rendering Debugger. Added DebugUI.Flags.IsHidden to allow conditional display of widgets in Rendering Debugger. Added \"Expand/Collapse All\" buttons to Rendering Debugger window menu. Added mouse & touch input support for Rendering Debugger runtime UI, and fix problems when InputSystem package is used. Add automatic spaces to enum display names used in Rendering Debugger and add support for InspectorNameAttribute. Adding new API functions inside DynamicResolutionHandler to get mip bias. This allows dynamic resolution scaling applying a bias on the frame to improve on texture sampling detail. Added a reminder if the data of probe volume might be obsolete. Added new API function inside DynamicResolutionHandler and new settings in GlobalDynamicResolutionSettings to control low res transparency thresholds. This should help visuals when the screen percentage is too low. Added common include file for meta pass functionality (case 1211436) Added OverridablePropertyScope (for VolumeComponentEditor child class only) to handle the Additional Property, the override checkbox and disable display and decorator attributes in one scope. Added IndentLevelScope (for VolumeComponentEditor child class only) to handle indentation of the field and the checkbox. Added an option to change the visibilty of the Volumes Gizmos (Solid, Wireframe, Everything), available at Preferences > Core Render Pipeline Added class for drawing shadow cascades UnityEditor.Rendering.ShadowCascadeGUI.DrawShadowCascades. Added UNITY_PREV_MATRIX_M and UNITY_PREV_MATRIX_I_M shader macros to support instanced motion vector rendering Added new API to customize the rtHandleProperties of a particular RTHandle. This is a temporary work around to assist with viewport setup of Custom post process when dealing with DLSS or TAAU Added IAdditionalData interface to identify the additional datas on the core package. Added new API to draw color temperature for Lights. Fixed Help boxes with fix buttons do not crop the label. Fixed missing warning UI about Projector component being unsupported (case 1300327). Fixed the display name of a Volume Parameter when is defined the attribute InspectorName Calculating correct rtHandleScale by considering the possible pixel rounding when DRS is on Problem on domain reload of Volume Parameter Ranges and UI values Fixed Right Align of additional properties on Volume Components Editors Fixed normal bias field of reference volume being wrong until the profile UI was displayed. Fixed L2 for Probe Volumes. When adding Overrides to the Volume Profile, only show Volume Components from the current Pipeline. Fixed assertion on compression of L1 coefficients for Probe Volume. Explicit half precision not working even when Unified Shader Precision Model is enabled. Fixed ACES filter artefact due to half float error on some mobile platforms. Fixed issue displaying a warning of different probe reference volume profiles even when they are equivalent. Fixed missing increment/decrement controls from DebugUIIntField & DebugUIUIntField widget prefabs. Fixed IES Importer related to new API on core. Fixed a large, visible stretch ratio in a LensFlare Image thumbnail. Fixed Undo from script refreshing thumbnail. Fixed cropped thumbnail for Image with non-uniform scale and rotation Skip wind calculations for Speed Tree 8 when wind vector is zero (case 1343002) Fixed memory leak when changing SRP pipeline settings, and having the player in pause mode. Fixed alignment in Volume Components Virtual Texturing fallback texture sampling code correctly honors the enableGlobalMipBias when virtual texturing is disabled. Fixed LightAnchor too much error message, became a HelpBox on the Inspector. Fixed library function SurfaceGradientFromTriplanarProjection to match the mapping convention used in SampleUVMappingNormalInternal.hlsl and fix its description. Fixed Volume Gizmo size when rescaling parent GameObject Fixed rotation issue now all flare rotate on positive direction (1348570) Fixed error when change Lens Flare Element Count followed by undo (1346894) Fixed Lens Flare Thumbnails Fixed Lens Flare 'radialScreenAttenuationCurve invisible' Fixed Lens Flare rotation for Curve Distribution Fixed potentially conflicting runtime Rendering Debugger UI command by adding an option to disable runtime UI altogether (1345783). Fixed Lens Flare position for celestial at very far camera distances. It now locks correctly into the celestial position regardless of camera distance (1363291) Fixed issues caused by automatically added EventSystem component, required to support Rendering Debugger Runtime UI input. (1361901) Changed Improved the warning messages for Volumes and their Colliders. Changed Window/Render Pipeline/Render Pipeline Debug to Window/Analysis/Rendering Debugger Changed Window/Render Pipeline/Look Dev to Window/Analysis/Look Dev Changed Window/Render Pipeline/Render Graph Viewer to Window/Analysis/Render Graph Viewer Changed Window/Render Pipeline/Graphics Compositor to Window/Rendering/Graphics Compositor Volume Gizmo Color setting is now under Colors->Scene->Volume Gizmo Volume Gizmo alpha changed from 0.5 to 0.125 Moved Edit/Render Pipeline/Generate Shader Includes to Edit/Rendering/Generate Shader Includes Moved Assets/Create/LookDev/Environment Library to Assets/Create/Rendering/Environment Library (Look Dev) Changed Nintendo Switch specific half float fixes in color conversion routines to all platforms. Improved load asset time for probe volumes. ClearFlag.Depth does not implicitely clear stencil anymore. ClearFlag.Stencil added. The RTHandleSystem no longer requires a specific number of sample for MSAA textures. Number of samples can be chosen independently for all textures. Platform ShaderLibrary API headers now have a new macro layer for 2d texture sampling macros. This layer starts with PLATFORM_SAMPLE2D definition, and it gives the possibility of injecting sampling behavior on a render pipeline level. For example: being able to a global mip bias for temporal upscalers. Update icon for IES, LightAnchor and LensFlare LensFlare (SRP) can be now disabled per element LensFlare (SRP) tooltips now refer to meters. Serialize the Probe Volume asset as binary to improve footprint on disk and loading speed. LensFlare Element editor now have Thumbnail preview Improved IntegrateLDCharlie() to use uniform stratified sampling for faster convergence towards the ground truth DynamicResolutionHandler.GetScaledSize function now clamps, and never allows to return a size greater than its input. Removed DYNAMIC_RESOLUTION snippet on lens flare common shader. Its not necessary any more on HDRP, which simplifies the shader. Made occlusion Radius for lens flares in directional lights, be independant of the camera's far plane. [11.0.0] - 2020-10-21 Fixed Fixed the default background color for previews to use the original color. Fixed spacing between property fields on the Volume Component Editors. Fixed ALL/NONE to maintain the state on the Volume Component Editors. Fixed the selection of the Additional properties from ALL/NONE when the option \"Show additional properties\" is disabled Fixed ACES tonemaping for Nintendo Switch by forcing some shader color conversion functions to full float precision. Fixed a bug in FreeCamera which would only provide a speed boost for the first frame when pressing the Shfit key. Added New View Lighting Tool, a component which allow to setup light in the camera space New function in GeometryTools.hlsl to calculate triangle edge and full triangle culling. Several utils functions to access SphericalHarmonicsL2 in a more verbose and intuitive fashion. [10.2.0] - 2020-10-19 Version Updated The version number for this package has increased due to a version update of a related graphics package. [10.1.0] - 2020-10-12 Added Added context options \"Move to Top\", \"Move to Bottom\", \"Expand All\" and \"Collapse All\" for volume components. Added the support of input system V2 Fixed Fixed the scene view to scale correctly when hardware dynamic resolution is enabled (case 1158661) Fixed game view artifacts on resizing when hardware dynamic resolution was enabled Fixed issue that caused UNITY_REVERSED_Z and UNITY_UV_STARTS_AT_TOP being defined in platforms that don't support it. Changed LookDev menu item entry is now disabled if the current pipeline does not support it. [10.0.0] - 2019-06-10 Added Add rough version of ContextualMenuDispatcher to solve conflict amongst SRP. Add api documentation for TextureCombiner. Add tooltips in LookDev's toolbar. Add XRGraphicsAutomatedTests helper class. Fixed Fixed compile errors for platforms with no VR support Replaced reference to Lightweight Render Pipeline by Universal Render Pipeline in the package description Fixed LighProbes when using LookDev. Fix LookDev minimal window size. Fix object rotation at instentiation to keep the one in prefab or used in hierarchy. Fixed shader compile errors when trying to use tessellation shaders with PlayStation VR on PS4. Fixed shader compile errors about LODDitheringTransition not being supported in GLES2. Fix WaveIsFirstLane() to ignore helper lanes in fragment shaders on PS4. Fixed a bug where Unity would crash if you tried to remove a Camera component from a GameObject using the Inspector window, while other components dependended on the Camera component. Fixed errors due to the debug menu when enabling the new input system. Fix LookDev FPS manipulation in view Fix LookDev zoom being stuck when going near camera pivot position Fix LookDev manipulation in view non responsive if directly using an HDRI Fix LookDev behaviour when user delete the EnvironmentLibrary asset Fix LookDev SunPosition button position Fix LookDev EnvironmentLibrary tab when asset is deleted Fix LookDev used Cubemap when asset is deleted Fixed the definition of rcp() for GLES2. Fixed copy/pasting of Volume Components when loading a new scene Fix LookDev issue when adding a GameObject containing a Volume into the LookDev's view. Fixed duplicated entry for com.unity.modules.xr in the runtime asmdef file Fixed the texture curve being destroyed from another thread than main (case 1211754) Fixed unreachable code in TextureXR.useTexArray Fixed GC pressure caused by VolumeParameter<T>.GetHashCode() Fixed issue when LookDev window is opened and the CoreRP Package is updated to a newer version. Fix LookDev's camera button layout. Fix LookDev's layout vanishing on domain reload. Fixed issue with the shader TransformWorldToHClipDir function computing the wrong result. Fixed division by zero in V_SmithJointGGX function. Fixed null reference exception in LookDev when setting the SRP to one not implementing LookDev (case 1245086) Fix LookDev's undo/redo on EnvironmentLibrary (case 1234725) Fix a compil error on OpenGL ES2 in directional lightmap sampling shader code Fix hierarchicalbox gizmo outside facing check in symetry or homothety mode no longer move the center Fix artifacts on Adreno 630 GPUs when using ACES Tonemapping Fixed a null ref in the volume component list when there is no volume components in the project. Fixed issue with volume manager trying to access a null volume. HLSL codegen will work with C# file using both the GenerateHLSL and C# 7 features. Changed Restored usage of ENABLE_VR to fix compilation errors on some platforms. Only call SetDirty on an object when actually modifying it in SRP updater utility Set depthSlice to -1 by default on SetRenderTarget() to clear all slices of Texture2DArray by default. ResourceReloader will now add additional InvalidImport check while it cannot load due to AssetDatabase not available. Replaced calls to deprecated PlayerSettings.virtualRealitySupported property. Enable RWTexture2D, RWTexture2DArray, RWTexture3D in gles 3.1 Updated macros to be compatible with the new shader preprocessor. Updated shaders to be compatible with Microsoft's DXC. Changed CommandBufferPool.Get() to create an unnamed CommandBuffer. (No profiling markers) Deprecating VolumeComponentDeprecad, using HideInInspector or Obsolete instead [7.1.1] - 2019-09-05 Added Add separated debug mode in LookDev. Changed Replaced usage of ENABLE_VR in XRGraphics.cs by a version define (ENABLE_VR_MODULE) based on the presence of the built-in VR module ResourceReloader now works on non-public fields. Removed normalize from UnpackNormalRGB to match UnpackNormalAG. Fixed shadow routines compilation errors when \"real\" type is a typedef on \"half\". Removed debug menu in non development build. [7.0.1] - 2019-07-25 Fixed Fixed a precision issue with the ACES tonemapper on mobile platforms. [7.0.0] - 2019-07-17 Added First experimental version of the LookDev. Works with all SRP. Only branched on HDRP at the moment. LookDev out of experimental [6.7.0-preview] - 2019-05-16 [6.6.0] - 2019-04-01 Fixed Fixed compile errors in XRGraphics.cs when ENABLE_VR is not defined [6.5.0] - 2019-03-07 [6.4.0] - 2019-02-21 Added Enabled support for CBUFFER on OpenGL Core and OpenGL ES 3 backends. [6.3.0] - 2019-02-18 [6.2.0] - 2019-02-15 [6.1.0] - 2019-02-13 [6.0.0] - 2019-02-23 Fixed Fixed a typo in ERROR_ON_UNSUPPORTED_FUNCTION() that was causing the shader compiler to run out of memory in GLES2. [Case 1104271] (https://issuetracker.unity3d.com/issues/mobile-os-restarts-because-of-high-memory-usage-when-compiling-shaders-for-opengles2) [5.2.0] - 2018-11-27 [5.1.0] - 2018-11-19 Added Added a define for determining if any instancing path is taken. Changed The Core SRP package is no longer in preview. [5.0.0-preview] - 2018-10-18 Changed XRGraphicConfig has been changed from a read-write control of XRSettings to XRGraphics, a read-only accessor to XRSettings. This improves consistency of XR behavior between the legacy render pipeline and SRP. XRGraphics members have been renamed to match XRSettings, and XRGraphics has been modified to only contain accessors potentially useful to SRP You can now have up to 16 additional shadow-casting lights. Fixed LWRP no longer executes shadow passes when there are no visible shadow casters in a Scene. Previously, this made the Scene render as too dark, overall. [4.0.0-preview] - 2018-09-28 Added Space transform functions are now defined in ShaderLibrary/SpaceTransforms.hlsl. Changed Removed setting shader inclue path via old API, use package shader include paths [3.3.0] - 2018-01-01 [3.2.0] - 2018-01-01 [3.1.0] - 2018-01-01 Added Add PCSS shadow filter Added Core EditMode tests Added Core unsafe utilities Improvements Improved volume UI & styling Fixed CoreUtils.QuickSort infinite loop when two elements in the list are equals. Changed Moved root files into folders for easier maintenance"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Camera-Switcher.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Camera-Switcher.html",
    "title": "Camera Switcher | FSM Unity Framework",
    "keywords": "Camera Switcher The CameraSwitcher component allows you to define a List of Cameras in the Scene and then use the Debug Window to switch between them in Play Mode. This is useful when you want a set of different fixed views for profiling purposes where you need to guarantee that the Camera view is in the same position between sessions. Properties Property Description Cameras Drag and drop GameObjects that have a Camera component attached to add them to this List of Cameras. The Debug Window can switch between the Cameras in this List."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/lens-flare-data-driven-asset.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/lens-flare-data-driven-asset.html",
    "title": "Lens Flare (SRP) Asset | FSM Unity Framework",
    "keywords": "Lens Flare (SRP) Asset Unity’s Scriptable Render Pipeline (SRP) includes the Lens Flare Element asset. You can use this asset to create lens flares in your scene and control their appearance. To create a Lens Flare Element asset, navigate to Assets > Create > SRP Lens Flare. To use this asset, assign it to the Lens Flare Data property of an SRP Lens Flare Override Component. Properties The Lens Flare Element asset has the following properties: Type Image Circle Polygon Common AxisTransform Distortion Multiple Elements Uniform Curve Random Type Property Description Type Select the type of Lens Flare Element this asset creates: • Image • Circle • Polygon Image Property Description Flare Texture The Texture this lens flare element uses. Preserve Aspect Ratio Fixes the width and height (aspect ratio) of the Flare Texture. You can use Distortion to change this property. Circle Property Description Gradient Controls the offset of the circular flare's gradient. This value ranges from 0 to 1. Falloff Controls the falloff of the circular flare's gradient. This value ranges from 0 to 1, where 0 has no falloff between the tones and 1 creates a falloff that is spread evenly across the circle. Inverse Enable this property to reverse the direction of the gradient. Polygon Property Description Gradient Controls the offset of the polygon flare's gradient. This value ranges from 0 to 1. Falloff Controls the falloff of the polygon flare's gradient. This value ranges from 0 to 1, where 0 has no falloff between the tones and 1 creates a falloff that is spread evenly across the polygon. Side Count Determines how many sides the polygon flare has. Roundness Defines how smooth the edges of the polygon flare are. This value ranges from 0 to 1, where 0 is a sharp polygon and 1 is a circle. Inverse Enable this property to reverse the direction of the gradient Color Property Description Tint Changes the tint of the lens flare. If this asset is attached to the light, this property is based on the light tint. Modulate By Light Color Allows light color to affect this Lens Flare Element. This only applies when the asset is used in a SRP Lens Flare Override Component that is attached to a point, spot, or area light. Intensity Controls the intensity of this element. Blend Mode Select the blend mode of the Lens Flare Element this asset creates: • Additive • Screen • Premultiplied • Lerp Transform Property Description Position Offset Defines the offset of the lens flare's position in screen space, relative to its source. Auto Rotate Enable this property to automatically rotate the Lens Flare Texture relative to its angle on the screen. Unity uses the Auto Rotate angle to override the Rotation parameter. To ensure the Lens Flare can rotate, assign a value greater than 0 to the Starting Position property. Rotation Rotates the lens flare. This value operates in degrees of rotation. Size Use this to adjust the scale of this lens flare element. This property is not available when the Type is set to Image and Preserve Aspect Ratio is enabled. Scale The size of this lens flare element in world space. AxisTransform Property Description Starting Position Defines the starting position of the lens flare relative to its source. This value operates in screen space. Angular Offset Controls the angular offset of the lens flare, relative to its current position. This value operates in degrees of rotation. Translation Scale Limits the size of the lens flare offset. For example, values of (1, 0) create a horizontal lens flare, and (0, 1) create a vertical lens flare. You can also use this property to control how quickly the lens flare appears to move. For example, values of (0.5, 0.5) make the lens flare element appear to move at half the speed. Distortion Property Description Enable Set this property to True to enable distortion. Radial Edge Size Controls the size of the distortion effect from the edge of the screen. Radial Edge Curve Blends the distortion effect along a curve from the center of the screen to the edges of the screen. Relative To Center Set this value to True to make distortion relative to the center of the screen. Otherwise, distortion is relative to the screen position of the lens flare. Multiple Elements Property Description Enable Enable this to allow multiple lens flare elements in your scene. Count Determines the number of identical lens flare elements Unity generates. A value of 1 appears the same as a single lens flare element. Distribution Select the method that Unity uses to generate multiple lens flare elements: •Uniform •Curve •Random Length Spread Controls how spread out multiple lens flare elements appear. Relative To Center If true the distortion is relative to center of the screen otherwise relative to lensFlare source screen position. Uniform Property Description Colors The range of colors that this asset applies to the lens flares. Rotation The angle of rotation (in degrees) applied to each element incrementally. Curve Property Description Colors The range of colors that this asset applies to the lens flares. You can use the Position Spacing curve to determine how this range affects each lens flare. Position Variation Adjust this curve to change the placement of the lens flare elements in the Lens Spread. Rotation The uniform angle of rotation (in degrees) applied to each element distributed along the curve. This value ranges from -180° to 180°. Scale Adjust this curve to control the size range of the lens flare elements. Random Property Description Seed The base value that this asset uses to generate randomness. Intensity Variation Controls the variation of brightness across the lens flare elements. A high value can make some elements might invisible. Colors The range of colors that this asset applies to the lens flares. This property is based on the Seed value. Position Variation Controls the position of the lens flares. The X value is spread along the same axis as Length Spread. A value of 0 means there is no change in the lens flare position. The Y value is spread along the vertical screen space axis based on the Seed value. Rotation Variation Controls the rotation variation of the lens flares, based on the Seed value. The Rotation and Auto Rotate parameters inherit from this property. Scale Variation Controls the scale of the lens flares based on the Seed value."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/lens-flare-data-driven-component.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/lens-flare-data-driven-component.html",
    "title": "Lens Flare (SRP) Component | FSM Unity Framework",
    "keywords": "Lens Flare (SRP) Component Unity’s Scriptable Render Pipeline (SRP) includes the SRP Lens Flare Override component to control a Lens Flare (SRP) Data asset. You can attach an Lens Flare (SRP) Component to any GameObject. Some properties only appear when you attach this component to a light. Properties General Property Description Lens Flare Data Select the Lens Flare (SRP) Asset asset this component controls. Intensity Multiplies the intensity of the lens flare. Scale Multiplies the scale of the lens flare. Attenuation by Light Shape Enable this property to automatically change the appearance of the lens flare based on the type of light you attached this component to. For example, if this component is attached to a spot light and the camera is looking at this light from behind, the lens flare will not be visible. This property is only available when this component is attached to a light. Attenuation Distance The distance between the start and the end of the Attenuation Distance Curve. This value operates between 0 and 1 in world space. Attenuation Distance Curve Fades out the appearance of the lens flare over the distance between the GameObject this asset is attached to, and the Camera. Scale Distance The distance between the start and the end of the Scale Distance Curve. This value operates between 0 and 1 in world space. Scale Distance Curve Changes the size of the lens flare over the distance between the GameObject this asset is attached to, and the Camera. Screen Attenuation Curve Reduces the effect of the lens flare based on its distance from the edge of the screen. You can use this to display a lens flare at the edge of your screen Occlusion Property Description Enable Enable this property to partially obscure the lens flare based on the depth buffer Occlusion Radius Defines how far from the light source Unity occludes the lens flare. This value is in world space. Sample Count The number of random samples the CPU uses to generate the Occlusion Radius. Occlusion Offset Offsets the plane that the occlusion operates on. A higher value moves this plane closer to Camera. This value is in world space. For example, if a lens flare is inside the light bulb, you can use this to sample occlusion outside the light bulb. Occlusion Remap Curve Specifies the curve used to remap the occlusion of the flare. By default, the occlusion is linear, between 0 and 1. This can be specifically useful to occlude flare more drastically when behind clouds. Allow Off Screen Enable this property to allow lens flares outside the Camera's view to affect the current field of view."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/light-anchor.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Common/light-anchor.html",
    "title": "Light Anchor | FSM Unity Framework",
    "keywords": "Light Anchor You can use a Light Anchor to light a scene in Rendered Camera Space. To use a Light Anchor, you must connect it to a Light. Properties Property Description Orbit Use the left icon to control the Orbit of the light. This tool becomes green when you move the icon. Elevation Use the middle icon to control the Elevation of the light. This tool becomes blue when you move the icon. Roll Use the right icon to control the Rollof the light. This tool becomes gray when you move the icon. This is especially useful if the light has an IES or a Cookie. Distance Controls the distance between the light and its anchor in world space. Up Direction Defines the space of the up direction of the anchor. When this value is set to Local, the Up Direction is relative to the camera. Common Assigns a preset to the light component based on the behaviour of studio lights."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Free-Camera.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Free-Camera.html",
    "title": "Free Camera | FSM Unity Framework",
    "keywords": "Free Camera The FreeCamera component provides you with an implementation for a simple free camera. When you add this component to a Camera, you can use the keyboard and mouse, or a controller, to control the Camera's position and rotation in Play Mode. Properties Property Description Look Speed Controller Set the look speed of the Camera when using a controller. Look Speed Mouse Set the look speed of the Camera when using a mouse. Move Speed Set the speed at which the Camera moves. Move Speed Increment Set the value of the increment that you can increase or decrease the Move Speed by. This is useful if you have a large Scene and the current Move Speed is not enough to easily traverse it. Turbo Set the value that this component multiplies the Move Speed by when you press the key or button assigned to \"Fire1\"."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Look-Dev-Environment-Library.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Look-Dev-Environment-Library.html",
    "title": "Environment Library | FSM Unity Framework",
    "keywords": "Environment Library An Environment Library is an Asset that contains a list of environments that you can use in Look Dev to simulate different lighting conditions. Each environment uses an HDRI (High Dynamic Range Image) for its skybox and also includes properties that you can use to fine-tune the environment. Creating an Environment Library To create an Environment Library Asset, either: Select Assets > Create > Rendering Environment Library (Look Dev). Open Look Dev and click the New Library button. Creating and editing an environment After you create an Environment Library, you can add environments to it which you can then use in Look Dev. To create environments or edit their properties, use the Look Dev window itself. To create and edit environments, you need to open an Environment Library in Look Dev. To do this, either: Go to the Look Dev window (menu: Window > Rendering > Look Dev) and drag your Environment Library from your Project window into the sidebar. In your Project window, click on your Environment Library Asset. Then, in the Inspector, click the Open in LookDev window button. If you already have environments in the Environment Library, you can see a list of them in the sidebar. When you click on any of the HDRI previews for an environment, a box appears at the bottom of the Environment Library list. This contains the selected environment's properties for you to edit. To add, remove, or duplicate environments, use the toolbar at the bottom of the Environment Library list, which contains the following buttons. Button Function Description Add Click this button to add a new environment to the bottom of the list. Remove Click this button to remove the environment currently selected. Note that the environment that you have selected is the one with the blue frame. Duplicate Click this button to duplicate the currently selected environment and add it as a new environment to the bottom of the list. Properties Property Description Sky with Sun Set the HDRI Texture that Look Dev uses for the sky lighting when using this environment. For information on how to import an HDRI Texture, see Importing an HDRI Texture. Sky without Sun Set the HDRI Texture that Look Dev uses for compositing the shadows when simulating a sun in the view. If you do not assign this value, Look Dev uses a lower intensity version of the same HDRI Texture in Sky with Sun. For information on how to import an HDRI Texture, see Importing an HDRI Texture. Rotation Set the offset longitude that Look Dev applies for the whole sky and sun position. Exposure Set the exposure that Look Dev uses when it renders the environment. Sun Position Set the position of the sun when compositing the shadows. The Sun button at the end of the line automatically places the sun on the brightest spot of the Sky with Sun HDRI Texture. Shadow Tint Use the color picker to set the color of the tint that Look Dev uses to color shadows. Importing an HDRI Texture To import an HDRI Texture into the Unity Editor, load an .hdr or .exr file into your Unity Project like you would any other image. In the Texture Importer Inspector window, set Texture Type to Default, set Texture Shape to Cube, and set Convolution Type to None. When you want to test an HDRI Texture Asset or a skybox cube map Material, drag and drop it into the Look Dev view."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Look-Dev.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/Look-Dev.html",
    "title": "Look Dev | FSM Unity Framework",
    "keywords": "Look Dev Look Dev is an image-based lighting tool that contains a viewer for you to check and compare Assets to ensure they work well in various lighting conditions. Look Dev uses the Scriptable Render Pipeline, so it can display the Asset in the same way as it looks in your Scene. You can load Assets into Look Dev either as Prefabs or from the Hierarchy window. Look Dev is only available in Edit mode. The Look Dev window closes when you enter Play mode. Asset validation Asset validation confirms whether Assets are authored correctly and behave as expected in different lighting environments. You must use an HDRI (high dynamic range image) to validate your Assets in Look Dev. An HDRI contains real-world lighting with incredibly high detail. As such, it offers perfect lighting that is difficult to create by hand. By using such an accurate lighting environment to test an Asset, you can determine whether the Asset itself or your Project's lighting is reducing the visual quality of your Scene. You can load two different Assets into Look Dev at the same time and compare them in two viewports. For example, an Art Director can check that a new Asset matches the art direction guidelines of a reference Asset. Using Look Dev To open Look Dev in the Unity Editor, select Window > Rendering > Look Dev. The first time you use Look Dev, you must either create a new Environment Library or load one. For information on how to create an Environment Library, see the Environment Library documentation. Viewports By default, there is only one viewport in Look Dev, but you can choose from a selection of split-screen views (see the Multi-view section). Controls Navigation with the Look Dev Camera works in a similar way to the Scene view Camera: Rotate around pivot: Left click and drag (this is similar to the Scene view except that you need to press the Alt key for the Scene view Camera). Pan camera: Middle click and drag. Zoom: Alt + right click and drag. Forward/backward: Mouse wheel. First Person mode: Right click + W, A,S, and D. Loading Assets into Look Dev Look Dev lets you view: Prefabs - To load a Prefab into Look Dev, drag it from the Project window into the Look Dev viewport. GameObjects - To load a copy of a Hierarchy GameObject, drag the GameObject from the Hierarchy into the Look Dev viewport. Viewport modes Use the toolbar in the top-left of the window to change which viewing mode Look Dev uses. Single viewport By default, Look Dev displays a single viewport which contains the Prefab or GameObject you are working with. If you are in another viewing mode, you can click either the number 1 or number 2 button to go back to single view. Each button corresponds to a viewport in Look Dev. Select button 1 to use viewport 1, and button 2 to use viewport 2. Multi-viewport Use multiple viewports to compare different environments and settings for the same Asset. You can arrange viewports: Vertically side-by-side. Use this mode to compare two different lighting conditions on the same Asset to check that the Asset behaves correctly. Horizontally side-by-side. Use this mode to compare two different lighting conditions for horizontal objects, like an environment Asset, to check that the Asset behaves correctly. Split-screen. Use this mode investigate texture problems using a debug Shader mode (for example, use one screen to view Normal or Albedo shading, and the other for environment-lit mode). Side-by-side and split-screen: Use this mode to compare two different versions of the same Asset using the same lighting conditions to see which changes improve the Asset’s quality. All three of these modes are useful to compare two different versions of the same Asset using the same lighting conditions to see which changes improve the Asset’s quality. To load a different Prefab or Hierarchy GameObject into each split-screen view, drag and drop the Asset into the viewport that you want to view it in. When using multiple viewports, it only makes sense to compare different Prefabs or GameObjects when you want to look at two versions of the same Asset. Comparing completely different Assets doesn’t give you a good idea of the difference in lighting or visual effect. Vertical or horizontal side-by-side Vertical and horizontal side-by-side viewports show an identical view of your Asset. Split-screen In a split-screen view, there is a red/blue manipulation Gizmo that separates the two viewports. For information on how to use this Gizmo, see Using the manipulation Gizmo. Multi-viewport Camera By default, Look Dev synchronizes the camera movement for both views. To decouple the Cameras from one another, and manipulate them independently, click the Synchronized Cameras button in-between the two numbered Camera buttons. To align the cameras with each other, or reset them, click on the drop-down arrow next to the viewport 2 icon: Using the manipulation Gizmo The manipulation Gizmo represents the separation plane between the two viewports. It has different behavior in split-screen mode, but you use it in the same way for both side-by-side or split-screen modes. Moving the separator To move the separator, click and drag the straight line of the Gizmo to the location you want. Changing the orientation and length To change the orientation and length of the manipulator Gizmo, click and drag the circle at either end of the manipulator. Changing the length of the Gizmo lets you set the orientation and blending values more precisely. ) Changing the split in increments To change the split in increments, click and hold the circle on the end of the manipulation Gizmo, then hold Shift as you move the mouse. This snaps the manipulation Gizmo to set angles in increments of 22.5°, which is useful for a perfectly horizontal, vertical or diagonal angle. Blending The central white circle on the separator allows you to blend between the two views. Left click on it and drag along the red line to blend the left-hand view with the right-hand view. Drag along the blue line to blend the right-hand view with the left-hand view (as shown in the image below). The white circle automatically snaps back into the center when you drag it back. This helps you get back to the default blending value quickly. HDRI environments in Look Dev Lighting in Look Dev uses an HDRI. The Look Dev view allows you to manipulate and easily switch between HDRIs to simulate different environments for the Asset you are working on. Look Dev uses the Environment Library Asset to store a list of environments, which are HDRIs with extra properties that you can use to further refine the environment. For information on how to create, edit, and assign Environment Libraries, see the Environment Library documentation. Implementing Look Dev for your custom Scriptable Render Pipeline In order to use Look Dev in your custom Scriptable Render Pipeline, you must implement the UnityEngine.Rendering.LookDev.IDataProvider interface. Function Description void FirstInitScene(StageRuntimeInterface stage) Look Dev calls this function after it initializes the Scene with a Light and Camera. It uses this function to add and configure extra components according to the needs of your Scriptable Render Pipeline. void UpdateSky(Camera camera, Sky sky, StageRuntimeInterface stage) Look Dev uses this function to update the environment when you change something in Look Dev. You can handle the sky in various ways, so add code that corresponds to your Scriptable Render Pipeline. IEnumerable** ** supportedDebugModes { get; } Use this function to specify the list of supported debug modes. You do not need to add None because Look Dev handles that automatically. void UpdateDebugMode(int debugIndex) Use this function to update the debug mode based on what the user selects. The debugIndex matches the list in supportedDebugModes. If the user selects None, then the debugIndex is -1; void GetShadowMask(ref RenderTexture output, StageRuntimeInterface stage) This function computes a shadow map. The given StageRuntimeInterface contains access to the Camera and a Light simulating the sun."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "SRP Core What's new 12 13 Camera components Free Camera Camera Switcher Render Graph Benefits of the render graph system Render graph fundamentals Writing a Render Pipeline RTHandle system RTHandle fundamentals Using the RTHandle system Custom Material Inspector Adding properties in the menu Synchronizing shader code and C# Look Dev Environment Library Light Anchor"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/View-Lighting-Tool.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/View-Lighting-Tool.html",
    "title": "| FSM Unity Framework",
    "keywords": "Light Anchor The Light Anchor can help to place light sources around subjects, in relation to a Camera and an anchor point. It's particularly effective for cinematic lighting, which often requires multiple light sources orbiting a subject. Using the Light Anchor Component To add a Light Anchor component to a GameObject in your Scene: Select a Light GameObject in the hierarchy to open its Inspector window. Go to Add Component > Rendering > Light Anchor By default, the Anchor's position is the same as the position of the GameObject the Light Anchor Component is attached to. Note: To use the Light Anchor, you must set the Tag of at least one Camera to \"MainCamera\". Use the Orbit and Elevation to control the orientation of the light, in degrees, relative to the main Camera's and Anchor's positions. If the Light has a Cookie or an IES Profile, use the Roll to change their orientation. Use the Distance to control how far from the anchor, in meters, you want to place the Light. You can use the Anchor Position Override to provide a GameObject’s Transform as an anchor point for the Light. This is useful if you want the Light to follow a specific GameObject in the Scene. Note: The above example uses the Main Camera as the reference Camera that adjusts the light rotation. The Common presets might create a different result in the Scene View if your view isn't aligned with the Main Camera. You can set a Position Offset for this custom Anchor. This is useful if the Transform position of the custom Anchor isn't centered appropriately for the light to orbit correctly around the custom Anchor. The Light Anchor component also includes a list of Presets that you can use to set the Light's orientation relative to the main Camera. Properties Property Description Orbit Use the left icon to control the Orbit of the light. This tool becomes green when you move the icon. Elevation Use the middle icon to control the Elevation of the light. This tool becomes blue when you move the icon. Roll Use the right icon to control the Roll of the light. This tool becomes gray when you move the icon. This is useful if the light has an IES or a Cookie. Distance Controls the distance between the light and its anchor in world space. Up Direction Defines the space of the up direction of the anchor. When you set this value to Local, the Up Direction is relative to the Camera. Anchor Position Override Allows you to use a GameObject's Transform as anchor position instead of the LightAnchor's Transform. When the Transform of the GameObject you assigned to this property changes, the Light Anchor's Transform also changes. Common Assigns a preset to the light component based on the behavior of studio lights."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/adding-properties.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/adding-properties.html",
    "title": "Adding properties to the Core Render Pipeline settings section | FSM Unity Framework",
    "keywords": "Adding properties to the Core Render Pipeline settings section To add properties in the Core Render Pipeline settings section (Edit > Preferences > Core Render Pipeline), create a class that implements the interface ICoreRenderPipelinePreferencesProvider. For example: class MyPreference : ICoreRenderPipelinePreferencesProvider { class Styles { public static readonly GUIContent myBoolLabel = EditorGUIUtility.TrTextContent(\"My check box\", \"The description of the property.\"); } public List<string> keywords => new List<string>() {Styles.myBoolLabel.text}; public GUIContent header => EditorGUIUtility.TrTextContent(\"My property section\", \"The description of my property section.\"); public static bool s_MyBoolPreference; public void PreferenceGUI() { EditorGUI.BeginChangeCheck(); var newValue = EditorGUILayout.Toggle(Styles.myBoolLabel, s_MyBoolPreference); if (EditorGUI.EndChangeCheck()) { s_MyBoolPreference = newValue; } } } Unity shows the new properties in the Core Render Pipeline settings section:"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/custom-material-inspector.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/custom-material-inspector.html",
    "title": "Custom Material Inspector | FSM Unity Framework",
    "keywords": "Custom Material Inspector Custom Material Inspectors enable you to define how Unity displays properties in the Material Inspector for a particular shader. This is useful if a shader includes a lot of properties and you want to organize them in the Inspector. The Universal Render Pipeline (URP) and High Definition Render Pipeline (HDRP) both support custom Material Inspectors, but the method to create them is slightly different. Creating a custom Material Inspector The implementation for custom Material Inspectors differs between URP and HDRP. For example, for compatibility purposes, every custom Material Inspector in HDRP must inherit from HDShaderGUI which does not exist in URP. For information on how to create custom Material Inspectors for the respective render pipelines, see: HDRP: HDRP custom Material Inspectors. URP: Unity Custom Shader GUI. Assigning a custom Material Inspector When you create a shader, either hand-written or using Shader Graph, both URP and HDRP provide a default editor for it to use. To override this default and provide your own custom Material Inspector, the method differs depending on whether you hand-wrote the shader or used Shader Graph. Using hand-written shaders To set a custom Material Inspector for a hand-written shader: Open the shader source file. Assign a string that contains the class name of the custom Material Inspector to the CustomEditor shader instruction. This is the same method as for the Built-in Renderer's custom shader GUI. For an example of how to do this, see the following shader code sample. In this example, the name of the custom Material Inspector class is ExampleCustomMaterialInspector: Shader \"Custom/Example\" { Properties { // Shader properties } SubShader { // Shader code } CustomEditor \"ExampleCustomMaterialInspector\" } Using Shader Graph To set a custom Material Inspector for a Shader Graph shader: Open the Shader Graph. In the Graph Inspector, open the Graph Settings tab. If Active Targets does not include the render pipeline your project uses, click the plus button then, in the drop-down, click the render pipeline. In the render pipeline section (HDRP or URP depending on the render pipeline your project uses) find the Custom Editor GUI property and provide it the name of the custom Material Inspector."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/generating-shader-includes.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/generating-shader-includes.html",
    "title": "Synchronizing shader code and C# | FSM Unity Framework",
    "keywords": "Synchronizing shader code and C# Unity can generate HLSL code based on C# structs to synchronize data and constants between shaders and C#. In Unity, the process of generating the HLSL code from C# code is called generating shader includes. When Unity generates shader includes, it parses all the C# files in the project and, for every file that contains a struct with a GenerateHLSL attribute, generates corresponding HLSL code. It places this HLSL code in a file with the same name as the origin, but uses the .cs.hlsl file extension. Generating shader includes To generate an HLSL equivalent for a C# struct: Add the GenerateHLSL attribute to the struct. To do this, above the line that declares the struct, add [GenerateHLSL(PackingRules.Exact, false)]. For an example on how to do this, see the sample code below. For more information about the GenerateHLSL attribute, see the API documentation. In the Unity Editor, go to Edit > Render Pipeline > Generate Shader Includes. The following code example is from the High Definition Render Pipeline (HDRP). It shows an extract of the C# representation of a directional light. The original file is LightDefinition.cs. When Unity generates the HLSL shader code, it places it in a new file called LightDefinition.cs.hlsl. // LightDefinition.cs [GenerateHLSL(PackingRules.Exact, false)] struct DirectionalLightData { public Vector3 positionRWS; public uint lightLayers; public float lightDimmer; public float volumetricLightDimmer; // Replaces 'lightDimer' public Vector3 forward; public Vector4 surfaceTextureScaleOffset; }; // LightDefinition.cs.hlsl // Generated from UnityEngine.Rendering.HighDefinition.DirectionalLightData // PackingRules = Exact struct DirectionalLightData { float3 positionRWS; uint lightLayers; float lightDimmer; float volumetricLightDimmer; float3 forward; float4 surfaceTextureScaleOffset; };"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/index.html",
    "title": "SRP Core | FSM Unity Framework",
    "keywords": "SRP Core The Scriptable Render Pipeline (SRP) is a Unity feature that allows you to write C# scripts to control the way Unity renders each frame. SRP Core is a package that makes it easier to create or customize an SRP. SRP Core contains reusable code, including boilerplate code for working with platform-specific graphics APIs, utility functions for common rendering operations, and the shader libraries used in the High Definition Render Pipeline (HDRP) and Universal Render Pipeline (URP). If you are creating a custom SRP from scratch or customizing a prebuilt SRP, using SRP Core will save you time. For more information on SRP, including a guide to getting started with a custom SRP, see the SRP documentation. For more information on Unity's prebuilt SRPs, see the Universal Render Pipeline (URP) documentation, or the High Definition Render Pipeline (HDRP) documentation."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-benefits.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-benefits.html",
    "title": "Benefits of the render graph system | FSM Unity Framework",
    "keywords": "Benefits of the render graph system Efficient memory management When you manage resource allocation manually, you have to account for scenarios when every rendering feature is active at the same time and thus allocate for the worst-case scenario. When particular rendering features are not active, the resources to process them are there, but the render pipeline does not use them. A render graph only allocates resources that the frame actually uses. This reduces the memory footprint of the render pipeline and means that there is no need to create complicated logic to handle resource allocation. Another benefit of efficient memory management is that, because a render graph can reuse resources efficiently, there are more resources available to create features for your render pipeline. Automatic synchronization point generation Asynchronous compute queues can run in parallel to the regular graphic workload and, as a result, may reduce the overall GPU time it takes to process a render pipeline. However, it can be difficult to manually define and maintain synchronization points between an asynchronous compute queue and the regular graphics queue. A render graph automates this process and, using the high-level declaration of the render pipeline, generates correct synchronization points between the compute and graphics queue. Maintainability One of the most complex issues in render pipeline maintenance is the management of resources. Because a render graph manages resources internally, it makes it much easier to maintain your render pipeline. Using the RenderGraph API, you can write efficient self-contained rendering modules that declare their input and output explicitly and are able to plug in anywhere in a render pipeline."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-fundamentals.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-fundamentals.html",
    "title": "Render graph fundamentals | FSM Unity Framework",
    "keywords": "Render graph fundamentals This document describes the main principles of a render graph and an overview of how Unity executes it. Main principles Before you can write render passes with the RenderGraph API, you need to know the following foundational principles: You no longer handle resources directly and instead use render graph system-specific handles. All RenderGraph APIs use these handles to manipulate resources. The resource types a render graph manages are RTHandles, ComputeBuffers, and RendererLists. Actual resource references are only accessible within the execution code of a render pass. The framework requires an explicit declaration of render passes. Each render pass must state which resources it reads from and/or writes to. There is no persistence between each execution of a render graph. This means that the resources you create inside one execution of the render graph cannot carry over to the next execution. For resources that need persistence (from one frame to another for example), you can create them outside of a render graph, like regular resources, and import them in. They behave like any other render graph resource in terms of dependency tracking, but the graph does not handle their lifetime. A render graph mostly uses RTHandles for texture resources. This has a number of implications on how to write shader code and how to set them up. Resource Management The render graph system calculates the lifetime of each resource with the high-level representation of the whole frame. This means that when you create a resource via the RenderGraph API, the render graph system does not create the resource at that time. Instead, the API returns a handle that represents the resource, which you then use with all RenderGraph APIs. The render graph only creates the resource just before the first pass that needs to write it. In this case, “creating” does not necessarily mean that the render graph system allocates resources. Rather, it means that it provides the necessary memory to represent the resource so that it can use the resource during a render pass. In the same manner, it also releases the resource memory after the last pass that needs to read it. This way, the render graph system can reuse memory in the most efficient manner based on what you declare in your passes. If the render graph system does not execute a pass that requires a specific resource, then the system does not allocate the memory for the resource. Render graph execution overview Render graph execution is a three-step process that the render graph system completes, from scratch, every frame. This is because a graph can change dynamically from frame to frame, for example, depending on the actions of the user. Setup The first step is to set up all the render passes. This is where you declare all the render passes to execute and the resources each render pass uses. Compilation The second step is to compile the graph. During this step, the render graph system culls render passes if no other render pass uses their outputs. This allows for less organized setups because you can reduce specific logic when you set up the graph. A good example of that is debug render passes. If you declare a render pass that produces a debug output that you don't present to the back buffer, the render graph system culls that pass automatically. This step also calculates the lifetime of resources. This allows the render graph system to create and release resources in an efficient way as well as compute the proper synchronization points when it executes passes on the asynchronous compute pipeline. Execution Finally, execute the graph. The render graph system executes all render passes that it did not cull, in declaration order. Before each render pass, the render graph system creates the proper resources and releases them after the render pass if later render passes do not use them."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-system.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-system.html",
    "title": "The render graph system | FSM Unity Framework",
    "keywords": "The render graph system The render graph system sits on top of Unity's Scriptable Render Pipeline (SRP). It allows you to author a custom SRP in a maintainable and modular way. Unity's High Definition Render Pipeline (HDRP) uses the render graph system. You use the RenderGraph API to create a render graph. A render graph is a high-level representation of the custom SRP's render passes, which explicitly states how the render passes use resources. Describing render passes in this way has two benefits: it simplifies render pipeline configuration, and it allows the render graph system to efficiently manage parts of the render pipeline, which can result in improved runtime performance. For more information on the benefits of the render graph system, see benefits of the render graph system. To use the render graph system, you need to write your code in a different way to a regular custom SRP. For more information on how to write code for the render graph system, see writing a render pipeline. For information on the technical principles behind the render graph system, see render graph fundamentals. Note: Render graph is currently experimental which means Unity might change its API during future development. This section contains the following pages: Render graph benefits Render graph fundamentals Writing a render pipeline"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-writing-a-render-pipeline.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/render-graph-writing-a-render-pipeline.html",
    "title": "Writing a render pipeline | FSM Unity Framework",
    "keywords": "Writing a render pipeline This page covers the process of how to use the RenderGraph API to write a render pipeline. For information about the RenderGraph API, see render graph system and render graph fundamentals. Initialization and cleanup of Render Graph To begin, your render pipeline needs to maintain at least one instance of RenderGraph. This is the main entry point for the API. You can use more than one instance of a render graph, but be aware that Unity does not share resources across RenderGraph instances so for optimal memory usage, only use one instance. using UnityEngine.Experimental.Rendering.RenderGraphModule; public class MyRenderPipeline : RenderPipeline { RenderGraph m_RenderGraph; void InitializeRenderGraph() { m_RenderGraph = new RenderGraph(“MyRenderGraph”); } void CleanupRenderGraph() { m_RenderGraph.Cleanup(); m_RenderGraph = null; } } To initialize a RenderGraph instance, call the constructor with an optional name to identify the render graph. This also registers a render graph-specific panel in the SRP Debug window which is useful for debugging the RenderGraph instance. When you dispose of a render pipeline, call the Cleanup() method on the RenderGraph instance to properly free all the resources the render graph allocated. Starting a render graph Before you add any render passes to the render graph, you first need to initialize the render graph. To do this, call the RecordAndExecute method. This method will return a disposable struct of type RenderGraphExecution that you can use with a scope. When the RenderGraphExecution struct exits the scope or its Dispose function is called, the render graph is executed. This pattern ensures that the render graph is always executed correctly even in the case of an exception during the recording of the graph. For details about this method's parameters, see the API documentation var renderGraphParams = new RenderGraphExecuteParams() { scriptableRenderContext = renderContext, commandBuffer = cmd, currentFrameIndex = frameIndex }; using (m_RenderGraph.RecordAndExecute(renderGraphParams)) { // Add your passes here } Creating resources for the render graph When you use a render graph, you never directly allocate resources yourself. Instead, the RenderGraph instance handles the allocation and disposal of its own resources. To declare resources and use them in a render pass, you use render graph specific APIs that return handles to the resource. There are two main types of resources that a render graph uses: Internal resources: These resources are internal to a render graph execution and you cannot access them outside of the RenderGraph instance. You also cannot pass these resources from one execution of a graph to another. The render graph handles the lifetime of these resources. Imported resources: These usually come from outside the render graph execution. Typical examples are the back buffer (provided by the camera) or buffers that you want the graph to use across multiple frames for temporal effects (like using the camera color buffer for temporal anti-aliasing). You are responsible for handling the lifetime of these resources. After you create or import a resource, the render graph system represents it as a resource type-specific handle (TextureHandle, ComputeBufferHandle, or RendererListHandle). This way, the render graph can use internal and imported resources in the same way in all of its APIs. public TextureHandle RenderGraph.CreateTexture(in TextureDesc desc); public ComputeBufferHandle RenderGraph.CreateComputeBuffer(in ComputeBufferDesc desc) public RendererListHandle RenderGraph.CreateRendererList(in RendererListDesc desc); public TextureHandle RenderGraph.ImportTexture(RTHandle rt); public TextureHandle RenderGraph.ImportBackbuffer(RenderTargetIdentifier rt); public ComputeBufferHandle RenderGraph.ImportComputeBuffer(ComputeBuffer computeBuffer); The main ways to create resources are described above, but there are variations of these functions. For the complete list, see the API documentation. Note that the specific function to use to import the camera back buffer is RenderTargetIdentifier. To create resources, each API requires a descriptor structure as a parameter. The properties in these structures are similar to the properties in the resources they represent (respectively RTHandle, ComputeBuffer, and RendererLists). However, some properties are specific to render graph textures. Here are the most important ones: clearBuffer: This property tells the graph whether to clear the buffer when the graph creates it. This is how you should clear textures when using the render graph. This is important because a render graph pools resources, which means any pass that creates a texture might get an already existing one with undefined content. clearColor: This property stores the color to clear the buffer to, if applicable. There are also two notions specific to textures that a render graph exposes through the TextureDesc constructors: xrReady: This boolean indicates to the graph whether this texture is for XR rendering. If true, the render graph creates the texture as an array for rendering into each XR eye. dynamicResolution: This boolean indicates to the graph whether it needs to dynamically resize this texture when the application uses dynamic resolution. If false, the texture does not scale automatically. You can create resources outside render passes, inside the setup code for a render pass, but not in the rendering code. Creating a resource outside of all render passes can be useful for cases where the first pass uses a given resource that depends on logic in the code that might change regularly. In this case, you must create the resource before any of those passes. A good example is using the color buffer for either a deferred lighting pass or a forward lighting pass. Both of these passes write to the color buffer, but Unity only executes one of them depending on the current rendering path chosen for the camera. In this case, you would create the color buffer outside both passes and pass it to the correct one as a parameter. Creating a resource inside a render pass is usually for resources the render pass produces itself. For example, a blur pass requires an already existing input texture, but creates the output itself and returns it at the end of the render pass. Note that creating a resource like that does not allocate GPU memory every frame. Instead, the render graph system reuses pooled memory. In the context of the render graph, think of resource creation more in terms of data flow in the context of a render pass than actual allocation. If a render pass creates a whole new output then it “creates” a new texture in the render graph. Writing a render pass Before Unity can execute the render graph, you must declare all the render passes. You write a render pass in two parts: setup and rendering. Setup During setup, you declare the render pass and all the data it needs to execute. The render graph represents data by a class specific to the render pass that contains all the relevant properties. These can be regular C# constructs (struct, PoDs, etc) and render graph resource handles. This data structure is accessible during the actual rendering code. class MyRenderPassData { public float parameter; public TextureHandle inputTexture; public TextureHandle outputTexture; } After you define the pass data, you can then declare the render pass itself: using (var builder = renderGraph.AddRenderPass<MyRenderPassData>(\"My Render Pass\", out var passData)) { passData.parameter = 2.5f; passData.inputTexture = builder.ReadTexture(inputTexture); TextureHandle output = renderGraph.CreateTexture(new TextureDesc(Vector2.one, true, true) { colorFormat = GraphicsFormat.R8G8B8A8_UNorm, clearBuffer = true, clearColor = Color.black, name = \"Output\" }); passData.outputTexture = builder.WriteTexture(output); builder.SetRenderFunc(myFunc); // details below. } You define the render pass in the using scope around the AddRenderPass function. At the end of the scope, the render graph adds the render pass to the internal structures of the render graph for later processing. The builder variable is an instance of RenderGraphBuilder. This is the entry point to build the information relating to the render pass. There are several important parts to this: Declaring resource usage: This is one of the most important aspects of the RenderGraph API. Here you explicitly declare whether the render pass needs read and/or write access to the resources. This allows the render graph to have an overall view of the whole rendering frame and thus determine the best use of GPU memory and synchronization points between various render passes. Declaring the rendering function: This is the function in which you call graphics commands. It receives the pass data you define for the render pass as a parameter as well as the render graph context. You set the rendering function for a render pass via SetRenderFunc and the function runs after the graph compiles. Creating transient resources: Transient, or internal, resources are resources you create for the duration of this render pass only. You create them in the builder rather than the render graph itself to reflect their lifetime. Creating transient resources uses the same parameters as the equivalent function in the RenderGraph APIs. This is particularly useful when a pass uses temporary buffers that should not be accessible outside of the pass. Outside the pass where you declare a transient resource, the handle to the resource becomes invalid and Unity throws errors if you try to use it. The passData variable is an instance of the type you provide when you declare the pass. This is where you set the data that the rendering code can access. Note that the render graph does not use the contents of passData right away, but later in the frame, after it registers all the passes and the render graph compiles and executes. This means that any reference the passData stores must be constant across the whole frame. Otherwise, if you change the content before the render pass executes, it does not contain the correct content during the render pass. For this reason, it is best practice to only store value types in the passData unless you are certain that a reference stays constant until the pass finishes execution. For an overview of the RenderGraphBuilder APIs, see the below table. For more details, see the API documentation: Function Purpose TextureHandle ReadTexture(in TextureHandle input) Declares that the render pass reads from the input texture you pass into the function. TextureHandle WriteTexture(in TextureHandle input) Declares that the render pass writes to the input texture you pass into the function. TextureHandle UseColorBuffer(in TextureHandle input, int index) Same as WriteTexture but also automatically binds the texture as a render texture at the provided binding index at the beginning of the pass. TextureHandle UseDepthBuffer(in TextureHandle input, DepthAccess flags) Same as WriteTexture but also automatically binds the texture as a depth texture with the access flags you pass into the function. TextureHandle CreateTransientTexture(in TextureDesc desc) Create a transient texture. This texture exists for the duration of the pass. RendererListHandle UseRendererList(in RendererListHandle input) Declares that this render pass uses the Renderer List you pass in. The render pass uses the RendererList.Draw command to render the list. ComputeBufferHandle ReadComputeBuffer(in ComputeBufferHandle input) Declares that the render pass reads from the input ComputeBuffer you pass into the function. ComputeBufferHandle WriteComputeBuffer(in ComputeBufferHandle input) Declares that the render pass writes to the input Compute Buffer you pass into the function. ComputeBufferHandle CreateTransientComputeBuffer(in ComputeBufferDesc desc) Create a transient Compute Buffer. This texture exists for the duration of the Compute Buffer. void SetRenderFunc (RenderFunc renderFunc) where PassData : class, new() Set the rendering function for the render pass. void EnableAsyncCompute(bool value) Declares that the render pass runs on the asynchronous compute pipeline. void AllowPassCulling(bool value) Specifies whether Unity should cull the render pass (default is true). This can be useful when the render pass has side effects and you never want the render graph system to cull. Rendering Code After you complete the setup, you can declare the function to use for rendering via the SetRenderFunc method on the RenderGraphBuilder. The function you assign must use the following signature: delegate void RenderFunc<PassData>(PassData data, RenderGraphContext renderGraphContext) where PassData : class, new(); You can either pass a render function as a static function or a lambda. The benefit of using a lambda function is that it can bring better code clarity because the rendering code is next to the setup code. Note that if you use a lambda, be very careful not to capture any parameters from the main scope of the function as that generates garbage, which Unity later locates and frees during garbage collection. If you use Visual Studio and hover over the arrow =>, it tells you if the lambda captures anything from the scope. Avoid accessing members or member functions because using either captures this. The render function takes two parameters: PassData data: This data is of the type you pass in when you declare the render pass. This is where you can access the properties initialized during the setup phase and use them for the rendering code. RenderGraphContext renderGraphContext. This stores references to the ScriptableRenderContext and the CommandBuffer that provide utility functions and allow you to write rendering code. Accessing resources in the render pass Inside the rendering function, you can access all the render graph resource handles stored inside the passData. The conversion to actual resources is automatic so, whenever a function needs an RTHandle, a ComputeBuffer, or a RendererList, you can pass the handle and the render graph converts the handle to the actual resource implicitly. Note that doing such implicit conversion outside of a rendering function results in an exception. This exception occurs because, outside of rendering, the render graph may have not allocated those resources yet. Using the RenderGraphContext The RenderGraphContext provides various functionality you need to write rendering code. The two most important are the ScriptableRenderContext and the CommandBuffer, which you use to call all rendering commands. The RenderGraphContext also contains the RenderGraphObjectPool. This class helps you to manage temporary objects that you might need for rendering code. Get temp functions Two functions that are particularly useful during render passes are GetTempArray and GetTempMaterialPropertyBlock. T[] GetTempArray<T>(int size); MaterialPropertyBlock GetTempMaterialPropertyBlock(); GetTempArray returns a temporary array of type T and size size. This can be useful to allocate temporary arrays for passing parameters to materials or creating a RenderTargetIdentifier array to create multiple render target setups without the need to manage the array’s lifetime yourself. GetTempMaterialPropertyBlock returns a clean material property block that you can use to set up parameters for a Material. This is particularly important because more than one pass might use a material and each pass could use it with different parameters. Because the rendering code execution is deferred via command buffers, copying material property blocks into the command buffer is mandatory to preserve data integrity on execution. The render graph releases and pools all the resources these two functions return automatically after the pass execution. This means you don’t have to manage them yourself and does not create garbage. Example render pass The following code example contains a render pass with a setup and render function: TextureHandle MyRenderPass(RenderGraph renderGraph, TextureHandle inputTexture, float parameter, Material material) { using (var builder = renderGraph.AddRenderPass<MyRenderPassData>(\"My Render Pass\", out var passData)) { passData.parameter = parameter; passData.material = material; // Tells the graph that this pass will read inputTexture. passData.inputTexture = builder.ReadTexture(inputTexture); // Creates the output texture. TextureHandle output = renderGraph.CreateTexture(new TextureDesc(Vector2.one, true, true) { colorFormat = GraphicsFormat.R8G8B8A8_UNorm, clearBuffer = true, clearColor = Color.black, name = \"Output\" }); // Tells the graph that this pass will write this texture and needs to be set as render target 0. passData.outputTexture = builder.UseColorBuffer(output, 0); builder.SetRenderFunc( (MyRenderPassData data, RenderGraphContext ctx) => { // Render Target is already set via the use of UseColorBuffer above. // If builder.WriteTexture was used, you'd need to do something like that: // CoreUtils.SetRenderTarget(ctx.cmd, data.output); // Setup material for rendering var materialPropertyBlock = ctx.renderGraphPool.GetTempMaterialPropertyBlock(); materialPropertyBlock.SetTexture(\"_MainTexture\", data.input); materialPropertyBlock.SetFloat(\"_FloatParam\", data.parameter); CoreUtils.DrawFullScreen(ctx.cmd, data.material, materialPropertyBlock); }); return output; } } Ending the frame Over the course of your application, the render graph needs to allocate various resources. It might use these resources for a time but then might not need them. For the graph to free up those resources, call the EndFrame() method once a frame. This deallocates any resources that the render graph has not used since the last frame. This also executes all internal processing the render graph requires at the end of the frame. Note that you should only call this once per frame and after all the rendering is complete (for example, after the last camera renders). This is because different cameras might have different rendering paths and thus need different resources. Calling the purge after each camera could result in the render graph releasing resources too early even though they might be necessary for the next camera."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system-fundamentals.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system-fundamentals.html",
    "title": "| FSM Unity Framework",
    "keywords": "RTHandle system fundamentals This document describes the main principles of the RTHandle (RTHandle) system. The RTHandle system is an abstraction on top of Unity's RenderTexture API. It makes it trivial to reuse render textures across Cameras that use various resolutions. The following principles are the foundation of how the RTHandle system works: You no longer allocate render textures yourself with a fixed resolution. Instead, you declare a render texture using a scale related to the full screen at a given resolution. The RTHandle system allocates the texture only once for the whole render pipeline so that it can reuse it for different Cameras. There is now the concept of reference size. This is the resolution the application uses for rendering. It is your responsibility to declare it before the render pipeline renders every Camera at a particular resolution. For information on how to do this, see the Updating the RTHandle system section. Internally, the RTHandle system tracks the largest reference size you declare. It uses this as the actual size of render textures. The largest reference size is the maximum size. Every time you declare a new reference size for rendering, the RTHandle system checks if it is larger than the current recorded largest reference size. If it is, the RTHandle system reallocates all render textures internally to fit the new size and replaces the largest reference size with the new size. An example of this process is as follows. When you allocate the main color buffer, it uses a scale of 1 because it is a full-screen texture. You want to render it at the resolution of the screen. A downscaled buffer for a quarter-resolution transparency pass would use a scale of 0.5 for both the x-axis and y-axis. Internally the RTHandle system allocates render textures using the largest reference size multiplied by the scale you declare for the render texture. After that and before each Camera renders, you tell the system what the current reference size is. Based on that and the scaling factor for all textures, the RTHandle system determines if it needs to reallocate render textures. As mentioned above, if the new reference size is larger than the current largest reference size, the RTHandle system reallocates all render textures. By doing this, the RTHandle system ends up with a stable maximum resolution for all render textures, which is most likely the resolution of your main Camera. The key takeaway of this is that the actual resolution of the render textures is not necessarily the same as the current viewport: it can be bigger. This has implications when you write a renderer using RTHandles, which the Using the RTHandle system documentation explains. The RTHandleSystem also allows you to allocate textures with a fixed size. In this case, the RTHandle system never reallocates the texture. This allows you to use the RTHandle API consistently for both automatically-resized textures that the RTHandle system manages and regular fixed size textures that you manage."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system-using.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system-using.html",
    "title": "| FSM Unity Framework",
    "keywords": "Using the RTHandle system This page covers how to use the RTHandle system to manage render textures in your render pipeline. For information about the RTHandle system, see RTHandle system and RTHandle system fundamentals. Initializing the RTHandle System All operations related to RTHandles require an instance of the RTHandleSystem class. This class contains all the APIs necessary to allocate RTHandles, release RTHandles, and set the reference size for the frame. This means that you must create and maintain an instance of RTHandleSystem in your render pipeline or make use of the static RTHandles class mentioned later in this section. To create your own instance of RTHandleSystem, see the following code sample: RTHandleSystem m_RTHandleSystem = new RTHandleSystem(); m_RTHandleSystem.Initialize(Screen.width, Screen.height); When you initialize the system, you must supply the starting resolution. The above code example uses the width and height of the screen. Because the RTHandle system only reallocates render textures when a Camera requires a resolution larger than the current maximum size, the internal RTHandle resolution can only increase from the value you pass in here. It is good practice to initialize this resolution to be the resolution of the main display. This means the system does not need to unnecessarily reallocate the render textures (and cause unwanted memory spikes) at the beginning of the application. You must only call the Initialize function once at the beginning of the application. After this, you can use the initialized instance to allocate textures. Because you allocate the majority of RTHandles from the same RTHandleSystem instance, the RTHandle system also provides a default global instance through the RTHandles static class. Rather than maintain your own instance of RTHandleSystem, this allows you to use the same API that you get with an instance, but not worry about the lifetime of the instance. Using the static instance, the initialization becomes this: RTHandles.Initialize(Screen.width, Screen.height); The code examples in the rest of this page use the default global instance. Updating the RTHandle System Before rendering with a Camera, you need to set the resolution the RTHandle system uses as a reference size. To do so, call the SetReferenceSize function. RTHandles.SetReferenceSize(width, height); Calling this function has two effects: If the new reference size you provide is bigger than the current one, the RTHandle system reallocates all the render textures internally to match the new size. After that, the RTHandle system updates internal properties that set viewport and render texture scales for when the system uses RTHandles as active render textures. Allocating and releasing RTHandles After you initialize an instance of RTHandleSystem, whether this is your own instance or the static default instance, you can use it to allocate RTHandles. There are three main ways to allocate an RTHandle. They all use the same Alloc method on the RTHandleSystem instance. Most of the parameters of these functions are the same as the regular Unity RenderTexture ones, so for more information see the RenderTexture API documentation. This section focuses on the parameters that relate to the size of the RTHandle: Vector2 scaleFactor: This variant requires a constant 2D scale for width and height. The RTHandle system uses this to calculate the resolution of the texture against the maximum reference size. For example, a scale of (1.0f, 1.0f) generates a full-screen texture. A scale of (0.5f 0.5f) generates a quarter-resolution texture. ScaleFunc scaleFunc: For cases when you don't want to use a constant scale to calculate the size of an RTHandle, you can provide a functor that calculates the size of the texture. The functor should take a Vector2Int as a parameter, which is the maximum reference size, and return a Vector2Int, which represents the size you want the texture to be. int width, int height: This is for fixed-size textures. If you allocate a texture like this, it behaves like any regular RenderTexture. There are also overrides that create RTHandles from RenderTargetIdentifier. RenderTextures, or Textures. These are useful when you want to use the RTHandle API to interact with all your textures, even though the texture might not be an actual RTHandle. The following code sample contains example uses of the Alloc function: // Simple Scale RTHandle simpleScale = RTHandles.Alloc(Vector2.one, depthBufferBits: DepthBits.Depth32, dimension: TextureDimension.Tex2D, name: \"CameraDepthStencil\"); // Functor Vector2Int ComputeRTHandleSize(Vector2Int screenSize) { return DoSpecificResolutionComputation(screenSize); } RTHandle rtHandleUsingFunctor = RTHandles.Alloc(ComputeRTHandleSize, colorFormat: GraphicsFormat.R32_SFloat, dimension: TextureDimension.Tex2D); // Fixed size RTHandle fixedSize = RTHandles.Alloc(256, 256, colorFormat: GraphicsFormat.R8G8B8A8_UNorm, dimension: TextureDimension.Tex2D); When you no longer need a particular RTHandle, you can release it. To do this, call the Release method. myRTHandle.Release(); Using RTHandles After you allocate an RTHandle, you can use it exactly like a regular RenderTexture. There are implicit conversions to RenderTargetIdentifier and RenderTexture, which means you can use them with regular related Unity APIs. However, when you use the RTHandle system, the actual resolution of the RTHandle might be different from the current resolution. For example, if the main Camera renders at 1920x1080 and a secondary Camera renders at 512x512, all RTHandle resolutions are based on the 1920x1080 resolution, even when rendering at lower resolutions. Because of this, take care when you set an RTHandle up as a render target. There are a number of APIs available in the CoreUtils class to help you with this. For example: public static void SetRenderTarget(CommandBuffer cmd, RTHandle buffer, ClearFlag clearFlag, Color clearColor, int miplevel = 0, CubemapFace cubemapFace = CubemapFace.Unknown, int depthSlice = -1) This function sets the RTHandle as the active render target but also sets up the viewport based on the scale of the RTHandle and the current reference size, not the maximum size. For example, when the reference size is 512x512, even if the maximum size is 1920x1080, a texture of scale (1.0f, 1.0f) uses the 512x512 size and therefore sets up a 512x512 viewport. A (0.5f, 0.5f) scaled texture sets up a viewport of 256x256 and so on. This means that, when using these helper functions, the RTHandle system generates the correct viewport based on the RTHandle parameters. This example is one of many different overrides for the SetRenderTarget function. For the full list of overrides, see the documentation. Using RTHandles in shaders When you sample from a full-screen render texture in a shader in the usual way, UVs span the whole 0 to 1 range. This is not always the case with RTHandles. The current rendering might only occur in a partial viewport. To take this into account, you must apply a scale to UVs when you sample RTHandles that use a scale. All the information necessary to handle RTHandles specificity inside shaders is in the RTHandleProperties structure that the RTHandleSystem instance provides. To access it, use: RTHandleProperties rtHandleProperties = RTHandles.rtHandleProperties; This structure contains the following properties: public struct RTHandleProperties { public Vector2Int previousViewportSize; public Vector2Int previousRenderTargetSize; public Vector2Int currentViewportSize; public Vector2Int currentRenderTargetSize; public Vector4 rtHandleScale; } This structure provides: The current viewport size. This is the reference size you set for rendering. The current render target size. This is the actual size of the render texture based on the maximum reference size. The rtHandleScale. This is the scale to apply to full-screen UVs to sample an RTHandle. Values for previous frames are also available. For more information, see Camera specific RTHandles. Generally, the most important property in this structure is rtHandleScale. It allows you to scale full-screen UV coordinates and use the result to sample an RTHandle. For example: float2 scaledUVs = fullScreenUVs * rtHandleScale.xy; However, because the partial viewport always starts at (0, 0), when you use integer pixel coordinates within the viewport to load content from a texture, there is no need to rescale them. Another important thing to consider is that, when you render a full-screen quad into a partial viewport, there is no benefit from standard UV addressing mechanisms such as wrap or clamp. This is because the texture might be bigger than the viewport. For this reason, take care when you sample pixels outside of the viewport. Custom SRP specific information There are no shader constants provided by default with SRP. So, when you use RTHandles with your own SRP, you must provide these constants to their shaders themselves. Camera specific RTHandles Most of the render textures that a rendering loop uses can be shared by all Cameras. If their content does not need to carry from one frame to another, this is fine. However, some render textures need persistence. A good example of this is using the main color buffer in subsequent frames for Temporal Anti-aliasing. This means that the Camera cannot share its RTHandle with other Cameras. Most of the time, this also means that these RTHandles must be at least double-buffered (written to during the current frame, read from during the previous frame). To address this problem, the RTHandle system includes BufferedRTHandleSystems. A BufferedRTHandleSystem is an RTHandleSystem that can multi-buffer RTHandles. The principle is to identify a buffer by a unique ID and provide APIs to allocate a number of instances of the same buffer then retrieve them from previous frames. These are history buffers. Usually, you must allocate one BufferedRTHandleSystem for each Camera. Each one owns their Camera-specific RTHandles. Not every Camera needs history buffers. For example, if a Camera does not need Temporal Anti-aliasing, you do not need to assign a BufferedRTHandleSystem to it. History buffers require memory which means you can save memory by not assigning history buffers to Cameras that do not need them. Another consequence is that the system only allocates history buffers at the resolution of the Camera that the buffers are for. If the main Camera is 1920x1080 and another Camera renders in 256x256 and needs a history color buffer, the second Camera only uses a 256x256 buffer and not a 1920x1080 buffer as the non-Camera specific RTHandles would. To create an instance of a BufferedRTHandleSystem, see the following code sample: BufferedRTHandleSystem m_HistoryRTSystem = new BufferedRTHandleSystem(); To allocate an RTHandle using a BufferedRTHandleSystem, the process is different from a normal RTHandleSystem: public void AllocBuffer(int bufferId, Func<RTHandleSystem, int, RTHandle> allocator, int bufferCount); The bufferId is a unique ID that the system uses to identify the buffer. The allocator is a function you provide to allocate the RTHandles when needed (all instances are not allocated upfront), and the bufferCount is the number of instances requested. From there, you can retrieve each RTHandle by its ID and instance index like so: public RTHandle GetFrameRT(int bufferId, int frameIndex); The frame index is between zero and the number of buffers minus one. Zero always represents the current frame buffer, one the previous frame buffer, two the one before that, and so on. To release a buffered RTHandle, call the Release function on the BufferedRTHandleSystem, passing in the ID of the buffer to release: public void ReleaseBuffer(int bufferId); In the same way that you provide the reference size for regular RTHandleSystems, you must do this for each instance of BufferedRTHandleSystem. public void SwapAndSetReferenceSize(int width, int height); This works the same way as regular RTHandleSystem but it also swaps the buffers internally so that the 0 index for GetFrameRT still references the current frame buffer. This slightly different way of handling Camera-specific buffers also has implications when you write shader code. With a multi-buffered approach like this, RTHandles from a previous frame might have a different size to the one from the current frame. For example, this can happen with dynamic resolution or even when you resize the window in the Editor. This means that when you access a buffered RTHandle from a previous frame, you must scale it accordingly. The scale Unity uses to do this is in RTHandleProperties.rtHandleScale.zw. Unity uses this in exactly the same way as xy for regular RTHandles. This is also the reason why RTHandleProperties contains the viewport and resolution of the previous frame. It can be useful when doing computation with history buffers. Dynamic Resolution One of the byproducts of the RTHandle System design is that you can also use it to simulate software dynamic resolution. Because the current resolution of the Camera is not directly correlated to the actual render texture objects, you can provide any resolution you want at the beginning of the frame and all render textures scale accordingly. Reset Reference Size Sometimes, you might need to render to a higher resolution than normal for a short period of time. If your application does not require this resolution anymore, the additional memory allocated is wasted. To avoid that, you can reset the current maximum resolution of an RTHandleSystem like so: RTHandles.ResetReferenceSize(newWidth, newHeight); This forces the RTHandle system to reallocate all RTHandles to the new provided size. This is the only way to shrink the size of RTHandles."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/rthandle-system.html",
    "title": "The RTHandle system | FSM Unity Framework",
    "keywords": "The RTHandle system Render target management is an important part of any render pipeline. In a complicated render pipeline where there are many interdependent render passes that use many different render textures, it is important to have a maintainable and extendable system that allows for easy memory management. One of the biggest issues occurs when a render pipeline uses many different Cameras, each with their own resolution. For example, off-screen Cameras or real-time reflection probes. In this scenario, if the system allocated render textures independently for each Camera, the total amount of memory would increase to unmanageable levels. This is particularly bad for complex render pipelines that use many intermediate render textures. Unity can use temporary render textures, but unfortunately, they do not suit this kind of use case because temporary render textures can only reuse memory if a new render texture uses the exact same properties and resolution. This means that when rendering with two different resolutions, the total amount of memory Unity uses is the sum of all resolutions. To solve these issues with render texture memory allocation, Unity's Scriptable Render Pipeline includes the RTHandle system. This system is an abstraction layer on top of Unity's RenderTexture API that handles render texture management automatically. This section contains the following pages: RTHandle system fundamentals Using the RTHandle system"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new-12.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new-12.html",
    "title": "What's new in SRP Core version 12 / Unity 2021.2 | FSM Unity Framework",
    "keywords": "What's new in SRP Core version 12 / Unity 2021.2 This page contains an overview of new features, improvements, and issues resolved in version 12 of the Core Render Pipeline package, embedded in Unity 2021.2. Improvements RTHandle System and MSAA The RTHandle System no longer requires you to specify the number of MSAA samples at initialization time. This means that you can now set the number of samples on a per texture basis, rather than for the whole system. In practice, this means that the initialization APIs no longer require MSAA related parameters. The Alloc functions have replaced the enableMSAA parameter and enables you to explicitly set the number of samples. New API to disable runtime Rendering Debugger UI It is now possible to disable the Rendering Debugger UI at runtime by using DebugManager.enableRuntimeUI. Added High performance sorting algorithms in CoreUnsafeUtils New high performance sorting algorithms in the CoreUnsafeUtils helper methods. The new sorting algorithms include: RadixSort - ideal for very large lists, more then 512 elements. MergeSort (non recursive) - ideal for mid size lists, less than 512 elements. InsertionSort - ideal for very small lists, less than 32 elements. The sorting algorithms only work on uint elements. They include methods that support standard c# arrays, NativeArray objects or raw pointers. RadixSort and MergeSort require support array data, which can be allocated by the user, or allocated automatically via ref parameter passing. InsertionSort is in-place and does not require support data. These algorithms are compatible with burst kernels when using raw pointers or NativeArray. Currently HDRP utilizes them to sort lights in the CPU lightloop."
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new-13.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new-13.html",
    "title": "What's new in SRP Core version 13 / Unity 2022.1 | FSM Unity Framework",
    "keywords": "What's new in SRP Core version 13 / Unity 2022.1 This page contains an overview of new features, improvements, and issues resolved in version 13 of the Core Render Pipeline package, embedded in Unity 2022.1. Added AMD Fidelity FX Super Sampling helper API - FSRUtils Introducing new stream lined API for AMD Fidelity FX Super Sampling. The new API is located in the static class FSRUtils and allows scriptable pipelines to have direct access / implement and integrate easilty FSR super sampler. For more information please review the API located in Runtime/Utitilies/FSRUtils.cs"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/Documentation~/whats-new.html",
    "title": "What's new in SRP Core | FSM Unity Framework",
    "keywords": "What's new in SRP Core This section contains information about changes to SRP Core. Each page contains a list of new features and, if relevant, a list of improvements and a list of resolved issues. The list of pages is as follows: 12 13"
  },
  "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.core@14.0.9/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.render-pipelines.core copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [Unreleased] Version Updated The version number for this package has increased due to a version update of a related graphics package. Started Changelog"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/Documentation~/index.html",
    "title": "Universal Render Pipeline Configuration Package | FSM Unity Framework",
    "keywords": "Universal Render Pipeline Configuration Package The Universal Render Pipeline (URP) uses this package to control the settings of some of its features. If you want to use this package to configure URP, you must link it as a local package. For information on how to set up and use the URP Config package, see URP Config. For documentation on URP itself, see URP documentation."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal-config@14.0.9/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.render-pipelines.universal-config copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [Unreleased] Version Updated The version number for this package has increased due to a version update of a related graphics package. [14.0.8] - 2023-09-27 This version is compatible with Unity 2022.3.11f1. Fixed Fixed TAA resource leak on entering/exiting the playmode. Fixed rare iOS shader building failure due to URP Lit Forward Pass shader varyings struct variable mismatch Fixed an issue with broken documentation links. Stripped BlitHDROverlay from build if HDR output is not allowed and stripping unused shader is allowed. Fixed an issue where switching Volume Update modes between Every Frame and Via Scripting gave an error. Fixed, URP & core package leaking materials when entering/exiting Play Mode. Fixed for the UI being drawn twice in some scenarios. Fixed an issue where assets were incorrectly being saved when making builds. Fixed incorrect MSAA sample count when using Deferred renderer but rendering to a target texture. Fixed ShaderGraph preview window not showing anything when using DepthNormals pass. 2D - Remove serialization and cache vertices and indices for sprite lights causing bloat in prefabs. Changed the ScreenSpace Decals sorting criteria to None to fix flickering issues. Fixed redundant blit is used due to postFX, although it is disabled in rendererData. Support DOTS_INSTANCING in DebugReplacement shader. Fixed a bug where color space conversion is applied twice in URP in specific conditions (HDR Output and Debug HDR Views enabled). Fixed using RenderTextureSubElement.Stencil in URP not binding properly. Fixed error message in filtered view when decals are enabled. Fixed Screen space Overlay UI rendered at the wrong size for scaling mode \"Constant Pixel Size\" or \"Constant Physical Size\", when HDR output is active. Fixed WebGL1 throwing errors when using depth copy texture. Fixed typo in RenderSingleCamera obsolete message. 2D - Fixed additional draw call when Foremost Sorting Layer is enabled during unlit. Fixed removal of renderer features if a renderer feature is missing. Fixed data-driven lens-flare occlusion and y-flip on opengl. Fixed an issue where rendering layers keywords were not enabled correctly when using Decals & SSAO in Deferred. Fixed an issue where incorrect Shader Keyword Prefiltering was used with SSAO when AfterOpaque was used. Fixed Native RenderPass errors when using RenderingLayers. Fixed exception for missing _Color Shader Property. Fixed runtime performance drops when multiple views that uses incompatible RTHandle descriptors are rendered within a frame. Fixed an issue where Shader ID's weren't reset properly in the DepthNormals pass. Fixed black screen issue when using URP hardware Dynamic Resolution with DX12. Fixed Native RenderPass errors when using RendererFeature which is executed in between GBuffer and Deferred Lighting passes. Fixed color and depth mismatch when scaling is on. Fixed an issue where IndexOutOfRangeException was thrown when creating a stacked camera from script. Fixed an issue where NullReferenceException was thrown when camera prefab referenced a camera outside the prefab in the camera stack. Fixed an issue where settings would disappear when deleting a child Camera of the Main Camera. Fixed an issue where additional lights were not rendering correctly when using a mix of shadow settings in deferred. Added GBuffer (fill) passes to ComplexLit and Unlit shader to prevent GBuffer data holes. Fixed render texture memory leak when rtHandle realloc failed to be added to pool. Fixed an issue where it wasn't possible to add a Renderer Feature on a renderer if another feature had a missing/broken script. Fixed an issue with Screen Space Decals where dark artefacts appeared in the editor. Fixed an issue where reflection probes were not updating correctly when using Forward+. Fixed an issue causing 'implicit truncation of vector type' warning when using ShaderGraph shaders in the Forward+ Rendering Path. Fixed HDR Output can't be turned off via the HDROutputSettings API in the editor. Add Shader Keywords for Soft Shadow quality levels and disable per-light quality level on untethered XR platforms Fix IndexOutOfRangeException error when using Native RenderPass on Deferred Fixed an issue where selecting a stacked camera caused the editor to freeze and sometimes crash. Fixed TAA Very High option flicker. [14.0.7] - 2023-05-23 This version is compatible with Unity 2022.2.22f1. Changed The two subshaders in main URP shaders (Lit, SimpleLit, BakedLit, ComplexLit, Particles) have now been combined in to one. The Forward+ rendering path now supports XR rendering, and cameras using orthographic projection. Fixed Fixed Render Targets being released when using multiple cameras. Fixed the Screen flicker in Scene view. Increased lighting BRDF specular max for half float math (mobile) to match the visual look of full float math (desktop) better. Disabled MSAA on devices without MSAA store support (Apple GPUs A8 and lower). Fixed an issue where using the Reflection Probe Node with the Forward+ rendering path would result in flickering on the object. Fixed TAA resource leak on entering/exiting the playmode. Fixed rare iOS shader building failure due to URP Lit Forward Pass shader varyings struct variable mismatch [14.0.6] - 2023-03-24 This version is compatible with Unity 2022.2.13f1. Fixed Fixed ComplexLit mixed lighting by matching ComplexLit shader keywords with the Lit shader. Fixed an issue causing materials using Shader Graphs with material override to disappear when using the Deferred rendering path if alpha clipping is enabled in the material. Fixed y-flipped shading on gizmos in game view. Fixed a light cookie out of bounds. Fixed an issue causing Dynamic Resolution to be disabled during URP rendering. Fixed a missing keyword in ParticlesSimpleLit for Lightmap shadow mixing. Fixed Reflection Probe baking throwing errors with Render Scale set not to 1 Fixed issue where disabling/enabling ShadowCaster2Ds can create duplicate shadows. Fixed an issue so that deferred rendering now works correctly in builds with Accurate GBuffer Normals enabled. Fixed the 2D Sprite Light & Freeform Light fast normal map quality setting to correctly use the normal map. Fixed a bug with the shadow mesh bounds of ShadowCaster2D so that shadows no longer disappear. Fixed GC.Allocs with sorting layers in Light2D. Prevent negative color and NaN write to TAA history. Fixed an issue where scenes were not marked dirty after changing the volume update setting on cameras. Fixed resource leak in URP deferred. Added vertex SH option to URP rendering and fixed HL2 forward light perf regression. Fixed an issue where instantiating and destroying cameras, with Volume Update Mode set to ViaScripting, would allocate each time 2d - Fix null exception when adding a sorting layer Fixed an issue where main light shadows were incorrect if scene and game windows were open. [14.0.5] - 2022-12-12 This version is compatible with Unity 2022.2.4f1. Changed Shader parameters used by additional lights are now removed when additional lights are disabled in URP Assets. Fixed Set default contribution to 0 for ColorLookup VolumeComponent, which makes the interpolation with the implicit default global volume behave as expected. Fixed Full Screen Pass functionality when used with XR. RenderObjects Render Features now render correctly when injected after rendering. Fixed an issue in deferred rendering mode where the Material inspector would log errors about color and depth dimensions not matching. Fixed an issue where a ParticlesUnlit.mat warning appeared when creating a new material. Fixed an issue with slower build-times caused by large Additional Light Shadows arrays in URP Shaders. Fixed a bug where lights with different blend styles may have missing shadows. Fixed mixed lights when using deferred rendering and shadow mask. Fixed releasing render targets when using multiple renderers. Fixed an issue where the Universal Renderer could incorrectly clear the render target during the forward opaques pass even if the render target contains valid rendering data produced by a pass that ran before opaque rendering. [14.0.4] - 2022-11-04 This version is compatible with Unity 2022.2.2f1. Added UniversalRenderPipeline.SingleCameraRequest. Use this as the RequestData parameter in SubmitRenderRequest to render a single camera. Changed Adapted URP to use Blitter interface for full screen draws. Removed DRAW_PRCEDURAL variant for shaders. Factored out full screen blits to utility function. Updated terrain SSAO tests for DX11 and DX12 by using the reference images from Vulkan. Improved edge quality for alpha-clipped materials when multisampling is used in URP. Reduced the number of memcpy operations from NativeArray access in URP for performance. Added tooltips for upscaling filters. Added Screen space for the Transform node. Integrated Foveated Rendering into URP for supported platforms. SSAO: The samples field has been changed to a dropdown: High (12 samples), Medium (8 samples) and Low (4 samples). SSAO: The final After Opaque passes have now been merged with the last blur pass. SSAO: Downsampling will now not only affect the AO pass but also the blur passes. SSAO: Depth test improved to avoid incorrectly adding AO in places where two objects are far away from one another. Moved Volume Update Mode out of Additional Settings. Messages regarding reducing resolution for additional punctual lights are now only displayed in debug builds. Avoid using Depth32Stencil8 format on Android. Fixed Fixed a crash on standalone profiler executing the URP Upgrader. Fixed so objects don't disappear when using Depth Priming and Rendering Debugger. Improved fallback to single shadow cascade on GLES2. Fixed materials that use Autodesk Interactive shader to convert correctly. Fixed a shader compilation error on certain platforms. (URP-1415). Fixed incorrect output when post process is enabled in URP 2D. Fixed vertex color for sprite shapes in URP 2D. Fixed Light2D upgrading issue with m_AlphaBlendOnOverlap property. Fixed Gizmo and grid artifact in editor view URP: Fixed SSAO being flipped in after opaque. URP: Fixed Decals being flipped. Fixed an issue with Depth Priming when executing the DepthNormals prepass with MSAA on. Fixed Gizmos in Game View when using Viewports (UUM-7069). Fixed SpeedTree Shadergraph causes errors spammed in console. Fixed specular highlight edges on Android. Fixed depth pre-pass being always executed on GLES devices. Fixed incorrect light brightness when using SimpleLit shader. Fixed alpha discard on Unlit Sprite targets for Shadergraph. Fixed 2D Spot Light artifacts in light. Fixed additional light perf regression on Quest. Fixed memory leak issue in URP deferred when resizing preview camera window. Fixed an issue where camera UI inspector's clearFlag is not respected. Fixed issue where selecting the URP asset could break HDRP blitter when HDRP is the active pipeline. Fixed an issue that the Shaders now correctly fallback to error shader. Fixed excessive banding from FSR in URP. Fixed decals correctly handle last batch. Fixed decal msaa error then camera is selected in deferred rendering path. Fixed render scale correctly work with screen size property. This includes decals. Fixed decals to produce correct world to tangent matrix. Fixed decals to pass correct viewDirectionWS to screen space and gbuffer lighting. Fixed decal screen space to work without intermediate texture and DBuffer to force using intermediate texture. Fixed instacing error when decals loaded, but not the decal shaders. Display Stats is now always shown in the first position on the Rendering Debugger. Fixed wireframe view in URP (UUM-2548). 2D - Fixed incorrect blit material set during Pixel Perfect Upscale pass. Disabled depth priming on GLES when MSAA is enabled. Disabled depth priming when baking reflection probes. Fixed a resource leak when switching between scenes with different pipeline assets. Fixed missing Depth Copy texture in Scene view. Fixed soft shadow filtering quality when using large empty shadow atlas. Use allocated atlas size instead of requested size. Fixed light banding artifacts on normal maps. Fixed render scale with SMAA. Removed RenderSingleCamera is now obsolete. Please use RenderPipeline.SubmitRenderRequest with RequestData of the SingleCameraRequest type. Graphics: Camera.SubmitRenderRequests is now obsolete. Please use RenderPipeline.SubmitRenderRequest with RequestData of a supported type such as RenderPipeline.StandardRequest. [14.0.3] - 2021-05-09 This version is compatible with Unity 2022.2.0b15. Changed Added new UI/UX for the converter framework. Changed so Unity exports shader variants information into a file in a temp folder. Fixed Fixed setters so they don't cause an infinite loop in URP pipeline asset. Fixed spot and point light harsh distance falloff artefact on some platforms due to fp16 precision issue. Fixed _InternalLut so it isn't released too early and logs warnings when using post-processing in stacked camera's in URP 2D. Reverted behavior to allow FinalBlit to be skipped when you have no ScriptableRenderPasses with AfterRendering as renderEvent while finalPostProcessing is not needed. Fixed the shader graph usage of Unity cross fade. Fixed a stencil test issue when a RendererObjects feature is injected after Post Processing. Fixed incorrect Depth for Camera Stacks. Fixed a capture pass issue so the recorder screenshot doesn't miss the post processing results. Fixed a capture pass issue so the recorder screenshot doesn't miss the post processing results. Fixed stale light cookie data when the last cookie is removed inside a prefab. Added a warning when there are more visible lights than maximum light cookies. Added multi_compile_instancing to the SimpleLit shader on SM 2.0. [14.0.2] - 2021-02-04 This version is compatible with Unity 2022.2.0a14. Added Added Soft Shadows filtering quality as per light option. Low, PCF 3x3 pixel area with fixed offsets which is recommended for mobile. Medium, Tent 5x5 pixel area as the default. High, Tent 7x7 pixel area. Added default DOTS compatible loading shader (FallbackLoading.shader) Add #pragma editor_sync_compilation directive to FallbackError.shader Added commandBuffer variable to RenderingData struct and switched all of the renderer to use that buffer instead of creating local command buffers. Added automatic Alpha-To-Coverage feature which improves visual quality for alpha-clipped opaque geometry when MSAA is enabled Changed PostProcessPass to internal visibility since it's in Internal namespace. Removed SHADER_API_MOBILE from shaders in cases where it affected quality. Removed SHADER_HINT_NICE_QUALITY from shaders. Removed low quality light fade for lighting consistency on both desktop and mobile. Removed SHADER_QUALITY_LOW, SHADER_QUALITY_MEDIUM, SHADER_QUALITY_HIGH from shaders. Everything is now \"SHADER_QUALITY_HIGH\". Merged MaterialError.shader and FallbackError.shader Fixed Fixed camera sorting layer render target not being allocated in the 2d renderer case 1389780 Fixed an issue with too many variants being included in ShaderGraph shaders used in URP. [case 1378545] Fixed an issue in where a user could stack cameras with different renderers and not get a warning in the editor (this is not supported). Fixed decal automatic technique to correctly work with webgl. case 1370326 Fixed ScreenSpaceShadows target which was not bound during draw. case 1388353 Use D24_UNorm_S8_UInt depth buffer format on some platforms to improve performance. [14.0.1] - 2021-12-07 Added Added support for user-selected upscaling filters. Current options are automatic, bilinear, and nearest-neighbor. Added batch mode support for the converters. Added FP16 camera render target option. Added support for user-selected upscaling filters. Current options are automatic, bilinear, and nearest-neighbor. Added support for FidelityFX Super Resolution 1.0 upscaling filter. Added Downscale and Max Iterations options for Bloom Changed Re-added the menu button to be able to convert selected materials. Reverted intermediate texture behavior. Shader Variant Log Level moved from the URP Asset to URP Global Settings. Particle alpha channel blend mode to match standard shader. Removed skipIterations from Bloom settings. It has now been replaced with maxIterations. Fixed Fix mismatch on some platforms between Editor-side and Runtime-side implementations of UnityEngine.Rendering.Universal.DecalRendererFeature.IsAutomaticDBuffer() [case 1364134] Fixed incorrect light indexing on Windows Editor with Android target. case 1378103 Fixed missing depth for Depth of Field in an overlay camera. case 1365623 Fixed FXAA quality issues when render scale is not 1.0. Fixed material converter not being able to be called in batch mode. [case 1375962] Fixed an issue where specular color was not matching behaviour in Legacy and HDRP. case 1326941 Fixed issue where ShadowCasterGroup2D would throw an exception when there were no shadow casters. case 1387201 Fixed a shader compiler issue with mismatching variable types when calling lerp. Fixed single channel compressed (BC4) cookies on main light. Fixed URP Deferred Fog pass does not work in XR singlepass. case 1390236 Fixed an issue where preview cameras were missing the descriptor for creating their RenderTexture case 1393818 Fixed max light count cpu/gpu mismatch on Windows Editor with Android target. case 1392965 Fixed missing shader keyword SHADOWS_SHADOWMASK for shader graph using deferred rendering. Fixed double alpha modulate for particle unlit shader. Fixed incorrect light indexing on Windows Editor with Android target. case 1378103 Fixed missing depth for Depth of Field in an overlay camera. case 1365623 Fixed FXAA quality issues when render scale is not 1.0. Fixed Screen Space Decal to work with fog. 1383719 [14.0.0] - 2021-11-17 Added Renderer Features can now use the HelpURLAttribute to specify a documentation URL to be used in the inspector. Added inspector documentation URLs to the SSAO, Decal, and Render Objects renderer features. Changed \"_USE_DRAW_PROCEDURAL\" to be used only in vertex shader in Post Processing related shaders as they are not needed for fragment shaders. In result we now generate less shader variants. Added support for user-selected upscaling filters. Current options are automatic, bilinear, and nearest-neighbor. Added missing documentation in UniversalRenderPipelineAsset. Reflection Probe sample showing how Probe Blending and box projection works. Fixed Fix shadow rendering correctly to work with shader stripping in WebGl. case 1381881 Fixed incorrect shadow batching and shadow length case 1387859 VFX: Incorrect Decal rendering when rendescale is different than one case 1343674 Fixed inspector documentation URLs for the URP asset and Universal Renderer asset. Fixed render scale setting unintentionally affecting the scene view camera. Fixed property wrappers around material properties. Fixed incorrect light indexing on Windows Editor with Android target. case 1378103 Fixed missing depth for Depth of Field in an overlay camera. case 1365623 Fixed FXAA quality issues when render scale is not 1.0. Fixed an issue where specular color was not matching behaviour in Legacy and HDRP. case 1326941 [13.1.2] - 2021-11-05 Added Added minimal picking support for DOTS 1.0 (on parity with Hybrid Renderer V2) Added support for RTHandle. Changed Converted internal render targets to use RTHandle targets instead of RenderTargetHandle and RenderTargetIdentifier. Set usage of RenderTargetHandle and public functions using RenderTargetIdentifier as obsolete for future removal. Split RendererFeatures AddRenderPasses into two functions with SetupRenderPasses so render targets can be used after allocation. The \"Add Renderer Feature\" menu now supports filtering. Removed the name input for the SSAO and Screen Space Shadows renderer features. Fixed Fixed an issue where 2D global lights with shadows enabled could break light layer batching case 1376487 Fixed broken soft shadow filtering. case 1374960 Fixed Lens Flare not accounting Render Scale setting. case 1376820 Fixed an issue where SSAO would throw a \"RenderingCommandBuffer: invalid pass index\" errors. case 1374215 Fixed performance regression for 2D shaders where alpha discard was disabled. [case 1335648] Fixed an issue with MSAA falling back to the incorrect value when sample count 2 is not supported on some Android GPUs Fixed decals to work with native render pass case 1353141 Fixed decals to work with render scale 1353885 Fixed an issue in where the _ScreenParams is not setup correctly. Fixed an issue where intermediate rendertextures were not scaled when a camera was rendering to a texture case 1342895 [13.1.1] - 2021-10-04 Added Added Depth Texture setting for Overlay Camera. Added Depth Priming support for Vulkan with MSAA. Added Shadows and Additional Lights off variants stripping. Added Adaptive Performance Decals scaler. Exposed public API for DebugDisplaySettings. Added Display Stats panel to Rendering Debugger that displays CPU/GPU frame timings and bottlenecks. Preserve Specular blend mode toggle for glass like materials where the specular reflection itself is not transparent. Emulate alpha for multiply blend mode by whitening the base map colors using the alpha value. Keyword _ALPHAMODULATE_ON is set for multiply blend mode. Changed Main light shadow, additional light shadow and additional light keywords are now enabled based on urp setting instead of existence in scene. This allows better variant stripping. Now using the SpeedTree8 PBRLit shadergraph as the default SpeedTree8 shader for Universal. Changed default target sorting layers of Light2D to \"Everything\". Newly added sorting layers will be included in Light2Ds that have target sorting layers already set to \"Everything\". Separated Premultiplied blend mode and Preserve Specular Lighting feature from each other. Premultiplied blend mode is now true straight premultiply mode. Preserve Specular Lighting, which applies alpha differently for diffuse and specular parts of lighting, is now a separate option for Alpha and Additive blend modes. The results of previous Premultiplied blend implementation can be achieved by using Alpha blend mode with Preserve Specular Lighting toggled on. Multiply blend now keeps DstAlpha as it's RGB only. Particle AlphaModulate() renamed to AlphaModulateAndPremultiply() as it does both. Moved separate AlphaModulate() and AlphaPremultiply() to URP shader library. Fix double alpha multiply for ParticleLit. Improved blending modes trigger a material update which tries to keep the existing look intact. This is not always possible and manual blend mode changes might be required. Fixed Fixed incorrect premultiply blend mode. case 1260085, case 1357703, case 1347301 Fixed a regression where ShaderGraph screen position was not correct in game view and when using XR [1369450] Fixed overwriting of preview camera background color. case 1357004 Fixed ShadowCaster now requires varying normalWS to include changed normals from vertex shader in shader graph. Fixed typo in numIterationsEnclosingSphere api name Fix for rendering thumbnails. case 1348209 Fixed a regression bug where XR camera postion can not be modified in beginCameraRendering [case 1365000] Fixed an issue in where installing the Adaptive Performance package caused errors to the inspector UI 1368161 Fixed a regression where filtering the scene view yielded incorrect visual results 1360233 Fixed disabled debug lighting modes on Vulkan and OpenGL following a shader compiler fix. [case 1334240] Fixed an issue in where the Convert Renderering Settings would cause a freeze. case 1353885 Fixed incorrect behavior of Reflections with Smoothness lighting debug mode. [case 1374181] Fixed a performance regression in the 2D renderer regarding the PostProcessPass [case 1347893] Fixed light banding artifacts on some mobile platforms. case 1375791 [13.1.0] - 2021-09-24 Added Added public api and updated docs for Light2D shape properties. Changed URP will no longer render via an intermediate texture unless actively required by a Renderer Feature. See the upgrade guide for compatibility options and how assets are upgraded. MaterialReimporter.ReimportAllMaterials now batches the asset database changes to improve performance. Fixed Fixed post processing with Pixel Perfect camera case 1363763 Fixed the LensFlare flicker with TAA on SceneView (case 1356734). Fixed an issue where Unlit and ParticlesUnlit shaders did not have HDR color selection for albedo case 1283767 [13.0.0] - 2021-09-01 Added URP global setting for stripping post processing shader variants. URP global setting for stripping off shader variants. Terrain grass shader alpha changed to always write 1 to alpha. Enabled alpha channel write mask. Changed Removed experimental tile deferred code. VFX: New shadergraph support directly on Universal target. Fixed Added warning for lit shader detailed abledo, if texture is not linear. 1342011 Fixed lit detail correctly upgraded from standard shader. 1323725 URP asset can now use multi-edit. case 1364966 Fixed an issue in where the current open scene didn't load after running the converters. [case 1365101] Added \"Conservative Enclosing Sphere\" setting to fix shadow frustum culling issue where shadows are erroneously culled in corners of cascades case 1153151 Fixed memory leak with XR combined occlusion meshes. [case 1366173] Fixed a bug with Sprite Targets in ShaderGraph not rendering correctly in game view [1352225] Changed Remove use of deprecated UNITY_USE_NATIVE_HDR keyword in shaders. [12.0.0] - 2021-01-11 Added Added support for default sprite mask shaders for the 2D Renderer in URP. Added View Vector node to mimic old behavior of View Direction node in URP. Added support for the PlayStation 5 platform. Enabled deferred renderer in UI. Added support for light layers, which uses Rendering Layer Masks to make Lights in your Scene only light up specific Meshes. 2D Light Texture Node. A Shader Graph node that enable sampling of the Light Textures generated by the 2D Renderer in a lit scene. Fixed an error where multisampled texture being bound to a non-multisampled sampler in XR. case 1297013 Added _SURFACE_TYPE_TRANSPARENT keyword to URP shaders. Added Depth and DepthNormals passes to particles shaders. Added support for SSAO in Particle and Unlit shaders. Added Decal support. This includes new Decal Projector component, Decal renderer feature and Decal shader graph. Added a SpeedTree 8 Shader Graph but did not set it as the default when importing or upgrading Speed Tree 8 assets. Because URP doesn't yet support per-material culling, this Shader Graph does not yet behave in the same way as the existing handwritten SpeedTree 8 shader for URP. Added optional Depth Priming. Allows the forward opaque pass of the base camera to skip shading certain fragments if they don't contribute to the final opaque output. Added blending and box projection for reflection probes. Added 'Store Actions' option that enables bandwidth optimizations on mobile GPU architectures. Added \"Allow Material Override\" option to Lit and Unlit ShaderGraph targets. When checked, allows Material to control the surface options (transparent/opaque, blend mode, etc). Added a new UI for Render Pipeline Converters. Used now for Built-in to Universal conversion. Added sections on Light Inspector. Reorder camera inspector to be in the same order as HDRP. Added new URP Debug Views under Window/Analysis/Rendering Debugger. Added support for controlling Volume Framework Update Frequency in UI on Cameras and URP Asset as well as through scripting. Added URP Global Settings Asset to the Graphics Settings - a common place for project-wide URP settings. Added possibility to rename light layer values. Added Light cookies support to directional, point and spot light. Directional light cookie is main light only feature. Added GetUniversalAdditionalLightData, a method that returns the additional data component for a given light or create one if it doesn't exist yet. VFX: Basic support of Lit output. Added Motion Vector render pass for URP. VFX: Fix light cookies integration. Added Lights 2D to the Light Explorer window. Two new URP specific scene templates, Basic which has a camera and directional light, then Standard which has the addition of a global volume with basic post effects setup. Added Render Settings Converter to the Render Pipeline Converter, this tool creates and assigns URP Assets based off rendering settings of a Builtin project. XR: Added Late Latching support to reduce VR latency (Quest). Fixed incorrect shadow fade in deferred rendering mode. Added a help button on material editor to show the shader documentation page Added \"Copy Depth Mode\" Universal Renderer Data option that allows to specify if URP should copy the depth after the opaques pass or after the transparents pass. This can lead to bandwidth savings on mobile. Changed Moved fog evaluation from vertex shader to pixel shader. This improves rendering of fog for big triangles and fog quality. This can change the look of the fog slightly. UNITY_Z_0_FAR_FROM_CLIPSPACE now remaps to [0, far] range on all platforms consistently. Previously OpenGL platforms did not remap, discarding small amount of range [-near, 0]. Moved all 2D APIs out of experimental namespace. ClearFlag.Depth does not implicitely clear stencil anymore. ClearFlag.Stencil added. The Forward Renderer asset is renamed to the Universal Renderer asset. The Universal Renderer asset contains the property Rendering Path that lets you select the Forward or the Deferred Rendering Path. Improved PixelPerfectCamera UI/UX Changed Pixel Snapping and Upscale Render Texture in the PixelPerfectCamera to a dropdown. Move Assets/Create/Rendering/Universal Render Pipeline/Pipeline Asset (2D Renderer) to Assets/Create/Rendering/URP Asset (with 2D Renderer) Move Assets/Create/Rendering/Universal Render Pipeline/2D Renderer to Assets/Create/Rendering/URP 2D Renderer Move Assets/Create/Rendering/Universal Render Pipeline/Renderer Feature to Assets/Create/Rendering/URP Renderer Feature Move Assets/Create/Rendering/Universal Render Pipeline/Post-process Data to Assets/Create/Rendering/URP Post-process Data Move Assets/Create/Rendering/Universal Render Pipeline/Pipeline Asset (Forward Renderer) to Assets/Create/Rendering/URP Asset (with Forward Renderer) Move Assets/Create/Rendering/Universal Render Pipeline/XR System Data to Assets/Create/Rendering/URP XR System Data Move Assets/Create/Rendering/Universal Render Pipeline/Forward Renderer to Assets/Create/Rendering/URP Forward Renderer Removing unused temporary depth buffers for Depth of Field and Panini Projection. Optimized the Bokeh Depth of Field shader on mobile by using half precision floats. Changed UniversalRenderPipelineCameraEditor to URPCameraEditor Made 2D shadow casting more efficient Reduced the size of the fragment input struct of the TerrainLitPasses and LitGBufferPass, SimpleLitForwardPass and SimpleLitGBufferPass lighting shaders. Bokeh Depth of Field performance improvement: moved some calculations from GPU to CPU. Advanced Options > Priority has been renamed to Sorting Priority Opacity as Density blending feature for Terrain Lit Shader is now disabled when the Terrain has more than four Terrain Layers. This is now similar to the Height-blend feature for the Terrain Lit Shader. DepthNormals passes now sample normal maps if used on the material, otherwise output the geometry normal. SSAO Texture is now R8 instead of ARGB32 if supported by the platform. Enabled subsurface scattering with GI on handwritten Universal ST8 shader. Material upgrader now also upgrades AnimationClips in the project that have curves bound to renamed material properties. 2D Lights now inherit from Light2DBase. The behavior of setting a camera's Background Type to \"Dont Care\" has changed on mobile. Previously, \"Dont Care\" would behave identically to \"Solid Color\" on mobile. Now, \"Dont Care\" corresponds to the render target being filled with arbitrary data at the beginning of the frame, which may be faster in some situations. Note that there are no guarantees for the exact content of the render target, so projects should use \"Dont care\" only if they are guaranteed to render to, or otherwise write every pixel every frame. Stripping shader variants per renderer features instead of combined renderer features. When MSAA is enabled and a depth texture is required, the opaque pass depth will be copied instead of scheduling a depth prepass. URP Asset Inspector - Advanced settings have been reordered under Show Additional Properties on each section. Changed the default name when a new urp asset is created. URP Asset Inspector - General section has been renamed to Rendering. Refactored some of the array resizing code around decal projector rendering to use new APIs in render core UniversalRendererData and ForwardRendererData GUIDs have been reversed so that users coming from 2019LTS, 2020LTS and 2021.1 have a smooth upgrade path, you may encounter issues coming from 2021.2 Alpha/Beta versions and are recommended to start with a fresh library if initial upgrade fails. Fixed Fixed an issue in PostProcessPass causing OnGUI draws to not show on screen. [case 1346650] Fixed an issue with the blend mode in Sprite-Lit-Default shader causing alpha to overwrite the framebuffer. case 1331392 Fixed pixel perfect camera rect not being correctly initialized. case 1312646 Camera Inspector Stack list edition fixes. Fix indentation of Emission map on material editor. Fixed additional camera data help url Fixed additional light data help url Fixed Opacity as Density blending artifacts on Terrain that that caused Terrain to have modified splat weights of zero in some areas and greater than one in others. case 1283124 Fixed an issue where Sprite type Light2Ds would throw an exeception if missing a sprite Fixed an issue where Sprite type Light2Ds were missing a default sprite Fixed an issue where ShadowCasters were sometimes being rendered twice in the editor while in playmode. Fixed an issue where ShadowCaster2D was generating garbage when running in the editor. case 1304158 Fixed an issue where the 2D Renderer was not rendering depth and stencil in the normal rendering pass Fixed an issue where 2D lighting was incorrectly calculated when using a perspective camera. Fixed an issue where objects in motion might jitter when the Pixel Perfect Camera is used. case 1300474 Fixed an issue where filtering in the scene view would not properly highlight the filtered objects. case 1324359 Fixed an issue where the scene view camera was not correctly cleared for the 2D Renderer. case 1311377 Fixed an issue where the letter box/pillar box areas were not properly cleared when the Pixel Perfect Camera is used. case 1291224 Fixed an issue where the Cinemachine Pixel Perfect Extension might cause the Orthographic Size of the Camera to jump to 1 when the Scene is loaded. case 1249076 Fixed an issue where 2D Shadows were casting to the wrong layers [case 1300753][https://issuetracker.unity3d.com/product/unity/issues/guid/1300753/] Fixed an issue where Light2D did not upgrade Shadow Strength, Volumetric Intensity, Volumetric Shadow Strength correctly case 1317755 Fixed an issue where render scale was breaking SSAO in scene view. case 1296710 Fixed GC allocations from XR occlusion mesh when using multipass. SMAA post-filter only clear stencil buffer instead of depth and stencil buffers. Fixed an issue where the inspector of Renderer Data would break after adding RenderObjects renderer feature and then adding another renderer feature. Fixed an issue where soft particles did not work with orthographic projection. case 1294607 Fixed wrong shader / properties assignement to materials created from 3DsMax 2021 Physical Material. (case 1293576) Normalized the view direction in Shader Graph to be consistent across Scriptable Render Pieplines. Fixed material upgrader to run in batch mode [case 1305402] Fixed gizmos drawing in game view. case 1302504 Fixed an issue in shaderGraph target where the ShaderPass.hlsl was being included after SHADERPASS was defined Fixed base camera to keep render texture in sync with camera stacks. case 1288105 Fixed base camera to keep viewport in sync with camera stacks. case 1311268 Fixed base camera to keep display index in sync with camera stacks. case 1252265 Fixed base camera to keep display index in sync with camera stacks for canvas. case 1291872 Fixed render pass reusage with camera stack on vulkan. case 1226940 Fixed camera stack UI correctly work with prefabs. case 1308717 Fixed an issue where Particle Lit shader had an incorrect fallback shader [case 1312459] Fixed an issue with backbuffer MSAA on Vulkan desktop platforms. Fixed shadow cascade blend culling factor. Fixed remove of the Additional Light Data when removing the Light Component. Fixed remove of the Additional Camera Data when removing the Camera Component. Fixed shadowCoord error when main light shadow defined in unlit shader graph case 1175274 Removed Custom.meta which was causing warnings. case 1314288 Fixed a case where shadow fade was clipped too early. Fixed an issue where SmoothnessSource would be upgraded to the wrong value in the material upgrader. Fixed multi editing of Bias property on lights. [case 1289620] Fixed an issue where bokeh dof is applied incorrectly when there is an overlay camera in the camera stack. case 1303572 Fixed SafeNormalize returning invalid vector when using half with zero length. [case 1315956] Fixed lit shader property duplication issue. case 1315032 Fixed undo issues for the additional light property on the UniversalRenderPipeline Asset. [case 1300367] Fixed an issue where SSAO would sometimes not render with a recently imported renderer. Fixed a regression where the precision was changed. case 1313942 Fixed an issue where motion blur would allocate memory each frame. case 1314613 Fixed an issue where using Camera.targetTexture with Linear Color Space on an Android device that does not support sRGB backbuffer results in a RenderTexture that is too bright. [case 1307710] Fixed issue causing missing shaders on DirectX 11 feature level 10 GPUs. case 1278390 Fixed errors when the Profiler is used with XR multipass. case 1313141 Fixed materials being constantly dirty. Fixed double sided and clear coat multi editing shader. Fixed issue where copy depth depth pass for gizmos was being skipped in game view case 1302504 Fixed an issue where transparent objects sampled SSAO. Fixed an issue where Depth Prepass was not run when SSAO was set to Depth Mode. Fixed an issue where changing camera's position in the BeginCameraRendering do not apply properly. [case 1318629] (https://issuetracker.unity3d.com/issues/camera-doesnt-move-when-changing-its-position-in-the-begincamerarendering-and-the-endcamerarendering-methods) Fixed depth of field pass usage on unsupported devices. case 1327076 Fixed an issue where SMAA did not work for OpenGL case 1318214 Fixed an issue with Shader Graph Lit shaders where the normalized view direction produced incorrect lighting. [1332804] Fixed return values from GetStereoProjectionMatrix() and SetStereoViewMatrix(). case 1312813 Fixed CopyDepthPass incorrectly always enqueued when deferred rendering mode was enabled when it should depends on the pipeline asset settings. Fixed renderer post processing option to work with asset selector re-assing. case 1319454 Fixed post processing to be enabled by default in the renderer when creating URP asset option. case 1333461 Fixed shaderGraph shaders to render into correct depthNormals passes when deferred rendering mode and SSAO are enabled. Fixed ordering of subshaders in the Unlit Shader Graph, such that shader target 4.5 takes priority over 2.0. case 1328636 Fixed issue where it will clear camera color if post processing is happening on XR [case 1324451] Fixed a case where camera dimension can be zero. case 1321168 Fixed renderer creation in playmode to have its property reloaded. [case 1333463] Fixed gizmos no longer allocate memory in game view. [case 1328852] Fixed an issue where shadow artefacts appeared between cascades on Terrain Detail objects. Fixed ShaderGraph materials to select render queue in the same way as handwritten shader materials by default, but allows for a user override for custom behavior. [case 1335795] Fixed sceneview debug mode rendering (case 1211436). URP Global Settings can now be unassigned in the Graphics tab (case 1343570). VFX: Fixed soft particles when HDR or Opaque texture isn't enabled VFX: Fixed OpenGL soft particles fallback when depth texture isn't available Fixed soft shadows shader variants not set to multi_compile_fragment on some shaders (gbuffer pass, speedtree shaders, WavingGrass shader). Fixed issue with legacy stereo matrices with XR multipass. [case 1342416] Fixed unlit shader function name ambiguity Fixed Terrain holes not appearing in shadows [case 1349305] VFX: Compilation issue with ShaderGraph and planar lit outputs case 1349894 Fixed an issue where _AfterPostProcessTexture was no longer being assigned in UniversalRenderer. Fixed an issue where TerrainLit was rendering color lighter than Lit [case 1340751] (https://issuetracker.unity3d.com/product/unity/issues/guid/1340751/) Fixed Camera rendering when capture action and post processing present. [case 1350313] Fixed artifacts in Speed Tree 8 billboard LODs due to SpeedTree LOD smoothing/crossfading [case 1348407] Fix sporadic NaN when using normal maps with XYZ-encoding case 1351020 Support undo of URP Global Settings asset assignation (case 1342987). Removed unsupported fields from Presets of Light and Camera [case 1335979]. Fixed graphical artefact when terrain height map is used with rendering layer mask for lighting. Fixed URP's vignette effect to respect XR's view center, since with Asymmetric FOV, the center of the view is not always the center of the texture case 1358336 Fixed an issue where screen space shadows has flickering with deferred mode case 1354681 Fixed shadowCascadeBlendCullingFactor to be 1.0 Fixed missing property values in a RendererFeature of screen space shadows by adding tooltip for it instead of showing them. [case 1327356] Changed Change Asset/Create/Shader/Universal Render Pipeline/Lit Shader Graph to Asset/Create/Shader Graph/URP/Lit Shader Graph Change Asset/Create/Shader/Universal Render Pipeline/Sprite Lit Shader Graph to Asset/Create/Shader Graph/URP/Sprite Lit Shader Graph Change Asset/Create/Shader/Universal Render Pipeline/Unlit Shader Graph to Asset/Create/Shader Graph/URP/Unlit Shader Graph Change Asset/Create/Shader/Universal Render Pipeline/Sprite Unlit Shader Graph to Asset/Create/Shader Graph/URP/Sprite Unlit Shader Graph Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Project Materials to 2D Renderer Materials to Edit/Rendering/Materials/Convert All Built-in Materials to URP 2D Renderer Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Scene Materials to 2D Renderer Materials to Edit/Rendering/Materials/Convert All Built-in Scene Materials to URP 2D Renderer Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Project URP Parametric Lights to Freeform to Edit/Rendering/Lights/Convert Project URP Parametric Lights to Freeform Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Scene URP Parametric Lights to Freeform to Edit/Rendering/Lights/Convert Scene URP Parametric Lights to Freeform Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Project Materials to URP Materials to Edit/Rendering/Materials/Convert All Built-in Materials to URP Moved Edit/Render Pipeline/Universal Render Pipeline/Upgrade Selected Materials to URP Materials to Edit/Rendering/Materials/Convert Selected Built-in Materials to URP Deprecated GetShadowFade in Shadows.hlsl, use GetMainLightShadowFade or GetAdditionalLightShadowFade. Improved shadow cascade GUI drawing with pixel perfect, hover and focus functionalities. Shadow fade now uses border value for calculating shadow fade distance and fall off linearly. Improved URP profiling scopes. Remove low impact scopes from the command buffer for a small performance gain. Fix the name and invalid scope for context.submit() scope. Change the default profiling name of ScriptableRenderPass to Unnamed_ScriptableRenderPass. Using the same MaterialHeaderScope for material editor as HDRP is using Removed Code to upgrade from LWRP to URP was removed. This means if you want to upgrade from LWRP you must first upgrade to previous versions of URP and then upgrade to this version. [11.0.0] - 2020-10-21 Added Added real-time Point Light Shadows. Added a supported MSAA samples count check, so the actual supported MSAA samples count value can be assigned to RenderTexture descriptors. Added the TerrainCompatible SubShader Tag. Use this Tag in your custom shader to tell Unity that the shader is compatible with the Terrain system. Added _CameraSortingLayerTexture global shader variable and related parameters Added preset shapes for creating a freeform light Added serialization of Freeform ShapeLight mesh to avoid CPU cost of generating them on the runtime. Added 2D Renderer Asset Preset for creating a Universal Renderer Asset Added an option to use faster, but less accurate approximation functions when converting between the sRGB and Linear color spaces. Added screen space shadow as renderer feature Added [DisallowMultipleRendererFeature] attribute for Renderer Features. Added support for Enlighten precomputed realtime Global Illumination. Changed Optimized 2D Renderer performance on mobile GPUs by reducing the number of render target switches. Optimized 2D Renderer performance by rendering the normal buffer at the same lower resolution as the light buffers. Improved Light2D UI/UX Improved 2D Menu layout Deprecated Light2D Parametric Light Deprecated Light2D point light cookie Renamed Light2D point light to spot light 2D Renderer: The per Blend Style render texture scale setting was replaced by a global scale setting for all Blend Styles. Optimized 2D Renderer performance by using a tiny light texture for layer/blend style pairs for which no light is rendered. Reorgnized the settings in 2D Renderer Data Inspector. FallOff Lookup Texture is now part of 2D RenderData. Creating a Shadow Caster 2D will use try and use sprite and physics bounds as the default shape Deleting all points in a Shadow Caster will cause the shape to use the bounds. Improved Geometry for Smooth Falloff of 2D Shape Lights. Updated the tooltips for Light 2D Inspector. Removed the Custom blend Mode option from the Blend Styles. New default Blend Styles when a new 2D Renderer Data asset is created. Added a supported MSAA samples count check, so the actual supported MSAA samples count value can be assigned to RenderTexture descriptors. Bloom in Gamma color-space now more closely matches Linear color-space, this will mean project using Bloom and Gamma color-space may need to adjust Bloom Intensity to match previous look. Autodesk Interactive Shader Graph files and folders containing them were renamed. The new file paths do not have spaces. Moved FinalPostProcessPass to AfterRenderingPostProcessing event from AfterRendering. This allows user pass to execute before and after FinalPostProcessPass and CapturePass to capture everything. Changed shader keywords of main light shadow from toggling to enumerating. Always use \"High\" quality normals, which normalizes the normal in pixel shader. \"Low\" quality normals looked too much like a bug. Re-enabled implicit MSAA resolve to backbuffer on Metal MacOS. Optimized 2D performance by rendering straight to the backbuffer if possible Changed Post Process Data to bool. When it is no enabled all post processing is stripped from build, when it is enabled you can still override resources there. Converted XR automated tests to use MockHMD. Improved 2D Renderer performance on mobile GPUs when using MSAA Reduced the size of the fragment input struct of the Terrain and Forward lighting shaders. Fixed Fixed an issue where additional lights would not render with WebGL 1 Fixed an issue where the 2D Renderer was incorrectly rendering transparency with normal maps on an empty background. Fixed an issue that that caused a null error when creating a Sprite Light. case 1307125 Fixed an issue where Sprites on one Sorting Layer were fully lit even when there's no 2D light targeting that layer. Fixed an issue where null reference exception was thrown when creating a 2D Renderer Data asset while scripts are compiling. case 1263040 Fixed an issue where no preview would show for the lit sprite master node in shadergraph Fixed an issue where no shader was generated for unlit sprite shaders in shadergraph Fixed an issue where Sprite-Lit-Default shader's Normal Map property wasn't affected by Tiling or Offset. case 1270850 Fixed an issue where normal-mapped Sprites could render differently depending on whether they're dynamically-batched. case 1286186 Removed the warning about mis-matched vertex streams when creating a default Particle System. case 1285272 Fixed latest mockHMD renderviewport scale doesn't fill whole view after scaling. [case 1286161] (https://issuetracker.unity3d.com/issues/xr-urp-renderviewportscale-doesnt-fill-whole-view-after-scaling) Fixed camera renders black in XR when user sets invalid MSAA value. Fixed an issue causing additional lights to stop working when set as the sun source. case 1278768 Fixed an issue causing passthrough camera to not render. case 1283894 Fixed an issue that caused a null reference when Lift Gamma Gain was being displayed in the Inspector and URP was upgraded to a newer version. case 1283588 Fixed an issue where soft particles were not rendered when depth texture was disabled in the URP Asset. case 1162556 Fixed an issue where soft particles were rendered opaque on OpenGL. case 1226288 Fixed an issue where the depth texture sample node used an incorrect texture in some frames. case 1268079 Fixed a compiler error in BakedLit shader when using Hybrid Renderer. Fixed an issue with upgrading material set to cutout didn't properly set alpha clipping. case 1235516 Fixed XR camera fov can be changed through camera inspector. Fixed an issue where Universal Render Pipeline with disabled antiAliasing was overwriting QualitySettings.asset on frequent cases. case 1219159 Fixed a case where overlay camera with output texture caused base camera not to render to screen. case 1283225 Fixed an issue where the scene view camera ignored the pipeline assets HDR setting. case 1284369 Fixed an issue where the Camera inspector was grabbing the URP asset in Graphics Settings rather than the currently active. Fixed an issue where the Light Explorer was grabbing the URP asset in Graphics Settings rather than the currently active. Fixed an issue causing materials to be upgraded multiple times. Fixed bloom inconsistencies between Gamma and Linear color-spaces. Fixed an issue in where all the entries in the Renderer List wasn't selectable and couldn't be deleted. Fixed Deferred renderer on some Android devices by forcing accurate GBuffer normals. [case 1288042] Fixed an issue where MSAA did not work in Editor Game View on Windows with Vulkan. Fixed issue where selecting and deselecting Forward Renderer asset would leak memory case 1290628 Fixed the default background color for previews to use the original color. Fixed an issue where the scene view would turn black when bloom was enabled. case 1298790 Fixed an issue where having \"Opaque Texture\" and MSAA enabled would cause the opaque texture to be rendered black on old Apple GPUs case 1247423 Fixed SAMPLE_TEXTURECUBE_ARRAY_LOD macro when using OpenGL ES. case 1285132 Fixed an issue such that it is now posible to enqueue render passes at runtime. Fixed SpeedTree LOD fade functionality. [case 1198135] [10.2.0] - 2020-10-19 Changed Changed RenderObjectsFeature UI to only expose valid events. Previously, when selecting events before BeforeRenderingPrepasses objects would not be drawn correctly as stereo and camera setup only happens before rendering opaques objects. Transparent Lit ShaderGraph using Additive blending will now properly fade with alpha [1270344] Fixed Fixed the Unlit shader not being SRP Batcher compatible on OpenGLES/OpenGLCore. case 1263720 Fixed an issue with soft particles not rendering correctly for overlay cameras with post processing. case 1241626 Fixed MSAA override on camera does not work in non-XR project if target eye is selected to both eye. [10.1.0] - 2020-10-12 Added support for the Shadowmask Mixed Lighting Mode (Forward only), which supports up to four baked-shadow Lights. Added ComplexLit shader for advanced material features and deferred forward fallback. Added Clear Coat feature for ComplexLit shader and for shader graph. Added Parallax Mapping to the Lit shader (Lit.shader). Added the Detail Inputs setting group in the Lit shader (Lit.shader). Added Smooth shadow fading. Added SSAO support for deferred renderer. The pipeline now outputs a warning in the console when trying to access camera color or depth texture when those are not valid. Those textures are only available in the context of ScriptableRenderPass. Added a property to access the renderer from the CameraData. Changed Shader functions SampleSH9, SampleSHPixel, SampleSHVertex are now gamma corrected in gamma space. As result LightProbes are gamma corrected too. The maximum number of visible lights when using OpenGL ES 3.x on Android now depends on the minimum OpenGL ES 3.x version as configured in PlayerSettings. The default value of the HDR property of a newly created Universal Render Pipeline Asset, is now set to true. Fixed Fixed an issue where the CapturePass would not capture the post processing effects. Fixed an issue were the filter window could not be defocused using the mouse. case 1242032 Fixed camera backgrounds not matching between editor and build when background is set to 'Uninitialized'. case 1224369 Fixed a case where main light hard shadows would not work if any other light is present with soft shadows.case 1250829 Fixed issue that caused color grading to not work correctly with camera stacking. case 1263193 Fixed an issue that caused an infinite asset database reimport when running Unity in command line with -testResults argument. Fixed ParticlesUnlit shader to use fog color instead of always black. [case 1264585] Fixed issue that caused some properties in the camera to not be bolded and highlighted when edited in prefab mode. case 1230082 Fixed issue where blur would sometimes flicker case 1224915 Fixed an issue in where the camera inspector didn't refresh properly when changing pipeline in graphic settings. case 1222668 Fixed depth of field to work with dynamic resolution. case 1225467 Fixed FXAA, SSAO, Motion Blur to work with dynamic resolution. Fixed an issue where Pixel lighting variants were stripped in builds if another URP asset had Additional Lights set to Per Vertex case 1263514 Fixed an issue where transparent meshes were rendered opaque when using custom render passes case 1262887 Fixed regression from 8.x.x that increased launch times on Android with GLES3. case 1269119 Fixed an issue with a render texture failing assertion when chosing an invalid format. case 1222676 Fixed an issue that caused the unity_CameraToWorld matrix to have z flipped values. case 1257518 Fixed not using the local skybox on the camera game object when the Skybox Material property in the Lighting window was set to null. Fixed an issue where, if URP was not in use, you would sometimes get errors about 2D Lights when going through the menus. Fixed GC when using XR single-pass automated tests. Fixed an issue that caused a null reference when deleting camera component in a prefab. case 1244430 Fixed resolution of intermediate textures when rendering to part of a render texture. case 1261287 Fixed indirect albedo not working with shadergraph shaders in some rare setups. case 1274967 Fixed XR mirroView sRGB issue when color space is gamma. Fixed an issue where XR eye textures are recreated multiple times per frame due to per camera MSAA change. Fixed an issue wehre XR mirror view selector stuck. Fixed LightProbes to have gamma correct when using gamma color space. case 1268911 Fixed GLES2 shader compilation. Fixed useless mip maps on temporary RTs/PostProcessing inherited from Main RT descriptor. Fixed issue with lens distortion breaking rendering when enabled and its intensity is 0. Fixed mixed lighting subtractive and shadowmask modes for deferred renderer. Fixed issue that caused motion blur to not work in XR. Fixed 2D renderer when using Linear rendering on Android directly to backbuffer. Fixed issue where multiple cameras would cause GC each frame. case 1259717 Fixed Missing camera cannot be removed after scene is saved by removing the Missing camera label. case 1252255 Fixed MissingReferenceException when removing Missing camera from camera stack by removing Missing camera label. case 1252263 Fixed slow down in the editor when editing properties in the UI for renderer features. case 1279804 Fixed test 130_UnityMatrixIVP on OpenGL ES 3 Fixed MSAA on Metal MacOS and Editor. [10.0.0] - 2020-06-10 Added Added the option to strip Terrain hole Shader variants. Added support for additional Directional Lights. The amount of additional Directional Lights is limited by the maximum Per-object Lights in the Render Pipeline Asset. Added Package Samples: 2 Camera Stacking, 2 Renderer Features Added default implementations of OnPreprocessMaterialDescription for FBX, Obj, Sketchup and 3DS file formats. Added Transparency Sort Mode and Transparency Sort Axis to 2DRendererData. Added support for a user defined default material to 2DRendererData. Added the option to toggle shadow receiving on transparent objects. Added XR multipass rendering. Multipass rendering is a requirement on many VR platforms and allows graceful fallback when single-pass rendering isn't available. Added support for Camera Stacking when using the Forward Renderer. This introduces the Camera Render Type property. A Base Camera can be initialized with either the Skybox or Solid Color, and can combine its output with that of one or more Overlay Cameras. An Overlay Camera is always initialized with the contents of the previous Camera that rendered in the Camera Stack. Added AssetPostprocessors and Shadergraphs to handle Arnold Standard Surface and 3DsMax Physical material import from FBX. Added [MainTexture] and [MainColor] shader property attributes to URP shader properties. These will link script material.mainTextureOffset and material.color to _BaseMap and _BaseColor shader properties. Added the option to specify the maximum number of visible lights. If you set a value, lights are sorted based on their distance from the Camera. Added the option to control the transparent layer separately in the Forward Renderer. Added the ability to set individual RendererFeatures to be active or not, use ScriptableRendererFeature.SetActive(bool) to set whether a Renderer Feature will execute, ScriptableRendererFeature.isActive can be used to check the current active state of the Renderer Feature. additional steps to the 2D Renderer setup page for quality and platform settings. If Unity Editor Analytics are enabled, Universal collects anonymous data about usage of Universal. This helps the Universal team focus our efforts on the most common scenarios, and better understand the needs of our customers. Added a OnCameraSetup() function to the ScriptableRenderPass API, that gets called by the renderer before rendering each camera Added a OnCameraCleanup() function to the ScriptableRenderPass API, that gets called by the renderer after rendering each camera Added Default Material Type options to the 2D Renderer Data Asset property settings. Added additional steps to the 2D Renderer setup page for quality and platform settings. Added option to disable XR autotests on test settings. Shader Preprocessor strips gbuffer shader variants if DeferredRenderer is not in the list of renderers in any Scriptable Pipeline Assets. Added an option to enable/disable Adaptive Performance when the Adaptive Performance package is available in the project. Added support for 3DsMax's 2021 Simplified Physical Material from FBX files in the Model Importer. Added GI to SpeedTree Added support for DXT5nm-style normal maps on Android, iOS and tvOS Added stencil override support for deferred renderer. Added a warning message when a renderer is used with an unsupported graphics API, as the deferred renderer does not officially support GL-based platforms. Added option to skip a number of final bloom iterations. Added support for Screen Space Ambient Occlusion and a new shader variant _SCREEN_SPACE_OCCLUSION. Added support for Normal Texture being generated in a prepass. Added a ConfigureInput() function to ScriptableRenderPass, so it is possible for passes to ask that a Depth, Normal and/or Opaque textures to be generated by the forward renderer. Added a float2 normalizedScreenSpaceUV to the InputData Struct. Added new sections to documentation: Writing custom shaders, and Using the beginCameraRendering event. Added support for GPU instanced mesh particles on supported platforms. Added API to check if a Camera or Light is compatible with Universal Render Pipeline. Changed Moved the icon that indicates the type of a Light 2D from the Inspector header to the Light Type field. Eliminated some GC allocations from the 2D Renderer. Added SceneSelection pass for TerrainLit shader. Remove final blit pass to force alpha to 1.0 on mobile platforms. Deprecated the CinemachineUniversalPixelPerfect extension. Use the one from Cinemachine v2.4 instead. Replaced PlayerSettings.virtualRealitySupported with XRGraphics.tryEnable. Blend Style in the 2DRendererData are now automatically enabled/disabled. When using the 2D Renderer, Sprites will render with a faster rendering path when no lights are present. Particle shaders now receive shadows The Scene view now mirrors the Volume Layer Mask set on the Main Camera. Drawing order of SRPDefaultUnlit is now the same as the Built-in Render Pipline. Made MaterialDescriptionPreprocessors private. UniversalRenderPipelineAsset no longer supports presets. Case 1197020. The number of maximum visible lights is now determined by whether the platform is mobile or not. Renderer Feature list is now redesigned to fit more closely to the Volume Profile UI, this vastly improves UX and reliability of the Renderer Features List. Default color values for Lit and SimpleLit shaders changed to white due to issues with texture based workflows. You can now subclass ForwardRenderer to create a custom renderer based on it. URP is now computing tangent space per fragment. Optimized the 2D Renderer to skip rendering into certain internal buffers when not necessary. You can now subclass ForwardRenderer to create a custom renderer based on it. URP shaders that contain a priority slider now no longer have an offset of 50 by default. The virtual ScriptableRenderer.FrameCleanup() function has been marked obsolete and replaced by ScriptableRenderer.OnCameraCleanup() to better describe when the function gets invoked by the renderer. DepthOnlyPass, CopyDepthPass and CopyColorPass now use OnCameraSetup() instead of Configure() to set up their passes before executing as they only need to get their rendertextures once per camera instead of once per eye. Updated shaders to be compatible with Microsoft's DXC. Mesh GPU Instancing option is now hidden from the particles system renderer as this feature is not supported by URP. The 2D Renderer now supports camera stacking. 2D shaders now use half-precision floats whenever precise results are not necessary. Removed the ETC1_EXTERNAL_ALPHA variant from Shader Graph Sprite shaders. Eliminated some unnecessary clearing of render targets when using the 2D Renderer. The rendering of 2D lights is more effient as sorting layers affected by the same set of lights are now batched. Removed the 8 renderer limit from URP Asset. Merged the deferred renderer into the forward renderer. Changing the default value of Skip Iterations to 1 in Bloom effect editor Use SystemInfo to check if multiview is supported instead of being platform hardcoded Default attachment setup behaviour for ScriptableRenderPasses that execute before rendering opaques is now set use current the active render target setup. This improves performance in some situations. Combine XR occlusion meshes into one when using single-pass (multiview or instancing) to reduce draw calls and state changes. Shaders included in the URP package now use local Material keywords instead of global keywords. This increases the amount of available global user-defined Material keywords. Fixed Fixed an issue that caused WebGL to render blank screen when Depth texture was enabled case 1240228 Fixed NaNs in tonemap algorithms (neutral and ACES) on platforms defaulting to lower precision. Fixed a performance problem with ShaderPreprocessor with large amount of active shader variants in the project Fixed an issue where linear to sRGB conversion occurred twice on certain Android devices. Fixed an issue where there were 2 widgets showing the outer angle of a spot light. Fixed an issue where Unity rendered fullscreen quads with the pink error shader when you enabled the Stop NaN post-processing pass. Fixed an issue where Terrain hole Shader changes were missing. Case 1179808. Fixed an issue where the Shader Graph SceneDepth node didn't work with XR single-pass (double-wide) rendering. See case 1123069. Fixed Unlit and BakedLit shader compilations in the meta pass. Fixed an issue where the Bokeh Depth of Field shader would fail to compile on a console platform. Fixed an issue where the Scene lighting button didn't work when you used the 2D Renderer. Fixed a performance regression when you used the 2D Renderer. Fixed an issue where the Freeform 2D Light gizmo didn't correctly show the Falloff offset. Fixed an issue where the 2D Renderer rendered nothing when you used shadow-casting lights with incompatible Renderer2DData. Fixed an issue where errors were generated when the Physics2D module was not included in the project's manifest. Fixed an issue where Prefab previews were incorrectly lit when you used the 2D Renderer. Fixed an issue where the Light didn't update correctly when you deleted a Sprite that a Sprite 2D Light uses. Fixed an issue where 2D Lighting was broken for Perspective Cameras. Fixed an issue where resetting a Freeform 2D Light would throw null reference exceptions. Case 1184536. Fixed an issue where Freeform 2D Lights were not culled correctly when there was a Falloff Offset. Fixed an issue where Tilemap palettes were invisible in the Tile Palette window when the 2D Renderer was in use. Case 1162550. Fixed issue where black emission would cause unneccesary inspector UI repaints. Case 1105661. Fixed user LUT sampling being done in Linear instead of sRGB. Fixed an issue when trying to get the Renderer via API on the first frame. Case 1189196. Fixed a material leak on domain reload. Fixed an issue where deleting an entry from the Renderer List and then undoing that change could cause a null reference. Case 1191896. Fixed an issue where the user would get an error if they removed the Additional Camera Data component. Case 1189926. Fixed post-processing with XR single-pass rendering modes. Fixed an issue where Cinemachine v2.4 couldn't be used together with Universal RP due to a circular dependency between the two packages. Fixed an issue that caused shaders containing HDRP string in their path to be stripped from the build. Fixed an issue that caused only selected object to render in SceneView when Wireframe drawmode was selected. Fixed Renderer Features UI tooltips. Case 1191901. Fixed multiple issues where Shader Graph shaders failed to build for XR in the Universal RP. Fixed an issue when using the 2D Renderer where some types of renderers would not be assigned the correct material. Fixed inconsistent lighting between the forward renderer and the deferred renderer, that was caused by a missing normalize operation on vertex normals on some speedtree shader variants. Fixed issue where XR Multiview failed to render when using URP Shader Graph Shaders Fixed lazy initialization with last version of ResourceReloader Fixed broken images in package documentation. Fixed an issue where viewport aspect ratio was wrong when using the Stretch Fill option of the Pixel Perfect Camera. case 1188695 Fixed an issue where setting a Normal map on a newly created material would not update. case 1197217 Fixed an issue where post-processing was not applied for custom renderers set to run on the \"After Rendering\" event case 1196219 Fixed an issue that caused an extra blit when using custom renderers case 1156741 Fixed an issue with transparent objects not receiving shadows when using shadow cascades. case 1116936 Fixed issue where using a ForwardRendererData preset would cause a crash. case 1201052 Fixed an issue where particles had dark outlines when blended together case 1199812 Fixed an issue with deleting shader passes in the custom renderer features list case 1201664 Fixed camera inverse view-projection matrix in XR mode, depth-copy and color-copy passes. Fixed an issue with the null check when UniversalRenderPipelineLightEditor.cs tries to access SceneView.lastActiveSceneView. Fixed an issue where the 'Depth Texture' drop down was incorrectly disabled in the Camera Inspector. Fixed an issue that caused errors if you disabled the VR Module when building a project. Fixed an issue where the default TerrainLit Material was outdated, which caused the default Terrain to use per-vertex normals instead of per-pixel normals. Fixed shader errors and warnings in the default Universal RP Terrain Shader. case 1185948 Fixed an issue where the URP Material Upgrader tried to upgrade standard Universal Shaders. case 1144710 Fixed an issue where some Materials threw errors when you upgraded them to Universal Shaders. case 1200938 Fixed issue where normal maps on terrain appeared to have flipped X-components when compared to the same normal map on a mesh. case 1181518 Fixed an issue where the editor would sometimes crash when using additional lights case 1176131 Fixed RemoveComponent on Camera contextual menu to not remove Camera while a component depend on it. Fixed an issue where right eye is not rendered to. case 1170619 Fixed issue where TerrainDetailLit.shader fails to compile when XR is enabled. Fixed an issue that allowed height-based blending on Terrains with more than 4 materials, which is not supported. Fixed an issue where opaque objects were outputting incorrect alpha values case 1168283 Fixed an issue where a depth texture was always created when post-processing was enabled, even if no effects made use of it. Fixed incorrect light attenuation on some platforms. Fixed an issue where the Volume System would not use the Cameras Transform when no Volume Trigger was set. Fixed an issue where post processing disappeared when using custom renderers and SMAA or no AA Fixed an issue where the 2D Renderer upgrader did not upgrade using the correct default material Fixed an issue with soft particles having dark blending when intersecting with scene geometry case 1199812 Fixed an issue with additive particles blending incorrectly case 1215713 Fixed an issue where camera preview window was missing in scene view. case 1211971 Fixed an issue with shadow cascade values were not readable in the render pipeline asset case 1219003 Fixed an issue where MSAA isn't applied until eye textures are relocated by changing their resolution. case 1197958 Fixed an issue where camera stacking didn't work properly inside prefab mode. case 1220509 Fixed the definition of mad() in SMAA shader for OpenGL. Fixed an issue where partical shaders failed to handle Single-Pass Stereo VR rendering with Double-Wide Textures. case 1201208 Fixed an issue that caused assets to be reimported if player prefs were cleared. case 1192259 Fixed missing Custom Render Features after Library deletion. case 1196338 Fixed not being able to remove a Renderer Feature due to tricky UI selection rects. case 1208113 Fixed an issue where the Camera Override on the Render Object Feature would not work with many Render Features in a row. case 1205185 Fixed UI clipping issue in Forward Renderer inspector. case 1211954 Fixed a Null ref when trying to remove a missing Renderer Feature from the Forward Renderer. case 1196651 Fixed data serialization issue when adding a Renderer Feature to teh Forward Renderer. case 1214779 Fixed issue with AssetPostprocessors dependencies causing models to be imported twice when upgrading the package version. Fixed an issue where NullReferenceException might be thrown when creating 2D Lights. case 1219374 Fixed an issue with a blurry settings icon. case 1201895 Fixed issue that caused the QualitySettings anti-aliasing changing without user interaction. case 1195272 Fixed an issue where Shader Graph shaders generate undeclared identifier 'GetWorldSpaceNormalizeViewDir' error. Fixed an issue where rendering into RenderTexture with Single Pass Instanced renders both eyes overlapping. Fixed an issue where Renderscale setting has no effect when using XRSDK. Fixed an issue where renderScale != 1 or Display.main.requiresBlitToBackbuffer forced an unnecessary blit on XR. Fixed an issue that causes double sRGB correction on Quest. case 1209292 Fixed an issue where terrain DepthOnly pass does not work for XR. Fixed an issue that caused depth texture to be flipped when sampling from shaders case 1225362 Fixed an issue with URP switching such that every avaiable URP makes a total set of supported features such that all URPs are taken into consideration. case 1157420 Fixed an issue where XR multipass repeatedly throws error messages \"Multi pass stereo mode doesn't support Camera Stacking\". Fixed an issue with shadows not appearing on terrains when no cascades were selected case 1226530 Fixed a shader issue that caused the Color in Sprite Shape to work improperly. Fixed an issue with URP switching such that every available URP makes a total set of supported features such that all URPs are taken into consideration. case 1157420 Metallic slider on the Lit shader is now linear meaning correct values are used for PBR. Fixed an issue where Post-Processing caused nothing to render on GLES2. Fixed an issue that causes viewport to not work correctly when rendering to textures. case 1225103 Fixed an issue that caused incorrect sampling of HDR reflection probe textures. Fixed UI text of RenderObjects feature to display LightMode tag instead of Shader Pass Name. case 1201696 Fixed an issue when Linear -> sRGB conversion would not happen on some Android devices. case 1226208 Fixed issue where using DOF at the same time as Dynamic Scaling, the depth buffer was smapled with incorrect UVs. case 1225467 Fixed an issue where an exception would be thrown when resetting the ShadowCaster2D component. case 1225339 Fixe an issue where using a Subtractive Blend Style for your 2D Lights might cause artifacts in certain post-processing effects. case 1215584 Fixed an issue where Cinemachine Pixel Perfect Extension didn't work when CinemachineBrain Update Method is anything other than Late Update. Fixed an issue where Sprite Shader Graph shaders weren't double-sided by default. Fixed an issue where particles using Sprite Shader Graph shaders were invisible. Fixed an issue where Scene objects might be incorrectly affected by 2D Lights from a previous Sorting Layer. Fixed an issue where errors would appear in the Console when entering Play Mode with a 2D Light selected in the Hierarchy. Case 1226918 Fixed an issue that caused Android GLES to render blank screen when Depth texture was enabled without Opaque texture case 1219325 Fixed an issue that caused transparent objects to always render over top of world space UI. case 1219877 Fixed issue causing sorting fudge to not work between shadergraph and urp particle shaders. case 1222762 Fixed shader compilation errors when using multiple lights in DX10 level GPU. case 1222302 Fixed an issue with shadows not being correctly calculated in some shaders. Fixed invalid implementation of one function in LWRP -> URP backward compatibility support. Fixed issue where maximum number of visible lights in C# code did not match maximum number in shader code on some platforms. Fixed OpenGL ES 3.0 support for URP ShaderGraph. case 1230890 Fixed an issue where multi edit camera properties didn't work. case 1230080 Fixed an issue where the emission value in particle shaders would not update in the editor without entering the Play mode. Fixed issues with performance when importing fbx files. Fixed issues with NullReferenceException happening with URP shaders. Fixed an issue that caused memory allocations when sorting cameras. case 1226448 Fixed an issue where grid lines were drawn on top of opaque objects in the preview window. Case 1240723. Fixed an issue where objects in the preview window were affected by layer mask settings in the default renderer. Case 1204376. Fixed an issue with reflections when using an orthographic camera case 1209255 Fixed issue that caused unity_AmbientSky, unity_AmbientEquator and unity_AmbientGround variables to be unintialized. Fixed issue that caused SHADERGRAPH_AMBIENT_SKY, SHADERGRAPH_AMBIENT_EQUATOR and SHADERGRAPH_AMBIENT_GROUND variables to be uninitialized. Fixed SceneView Draw Modes not being properly updated after opening new scene view panels or changing the editor layout. Fixed GLES shaders compilation failing on Windows platform (not a mobile platform) due to uniform count limit. Fixed an issue that caused the inverse view and projection matrix to output wrong values in some platforms. case 1243990 Fixed an issue where the Render Scale setting of the pipeline asset didn't properly change the resolution when using the 2D Renderer. case 1241537 Fixed an issue where 2D lights didn't respect the Camera's Culling Mask. case 1239136 Fixed broken documentation links for some 2D related components. Fixed an issue where Sprite shaders generated by Shader Graph weren't double-sided. case 1261232 Fixed an issue where the package would fail to compile if the Animation module was disabled. case 1227068 Fixed an issue where Stencil settings wasn't serialized properly in sub object case 1241218 Fixed an issue with not being able to remove Light Mode Tags case 1240895 Fixed an issue where preset button could still be used, when it is not supposed to. case 1246261 Fixed an issue where Model Importer Materials used the Standard Shader from the Built-in Render Pipeline instead of URP Lit shader when the import happened at Editor startup. Fixed an issue where only unique names of cameras could be added to the camera stack. Fixed issue that caused shaders to fail to compile in OpenGL 4.1 or below. Fixed an issue where camera stacking with MSAA on OpenGL resulted in a black screen. case 1250602 Optimized shader compilation times by compiling different variant sets for vertex and fragment shaders. Fixed shadows for additional lights by limiting MAX_VISIBLE_LIGHTS to 16 for OpenGL ES 2.0 and 3.0 on mobile platforms. case 1244391 Fixed Lit/SimpleLit/ParticlesLit/ParticlesSimpleLit/ParticlesUnlit shaders emission color not to be converted from gamma to linear color space. [case 1249615] Fixed missing unity_MatrixInvP for shader code and shaderGraph. Fixed XR support for deferred renderer. Fixing RenderObject to reflect name changes done at CustomForwardRenderer asset in project view. case 1246256 Fixing camera overlay stacking adding to respect unity general reference restrictions. case 1240788 Fixed profiler marker errors. case 1240963 Fixed issue that caused the pipeline to not create _CameraColorTexture if a custom render pass is injected. case 1232761 Fixed target eye UI for XR rendering is missing from camera inspector. case 1261612 Fixed an issue where terrain and speedtree materials would not get upgraded by upgrade project materials. case 1204189 Fixed an issue that caused renderer feature to not render correctly if the pass was injected before rendering opaques and didn't implement Configure method. case 1259750 Fixed an issue where postFX's temp texture is not released properly. Fixed an issue where ArgumentOutOfRangeException errors were thrown after removing Render feature case 1268147 Fixed an issue where depth and depth/normal of grass isn't rendered to depth texture. Fixed an issue that impacted MSAA performance on iOS/Metal case 1219054 Fixed an issue that caused a warning to be thrown about temporary render texture not found when user calls ConfigureTarget(0). case 1220871 Fixed performance issues in the C# shader stripper. [7.1.1] - 2019-09-05 Upgrade Guide The render pipeline now handles custom renderers differently. You must now set up renderers for the Camera on the Render Pipeline Asset. Render Pipeline Assets upgrades automatically and either creates a default forward renderer in your project or links the existing custom one that you've assigned. If you have custom renderers assigned to Cameras, you must now add them to the current Render Pipeline Asset. Then you can select which renderer to use on the Camera. Added Added shader function GetMainLightShadowParams. This returns a half4 for the main light that packs shadow strength in x component and shadow soft property in y component. Added shader function GetAdditionalLightShadowParams. This returns a half4 for an additional light that packs shadow strength in x component and shadow soft property in y component. Added a Debug Level option to the Render Pipeline Asset. With this, you can control the amount of debug information generated by the render pipeline. Added ability to set the ScriptableRenderer that the Camera renders with via C# using UniversalAdditionalCameraData.SetRenderer(int index). This maps to the Renderer List on the Render Pipeline Asset. Added shadow support for the 2D Renderer. Added ShadowCaster2D, and CompositeShadowCaster2D components. Added shadow intensity and shadow volume intensity properties to Light2D. Added new Gizmos for Lights. Added CinemachineUniversalPixelPerfect, a Cinemachine Virtual Camera Extension that solves some compatibility issues between Cinemachine and Pixel Perfect Camera. Added an option that disables the depth/stencil buffer for the 2D Renderer. Added manipulation handles for the inner cone angle for spot lights. Added documentation for the built-in post-processing solution and Volumes framework (and removed incorrect mention of the PPv2 package). Changed Increased visible lights limit for the forward renderer. It now supports 256 visible lights except in mobile platforms. Mobile platforms support 32 visible lights. Increased per-object lights limit for the forward renderer. It now supports 8 per-object lights in all platforms except GLES2. GLES2 supports 4 per-object lights. The Sprite-Lit-Default shader and the Sprite Lit Shader Graph shaders now use the vertex tangents for tangent space calculations. Temporary render textures for cameras rendering to render textures now use the same format and multisampling configuration as camera's target texture. All platforms now use R11G11B10_UFloat format for HDR render textures if supported. There is now a list of ScriptableRendererData on the Render Pipeline Asset as opposed to a renderer type. These are available to all Cameras and are included in builds. The renderer override on the Camera is now an enum that maps to the list of ScriptableRendererData on the Render Pipeline Asset. Pixel Perfect Camera now allows rendering to a render texture. Light2D GameObjects that you've created now have a default position with z equal to 0. Documentation: Changed the \"Getting Started\" section into \"Install and Configure\". Re-arranged the Table of Content. Fixed Fixed LightProbe occlusion contribution. case 1146667 Fixed an issue that caused a log message to be printed in the console when creating a new Material. case 1173160 Fixed an issue where OnRenderObjectCallback was never invoked. case 1122420 Fixed an issue where Sprite Masks didn't function properly when using the 2D Renderer. case 1163474 Fixed memory leaks when using the Frame Debugger with the 2D Renderer. Fixed an issue where materials using _Time did not animate in the scene. 1175396 Fixed an issue where the Particle Lit shader had artifacts when both soft particles and HDR were enabled. 1136285 Fixed an issue where the Area Lights were set to Realtime, which caused them to not bake. 1159838 Fixed an issue where the Disc Light did not generate any light. 1175097 Fixed an issue where the alpha was killed when an opaque texture was requested on an offscreen camera with HDR enabled case 1163320. Fixed an issue that caused Orthographic camera with far plane set to 0 to span Unity console with errors. case 1172269 Fixed an issue causing heap allocation in RenderPipelineManager.DoRenderLoop case 1156241 Fixed an issue that caused shadow artifacts when using large spot angle values case 1136165 Fixed an issue that caused self-shadowing artifacts when adjusting shadow near-plane on spot lights. Fixed an issue that caused specular highlights to disappear when the smoothness value was set to 1.0. case 1161827 Fixed an issue in the Material upgrader that caused transparent Materials to not upgrade correctly to Universal RP. case 1170419. Fixed an issue causing shadows to be incorrectly rendered when a light was close to the shadow caster. Fixed post-processing for the 2D Renderer. Fixed an issue in Light2D that caused a black line to appear for a 360 degree spotlight. Fixed a post-processing rendering issue with non-fullscreen viewport. case 1177660 Fixed an issue where Undo would not undo the creation of Additional Camera Data. case 1158861 Fixed an issue where selecting the same drop-down menu item twice would trigger a change event. case 1158861 Fixed an issue where selecting certain objects that use instancing materials would throw console warnings. case 1127324 Fixed a GUID conflict with LWRP. case 1179895 Fixed an issue where the Terrain shader generated NaNs. Fixed an issue that caused the Opaque Color pass to never render at half or quarter resolution. Fixed and issue where stencil state on a ForwardRendererData was reset each time rendering happened. [7.0.1] - 2019-07-25 Changed Platform checks now provide more helpful feedback about supported features in the Inspectors. Fixed Fixed specular lighting related artifacts on Mobile case 1143049 and case 1164822. Post-processing is no longer enabled in the previews. Unity no longer force-enables post-processing on a camera by default. Fixed an issue that caused the Scene to render darker in GLES3 and linear color space. case 1169789 [7.0.0] - 2019-07-17 Universal Render Pipeline LWRP has been renamed to the \"Universal Render Pipeline\" (UniversalRP). UniversalRP is the same as LWRP in terms of features and scope. Classes have moved to the Universal namespace (from LWRP). Upgrade Guide Upgrading to URP is designed to be almost seamless from the user side. LWRP package still exists, this forwards includes and classes to the UniversalRP Package. Please see the more involved upgrade guide (https://docs.google.com/document/d/1Xd5bZa8pYZRHri-EnNkyhwrWEzSa15vtnpcg--xUCIs/). Added Initial Stadia platform support. Added a menu option to create a new ScriptableRendererFeature script. To do so in the Editor, click on Asset > Create > Rendering > Lightweight Render Pipeline > Renderer Feature. Added documentation for SpeedTree Shaders in LWRP. Added extended features to LWRP Terrain Shader, so terrain assets can be forward-compatible with HDRP. Enabled per-layer advanced or legacy-mode blending in LWRP Terrain Shader. Added the documentation page \"Rendering in LWRP\", which describes the forward rendering camera loop. Added documentation overview for how Post Processing Version 2 works in LWRP. Added documentation notes and FAQ entry on the 2D Renderer affecting the LWRP Asset. Changed Replaced beginCameraRendering callbacks by non obsolete implementation in Light2D Updated ScriptableRendererFeature and ScriptableRenderPass API docs. Changed shader type Real to translate to FP16 precision on some platforms. Fixed Fixed a case where built-in Shader time values could be out of sync with actual time. case 1142495 Fixed an issue that caused forward renderer resources to not load properly when you upgraded LWRP from an older version to 7.0.0. case 1154925 Fixed GC spikes caused by LWRP allocating heap memory every frame. Fixed distortion effect on particle unlit shader. Fixed NullReference exception caused when trying to add a ScriptableRendererFeature. Fixed issue with certain LWRP shaders not showing when using forward/2D renderer. Fixed the shadow resolve pass and the final pass, so they're not consuming unnecessary bandwidth. case 1152439 Added missing page for 2D Lights in LWRP. Tilemap tiles no longer appear black when you use the 2D renderer. Sprites in the preview window are no longer lit by 2D Scene lighting. Fixed warnings for unsupported shadow map formats for GLES2 API. Disabled shadows for devices that do not support shadow maps or depth textures. Fixed support for LWRP per-pixel terrain. case 1110520 Fixed some basic UI/usability issues with LWRP terrain Materials (use of warnings and modal value changes). Fixed an issue where using LWRP and Sprite Shape together would produce meta file conflicts. Fixed specular calculation fp16 overflow on some platforms Fixed shader compilation errors for Android XR projects. Updated the pipeline Asset UI to cap the render scale at 2x so that it matches the render pipeline implementation limit. [6.7.0] - 2019-05-16 Added Added SpeedTree Shaders. Added two Shader Graph master nodes: Lit Sprite and Unlit Sprite. They only work with the 2D renderer. Added documentation for the 2D renderer. Changed The 2D renderer and Light2D component received a number of improvements and are now ready to try as experimental features. Updated the Feature Comparison Table page to reflect the current state of LWRP features. Fixed When in playmode, the error 'Non matching Profiler.EndSample' no longer appears. case 1140750 LWRP Particle Shaders now correctly render in stereo rendering modes. case 1106699 Shaders with 'debug' in the name are no longer stripped automatically. case 1112983 Fixed tiling issue with selection outline and baked cutout shadows. in the Shadergraph Unlit Master node, Premultiply no longer acts the same as Alpha. case 1114708 Fixed an issue where Lightprobe data was missing if it was needed per-pixel and GPU instancing was enabled. The Soft ScreenSpaceShadows Shader variant no longer gets stripped form builds. case 1138236 Fixed a typo in the Particle Unlit Shader, so Soft Particles now work correctly. Fixed emissive Materials not being baked for some meshes. case 1145297 Camera matrices are now correctly set up when you call rendering functions in EndCameraRendering. case 1146586 Fixed GI not baking correctly while in gamma color space. Fixed a NullReference exception when adding a renderer feature that is contained in a global namespace. case 1147068 Shaders are now set up for VR stereo instancing on Vulkan. case 1142952. VR stereo matrices and vertex inputs are now set up on Vulkan. case 1142952. Fixed the Material Upgrader so it's now run upon updating the LWRP package. 1148764 Fixed a NullReference exception when you create a new Lightweight Render Pipeline Asset. case 1153388 [6.6.0] - 2019-04-01 Added Added support for Baked Indirect mixed lighting. You can now use Light Probes for occlusion. This means that baked lights can now occlude dynamic objects. Added RenderObjects. You can add RenderObjects to a Renderer to perform custom rendering. (WIP) Added an experimental 2D renderer that implements a 2D lighting system. (WIP) Added a Light2D component that works with the 2D renderer to add lighting effects to 2D sprites. Fixed Fixed a project import issue in the LWRP template. Fixed the warnings that appear when you create new Unlit Shader Graphs using the Lightweight Render Pipeline. Fixed light attenuation precision on mobile platforms. Fixed split-screen rendering on mobile platforms. Fixed rendering when using an off-screen camera that renders to a depth texture. Fixed the exposed stencil render state in the renderer. Fixed the default layer mask so it's now applied to a depth pre-pass. Made several improvements and fixes to the render pass UI. Fixed artifacts that appeared due to precision errors in large scaled objects. Fixed an XR rendering issue where Unity required a depth texture. Fixed an issue that caused transparent objects to sort incorrectly. [6.5.0] - 2019-03-07 Added You can now create a custom forward renderer by clicking on Assets/Create/Rendering/Lightweight Render Pipeline/Forward Renderer. This creates an Asset in your Project. You can add additional features to it and drag-n-drop the renderer to either the pipeline Asset or to a camera. You can now add ScriptableRendererFeature to the ScriptableRenderer to extend it with custom effects. A feature is an ScriptableObject that can be drag-n-dropped in the renderer and adds one or more ScriptableRenderPass to the renderer. ScriptableRenderer now exposes interface to configure lights. To do so, implement SetupLights when you create a new renderer. ScriptableRenderer now exposes interface to configure culling. To do so, implement SetupCullingParameters when you create a new renderer. ScriptableRendererData contains rendering resources for ScriptableRenderer. A renderer can be overridden globally for all cameras or on a per-camera basis. ScriptableRenderPass now has a RenderPassEvents. This controls where in the pipeline the render pass is added. ScriptableRenderPass now exposes ConfigureTarget and ConfigureClear. This allows the renderer to automatically figure out the currently active rendering targets. ScriptableRenderPass now exposes Blit. This performs a blit and sets the active render target in the renderer. ScriptableRenderPass now exposes RenderPostProcessing. This renders post-processing and sets the active render target in the renderer. ScriptableRenderPass now exposes CreateDrawingSettings as a helper for render passes that need to call ScriptableRenderContext.DrawRenderers. Changed Removed RegisterShaderPassName from ScriptableRenderPass. Instead, CreateDrawingSettings now takes one or a list of ShaderTagId. Removed remaining experimental namespace from LWRP. All APIrelated to ScriptableRenderer, ScriptableRenderPass, and render pass injection is now out of preview. Removed SetRenderTarget from ScriptableRenderPass. You should never call it. Instead, call ConfigureTarget, and the renderer automatically sets up targets for you. Removed RenderFullscreenQuad from ScriptableRenderer. Use CommandBuffer.DrawMesh and RenderingUtils.fullscreenMesh instead. Removed RenderPostProcess from ScriptableRenderer. Use ScriptableRenderPass.RenderPostProcessing instead. Removed postProcessingContext property from ScriptableRenderer. This is now exposed in RenderingUtils.postProcessingContext. Removed GetCameraClearFlag from ScriptableRenderer. Fixed Fixed y-flip in VR when post-processing is active. Fixed occlusion mesh for VR not rendering before rendering opaques. Enabling or disabling SRP Batcher in runtime works now. Fixed video player recorder when post-processing is enabled. [6.4.0] - 2019-02-21 [6.3.0] - 2019-02-18 [6.2.0] - 2019-02-15 Changed Code refactor: all macros with ARGS have been swapped with macros with PARAM. This is because the ARGS macros were incorrectly named. [6.1.0] - 2019-02-13 [6.0.0] - 2019-02-23 Added You can now implement a custom renderer for LWRP. To do so, implement an IRendererData that contains all resources used in rendering. Then create an IRendererSetup that creates and queues ScriptableRenderPass. Change the renderer type either in the Pipeline Asset or in the Camera Inspector. LWRP now uses the Unity recorder extension. You can use this to capture the output of Cameras. You can now inject a custom render pass before LWRP renders opaque objects. To do so, implement an IBeforeRender interface. Distortion support in all Particle Shaders. An upgrade system for LWRP Materials with MaterialPostprocessor. An upgrade path for Unlit shaders Tooltips for Shaders. SRP Batcher support for Particle Shaders. Docs for these Shaders: Baked Lit, Particles Lit, Particles Simple Lit, and Particles Unlit. LWRP now supports dynamic resolution scaling. The target platform must also support it. LWRP now includes version defines for both C# and Shaders in the format of LWRP_X_Y_Z_OR_NEWER. For example, LWRP_5_3_0_OR_NEWER defines version 5.3.0. The Terrain Lit Shader now samples Spherical Harmonics if you haven't baked any lightmaps for terrain. Added a Priority option, which you can use to tweak the rendering order. This is similar to render queue in the built-in render pipeline. These Shaders now have this option: Lit, Simple Lit, Baked Lit, Unlit, and all three Particle Shaders. Added support for overriding terrain detail rendering shaders, via the render pipeline editor resources asset. Changed You can now only initialize a camera by setting a Background Type. The supported options are Skybox, Solid Color, and Don't Care. LWRP now uses non-square shadowmap textures when it renders directional shadows with 2 shadow cascades. LWRP now uses RGB111110 as the HDR format on mobile devices, when this format is supported. Removed IAfterDepthPrePass interface. We’ve redesigned the Shader GUI. For example, all property names in Shaders are now inline across the board The Simple Lit Shader now has Smoothness, which can be stored in the alpha of specular or albedo maps. The Simple Lit and Particles Simple Lit Shaders now take shininess from the length (brightness) of the specular map. The Double sided property is now Render Face. This means you can also do front face culling. Changed the docs for Lit Shader, Simple Lit Shader and Unlit Shader according to Shader GUI changes. When you create a new LWRP Asset, it will now be initialized with settings that favor performance on mobile platforms. Updated the FAQ section and the Built-in/LWRP feature comparison table. Fixed Several tweaks to reduce bandwidth consumption on mobile devices. The foldouts in the Lightweight Asset inspector UI now remember their state. Added missing meta file for GizmosRenderingPass.cs. Fixed artifacts when using multiple or Depth Only cameras. Case 1072615 Fixed a typo in ERROR_ON_UNSUPPORTED_FUNCTION() that was causing the shader compiler to run out of memory in GLES2. Case 1104271 LWRP now renders shadows on scaled objects correctly. Case 1109017 LWRP now allows some Asset settings to be changed at runtime. Case 1105552 Realtime shadows now work in GLES2. Case 1087251 Framedebugger now renders correctly when stepping through drawcalls. Cameras that request MSAA and Opaque Textures now use less frame bandwidth when they render. Fixed rendering in the gamma color space, so it doesn't appear darker. Particles SImple Lit and Particles Unlit Shaders now work correctly. Soft Particles now work correctly. Camera fading for particles. Fixed a typo in the Unlit IgnoreProjector tag. Particles render in both eyes with stereo instancing Fixed specular issues on mobile. case 1109017 Fixed issue causing LWRP to create MSAA framebuffer even when MSAA setting was disabled. Post-processing in mobile VR is now forced to be disabled. It was causing many rendering issues. Fixed Editor Previews breaking in Play Mode when VR is enabled. Case 1109009 A camera's HDR enable flag is now respected when rendering in XR. Terrain detail rendering now works correctly when LWRP is installed but inactive. [5.2.0] - 2018-11-27 Added LWRP now handles blits that are required by the device when rendering to the backbuffer. You can now enable the SRP Batcher. To do so, go to the Pipeline Asset. Under Advanced, toggle SRP Batcher. Changed Renamed shader variable unity_LightIndicesOffsetAndCount to unity_PerObjectLightData. Shader variables unity_4LightIndices0 and unity_4LightIndices1 are now declared as unity_PerObjectLightIndices array. [5.1.0] - 2018-11-19 Added The user documentation for LWRP is now in this GitHub repo, instead of in the separate GitHub wiki. You can find the most up-to-date pages in the TableOfContents.md file. Pages not listed in that file are still in progress. Changed The LWRP package is no longer in preview. LWRP built-in render passes are now internal. Changed namespace from UnityEngine.Experimental.Rendering.LightweightPipeline to UnityEngine.Rendering.LWRP. Changed namespace from UnityEditor.Experimental.Rendering.LightweightPipeline to UnityEditor.Rendering.LWRP. Fixed LWRP now respects the iOS Player setting Force hard shadows. When you enable this setting, hardware filtering of shadows is disabled. Scene view mode now renders baked lightmaps correctly. Case 1092227 Shadow bias calculations are now correct for both Shader Graph and Terrain shaders. Blit shader now ignores culling. When you select Per Vertex option for Additional Lights, the Per Object Limit option is not greyed out anymore. When you change camera viewport height to values above 1.0, the Unity Editor doesn't freeze anymore. Case 1097497 When you use AR with LWRP, the following error message is not displayed in the console anymore: \"The camera list passed to the render pipeline is either null or empty.\" [5.0.0-preview] - 2018-09-28 Added Added occlusion mesh rendering/hookup for VR You can now configure default depth and normal shadow bias values in the pipeline asset. You can now add the LWRPAdditionalLightData component to a Light to override the default depth and normal shadow bias. You can now log the amount of shader variants in your build. To do so, go to the Pipeline Asset. Under Advanced, select and set the Shader Variant Log Level. Changed Removed the supportedShaderFeatures property from LWRP core. The shader stripper now figures out which variants to strip based on the current assigned pipeline Asset in the Graphics settings. Fixed The following error does not appear in console anymore: (\"Begin/End Profiler section mismatch\") When you select a material with the Lit shader, this no longer causes the following error in the console: (\"Material doesn't have...\"). case 1092354 In the Simple Lit shader, per-vertex additional lights are now shaded properly. Shader variant stripping now works when you're building a Project with Cloud Build. This greatly reduces build times from Cloud Build. Dynamic Objects now receive lighting when the light mode is set to mixed. MSAA now works on Desktop platforms. The shadow bias value is now computed correctly for shadow cascades and different shadow resolutions. case 1076285 When you use Area Light with LWRP, Cast Shadows no longer overlaps with other UI elements in the Inspector. case 1085363 Changed Read/write XRGraphicsConfig -> Read-only XRGraphics interface to XRSettings. [4.0.0-preview] - 2018-09-28 Added When you have enabled Gizmos, they now appear correctly in the Game view. Added requiresDepthPrepass field to RenderingData struct to tell if the runtime platform requires a depth prepass to generate a camera depth texture. The RenderingData struct now holds a reference to CullResults. When HDR is enabled in the Camera but disabled in the Asset, an information box in the Camera Inspector informs you about it. When MSAA is enabled in the Camera but disabled in the Asset, an information box in the Camera Inspector informs you about it. Enabled instancing on the terrain shader. Sorting of opaque objects now respects camera opaqueSortMode setting. Sorting of opaque objects disables front-to-back sorting flag, when camera settings allow that and the GPU has hidden surface removal. LWRP now has a Custom Light Explorer that suits its feature set. LWRP now supports Vertex Lit shaders for detail meshes on terrain. LWRP now has three interactive Autodesk shaders: Autodesk Interactive, Autodesk Interactive Masked and Autodesk Interactive Transparent. [Shader API] The GetMainLight and GetAdditionalLight functions can now compute shadow attenuation and store it in the new shadowAttenuation field in LightData struct. [Shader API] Added a VertexPositionInputs struct that contains vertex position in difference spaces (world, view, hclip). [Shader API] Added a GetVertexPositionInputs function to get an initialized VertexPositionInputs. [Shader API] Added a GetPerObjectLightIndex function to return the per-object index given a for-loop index. [Shader API] Added a GetShadowCoord function that takes a VertexPositionInputs as input. [ShaderLibrary] Added VertexNormalInputs struct that contains data for per-pixel normal computation. [ShaderLibrary] Added GetVertexNormalInputs function to return an initialized VertexNormalInputs. Changed The RenderingData struct is now read-only. ScriptableRendereralways performs a Clear before calling IRendererSetup::Setup. ScriptableRenderPass::Execute no longer takes CullResults as input. Instead, use RenderingDataas input, since that references CullResults. IRendererSetup_Setup no longer takes ScriptableRenderContext and CullResults as input. Shader includes are now referenced via package relative paths instead of via the deprecated shader export path mechanism https://docs.unity3d.com/2018.3/Documentation/ScriptReference/ShaderIncludePathAttribute.html. The LWRP Asset settings were re-organized to be more clear. Vertex lighting now controls if additional lights should be shaded per-vertex or per-pixel. Renamed all Local Lights nomenclature to Additional Lights. Changed shader naming to conform to our SRP shader code convention. [Shader API] Renamed SpotAttenuation function to AngleAttenuation. [Shader API] Renamed _SHADOWS_ENABLED keyword to _MAIN_LIGHT_SHADOWS [Shader API] Renamed _SHADOWS_CASCADE keyword to _MAIN_LIGHT_SHADOWS_CASCADE [Shader API] Renamed _VERTEX_LIGHTS keyword to _ADDITIONAL_LIGHTS_VERTEX. [Shader API] Renamed _LOCAL_SHADOWS_ENABLED to _ADDITIONAL_LIGHT_SHADOWS [Shader API] Renamed GetLight function to GetAdditionalLight. [Shader API] Renamed GetPixelLightCount function to GetAdditionalLightsCount. [Shader API] Renamed attenuation to distanceAttenuation in LightData. [Shader API] Renamed GetLocalLightShadowStrength function to GetAdditionalLightShadowStrength. [Shader API] Renamed SampleScreenSpaceShadowMap functions to SampleScreenSpaceShadowmap. [Shader API] Renamed MainLightRealtimeShadowAttenuation function to MainLightRealtimeShadow. [Shader API] Renamed light constants from Directional and Local to MainLight and AdditionalLights. [Shader API] Renamed GetLocalLightShadowSamplingData function to GetAdditionalLightShadowSamplingData. [Shader API] Removed OUTPUT_NORMAL macro. [Shader API] Removed lightIndex and substractiveAttenuation from LightData. [Shader API] Removed ComputeShadowCoord function. GetShadowCoord is provided instead. All LightweightPipeline references in API and classes are now named LightweightRenderPipeline. Files no longer have the Lightweight prefix. Renamed Physically Based shaders to Lit, ParticlesLit, and TerrainLit. Renamed Simple Lighting shaders to SimpleLit, and ParticlesSimpleLit. [ShaderLibrary] Renamed InputSurfacePBR.hlsl, InputSurfaceSimple.hlsl, and InputSurfaceUnlit to LitInput.hlsl, SimpleLitInput.hlsl, and UnlitInput.hlsl. These files were moved from the ShaderLibrary folder to theShaders. [ShaderLibrary] Renamed LightweightPassLit.hlsl and LightweightPassLitSimple.hlsl to LitForwardPass.hlsl and SimpleLitForwardPass.hlsl. These files were moved from the ShaderLibrary folder to Shaders. [ShaderLibrary] Renamed LightweightPassMetaPBR.hlsl, LightweightPassMetaSimple.hlsl and LighweightPassMetaUnlit to LitMetaPass.hlsl, SimpleLitMetaPass.hlsl and UnlitMetaPass.hlsl. These files were moved from the ShaderLibrary folder to Shaders. [ShaderLibrary] Renamed LightweightPassShadow.hlsl to ShadowCasterPass.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed LightweightPassDepthOnly.hlsl to DepthOnlyPass.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed InputSurfaceTerrain.hlsl to TerrainLitInput.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed LightweightPassLitTerrain.hlsl to TerrainLitPases.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed ParticlesPBR.hlsl to ParticlesLitInput.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed InputSurfacePBR.hlsl to LitInput.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed InputSurfaceUnlit.hlsl to UnlitInput.hlsl. This file was moved to the Shaders folder. [ShaderLibrary] Renamed InputBuiltin.hlsl to UnityInput.hlsl. [ShaderLibrary] Renamed LightweightPassMetaCommon.hlsl to MetaInput.hlsl. [ShaderLibrary] Renamed InputSurfaceCommon.hlsl to SurfaceInput.hlsl. [ShaderLibrary] Removed LightInput struct and GetLightDirectionAndAttenuation. Use GetAdditionalLight function instead. [ShaderLibrary] Removed ApplyFog and ApplyFogColor functions. Use MixFog and MixFogColor instead. [ShaderLibrary] Removed TangentWorldToNormal function. Use TransformTangentToWorld instead. [ShaderLibrary] Removed view direction normalization functions. View direction should always be normalized per pixel for accurate results. [ShaderLibrary] Renamed FragmentNormalWS function to NormalizeNormalPerPixel. Fixed If you have more than 16 lights in a scene, LWRP no longer causes random glitches while rendering lights. The Unlit shader now samples Global Illumination correctly. The Inspector window for the Unlit shader now displays correctly. Reduced GC pressure by removing several per-frame memory allocations. The tooltip for the the camera MSAA property now appears correctly. Fixed multiple C# code analysis rule violations. The fullscreen mesh is no longer recreated upon every call to ScriptableRenderer.fullscreenMesh. [3.3.0-preview] - 2018-01-01 Added Added callbacks to LWRP that can be attached to a camera (IBeforeCameraRender, IAfterDepthPrePass, IAfterOpaquePass, IAfterOpaquePostProcess, IAfterSkyboxPass, IAfterTransparentPass, IAfterRender) ###Changed Clean up LWRP creation of render textures. If we are not going straight to screen ensure that we create both depth and color targets. UNITY_DECLARE_FRAMEBUFFER_INPUT and UNITY_READ_FRAMEBUFFER_INPUT macros were added. They are necessary for reading transient attachments. UNITY_MATRIX_I_VP is now defined. Renamed LightweightForwardRenderer to ScriptableRenderer. Moved all light constants to _LightBuffer CBUFFER. Now _PerCamera CBUFFER contains all other per camera constants. Change real-time attenuation to inverse square. Change attenuation for baked GI to inverse square, to match real-time attenuation. Small optimization in light attenuation shader code. Fixed Lightweight Unlit shader UI doesn't throw an error about missing receive shadow property anymore. [3.2.0-preview] - 2018-01-01 Changed Receive Shadows property is now exposed in the material instead of in the renderer. The UI for Lightweight asset has been updated with new categories. A more clean structure and foldouts has been added to keep things organized. Fixed Shadow casters are now properly culled per cascade. (case 1059142) Rendering no longer breaks when Android platform is selected in Build Settings. (case 1058812) Scriptable passes no longer have missing material references. Now they access cached materials in the renderer.(case 1061353) When you change a Shadow Cascade option in the Pipeline Asset, this no longer warns you that you've exceeded the array size for the _WorldToShadow property. Terrain shader optimizations. [3.1.0-preview] - 2018-01-01 Fixed Fixed assert errors caused by multi spot lights Fixed LWRP-DirectionalShadowConstantBuffer params setting [3.0.0-preview] - 2018-01-01 Added Added camera additional data component to control shadows, depth and color texture. pipeline now uses XRSEttings.eyeTextureResolutionScale as renderScale when in XR. New pass architecture. Allows for custom passes to be written and then used on a per camera basis in LWRP Changed Shadow rendering has been optimized for the Mali Utgard architecture by removing indexing and avoiding divisions for orthographic projections. This reduces the frame time by 25% on the Overdraw benchmark. Removed 7x7 tent filtering when using cascades. Screenspace shadow resolve is now only done when rendering shadow cascades. Updated the UI for the Lighweight pipeline asset. Update assembly definitions to output assemblies that match Unity naming convention (Unity.*). Fixed Post-processing now works with VR on PC. Console platform compiler error Fixed VR multiview rendering by forcing MSAA to be off. There's a current issue in engine that breaks MSAA and Texture2DArray. Fixed UnityPerDraw CB layout GLCore compute buffer compiler error Occlusion strength not being applied on LW standard shaders CopyDepth pass is being called even when a depth from prepass is available GLES2 shader compiler error in IntegrationTests Can't set RenderScale and ShadowDistance by script VR Single Pass Instancing shadows Fixed compilation errors on platforms with limited XRSetting support. [2.0.0-preview] - 2018-01-01 Added Explicit render target load/store actions were added to improve tile utilization Camera opaque color can be requested on the pipeline asset. It can be accessed in the shader by defining a _CameraOpaqueTexture. This can be used as an alternative to GrabPass. Dynamic Batching can be enabled in the pipeline asset Pipeline now strips unused or invalid variants and passes based on selected pipeline capabilities in the asset. This reduces build and memory consuption on target. Shader stripping settings were added to pipeline asset Changed Pipeline Pipeline code is now more modular and extensible. A ForwardRenderer class is initialized by the pipeline with RenderingData and it's responsible for enqueueing and executing passes. In the future pluggable renderers will be supported. On mobile 1 directional light + up to 4 local lights (point or spot) are computed On other platforms 1 directional light + up to 8 local lights are computed Multiple shadow casting lights are supported. Currently only 1 directional + 4 spots light shadows. Shading Framework Directional Lights are always considered a main light in shader. They have a fast shading path with no branching and no indexing. GetMainLight() is provided in shader to initialize Light struct with main light shading data. Directional lights have a dedicated shadowmap for performance reasons. Shadow coord always comes from interpolator. MainLigthRealtimeShadowAttenuation(float4 shadowCoord) is provided to compute main light realtime shadows. Spot and Point lights are always shaded in the light loop. Branching on uniform and indexing happens when shading them. GetLight(half index, float3 positionWS) is provided in shader to initialize Light struct for spot and point lights. Spot light shadows are baked into a single shadow atlas. Shadow coord for spot lights is always computed on fragment. Use LocalLightShadowAttenuation(int lightIndex, float3 positionWS) to comppute realtime shadows for spot lights. Fixed Issue that was causing VR on Android to render black Camera viewport issues UWP build issues Prevent nested camera rendering in the pipeline [1.1.4-preview] - 2018-01-01 Added Terrain and grass shaders ported Updated materials and shader default albedo and specular color to midgrey. Exposed _ScaledScreenParams to shader. It works the same as _ScreenParams but takes pipeline RenderScale into consideration Performance Improvements in mobile Fixed SRP Shader library issue that was causing all constants to be highp in mobile shader error that prevented LWRP to build to UWP shader compilation errors in Linux due to case sensitive includes Rendering Texture flipping issue Standard Particles shader cutout and blending modes crash caused by using projectors issue that was causing Shadow Strength to not be computed on mobile Material Upgrader issue that caused editor to SoftLocks GI in Unlit shader Null reference in the Unlit material shader GUI [1.1.2-preview] - 2018-01-01 Changed Performance improvements in mobile Fixed Shadows on GLES 2.0 CPU performance regression in shadow rendering Alpha clip shadow issues Unmatched command buffer error message Null reference exception caused by missing resource in LWRP Issue that was causing Camera clear flags was being ignored in mobile [1.1.1-preview] - 2018-01-01 Added Added Cascade Split selection UI Added SHADER_HINT_NICE_QUALITY. If user defines this to 1 in the shader Lightweight pipeline will favor quality even on mobile platforms. Changed Shadowmap uses 16bit format instead of 32bit. Small shader performance improvements Fixed Subtractive Mode Shadow Distance does not accept negative values anymore [0.1.24] - 2018-01-01 Added Added Light abstraction layer on lightweight shader library. Added HDR global setting on pipeline asset. Added Soft Particles settings on pipeline asset. Ported particles shaders to SRP library Changed HDR RT now uses what format is configured in Tier settings. Refactored lightweight standard shaders and shader library to improve ease of use. Optimized tile LOAD op on mobile. Reduced GC pressure Reduced shader variant count by ~56% by improving fog and lightmap keywords Converted LW shader library files to use real/half when necessary. Fixed Realtime shadows on OpenGL Shader compiler errors in GLES 2.0 Issue sorting issues when BeforeTransparent custom fx was enabled. VR single pass rendering. Viewport rendering issues when rendering to backbuffer. Viewport rendering issues when rendering to with MSAA turned off. Multi-camera rendering. [0.1.23] - 2018-01-01 Added UI Improvements (Rendering features not supported by LW are hidden) Changed Shaders were ported to the new SRP shader library. Constant Buffer refactor to use new Batcher Shadow filtering and bias improved. Pipeline now updates color constants in gamma when in Gamma colorspace. Optimized ALU and CB usage on Shadows. Reduced shader variant count by ~33% by improving shadow and light classification keywords Default resources were removed from the pipeline asset. Fixed Fixed shader include path when using SRP from package manager. Fixed spot light attenuation to match Unity Built-in pipeline. Fixed depth pre-pass clearing issue. [0.1.12] - 2018-01-01 Added Standard Unlit shader now has an option to sample GI. Added Material Upgrader for stock Unity Mobile and Legacy Shaders. UI improvements Changed Realtime shadow filtering was improved. Fixed Fixed an issue that was including unreferenced shaders in the build. Fixed a null reference caused by Particle System component lights."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DLightProperties.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DLightProperties.html",
    "title": "Common properties of 2D Lights | FSM Unity Framework",
    "keywords": "Common properties of 2D Lights Each 2D Light Type has various properties and options to customize their appearance and behavior. This page documents the properties that are common to all 2D Light Types. following are the common properties used by the different Light types. For properties specific to each of the available Light Types, refer to their respective sections: Freeform Sprite Spot (Note: The Point Light Type has been renamed to the Spot Light Type from URP 11 onwards.) Global Creating a Light Create a 2D Light GameObject by going to GameObject > Light > 2D and selecting one of the five available types: Freeform: You can edit the shape of this Light type with a spline editor. Sprite: You can select a Sprite to create this Light type. Spot: You can control the inner and outer radius, direction and angle of this Light type. Global: This 2D Light affects all rendered Sprites on all targeted sorting layers. The following are the common properties used by the different Light types. Property Function Light Type Select the type of Light you want the selected Light to be. The available types are Freeform, Sprite, Parametric, Spot, and Global. Color Use the color picker to set the color of the emitted light. Intensity Enter the desired brightness value of the Light. The default value is 1. Overlap Operation Select the overlap operation used by this light The operations available are Additive, and Alpha Blend. Target Sorting Layers Select the sorting layers that this Light targets and affects. Blend Style Select the blend style used by this Light. Different blend styles can be customized in the 2D Renderer Asset. Light Order (unavailable for Global Lights) Enter a value here to specify the rendering order of this Light relative to other Lights on the same sorting layer(s). Lights with lower values are rendered first, and negative values are valid. Shadow Strength Use the slider to control the amount of light that Shadow Caster 2Ds block when they obscure this Light. The value scales from 0 (no light is blocked) to 1 (all light is blocked). Volumtric Intensity Use the slider to select the opacity of the volumetric lighting. The value scales from 0 (transparent) to 1 (opaque). Volumetric Shadow Strength Use the slider to control the amount of volumetric light that Shadow Caster 2Ds block when they obscure this Light. The value scales from 0 (no light is blocked) to 1 (all light is blocked). Normal Map Quality Select either Disabled (degfault)m Accurate or Fast to adjust the accuracy of the lighting calculations used. Normal Map Distance (available when Use Normal Map quality is not disabled) Enter the desired distance (in Unity units) between the Light and the lit Sprite. This does not Transform the position of the Light in the Scene. Overlap Operation This property controls the way in the selected Light interacts with other rendered Lights. You can toggle between the two modes by enabling or disabling this property. The effects of both modes are shown in the examples below: Overlap Operation set to Additive Overlap Operation set to Alpha Blend When Overlap Operation is set to Additive, the Light is blended with other Lights additively, where the pixel values of intersecting Lights are added together. This is the default Light blending behavior. When Overlap Operation is set to Alpha Blend, Lights are blended together based on their alpha values. This can be used to completely overwrite one Light with another where they intersect, but the render order of the Lights is also dependent on the Light Order of the different Lights. Light Order The Light Order value determines the position of the Light in the Render queue relative to other Lights that target the same sorting layer(s). Lower numbered Lights are rendered first, with higher numbered Lights rendered above those below. This especially affects the appearance of blended Lights when Overlap Operation is set to Alpha Blend. Intensity Light intensity are available to all types of Lights. Color adjusts the lights color, while intensity allows this color to go above 1. This allows lights which use multiply to brighten a sprite beyond its original color. Use Normal Map All lights except for global lights can be toggled to use the normal maps in the sprites material. When enabled, Distance and Accuracy will be visible as new properties. Use Normal Map: __Disabled Use Normal Map: Enabled Distance Distance controls the distance between the light and the surface of the Sprite, changing the resulting lighting effect. This distance does not affect intensity, or transform the position of the Light in the Scene. The following examples show the effects of changing the Distance values. Distance: 0.5 Distance: 2 Distance: 8 Quality Light quality allows the developer to choose between performance and accuracy. When choosing performance, artefacts may occur. Smaller lights and larger distance values will reduce the difference between fast and accurate. Volume Opacity Volumetric lighting is available to all Light types. Use the Volume Opacity slider to control the visibility of the volumetric light. At a value of zero, no Light volume is shown while at a value of one, the Light volume appears at full opacity. Shadow Intensity The Shadow Intensity property controls the amount of light that Shadow Caster 2Ds block from the Light source which affects the intensity of their shadows. This is available on all non global Light types. Use this slider to control the amount of light that Shadow Caster 2Ds block when they interact with or block this Light. The slider ranges from 0 to 1. At 0, Shadow Caster 2Ds do not block any light coming from the Light source and they create no shadows. At the maximum value of 1, Shadow Caster 2Ds block all light from the Light source and create shadows at full intensity. Shadow Intensity = 0.0 Shadow Intensity = 0.5 Shadow Intensity = 1.0 Shadow Volume Intensity Shadow Volume Intensity determines the amount of volumetric light Shadow Caster 2Ds block from the Light source. It is available on all non global lights, and when Volume Opacity is above zero. Use this slider to control the amount of volumetric light that Shadow Caster 2Ds block when they interact with or block this Light. The slider ranges from 0 to 1. At 0, Shadow Caster 2Ds do not block any light coming from the Light source and they create no shadows. At the maximum value of 1, Shadow Caster 2Ds block all light from the Light source and create shadows at full intensity. Target Sorting Layers Lights only light up the Sprites on their targeted sorting layers. Select the desired sorting layers from the drop-down menu for the selected Light. To add or remove sorting layers, refer to the Tag Manager - Sorting Layers for more information."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DRendererData_overview.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DRendererData_overview.html",
    "title": "2D Renderer Data Asset | FSM Unity Framework",
    "keywords": "2D Renderer Data Asset The 2D Renderer Data Asset contains the settings that affect the way 2D Lights are applied to lit Sprites. You can set the way Lights emulate HDR lighting with the HDR Emulation Scale, or customize your own Light Blend Styles. Refer to their respective pages for more information about their properties and options. Default Material Type Unity assigns a Material of the selected Default Material Type to Sprites when they are created. The available options have the following properties and functions. Lit: Unity assigns a Material with the Lit type (default Material: Sprite-Lit-Default). 2D Lights affect Materials of this type. Unlit: Unity assigns a Material with the Unlit type (default Material: Sprite-Lit-Default). 2D Lights do not affect Materials of this type. Custom: Unity assigns a Material with the Custom type. When you select this option, Unity shows the Default Custom Material box. Assign the desired Material to this box. Use Depth/Stencil Buffer This option is enabled by default. Clear this option to disable the Depth/Stencil Buffer. Doing so might improve your project’s performance, especially on mobile platforms. You should clear this option if you are not using any features that require the Depth/Stencil Buffer (such as Sprite Mask). Camera Sorting Layer Texture The 2D Renderer Data specifies how Unity supplies the shader variable CameraSortingLayerTexture for use in custom shaders. It is recommended that you use this data in the same frame and on the following layers, as using CameraSortingLayerTexture before it has been captured may result in unexpected results. Foremost Sorting Layer All Layers captured for use in the supplied Texture will be drawn from the very back Layer up to and including the Layer specified by Foremost Sorting Layer. Downsampling Method Downsampling reduces the Texture resolution used by CameraSortingLayerTexture. The options are: None, 2x Bilinear, 4x Box, 4x Bilinear. Renderer Features The 2D Renderer supports URP Renderer Features. The setup for the features are called before any of the 2D built-in passes are queued. Refer to the URP Renderer Features documentation for more information."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DShadows.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2DShadows.html",
    "title": "| FSM Unity Framework",
    "keywords": "Shadow Caster 2D The Shadow Caster 2D component defines the shape and properties that a Light uses to determine its cast shadows. Add the Shadow Caster 2D component to a GameObject by going to menu: Component > Rendering > 2D > Shadow Caster 2D. Property Function Use Renderer Silhouette Enable this and Self Shadows to include the GameObject Renderer's silhouette as part of the shadow. Enable this and disable Self Shadows to exclude the Renderer's silhouette from the shadow. This option is only available when a valid Renderer is present. Casts Shadows Enable this to have the Renderer cast shadows. Self Shadows Enable this to have the Renderer cast shadows on itself. Use Renderer Silhouette disabled, Self Shadow disabled Use Renderer Silhouette enabled, Self Shadow disabled Use Renderer Silhouette disabled, Self Shadows enabled Use Renderer Silhouette enabled, Self Shadows enabled Composite Shadow Caster 2D The Composite Shadow Caster 2D merges the shape of multiple Shadow Caster 2Ds together as a single Shadow Caster 2D. Add the Composite Shadow Caster 2D component to a GameObject by going to menu: Component > Rendering > 2D > Composite Shadow Caster 2D, then parent GameObjects with the Shadow Caster 2D component to it. The Composite component merges all Shadow Caster 2Ds within this hierarchy, including any Shadow Caster 2Ds on the parent as well. Without Composite Shadow Caster 2D With Composite Shadow Caster 2D"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-customlit.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-customlit.html",
    "title": "Custom Lighting in 2D | FSM Unity Framework",
    "keywords": "Custom Lighting in 2D The default lighting model in 2D renderer is meant for generic use and was design to provide some level flexibility. However, it is not infinitely flexible and may not be able to meet the needs for more custom or advance effects. You can now make your own 2D Lighting model. Sprite Custom Lit Shader Graph The new Shader Graph target \"Custom Lit Shader Graph\" provides a great starting point to create a custom lithing model shader. It does not sample the Light Textures but it does have a Normal pass and a fallback Forward pass for use in non 2D Renderer. 2D Light Texture 2D Light Textures are Render Textures created by the 2D Renderer that contain the visible lights in the scene. There are up to 4 textures each representing a blend style in the 2D Renderer Data The built in Lit shaders will sample these textures and combined them with the Sprite's textures to create the lighting effect. 2D Light Texture Node To sample the Light Texture use the new \"2D Light Texture\" node in Shader Graph. The output of the node is the same as the output of a \"Texture 2D\" and should be fed into a \"Texture Sampler\". Creating the Emissive Effect with Custom Lit Shader The emissive effect is the perfect example of utilizing the Custom Lit Shader to create a custom effect. By combining the a mask texture to identify areas of the Sprite that should not receive lighting effect. The \"Secondary Texture\" feature is a great way to load the emissive mask."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-index.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-index.html",
    "title": "| FSM Unity Framework",
    "keywords": "#2D Graphics Features 2D features included with URP are the 2D Lighting graphics pipeline which allows you to create 2D Lights and 2D lighting effects; and the 2D Pixel Perfect Camera for implementing the pixelated visual style with your projects. The following are the different 2D Light Types included in the package's Light 2D component: Freeform Sprite Spot (Note: The Point Light Type has been renamed to the Spot Light Type from URP 11 onwards.) Global Important: The Parametric Light Type is deprecated from URP 11 onwards. To convert existing Parametric lights to Freeform lights, go to Edit > Render Pipeline > Universal Render Pipeline > Upgrade Project/Scene Parametric Lights to Freeform The package includes the 2D Renderer Data Asset which contains the Blend Styles parameters, and allows you to create up to four custom Light Operations for your Project. Note: If you have the experimental 2D Renderer enabled (menu: Graphics Settings > add the 2D Renderer Asset under Scriptable Render Pipeline Settings), some of the options related to 3D rendering in the URP Asset don't have any impact on your final app or game."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-pixelperfect.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/2d-pixelperfect.html",
    "title": "2D Pixel Perfect | FSM Unity Framework",
    "keywords": "2D Pixel Perfect The 2D Pixel Perfect package contains the Pixel Perfect Camera component, which ensures your pixel art remains crisp and clear at different resolutions, and stable in motion. It is a single component that makes all the calculations Unity needs to scale the viewport with resolution changes, so that you don’t need to do it manually. You can use the component settings to adjust the definition of the rendered pixel art within the camera viewport, and you can use the Run in Edit Mode feature to preview any changes immediately in the Game view. Attach the Pixel Perfect Camera component to the main Camera GameObject in the Scene, it is represented by two green bounding boxes centered on the Camera gizmo in the Scene view. The solid green bounding box shows the visible area in Game view, while the dotted bounding box shows the Reference Resolution. The Reference Resolution is the original resolution your Assets are designed for, its effect on the component's functions is detailed further in the documentation. Before using the component, first ensure your Sprites are prepared correctly for best results with the the following steps. Preparing Your Sprites After importing your textures into the project as Sprites, set all Sprites to the same Pixels Per Unit value. In the Sprites' Inspector window, set their Filter Mode to ‘Point’. Set their Compression to 'None'. Follow the steps below to correctly set the pivot for a Sprite Open the Sprite Editor for the selected Sprite. If __Sprite Mode __is set to ‘Multiple’ and there are multiple Sprite elements, then you need to set a pivot point for each individual Sprite element. Under the Sprite settings, set Pivot to ‘Custom’, then set Pivot Unit Mode to ‘Pixels’. This allows you to set the pivot point's coordinates in pixels, or drag the pivot point around freely in the Sprite Editor and have it automatically snap to pixel corners. Repeat for each Sprite element as necessary. Snap Settings To ensure the pixelated movement of Sprites are consistent with each other, follow the below steps to set the proper snap settings for your project. To open the Snap settings, go to Edit > Snap Settings. Set the Move X/Y/Z properties to 1 divided by the Pixel Perfect Camera’s Asset Pixels Per Unit (PPU) value. For example, if the Asset PPU is 100, you should set the Move X/Y/Z properties to 0.01 (1 / 100 = 0.01). Unity does not apply Snap settings retroactively. If there are any pre-existing GameObjects in the Scene, select each of them and select Snap All Axes to apply the Snap settings. Properties The component's Inspector window Property Function Asset Pixels Per Unit This is the amount of pixels that make up one unit of the Scene. Match this value to the Pixels Per Unit values of all Sprites in the Scene. Reference Resolution This is the original resolution your Assets are designed for. Crop Frame Describes what to do when there is a difference in aspect ratio. Grid Snapping Describes how to handle snapping. Current Pixel Ratio Shows the size ratio of the rendered Sprites compared to their original size. Additional Property Details Reference Resolution This is the original resolution your Assets are designed for. Scaling up Scenes and Assets from this resolution preserves your pixel art cleanly at higher resolutions. Grid Snapping Upscale Render Texture By default, the Scene is rendered at the pixel perfect resolution closest to the full screen resolution. Enable this option to have the Scene rendered to a temporary texture set as close as possible to the Reference Resolution, while maintaining the full screen aspect ratio. This temporary texture is then upscaled to fit the entire screen. The result is unaliased and unrotated pixels, which may be a desirable visual style for certain game projects. Pixel Snapping Enable this feature to snap Sprite Renderers to a grid in world space at render-time. The grid size is based on the Assets Pixels Per Unit value. Pixel Snapping prevents subpixel movement and make Sprites appear to move in pixel-by-pixel increments. This does not affect any GameObjects' Transform positions. Crop Frame Crops the viewport based on the option selected, adding black bars to match the Reference Resolution. Black bars are added to make the Game view fit the full screen resolution. Uncropped Cropped"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Building-For-Consoles.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Building-For-Consoles.html",
    "title": "Building your Project for Closed platforms | FSM Unity Framework",
    "keywords": "Building your Project for Closed platforms If you have a license to develop games for Closed platforms that require you to meet the confidentiality and legal agreements of the platform provider, then see the relevant developer forums for a link to the console specific render pipeline package. Platform package installation Closed platform packages are not available in the package registry or the Package Manager. To install a Closed platform package: Download the package from the relevant platform developer forum. Use the Package Manager to install the package locally. For information on how to install packages locally, see Installing a local package."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/EffectList.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/EffectList.html",
    "title": "Effect List | FSM Unity Framework",
    "keywords": "Effect List These are the post-processing effects that are available in the Universal Render Pipeline (URP): Bloom Channel Mixer Chromatic Aberration Color Adjustments Color Curves Depth of Field Film Grain Lens Distortion Lift Gamma Gain Motion Blur Panini Projection Shadows Midtones Highlights Split Toning Tonemapping Vignette White Balance Lens Flare"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/HDREmulationScale.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/HDREmulationScale.html",
    "title": "| FSM Unity Framework",
    "keywords": "HDR emulation scale All Lights in the 2D lighting system support HDR. While a typical RGBA32 color channel has the range of zero to one, a HDR channel can go beyond one. Light RGB(1,1,1) HDR Light RGB(1,1,1) + Light RGB(2,2,2) However, not all platforms natively support HDR textures. HDR Emulation Scale allows those platforms to use HDR lighting by compressing the number of expressible colors in exchange for extra intensity range. Scale describes this extra intensity range. Increasing this value too high may cause undesirable banding to occur. Light Intensity scale examples: HDR Reference Light Intensity Scale 1 (No HDR) Light Intensity Scale 4 Light Intensity Scale 12 When choosing a value for HDR Emulation Scale, the developer should choose the combined maximum brightness for the lights in the scene as the value."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/InstallURPIntoAProject.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/InstallURPIntoAProject.html",
    "title": "Installing the Universal Render Pipeline into an existing Project | FSM Unity Framework",
    "keywords": "Installing the Universal Render Pipeline into an existing Project You can download and install the latest version of the Universal Render Pipeline (URP) to your existing Project via the Package Manager system, and then install it into your Project. If you don’t have an existing Project, see documentation on how to start a new URP Project from a Template. Before you begin URP uses its own integrated post-processing solution. If you have the Post Processing Version 2 package installed in your Project, you need to delete the Post Processing Version 2 package before you install URP into your Project. When you have installed URP, you can then recreate your post-processing effects. URP does not currently support custom post-processing effects. If your Project uses custom post-processing effects, these cannot currently be recreated in URP. Custom post-processing effects will be supported in a forthcoming release of URP. Installing URP In Unity, open your Project. In the top navigation bar, select Window > Package Manager to open the Package Manager window. In the Packages menu, select Unity Registry. This shows the list of available packages for the version of Unity that you are currently running. Select Universal RP from the list of packages. In the bottom right corner of the Package Manager window, select Install. Unity installs URP directly into your Project. Configuring URP Before you can start using URP, you need to configure it. To do this, you need to create a Scriptable Render Pipeline Asset and adjust your Graphics settings. Creating the Universal Render Pipeline Asset The Universal Render Pipeline Asset (URP Asset) contains the global rendering and quality settings of your project, and creates the rendering pipeline instance. The rendering pipeline instance contains intermediate resources and the render pipeline implementation. To create a Universal Render Pipeline Asset: In the Editor, go to the Project window. Right-click in the Project window, and select Create > Rendering > URP Asset. Alternatively, navigate to the menu bar at the top, and select Assets > Create > Rendering > URP Asset. You can either leave the default name for the new Universal Render Pipeline Asset, or type a new one. Set URP as the active render pipeline To set URP as the active render pipeline: In your project, locate the Render Pipeline Asset that you want to use. Tip: to find all URP Assets in a project, use the following query in the search field: t:universalrenderpipelineasset. Select Edit > Project Settings > Graphics. In the Scriptable Render Pipeline Settings field, select the URP Asset. When you select the URP Asset, the available Graphics settings change immediately. Optional: Set an override URP Assets for different quality levels: Select Edit > Project Settings > Quality. Select a quality level. In the Render Pipeline Asset field, select the Render Pipeline Asset. Upgrading your shaders If your project uses the prebuilt Standard Shader, or custom Unity shaders made for the Built-in Render Pipeline, you must convert them to URP-compatible Unity shaders. For more information on this topic, see Upgrading your Shaders."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/InstallingAndConfiguringURP.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/InstallingAndConfiguringURP.html",
    "title": "Getting started | FSM Unity Framework",
    "keywords": "Getting started To use the Universal Render Pipeline (URP), you can start a new Project or upgrade an existing Project. You can do this in the following ways: Create a new URP Project from a Template. If you are starting a new Project from scratch, this is the best choice. When you do this, Unity automatically installs and configures URP for you. Install URP into an existing Unity Project. If you have started a Project using the Built-in Render Pipeline, you can install URP and configure your Project to use URP. When you do this, you must configure URP yourself. You will need to manually convert or recreate parts of your Project (such as lit shaders or post-processing effects) to be compatible with URP. Note: URP does not currently support custom post-processing effects. If your Project uses custom post-processing effects, these cannot currently be recreated in URP. Custom post-processing effects will be supported in a forthcoming release of URP. Note: Projects made using URP are not compatible with the High Definition Render Pipeline (HDRP) or the Built-in Render Pipeline. Before you start development, you must decide which render pipeline to use in your Project. For information on choosing a render pipeline, see the Render Pipelines section of the Unity Manual."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/LightBlendStyles.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/LightBlendStyles.html",
    "title": "Light Blend Styles | FSM Unity Framework",
    "keywords": "Light Blend Styles Blend Styles determine the way a particular Light interacts with Sprites in the Scene. All Lights in the Scene must pick from one of the available Blend Styles. The Universal Render Pipeline (URP) 2D Asset can currently contain a total of four different Light Blend Styles, starting with the 'Default' Blend Style being available. Property Function Name The name that appears when choosing a Blend Style for a Light2D. Mask Texture Channel Which mask channel to use when applying this Blend Style to a Sprite. Render Texture Scale Scale for the internal render texture created for this Blend Style. Blend Mode What blending mode the Light2D uses when this Blend Style is selected. Blend Mode Blend Modes controls the way a Sprite is lit by light. The following examples show the four predefined Blend Modes: Original Sprite Multiply Additive Subtractive Custom Blend Modes All of the Blend Modes available can be expressed with Custom Blend Factors. The factors for the existing Blend Modes are as follows: Multiplicative Modulate = 1 Additive = 0 Additive Modulate = 0 Additive = 1 Subtractive Modulate = 0 Additive = -1 Mask Texture Channel Masks control where Lights can affect a Sprite. There are 4 channels to select from as the mask channel - Red, Blue, Green, and Alpha. In a mask max value means full light, min value means no light. Original Rock Color Rock with a mask Additive Light Blending Additive Light Blending with a mask Render Texture Scale Render Texture Scale adjusts the size of the internal render Texture used for a given Blend Mode. Lowering the Render Texture Scale can increase performance and decrease memory usage for Scenes that contain large Lights. Lowering the Texture Scale to too low a value may cause visual artefacts or a shimmering effect when there is motion in the Scene."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/LightTypes.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/LightTypes.html",
    "title": "| FSM Unity Framework",
    "keywords": "Freeform Select the Freeform Light type to create a Light from an editable polygon with a spline editor. To begin editing your shape, select the Light and find the button in its Inspector window. Select it to enable the shape editing mode. Add new control points by clicking the mouse along the inner polygon’s outline. Remove control points selecting the point and pressing the Delete key. The following additional properties are available to the Freeform Light type. Property Function Falloff Adjust the amount the blending from solid to transparent, starting from the center of the shape to its edges. Falloff Intensity Adjusts the falloff curve of the Light. Falloff Offset Sets the offset for the outer falloff shape. Freeform Light in edit mode Resulting Light Effect When creating a Freeform Light, take care to avoid self-intersection as this may cause unintended lighting results. Self-intersection may occur by creating outlines where edges cross one another, or by enlarging falloff until it overlaps itself. To prevent such issues, it is recommended to edit the shape of the Light until the conditions creating the self-intersection no longer occur. Outline self-intersection in Edit mode. Light effect with a black triangular artifact Falloff overlap in Edit mode Light effect with double lighted areas with overlapping falloff Parametric The Parametric light type has been deprecated. To convert existing Parametric lights to Freeform lights, Edit > Rendering > Lights > Upgrade Project/Scene URP Parametric Lights to Freeform Sprite Select the Sprite Light type to create a Light based on a selected Sprite by assigning the selected Sprite to the additional Sprite property. Property Function Sprite Select a Sprite as the Light source. Selected Sprite Resulting Light effect Spot Select the Spot Light type for great control over the angle and direction of the selected Light with the following additional properties. Property Function Radius Inner Set the inner radius here or with the gizmo. Light within the inner radius will be at maximum intensity. Radius Outer Set the outer radius here or with the gizmo. Light intensity decreases to zero as it approaches the outer radius. Inner / Outer Spot Angle Set the angles with this slider or with the gizmo. Light within the inner and outer angles will be at the intensity specified by inner and outer radius. Point Light in Edit mode Resulting Light effect Global Global Lights light all objects on the targeted sorting layers. Only one global Light can be used per Blend Style, and per sorting layer."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Lights-2D-intro.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Lights-2D-intro.html",
    "title": "Introduction to the 2D Lighting system | FSM Unity Framework",
    "keywords": "Introduction to the 2D Lighting system The 2D Lighting system included with URP consists of a set of artist friendly tools and runtime components that help you quickly create a lit 2D Scene through core Unity components such as the Sprite Renderer, and 2D Light components that act as 2D counterparts to familiar 3D Light components. These tools are designed to integrate seamlessly with 2D Renderers such as the Sprite Renderer, Tilemap Renderer, and Sprite Shape Renderer. This system of tools and components are optimized for mobile systems, and for running on multiple platforms. Differences from 3D Lights There are a number of key differences between the implementation and behavior of 2D Lights and 3D Light, which consists of the following: New 2D specific components and render pass The 2D lighting systems includes its own set of 2D Light components, Shader Graph sub-targets and a custom 2D render pass that are specifically designed for 2D lighting and rendering. Editor tooling for the 2D Lights and pass configuration are also included in the package. Coplanar The 2D lighting model was designed specifically to work with 2D worlds that are coplanar and multi-layered. A 2D Light does not require depth separation between it and the object it is lighting. The 2D shadow system also works in coplanar and does not require depth separation. Not physically based The lighting calculation in 2D Lights is not physics based as it is with 3D Lights. The details of the lighting model calculation can be found here. No interoperability with 3D Lights and 3D Renderers Currently both 3D and 2D Lights can only affect 3D and 2D Renderers respectively. 2D Lighting does not work on or effect 3D Renderers such as the Mesh Renderer, while 3D Lighting will similarly have no effect on 2D Renderers such as the Sprite Renderer. While interoperability between the respective Lights and Renderers may be developed in the future, currently a combination of 2D and 3D Lights and 2D and 3D Renderers in a single Scene can be achieved by using the camera stacking technique. Technical details of the 2D Lighting graphics pipeline The 2D Lighting graphics pipeline rendering process can be broken down into 2 distinct phases: Drawing the Light Render Textures Drawing the Renderers Light Render Textures are Render Textures that contain information about the Light’s color and shape in screen space. These two phases are only repeated for each distinctly lit set of Light Layers. In other words, if Sorting Layers 1 through 4 have the exact same set of Lights, it will only perform the above set of operations once. The default setup allows a number of batches to be drawn ahead of time before drawing the Renderers to reduce target switching. The ideal setup would allow the pipeline to render the Light Render Textures for all the batches and only then move on to draw the Renderers. This prevents loading and unloading of the color target. Refer to Optimization for more detailed information. Pre-phase: Calculate Sorting Layer batching Before proceeding with the rendering phases, the 2D Lighting graphics pipeline first analyses the Scene to assess which Layers can be batched together in a single draw operation. The following is the criteria that determine whether Layers are batched together: They are consecutive Layers. They share the exact same set of Lights. It is highly recommended to batch as many Layers as possible to minimize the number of Light Render Textures draw operations and improve performance. Phase 1: Draw Light Render Textures After the pre-phase batching, the pipeline then draws the Light Textures for that batch. This essentially draws the Light’s shape onto a Render Texture. The light’s color and shape can be blended onto the target Light Render Texture using Additive or Alpha Blended depending on the light’s setup. It is worth noting that a Light Render Texture is only created when at least one 2D Light is targeting it. For example, if all the lights of a Layer only uses Blendstyle #1, then only one Light Render Texture is created. Phase 2: Draw Renderers Once all the Light Render Textures have been drawn, the pipeline proceeds to draw the Renderers. The system will keep track of which set of Renderers are drawn by which set of Light Render Textures. They are associated during the batching process in the pre-phase. When the Renderers are being drawn, it will have access to all (one for each blend style) the available Light Render Textures. In the shader, the final color is calculated by combining the input color with colors from the Light Render Texture using the specified operation. An example of a setup with four active blend styles illustrating how multiple blend styles come together. In most cases, you would typically only need two blend styles for your desired effect. Optimization In addition to the standard optimization techniques such as reducing draw calls, culling and optimizing shaders, there are several techniques and considerations that are unique to the 2D Lighting graphics pipeline. Number of blend styles The easiest way to increase rendering performance is to reduce the number of blend styles used. Each blend style is a Render Texture that needs to be rendered and subsequently uploaded. Reducing the number of blend styles has a direct impact on the performance. For simple scenes a single blend style could suffice. It is also common to use up to 2 blend styles in a scene. Light Render Texture scale The 2D Lighting system relies on screen space Light Render Texture to capture light contribution. This means there are a lot of Render Texture drawing subsequent uploading. Choosing the right Render Texture size directly impacts the performance. By default it is set at 0.5x of screen resolution. Smaller Light Render Texture size will give better performance at the cost of visual artifact. Half screen size resolution provides a good performance with almost no noticeable artifact in most situations. Experiment and find a scale suitable for your project. Layer Batching To further reduce the number of Light Render Textures, it is crucial to make the Sorting Layer batchable. Layers that are batched together share the same set of Light Render Textures. Uniquely lit layers will have its own set thus increasing the amount of work needed. Layers can be batch together if they share the same set of lights. Pre-rendering of Light Render Texture Multiple sets of Light Render Textures can be rendered ahead of drawing the Renderers. In an ideal situation, all the Light Render Textures will be rendered upfront and only then will the pipeline proceed with drawing the Renderers onto the final color output. This reduces the need to load/unload/reload of final color output. In a very complex setup with many distinctly lit Layers, it may not be practical to pre-render all Light Render Textures. The limit can be configured in the 2D Renderer Data inspector. Normal Maps Using normal maps to simulate depth is currently a very expensive operation. If it is enabled, a full size Render Texture is created during a depth pre-pass and the Renderers are drawn onto it. This is done for each Layer batch. If the normal mapping effect to simulate depth perception is not needed, ensure that all lights have the normal map option disabled."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Channel-Mixer.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Channel-Mixer.html",
    "title": "Channel Mixer | FSM Unity Framework",
    "keywords": "Channel Mixer The Channel Mixer effect modifies the influence of each input color channel on the overall mix of the output channel. For example, if you increase the influence of the green channel on the overall mix of the red channel, all areas of the final image that are green (including neutral/monochrome) tint to a more reddish hue. Using Channel Mixer Channel Mixer uses the Volume framework, so to enable and modify Channel Mixer properties, you must add a Channel Mixer override to a Volume in your Scene. To add Channel Mixer to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Channel Mixer. URP now applies Channel Mixer to any Camera this Volume affects. Properties Output channels Before you modify the influence of each input channel, you must select the output color channel to influence. To do this, click the button for the channel that you want to set the influence for. Property Description Red Use the slider to set the influence of the red channel on the selected output channel. Green Use the slider to set the influence of the green channel on the selected output channel. Blue Use the slider to set the influence of the blue channel on the selected output channel."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Color-Adjustments.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Color-Adjustments.html",
    "title": "Color Adjustments | FSM Unity Framework",
    "keywords": "Color Adjustments Use this effect to tweak the overall tone, brightness, and contrast of the final rendered image. Scene without Color Adjustments effect. Scene with Color Adjustments effect. Using Color Adjustments Color Adjustments uses the Volume framework, so to enable and modify Color Adjustments properties, you must add a Color Adjustments override to a Volume in your Scene. To add Color Adjustments to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Color Adjustments. URP now applies Color Adjustments to any Camera this Volume affects. Properties Property Description Post Exposure Adjusts the overall exposure of the Scene in EV (not EV100). URP applies this after the HDR effect and before tonemapping, which means that it does not affect previous effects in the chain. Contrast Use the slider to expand or shrink the overall range of tonal values. Larger positive values expand the tonal range and lower negative values shrink the tonal range. Color Filter Use the color picker to select which color the Color Adjustment effect should use to multiply the render and tint the result. Hue Shift Use the slider to shift the hue of all colors. Saturation Use the slider to push the intensity of all colors."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Color-Curves.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Color-Curves.html",
    "title": "Color Curves | FSM Unity Framework",
    "keywords": "Color Curves Grading curves are an advanced way to adjust specific ranges in hue, saturation, or luminosity. You can adjust the curves in eight available graphs to achieve effects such as specific hue replacement or desaturating certain luminosities. Using Color Curves Color Curves uses the Volume framework, so to enable and modify Color Curves properties, you must add a Color Curves override to a Volume in your Scene. To add Color Curves to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Color Curves. URP now applies Color Curves to any Camera this Volume affects. Properties Curve Description Master This curve affects the luminance across the whole image. The x-axis of the graph represents input luminance and the y-axis represents output luminance. You can use this to further adjust the appearance of basic attributes such as contrast and brightness across all color channels at the same time. Red This curve affects the red channel intensity across the whole image. The x-axis of the graph represents input intensity and the y-axis represents output intensity for the red channel. Green This curve affects the green channel intensity across the whole image. The x-axis of the graph represents input intensity and the y-axis represents output intensity for the green channel. Blue This curve affects the blue channel intensity across the whole image. The x-axis of the graph represents input intensity and the y-axis represents output intensity for the blue channel. Hue Vs Hue This curve shifts the input hue (x-axis) according to the output hue (y-axis). You can use this to fine tune hues of specific ranges or perform color replacement. Hue Vs Sat This curve adjusts saturation (y-axis) according to the input hue (x-axis). You can use this to tone down particularly bright areas or create artistic effects such as monochromatic except a single dominant color. Sat Vs Sat This curve adjusts saturation (y-axis) according to the input saturation (x-axis). You can use this to fine tune saturation adjustments made with Color Adjustments. Lum Vs Sat This curve adjusts saturation (y-axis) according to the input luminance (x-axis). You can use this to desaturate areas of darkness to provide an interesting visual contrast."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Film-Grain.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Film-Grain.html",
    "title": "Film Grain | FSM Unity Framework",
    "keywords": "Film Grain Scene with Film Grain effect turned off. Scene with Film Grain effect turned on. The Film Grain effect simulates the random optical texture of photographic film, usually caused by small particles being present on the physical film. Using Film Grain Film Grain uses the Volume framework, so to enable and modify Film Grain properties, you must add a Film Grain override to a Volume in your Scene. To add Film Grain to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Film Grain. URP now applies Film Grain to any Camera this Volume affects. Properties Property Description Type Use the drop-down to select the type of grain to use. You can select from a list of presets that URP includes, or select Custom to provide your own grain Texture. Texture Assign a Texture that this effect uses as a custom grain Texture.This property is only available when Type is set to Custom. Intensity Use the slider to set the strength of the Film Grain effect. Response Use the slider to set the noisiness response curve. The higher you set this value, the less noise there is in brighter areas. Response value 1 (left), and 0 (right)."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Lens-Distortion.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Lens-Distortion.html",
    "title": "Lens Distortion | FSM Unity Framework",
    "keywords": "Lens Distortion Scene with Lens Distortion effect turned off. Scene with Lens Distortion effect turned on. The Lens Distortion effect distorts the final rendered picture to simulate the shape of a real-world camera lens. Using Lens Distortion Lens Distortion uses the Volume framework, so to enable and modify Lens Distortion properties, you must add a Lens Distortion override to a Volume in your Scene. To add Lens Distortion to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Lens Distortion. URP now applies Lens Distortion to any Camera this Volume affects. Properties Property Description Intensity Use the slider to set the overall strength of the distortion effect. X Multiplier Use the slider to set the distortion intensity on the x-axis. This value acts as a multiplier so you can set this value to 0 to disable distortion on this axis, Y Multiplier Use the slider to set the distortion intensity on the y-axis. This value acts as a multiplier so you can set this value to 0 to disable distortion on this axis, Center Set the center point of the distortion effect on the screen. Scale Use the slider to set the value for global screen scaling. This zooms the render to hide the borders of the screen. When you use a high distortion, pixels on the borders of the screen can break because they rely on information from pixels outside the screen boundaries that don't exist. This property is useful for hiding these broken pixels around the screen border."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Lift-Gamma-Gain.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Lift-Gamma-Gain.html",
    "title": "Lift Gamma Gain | FSM Unity Framework",
    "keywords": "Lift Gamma Gain This effect allows you to perform three-way color grading. The Lift Gamma Gain trackballs follow the ASC CDL standard. When you adjust the position of the point on the trackball, it shifts the hue of the image towards that color in the given tonal range. Use the different trackballs to affect different ranges within the image. Adjust the slider under the trackball to offset the color lightness of that range. Using Lift Gamma Gain Lift Gamma Gain uses the Volume framework, so to enable and modify the lift, gamma, or gain of the render, you must add a Lift Gamma Gain override to a Volume in your Scene. To add Lift Gamma Gain to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Lift Gamma Gain. URP now applies Lift Gamma Gain to any Camera this Volume affects. Properties Property Description Lift Use this to control the dark tones. This has a more exaggerated effect on shadows. Use the trackball to select which color URP should shift the hue of the dark tones to. Use the slider to offset the color lightness of the trackball color. Gamma Use this to control the mid-range tones with a power function. Use the trackball to select which color URP should use to shift the hue of the mid-tones to. Use the slider to offset the color lightness of the trackball color. Gain Use this to increase the signal and make highlights brighter. Use the trackball to select which color that URP uses to shift the hue of the highlights to. Use the slider to offset the color lightness of the trackball color."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Motion-Blur.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Motion-Blur.html",
    "title": "Motion Blur | FSM Unity Framework",
    "keywords": "Motion Blur Scene with Motion Blur effect turned off. Scene with Motion Blur effect turned on. The Motion Blur effect simulates the blur that occurs in an image when a real-world camera films objects moving faster than the camera’s exposure time. This is usually due to rapidly moving objects, or a long exposure time. Universal Render Pipeline (URP) only blurs camera motions. Using Motion Blur Motion Blur uses the Volume system, so to enable and modify Motion Blur properties, you must add a Motion Blur override to a Volume in your Scene. To add Motion Blur to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing, and click on Motion Blur. URP now applies Motion Blur to any Camera this Volume affects. Properties Property Description Quality Set the quality of the effect. Lower presets give better performance, but at a lower visual quality. Intensity Set the strength of the motion blur filter to a value from 0 to 1. Higher values give a stronger blur effect, but can cause lower performance, depending on the Clamp parameter. Clamp Set the maximum length that the velocity resulting from Camera rotation can have. This limits the blur at high velocity, to avoid excessive performance costs. The value is measured as a fraction of the screen's full resolution. The value range is 0 to 0.2. The default value is 0.05. Troubleshooting performance issues To decrease the performance impact of Motion Blur, you can: Reduce the Quality. A lower quality setting gives higher performance but may exhibit more visual artifacts. Decrease the Clamp to reduce the maximum velocity that Unity takes into account. Lower values give higher performance."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Panini-Projection.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Panini-Projection.html",
    "title": "Panini Projection | FSM Unity Framework",
    "keywords": "Panini Projection Scene with Panini Projection effect turned off. Scene with Panini Projection effect turned on. This effect helps you to render perspective views in Scenes with a very large field of view. Panini projection is a cylindrical projection, which means that it keeps vertical straight lines straight and vertical. Unlike other cylindrical projections, panini projection keeps radial lines through the center of the image straight too. For more information about panini projection, see PanoTools’ wiki documentation on General Panini Projection. Using Panini Projection Panini Projection uses the Volume framework, so to enable and modify Panini Projection properties, you must add a Panini Projection override to a Volume in your Scene. To add Panini Projection to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Panini Projection. URP now applies Panini Projection to any Camera this Volume affects. Properties Property Description Distance Use the slider to set the strength of the distortion. Crop to Fit Use the slider to crop the distortion to fit the screen. A value of 1 crops the distortion to the edge of the screen, but results in a loss of precision in the center if you set Distance to a high value."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Shadows-Midtones-Highlights.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Shadows-Midtones-Highlights.html",
    "title": "Shadows Midtones Highlights | FSM Unity Framework",
    "keywords": "Shadows Midtones Highlights The Shadows Midtones Highlights effect separately controls the shadows, midtones, and highlights of the render. Unlike Lift, Gamma, Gain, you can use this effect to precisely define the tonal range for shadows, midtones, and highlights. Using Shadows Midtones Highlights Shadows Midtones Highlights uses the Volume framework, so to enable and modify the shadows, midtones, or highlights of the render, you must add a Shadows Midtones Highlights override to a Volume in your Scene. To add Shadows Midtones Highlights to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Shadows Midtones Highlights. URP now applies Shadows Midtones Highlights to any Camera this Volume affects. Properties Property Description Shadows Use this to control the shadows.Use the trackball to select the color URP should shift the hue of the shadows to.Use the slider to offset the color lightness of the trackball color. Midtones Use this to control the midtones.Use the trackball to select the color URP should shift the hue of the midtones to.Use the slider to offset the color lightness of the trackball color. Highlights Use this to control the highlights.Use the trackball to select the color URP should shift the hue of the highlights to.Use the slider to offset the color lightness of the trackball color. Graph view This graph shows the overall contribution of the Shadows (blue), Midtones (green), and Highlights (yellow). This is useful to visualize the transitions between the tonal regions. Shadow Limits Property Description Start Set the start point of the transition between the shadows and the midtones of the render. End Set the end point of the transition between the shadows and the midtones of the render. Highlight Limits Property Description Start Set the start point of the transition between the midtones and the highlights of the render. End Set the end point of the transition between the midtones and the highlights of the render."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Split-Toning.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-Split-Toning.html",
    "title": "Split Toning | FSM Unity Framework",
    "keywords": "Split Toning This effect tints different areas of the image based on luminance values, to help you achieve a more distinctive look. You can use this to add different color tones to the shadows and highlights in your Scene. Using Split Toning Split Toning uses the Volume framework, so to enable and modify Split Toning properties, you must add a Split Toning override to a Volume in your Scene. To add Split Toning to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Split Toning. URP now applies Split Toning to any Camera this Volume affects. Properties When you adjust the color in the color picker for each property, you should only adjust the Hue and Saturation. Value also changes the overall image brightness. Property Description Shadows Use the color picker to select the color that URP uses for tinting shadows. Highlights Use the color picker to select the color that URP uses for tinting highlights. Balance Use the slider to set the balance between Shadows and Highlights. Lower values result in more pronounced shadow toning is compared to highlight toning. Higher values result in the opposite effect, with more pronounced highlight toning compared to shadow toning."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-White-Balance.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Post-Processing-White-Balance.html",
    "title": "White Balance | FSM Unity Framework",
    "keywords": "White Balance The White Balance component applies a white balance effect that removes unrealistic color casts, so that items that would appear white in real life render as white in your final image. You can also use white balance to create an overall colder or warmer feel in the final render. Using White Balance White Balance uses the Volume framework, so to enable and modify White Balance properties, you must add a White Balance override to a Volume in your Scene. To add White Balance to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on White Balance. URP now applies White Balance to any Camera this Volume affects. Properties Property Description Temperature Use the slider to set the white balance to a custom color temperature. Higher values result in a warmer color temperature and lower values result in a colder color temperature. See Wikipedia: Color balance for more information about color temperature. Tint Use the slider to compensate for a green or magenta tint."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/PrepShader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/PrepShader.html",
    "title": "Preparing Sprites For Lighting | FSM Unity Framework",
    "keywords": "Preparing Sprites For Lighting To light Sprites with 2D Lights, the Sprite Renderer component of the Sprite is assigned a material with a Shader that reacts to 2D Lights. With the 2D Lights preview package installed, dragging Sprites onto the Scene automatically assigns the ‘Sprite-Lit-Default’ material to them which enables them to interact and appear lit by 2D Lights. Alternatively, you can create a custom Shader that reacts to Lights with the Shader Graph package. The Shader Graph package is available for download via the Package Manager. Upgrading to a compatible Shader If you are installing the 2D Lights package into a Project with pre-existing Prefabs, materials or Scenes, you will need to upgrade any materials used to a lighting compatible Shader. The following functions automatically upgrade a Scene or Project automatically in a one way process. Upgraded Scenes or Projects cannot be reverted to their previous state. Upgrading a Scene To upgrade the currently opened Scene, go to Edit> Render Pipeline > UniversalRP 2D Renderer > Upgrade Scene To 2D Renderer Upgrading a Project To upgrade all Prefabs and materials in your Project, go to Edit > Render Pipeline > UniversalRP 2D Renderer > Upgrade Project To 2D Renderer"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/SecondaryTextures.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/SecondaryTextures.html",
    "title": "Setting up normal map and mask Textures | FSM Unity Framework",
    "keywords": "Setting up normal map and mask Textures 2D Lights can interact with normal map and mask Textures linked to Sprites to create advanced lighting effects, such as normal mapping. Assign these additional Textures to Sprites by using the Sprite Editor’s Secondary Textures module. First select a Sprite, and open the Sprite Editor from its Inspector window. Then select the Secondary Textures module from the drop-down menu at the top left of the editor window. Adding a Secondary Texture In the Secondary Textures editor, select a Sprite to add Secondary Textures to. With a Sprite selected, the Secondary Textures panel appears at the bottom right of the editor window. The panel displays the list of Secondary Textures currently assigned to the selected Sprite. To add a new Secondary Texture to the Sprite, select + at the bottom right of the list. This adds a new entry to the list with the ‘Name’ and ‘Texture’ boxes. Enter a custom name into the Name box, or select the arrow to the right of the Name box to open the drop-down list of suggested names. These suggested names can include suggestions from installed Unity packages, as the Secondary Textures may need to have specific names to interact correctly with the Shaders in these packages to produce their effects. The 2D Lights package suggests the names ‘MaskTex’ and ‘NormalMap’. Select the name that matches the function of the selected Texture - select ‘MaskTex’ for a masking Texture, or ‘NormalMap’ for a normal map Texture. Naming these Textures correctly allow them to interact with the 2D Lights Shaders to properly produce the various lighting effects. To select the Texture Asset for this Secondary Texture entry, drag the Texture Asset directly onto the Texture field, or open the Object Picker window by selecting the circle to the right of the Texture box. Secondary Textures are sampled with the same UV coordinates as the Texture of the selected Sprite. Align the Secondary Textures with the main Sprite Texture to ensure that additional Texture effects are displayed correctly. To preview the Secondary Texture in the Sprite Editor window, select an entry in the list. This automatically hides the Sprite’s main Texture. Click outside of the Secondary Textures list to deselect the entry, and the main Sprite Texture becomes visible again. Deleting a Secondary Texture To delete a Secondary Texture, select it from the list and then select - at the bottom right of the window. This automatically removes the entry. Applying Select Apply at the top of the editor to save your entries. Invalid entries without a Name or an assigned Texture are automatically removed when changes are applied."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Setup.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Setup.html",
    "title": "Requirements and setup | FSM Unity Framework",
    "keywords": "Requirements and setup Install the following Editor and package versions to begin working with the 2D Renderer: Unity 2021.2.0b1 or later Universal Render Pipeline version 10 or higher (available via the Package Manager) 2D Renderer Setup Create a new Project using the 2D template. Create a new Pipeline Asset and Renderer Asset by going to the Assets menu and selecting Create > Rendering > URP Asset (with 2D Renderer). Enter the name for both the Pipeline and Renderer Assets. The name is automatically applied to both, with \"_Renderer\" appended to the name of the Renderer Asset. The Renderer Asset is automatically assigned to the Pipeline Asset. To set the graphics quality settings, there are two options: Option 1: For a single setting across all platforms Go to Edit > Project Settings and select the Graphics category. Drag the Pipeline Asset created earlier to the Scriptable Render Pipeline Settings box, and adjust the quality settings. Option 2: For settings per quality level Go to Edit > Project Settings and select the Quality category. Select the quality levels to be included in your Project. Drag the Pipeline Asset created earlier to the Rendering box. Repeat steps 2-3 for each quality level and platform included in your Project. The 2D Renderer is now set up for your Project. Note: If you use the 2D Renderer in your Project, some of the options related to 3D rendering in the Universal Render Pipeline Asset will not affect or impact on your final app or game."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/ShaderGraph.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/ShaderGraph.html",
    "title": "2D Renderer and Shader Graph | FSM Unity Framework",
    "keywords": "2D Renderer and Shader Graph Creating a Lit Shader Create a new Asset by selecting Assets > Create > Shader Graph > URP > Sprite Lit Shader Graph. The Shader Graph Asset is then created in the Asset window. Double-click the new Asset to open the Shader Graph. Create three Sample Texture 2D Nodes by right-clicking on the Shader Graph window and selecting Create Node, then search for and select the Sample Texture 2D option. Change the Type of one of the Nodes to Normal. Attach the RGBA(4) Output Slot of the Default Type Nodes as shown below. Note that you should attach the Normal Type Node's Output Slot to the Normal(Tangent Space)(3) Input Slot. Create three Texture 2D properties by selecting the + on the Blackboard, and then select Texture 2D. Name them 'MainTex', 'MaskTex', and 'NormalMap' for this example. Drag each of the Texture 2D properties onto the editor window. Attach each of the properties to the Input Slots of the Sample Texture 2D Nodes as shown below. Note that the 'NormalMap' property must be attached to the Normal Type Node only. Select Save Asset to save the Shader. You can now apply the newly built Shader to materials."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Shadows-in-URP.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Shadows-in-URP.html",
    "title": "Shadows in the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Shadows in the Universal Render Pipeline The Universal Render Pipeline’s Lights can cast shadows from one GameObject onto another. They emphasize the position and scale of GameObjects, which adds a degree of depth and realism to a Scene that could otherwise look flat. Shadow map resolution The resolution of a Light’s shadow map determines the size of its shadow maps. The larger the shadow map, the more precise the shadows can be, and the better the Universal Render Pipeline can capture small details in the shadow casting geometry. Rendering shadow maps at higher resolutions make them look sharper. The number of shadow maps Universal RP renders per Light depends on the Type of the Light: A Spot Light renders one shadow map. A Point Light renders six shadow maps (the number of faces in a cubemap). A Directional Light renders one shadow map per cascade. Set the cascade count of Directional Lights from the Universal Render Pipeline Asset of your project. Universal RP will try to use the best resolution according to the number of shadow maps that are needed in the scene, and the size of the shadow atlases. Shadow atlases Universal RP renders all real-time shadows for a frame using one common shadow map atlas for all punctual light shadows (i.e shadows for Spot Lights and Point Lights), and an other shadow map atlas for Directional Light shadows. Set the size of these atlases in your Unity Project’s Universal Render Pipeline Asset. The atlas size determines the maximum resolution of shadows in your Scene. For example, an atlas of size 1024 x 1024 can fit: Four shadow maps of 512 x 512 pixels. Sixteen shadow maps of 256 x 256 pixels. Matching shadow atlas resolution to Built-In RP settings In projects that used the Built-In Render Pipeline, you controlled shadow maps resolution by selecting a shadow resolution level (\"Low\", \"Medium\", \"High\", \"Very High\") in your project's Quality Settings. For each shadow map, Unity then decided which resolution to actually use, based on the algorithm explained in the Built-In RP Manual Page about Shadow Mapping. You could then inspect in the Frame Debugger the resolution actually used for a specific shadow map. In Universal Render Pipeline, you specify the resolution of the Shadow Atlases. Therefore you can control the amount of video memory your application will allocate for shadows. If you want to make sure that the resolution Universal RP uses for a specific punctual light shadow in your project, will not go under a specific value: Consider the number of shadow maps required in the scene, and select a big enough shadow atlas resolution. For example: if your scene has four Spot Lights and one Point light ; and you want each shadow map resolution to be at least 256x256. Your scene needs to render ten shadow maps (one for each Spot Light, and six for the Point Light), each with resolution 256x256. Using a shadow atlas of size 512x512 would not be enough, because it can contain only four maps of size 256x256. Therefore, you should use a shadow atlas of size 1024x1024, that can contain up to sixteen maps of size 256x256. Shadow Bias Shadow maps are essentially textures projected from the point of view of the Light. Universal RP uses a bias in the projection so that the shadow casting geometry does not self-shadow itself. In Universal RP, each individual Light component controls its own shadow biasing using the following parameters: Depth Bias Normal Bias Near Plane Find these settings under the Shadows section. If properties are not visible, change the Bias setting from \"Use Pipeline Settings\" to \"Custom\" to expose them. Using high shadow bias values can result in light \"leaking\" through Meshes. This occurs where there is a visible gap between the shadow and its caster, and leads to shadow shapes that do not accurately represent their casters. Configure shadows for better performance Refer to Configure for better performance for more information about how to adjust shadow settings for better performance."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "Universal Render Pipeline Requirements Building for Closed platforms What's new in URP Features Feature Comparison with the Built-in Render Pipeline Getting started Create a Project from a Template Install URP into an existing Project Package samples URP Package Samples Scene Templates Understand performance Configure for better performance Render Pipeline Concepts The URP Asset URP Global Settings Universal Renderer Deferred Rendering Path Forward+ Rendering Path Renderer Feature How to add a Renderer Feature Decal Decal Shader Graph Screen Space Shadows Render Objects Renderer Feature Screen Space Ambient Occlusion Full Screen Pass Upgrade guides Render Pipeline Converter Upgrading to URP 14 Upgrading to URP 13 Upgrading to URP 12.0.x Upgrading to URP 11.0.x Upgrading to URP 10.1.x Upgrading to URP 10.0.x Upgrading to URP 9.0.x Upgrading to URP 8.2.0 Upgrading to URP 8.1.0 Upgrading to URP 8.0.0 Upgrading to URP 7.4.0 Upgrading to URP 7.3.0 Upgrading to URP 7.2.0 Upgrading from LWRP to URP Rendering How to use Render Objects Renderer Feature Rendering Layers Lighting Light component reference Lighting Mode The Universal Additional Light Data component Shadows in the Universal Render Pipeline Reflection probes Lens Flare asset Cameras The Universal Additional Camera Data component Render Type Working with multiple cameras Camera Stacking Rendering from multiple Cameras to the same render target Rendering to a Render Texture Clearing, rendering order and overdraw Anti-aliasing Camera component reference Post-processing How to configure HDR Output Volumes Volume Profile Volume Overrides Effect List Ambient Occlusion Bloom Channel Mixer Chromatic Aberration Color Adjustments Color Curves Depth of Field Film Grain Lens Distortion Lift, Gamma, and Gain Motion Blur Panini Projection Shadows Midtones Highlights Split Toning Tonemapping Vignette White Balance Lens Flare Custom Post-processing How to create a custom post-processing effect Shaders and Materials Shading Models Material Variants Complex Lit Lit Simple Lit Baked Lit Unlit Terrain Lit Particles Lit Particles Simple Lit Particles Unlit Decal Upgrading shaders from Built-in Shader stripping Writing custom shaders Creating a sample scene URP basic unlit shader URP unlit shader with color input Drawing a texture Visualizing normal vectors Reconstruct the world space positions URP ShaderLab Pass tags How to Perform fullscreen blit Use Render Objects Renderer Feature Create custom Renderer Feature Create a custom post-processing effect Change Quality settings with code Customizing URP Injection points Inject a custom pass via scripting Blit overview beginCameraRendering event Create custom Renderer Feature Optimization Rendering Debugger Optimize for better performance 2D graphic features Introduction to Lights 2D Requirements and setup Configuring the 2D Renderer Asset HDR emulation scale Light Blend Styles Preparing and upgrading Normal map and mask Textures Common Lights 2D properties Lights 2D types and specific properties 2D Shadows 2D Renderer and Shader Graph 2D Pixel Perfect Cinemachine Pixel Perfect extension Frequently asked questions (FAQ) Known issues"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/URP-Config-Package.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/URP-Config-Package.html",
    "title": "The Universal Render Pipeline Config package | FSM Unity Framework",
    "keywords": "The Universal Render Pipeline Config package The Universal Render Pipeline (URP) uses a separate package to control the settings of some of its features. For example, you can use it to: Change the max number of visible light. Using the URP Config package To use the URP Config package in your URP Project, you need to create a local copy of it and make your Project's package manifest reference it like so: In your Project's directory, move and rename the folder \"/Library/PackageCache/com.unity.render-pipelines.universal-config@[versionnumber]\" to \"/Packages/com.unity.render-pipelines.universal-config\". Note: Using the Package Manager to upgrade your URP package does not automatically upgrade the local config package. To manually upgrade the local config package: Make a copy of your current config package. Delete the com.unity.render-pipelines.universal-config folder in your Packages/ folder. Copy again the folder from the Library/PackageCache/ like mentionned above. Apply your modifications by hand to the new config package. Configuring URP using the config package You can edit the ShaderConfig.cs file to configure the properties of your URP Project. If you edit this file, you must also update the equivalent ShaderConfig.cs.hlsl header file (which URP Shaders use) so that it mirrors the definitions that you set in ShaderConfig.cs. You can update the ShaderConfig.cs.hlsl file in two ways. You can either make Unity generate the ShaderConfig.cs.hlsl file from the ShaderConfig.cs file, which makes sure that the two files are synchronized, or edit the ShaderConfig.cs.hlsl file directly, which is faster but it is up to you to synchronize the files when you make changes. To ensure that the two files are synchronized, you should follow the first method. To do this: Go to Packages > com.unity.render-pipelines.universal-config > Runtime and open ShaderConfig.cs. Edit the values of the properties that you want to change and then save the file. Back in Unity, select Edit > RenderPipeline > Generate Include Files. Unity automatically configures your Project and Shaders to use the new configuration."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/VolumeOverrides.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/VolumeOverrides.html",
    "title": "Volume Overrides | FSM Unity Framework",
    "keywords": "Volume Overrides Volume Overrides let you change or extend the default properties in a Volume Profile. URP implements post-processing effects as Volume Overrides. For example, the following image shows the Vignette post-processing effect in the URP Template SampleScene. In a the Volume Override, checkboxes to the left of each property let you enable or disable specific properties. If you disable a property, URP uses the Volume’s default value for that property instead. To turn all properties on or off, use the All or None shortcuts above the property list. How to add a Volume Override to a Volume component To add a Volume Override to a Volume component: Select a GameObject with the Volume component. In the Inspector window, click Add Override. Use the search field to search for an Override, or select an Override from the menu."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/VolumeProfile.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/VolumeProfile.html",
    "title": "| FSM Unity Framework",
    "keywords": "#Volume Profile A Volume Profile is a Scriptable Object which contains properties that Volumes use to determine how to render the Scene environment for Cameras they affect. A Volume references a Volume Profile in its Profile field and uses values from the Volume Profile it references. A Volume Profile organizes its properties into structures which control different environment settings. These structures all have default values that you can use, but you can use Volume Overrides to override these values and customize the environment settings. ##Create and custom a Volume Profile There are multiple ways to create a Volume Profile. Unity creates, and links, a Volume Profile automatically when you create a Scene Settings GameObject (menu: Rendering > Scene Settings). You can also create a Volume Profile manually. Navigate to menu: Assets > Create > Volume Profile. Open the Volume Profile in the Inspector to edit its properties. To do this, you can either: • Select the Volume Profile in the Assets folder. • Select a GameObject with a Volume component that has a Volume Profile set in its Profile field. When you view the Volume Profile in the Inspector, you can only see values from the Volume overrides that the Volume Profile includes; the Volume Profile hides all other values. You must add Volume override components in order to edit the default properties of the Volume Profile. To add a Volume override component, click on the Add Override button, and select which Volume override you want to add to the Volume Profile. For example, click on the Add Override button and select the Motion Blur Volume override. This exposes properties relating to the Motion Blur effect in URP."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Volumes.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/Volumes.html",
    "title": "Volumes | FSM Unity Framework",
    "keywords": "Volumes The Universal Render Pipeline (URP) uses the Volume framework. Volumes can override or extend Scene properties depending on the Camera position relative to each Volume. URP uses the Volume framework for post-processing effects. URP implements dedicated GameObjects for Volumes: Global Volume, Box Volume, Sphere Volume, Convex Mesh Volume. The Volume component contains the Mode property that defines whether the Volume is Global or Local. With Mode set to Global, Volumes affect the Camera everywhere in the Scene. With Mode set to Local, Volumes affect the Camera if the Camera is within the bounds of the Collider. For more information, see How to use Local Volumes. You can add a Volume component to any GameObject. A Scene can contain multiple GameObjects with Volume components. You can add multiple Volume components to a GameObject. The Volume component references a Volume Profile, which contains the Scene properties. A Volume Profile contains default values for every property and hides them by default. Volume Overrides let you change or extend the default properties in a Volume Profile. At runtime, URP goes through all of the enabled Volume components attached to active GameObjects in the Scene, and determines each Volume's contribution to the final Scene settings. URP uses the Camera position and the Volume component properties to calculate the contribution. URP interpolates values from all Volumes with a non-zero contribution to calculate the final property values. Volume component properties Volumes components contain properties that control how they affect Cameras and how they interact with other Volumes. Property Description Mode Use the drop-down to select the method that URP uses to calculate whether this Volume can affect a Camera: • Global: Makes the Volume have no boundaries and allow it to affect every Camera in the Scene. • Local: Allows you to specify boundaries for the Volume so that the Volume only affects Cameras inside the boundaries. Add a Collider to the Volume's GameObject and use that to set the boundaries. Blend Distance The furthest distance from the Volume’s Collider that URP starts blending from. A value of 0 means URP applies this Volume’s overrides immediately upon entry. This property only appears when you select Local from the Mode drop-down. Weight The amount of influence the Volume has on the Scene. URP applies this multiplier to the value it calculates using the Camera position and Blend Distance. Priority URP uses this value to determine which Volume it uses when Volumes have an equal amount of influence on the Scene. URP uses Volumes with higher priorities first. Profile A Volume Profile Asset that contains the Volume Components that store the properties URP uses to handle this Volume. Volume Profiles The Profile field stores a Volume Profile, which is an Asset that contains the properties that URP uses to render the Scene. You can edit this Volume Profile, or assign a different Volume Profile to the Profile field. You can also create a Volume Profile or clone the current one by clicking the New and Clone buttons respectively. How to use Local Volumes This section describes how to use a Local Volume to implement a location-based post-processing effect. In this example, URP applies a post-processing effect when the Camera is within a certain Box Collider. In the Scene, create a new Box Volume (GameObject > Volume > Box Volume). Select the Box Volume. In Inspector, In the Volume component, In the Profile field, click New. Unity creates the new Volume Profile and adds the Add Override button to the Volume component. If you have other Volumes in the Scene, change the value of the Priority property to ensure that the Overrides from this Volume have higher priority than those of other Volumes. Click Add Override. In the Volume Overrides dialog box, select a post-processing effect. In the Collider component, adjust the Size and the Center properties so that the Collider occupies the volume where you want the local post-processing effect to be. Ensure that the Is Trigger check box is selected. Now, when the Camera is within the bounds of the Volume's Box Collider, URP uses the Volume Overrides from the Box Volume."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/anti-aliasing.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/anti-aliasing.html",
    "title": "Anti-aliasing in the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Anti-aliasing in the Universal Render Pipeline Aliasing is a side effect that happens when a digital sampler samples real-world information and attempts to digitize it. For example, when you sample audio or video, aliasing means that the shape of the digital signal doesn't match the shape of the original signal. This means when Unity renders a line, it may appear jagged as the pixels don't align perfectly with the line's intended path across the screen. An example of the rasterization process creating aliasing. To prevent aliasing, the Universal Render Pipeline (URP) has multiple methods of anti-aliasing, each with their own effectiveness and resource intensity. The anti-aliasing methods available are: Fast Approximate Anti-aliasing (FXAA) Subpixel Morphological Anti-aliasing (SMAA) Temporal Anti-aliasing (TAA) Multisample Anti-aliasing (MSAA) Fast Approximate Anti-aliasing (FXAA) FXAA uses a full screen pass that smooths edges on a per-pixel level. This is the least resource intensive anti-aliasing technique in URP. To select FXAA for a Camera: Select the Camera in the Scene view or Hierarchy window and view it in the Inspector. Navigate to Rendering > Anti-aliasing, and select Fast Approximate Anti-aliasing (FXAA). Subpixel Morphological Anti-aliasing (SMAA) SMAA finds patterns in the borders of an image and blends the pixels on these borders according to the pattern it finds. This anti-aliasing method has much sharper results than FXAA. To select SMAA for a Camera: Select the Camera in the Scene view or Hierarchy window and view it in the Inspector. Navigate to Rendering > Anti-aliasing, and select Subpixel Morphological Anti-aliasing (SMAA). Temporal Anti-aliasing (TAA) TAA uses frames from a color history buffer to smooth edges over the course of multiple frames. This means edges in motion are smoother and there's less flickering. Because TAA calculates its effects over time, it often creates ghosting artifacts in extreme situations, such as when a GameObject moves quickly in front of a surface that contrasts with it. To select TAA for a Camera: Select the Camera in the Scene view or Hierarchy window and view it in the Inspector. Navigate to Rendering > Anti-aliasing, and select Temporal Anti-aliasing (TAA). The following features cannot be used with TAA: Multisample anti-aliasing (MSAA) Camera Stacking Dynamic Resolution Multisample Anti-aliasing (MSAA) MSAA samples the depth and stencil values of every pixel and combines these samples to produce the final pixel. Crucially, MSAA solves spatial aliasing issues and is better at solving triangle-edge aliasing issues than the other techniques. However, it does not fix shader aliasing issues such as specular or texture aliasing. MSAA is more resource intensive than other forms of anti-aliasing on most hardware. However, when run on a tiled GPU with no post-processing anti-aliasing or custom render features in use, MSAA is a cheaper option than other anti-aliasing types. MSAA is a hardware anti-aliasing method. This means you can use it with the other methods, as they are post-processing effects. However, you can't use MSAA with TAA. To enable MSAA: Open a URP Asset in the Inspector. Navigate to Quality > Anti Aliasing (MSAA) and select the level of MSAA you want. For more information on the available settings, see the MSAA setings in the URP Asset. Note: On mobile platforms that don't support the StoreAndResolve store action, if Opaque Texture is selected in the URP Asset, Unity ignores the MSAA property at runtime (as if MSAA is set to Disabled)."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/baked-lit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/baked-lit-shader.html",
    "title": "Baked Lit Shader | FSM Unity Framework",
    "keywords": "Baked Lit Shader In the Universal Render Pipeline (URP), use this Shader for stylised games or apps that only require baked lightingvia lightmaps and Light Probes. This shader does not use Physically Based Shading and has no real-time lighting, so all real-time relevant shader keywords and variants are stripped from the Shader code, which makes it faster to calculate. Using the Baked Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Baked Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how the Material is rendered on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Select how Unity calculates the color of each pixel of a transparent Material when it blends the Material with the background. In the context of Blending Modes, Source refers to the transparent Material where the Blending Mode is set and Destination refers to anything that Material overlaps with. Alpha Alpha blending mode. Alpha uses the Material's alpha value to change how transparent an object is. 0 is fully transparent. 255 is fully opaque, this is translated to a value of 1 when used with the blending equations. The Material is always rendered in the Transparent render pass regardless of it's alpha value. Alpha equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB × (1 − SourceAlpha) Premultiply Premultiply blending mode. Premultiply first multiplies the RGB values of the transparent Material by its alpha value then applies a similar effect to the Material as Alpha. The equation for Premultiply also allows areas of the transparent Material with an alpha value of 0 to have an additive blend effect. This can help reduce artifacts that may appear at the edge of the overlap between opaque and transparent pixels. Premultiply equation: OutputRGBA = SourceRGB + DestinationRGB × (1 − SourceAlpha) Additive Additive blending mode. Additive adds the color values of the Materials together to create the blend effect. The alpha value determines the strength of the source Material's color before the blend is calculated. Additive equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB Multiply Multiply blending mode. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. This mode uses the Material’s alpha value to adjust how much the colors blend. An alpha value of 1 results in unadjusted multiplication of the colors while lower values blend the colors towards white. Multiply equation: OutputRGBA = SourceRGB × DestinationRGB Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface, also known as the diffuse map. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. Tiling A 2D multiplier value that scales the Texture to fit across a mesh according to the U and V axes. This is good for surfaces like floors and walls. The default value is 1, which means no scaling. Set a higher value to make the Texture repeat across your mesh. Set a lower value to stretch the Texture. Try different values until you reach your desired effect. Offset The 2D offset that positions the Texture on the mesh. To adjust the position on your mesh, move the Texture across the U or V axes. Advanced The Advanced settings affect the underlying calculations of your rendering. They do not have a visible effect on your surface. Property Description Enable GPU Instancing Makes URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-component-reference.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-component-reference.html",
    "title": "Camera component reference | FSM Unity Framework",
    "keywords": "Camera component reference In the Universal Render Pipeline (URP), Unity exposes different properties of the Camera component in the Inspector depending on the Camera type. To change the type of the Camera, change the property Render Type. Base Render Type Projection Physical Camera Rendering Stack Environment Output Overlay Render Type Projection Physical Camera Rendering Environment Projection Property Description Projection Toggle the camera's capability to simulate perspective. Perspective Camera will render objects with perspective intact. Orthographic Camera will render objects uniformly, with no sense of perspective. Field of View Axis Set the axis Unity measures the Camera's field of view along. Available options: Vertical Horizontal This property is only visible when Projection is set to Perspective. Field of View Set the width of the Camera's view angle, measured in degrees along the selected axis. This property is only visible when Projection is set to Perspective. Size Set the viewport size of the Camera. This property is only visible when Projection is set to Orthographic. Clipping Planes Set the distances from the camera where rendering starts and stops. Near The closest point relative to the Camera that drawing will occur. Far The furthest point relative to the Camera that drawing will occur. Physical Camera Enable Physical Camera properties for this camera. When the Physical Camera properties are enabled, Unity calculates the Field of View using the properties that simulate real-world camera attributes: Focal Length, Sensor Size, and Lens Shift. Physical Camera properties are not visible in the Inspector until you tick this box. This property is only visible when Projection is set to Perspective. Physical Camera Property Description Camera Body Sensor Type Specify the real-world camera format you want the camera to simulate. Choose the desired format from the list. When you choose a camera format, Unity sets the the Sensor Size > X and Y properties to the correct values automatically. If you change the Sensor Size values manually, Unity automatically sets this property to Custom. Sensor Size Set the size, in millimeters, of the camera sensor. Unity sets the X and Y values automatically when you choose the Sensor Type. You can enter custom values if needed. X Set the horizontal size of the camera sensor. Y Set the vertical size of the camera sensor. ISO Set the light sensitivity of the Camera sensor. Shutter Speed Set the amount of time the Camera sensor captures light. Shutter Speed Unit Select the unit of measurement for Shutter Speed. Available options: Second 1/Second Gate Fit Options for changing the size of the resolution gate (size/aspect ratio of the game view) relative to the film gate (size/aspect ratio of the Physical Camera sensor). For further information about resolution gate and film gate, see documentation on Physical Cameras. Vertical Fits the resolution gate to the height of the film gate. If the sensor aspect ratio is larger than the game view aspect ratio, Unity crops the rendered image at the sides. If the sensor aspect ratio is smaller than the game view aspect ratio, Unity overscans the rendered image at the sides. When you choose this setting, changing the sensor width (Sensor Size > X property) has no effect on the rendered image. Horizontal Fits the resolution gate to the width of the film gate. If the sensor aspect ratio is larger than the game view aspect ratio, Unity overscans the rendered image on the top and bottom. If the sensor aspect ratio is smaller than the game view aspect ratio, Unity crops the rendered image on the top and bottom. When you choose this setting, changing the sensor height (Sensor Size > Y property) has no effect on the rendered image. Fill Fits the resolution gate to either the width or height of the film gate, whichever is smaller. This crops the rendered image. Overscan Fits the resolution gate to either the width or height of the film gate, whichever is larger. This overscans the rendered image. None Ignores the resolution gate and uses the film gate only. This stretches the rendered image to fit the game view aspect ratio. Lens Focal Length Set the distance, in millimeters, between the camera sensor and the camera lens. Lower values result in a wider Field of View, and vice versa. When you change this value, Unity automatically updates the Field of View property accordingly. Shift Shift the lens horizontally or vertically from center. Values are multiples of the sensor size; for example, a shift of 0.5 along the X axis offsets the sensor by half its horizontal size. You can use lens shifts to correct distortion that occurs when the camera is at an angle to the subject (for example, converging parallel lines). Shift the lens along either axis to make the camera frustum oblique. X Set the horizontal offset of the lens from the Camera sensor. Y Set the vertical offset of the lens from the Camera sensor. Aperture Set the f-stop (f-number) of the lens. A lower value gives a wider lens aperture. Focus Distance Set the distance from the camera where objects appear sharp when you enable Depth of Field. Aperture Shape Blade Count The number of blades in the lens aperture. A higher value gives a rounder aperture shape. Curvature Set the curvature of the lens aperture blades. Barrel Clipping Set the self-occlusion of the lens. A higher value creates a cat's eye effect. Anamorphism Set the stretch of the sensor. A higher value increases the stretch of the sensor to simulate an anamorphic look. Rendering Property Description Renderer Control which renderer this Camera uses. Post Processing Enable post-processing effects. Anti-Aliasing Select the method that this Camera uses for post-process anti-aliasing. A Camera can still use Multisample Anti-aliasing (MSAA), which is a hardware feature, at the same time as post-process anti-aliasing unless you use Temporal Anti-aliasing. The following Anti-aliasing options are available: None: This Camera can process MSAA but does not process any post-process anti-aliasing. Fast Approximate Anti-aliasing (FXAA): Performs a full screen pass which smooths edges on a per-pixel level. Subpixel Morphological Anti-aliasing (SMAA): Finds edge patterns in the image and blends the pixels on these edges according to those patterns. Temporal Anti-aliasing (TAA): Uses previous frames accumulated into a color history buffer to smooth edges over the course of multiple frames. For more information, see Anti-aliasing in the Universal Render Pipeline. This property is only visible when Render Type is set to Base. Quality (SMAA) Select the quality of SMAA. The difference in resource intensity is fairly small between Low and High. Available options: Low Medium High This property only appears when you select Subpixel Morphological Anti-aliasing (SMAA) from the Anti-aliasing drop-down. Quality (TAA) Select the quality of TAA. Available options: Very Low Low Medium High Very High This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down. Contrast Adaptive Sharpening Enable high quality post sharpening to reduce TAA blur. This setting is overridden when you enable AMD FidelityFX Super Resolution (FSR) upscaling in the URP Asset as FSR handles sharpening when it performs post-upscale sharpening. This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down. Base Blend Factor Set how much the history buffer blends with the current frame result. Higher values mean more history contribution, which improves the anti-aliasing, but also increases the chance of ghosting. This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down and enable Show Additional Properties in the Inspector. Jitter Scale Set the scale of the jitter applied when TAA is enabled. A lower value reduces visible flickering and jittering, but also reduces the effectiveness of the anti-aliasing. This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down and enable Show Additional Properties in the Inspector. Mip Bias Set how much texture mipmap selection is biased when rendering. A positive bias makes a texture appear more blurry, while a negative bias sharpens the texture. However, a lower value also has a negative impact on performance. Note: Requires mipmaps in textures. This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down and enable Show Additional Properties in the Inspector. Variance Clamp Scale Set the size of the color volume Unity uses to find nearby pixels when the color history is incorrect or unavailable. To do this, the clamp limits how much a pixel's color can vary from the color of the pixels that surround it. Lower values can reduce ghosting, but produce more flickering. Higher values reduce flickering, but are prone to blur and ghosting. This property only appears when you select Temporal Anti-aliasing (TAA) from the Anti-aliasing drop-down and enable Show Additional Properties in the Inspector. Stop NaNs Enable the checkbox to make this Camera replace values that are Not a Number (NaN) with a black pixel. This stops certain effects from breaking, but is a resource-intensive process. Only enable this feature if you experience NaN issues that you can not fix. This property is only visible when Render Type is set to Base. Dithering Enable the checkbox to apply 8-bit dithering to the final render. This can help reduce banding on wide gradients and low light areas. This property is only visible when Render Type is set to Base. Clear Depth Enable to clear depth from previous camera on rendering. This property is only visible when Render Type is set to Overlay. Render Shadows Enable shadow rendering. Priority A Camera with a higher priority is drawn on top of a Camera with a lower priority. Priority has a range from -100 to 100. This property is only visible when Render Type is set to Base. Opaque Texture Control whether the Camera creates a CameraOpaqueTexture, which is a copy of the rendered view. Available options: Off: Camera does not create a CameraOpaqueTexture. On: Camera creates a CameraOpaqueTexture. Use Pipeline Settings: The Render Pipeline Asset determines the value of this setting. This property is only visible when Render Type is set to Base. Depth Texture Control whether the Camera creates CameraDepthTexture, which is a copy of the rendered depth values. Available options: Off: Camera does not create a CameraDepthTexture. On: Camera creates a CameraDepthTexture. Use Pipeline Settings: The Render Pipeline Asset determines the value of this setting. Culling Mask Select which Layers the Camera renders to. Occlusion Culling Enable Occlusion Culling. Stack Note: This section is only available if Render Type is set to Base A camera stack allows to composite results of several cameras together. The camera stack consists of a Base camera and any number of additional Overlay cameras. You can use the stack property add Overlay cameras to the stack and they will render in the order as defined in the stack. For more information on configuring and using Camera Stacks, see Camera Stacking. Environment Property Description Background Type Control how to initialize the color buffer at the start of this Camera's render loop. For more information, see the documentation on clearing. This property is only visible when Render Type is set to Base. Skybox Initializes the color buffer by clearing to a Skybox. Defaults to a background color if no Skybox is found. Solid Color Initializes the color buffer by clearing to a given color. If you select this property, Unity shows the following extra property: Background: the Camera clears its color buffer to this color before rendering. Uninitialized Does not initialize the color buffer. This means that the load action for that specific RenderTarget will be DontCare instead of Load or Clear. DontCare specifies that the previous contents of the RenderTarget don't need to be preserved. Only use this option in order to optimize performance in situations where your Camera or Camera Stack will draw to every pixel in the color buffer, otherwise the behaviour of pixels the Camera doesn't draw is undefined. Note: The results might look different between Editor and Player, as the Editor doesn't run on Tile-Based Deferred Rendering (TBDR) GPUs (found in mobile devices). If you use this option on TBDR GPUs, it causes uninitialized tile memory and the content is undefined. Volumes The settings in this section define how Volumes affect this Camera. Update Mode Select how Unity updates Volumes. Available options: Every Frame: Update Volumes with every frame Unity renders. Via Scripting: Only update volumes when triggered by a script. Use Pipeline Settings: Use the default setting for the Render Pipeline. Volume Mask Use the drop-down to set the Layer Mask that defines which Volumes affect this Camera. Volume Trigger Assign a Transform that the Volume system uses to handle the position of this Camera. For example, if your application uses a third person view of a character, set this property to the character's Transform. The Camera then uses the post-processing and Scene settings for Volumes that the character enters. If you do not assign a Transform, the Camera uses its own Transform instead. Output Note: This section is only available if Render Type is set to Base Note: When a Camera's Render Type is set to Base and its Render Target is set to Texture, Unity does not show the following properties in the Inspector for the Camera: Target Display HDR MSAA Allow Dynamic Resolution This is because the Render Texture determines these properties. To change them, do so on the Render Texture Asset. Property Description Output Texture Render this Camera's output to a RenderTexture if this field is assigned, otherwise render to the screen. Target Display Define which external device to render to. Between 1 and 8. This property only appears when you select Screen from the Output Target drop-down. Target Eye Select the target eye for this camera. Available options: Both: Allows XR rendering from the selected Camera. None: Disables XR rendering for the selected Camera. Viewport Rect Four values that indicate where on the screen this camera view is drawn. Measured in Viewport Coordinates (values 0-1). This property only appears when you select Screen from the Output Target drop-down. X The beginning horizontal position Unity uses to draw the camera view. Y The beginning vertical position Unity uses to draw the camera view. W (Width) Width of the camera output on the screen. H (Height) Height of the camera output on the screen. HDR Enable High Dynamic Range rendering for this camera. This property only appears when you select Screen from the Output Target drop-down. MSAA Enable Multisample Anti-aliasing for this camera. This property only appears when you select Screen from the Output Target drop-down. Allow Dynamic Resolution Enable URP Dynamic Resolution rendering for this Camera. This property only appears when you select Screen from the Output Target drop-down."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-stacking.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-stacking.html",
    "title": "Camera Stacking | FSM Unity Framework",
    "keywords": "Camera Stacking In the Universal Render Pipeline (URP), you use Camera Stacking to layer the output of multiple Cameras and create a single combined output. Camera Stacking allows you to create effects such as a 3D model in a 2D UI, or the cockpit of a vehicle. A Camera Stack consists of a Base Camera and one or more Overlay Cameras. A Camera Stack overrides the output of the Base Camera with the combined output of all the Cameras in the Camera Stack. As such, anything that you can do with the output of a Base Camera, you can do with the output of a Camera Stack. For example, you can render a Camera Stack to a given render target, apply post-process effects, and so on. URP performs several optimizations within a Camera, including rendering order optimizations to reduce overdraw. However, when you use a Camera Stack, you effectively define the order in which those Cameras are rendered. You must therefore be careful not to order the Cameras in a way that causes excessive overdraw. For more information on overdraw in URP, see Advanced information. For examples of how to use Camera Stacking, see the Camera Stacking samples in URP Package Samples. Adding a Camera to a Camera Stack Create a Camera in your Scene. Its Render Type defaults to Base, making it a Base Camera. Create another Camera in your Scene, and select it. In the Camera Inspector, change the Camera’s Render Type to Overlay. Select the Base Camera again. In the Camera Inspector, scroll to the Stack section, click the plus (+) button, and click the name of the Overlay Camera. The Overlay Camera is now part of the Base Camera's Camera Stack. Unity renders the Overlay Camera's output on top of the Base Camera's output. You can add a Camera to a Camera Stack in a script by directly manipulating the cameraStack property of the Base Camera's Universal Additional Camera Data component, like this: var cameraData = camera.GetUniversalAdditionalCameraData(); cameraData.cameraStack.Add(myOverlayCamera); Removing a Camera from a Camera Stack Create a Camera Stack that contains at least one Overlay Camera. For instructions, see Adding a Camera to a Camera Stack. Select the Camera Stack's Base Camera. In the Camera Inspector, scroll to the Stack section, click the name of the Overlay Camera you want to remove, and then click the minus (-) button. The Overlay Camera remains in the Scene, but is no longer part of the Camera Stack. You can remove a Camera from a Camera Stack in a script by directly manipulating the cameraStack property of the Base Camera's Universal Additional Camera Data component, like this: var cameraData = camera.GetUniversalAdditionalCameraData(); cameraData.cameraStack.Remove(myOverlayCamera); Changing the order of Cameras in a Camera Stack Create a Camera Stack that contains more than one Overlay Camera. For instructions, see Adding a Camera to a Camera Stack. Select the Base Camera in the Camera Stack. In the Camera Inspector, scroll to the Stack section. Use the handles next to the names of the Overlay Cameras to reorder the list of Overlay Cameras. The Base Camera renders the base layer of the Camera Stack, and the Overlay Cameras in the stack render on top of this in the order that they are listed, from top to bottom. You can reorder a Camera Stack in a script by directly manipulating the cameraStack property of the Base Camera's Universal Additional Camera Data component. Adding the same Overlay Camera to multiple stacks To add an Overlay Camera to multiple Camera Stacks: Create a Camera Stack that contains at least one Overlay Camera. For instructions, see Adding a Camera to a Camera Stack. Create a Camera in your Scene. Its Render Type defaults to Base, making it a Base Camera. Select the new Base Camera. In the Camera Inspector, scroll to the Stack section, click the plus (+) button, and click the name of the Overlay Camera that you want to use in both Camera Stacks. The Overlay Camera is now rendering in both Camera Stacks. You can also add a Camera to a Camera Stack in a script by directly manipulating the cameraStack property of the Base Camera's Universal Additional Camera Data component, like this: var cameraData = camera.GetUniversalAdditionalCameraData(); cameraData.cameraStack.Add(myOverlayCamera);"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-types-and-render-type.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/camera-types-and-render-type.html",
    "title": "Render Type | FSM Unity Framework",
    "keywords": "Render Type There are two types of Camera in the Universal Render Pipeline (URP): A Base Camera is a general purpose Camera that renders to a render target (a screen, or a Render Texture). An Overlay Camera renders on top of another Camera's output. You can combine the output of a Base Camera with the output of one or more Overlay Cameras. This is called Camera stacking. Use a Camera’s Render Type property to make it a Base Camera or an Overlay Camera. To change the type of a Camera in the Unity Editor: Create or select a Camera in your Scene. In the Camera Inspector, use the Render Type drop-down menu to select a different type of Camera. Select either: Base to change the Camera to a Base Camera Overlay to change the Camera to an Overlay Camera You can change a Camera’s type in a script, by setting the renderType property of the Camera's Universal Additional Camera Data component, like this: var cameraData = camera.GetUniversalAdditionalCameraData(); cameraData.renderType = CameraRenderType.Base; Base Camera Base Camera is the default type of Camera in URP. A Base Camera is a general purpose Camera that renders to a given render target. To render anything in URP, you must have at least one Base Camera in your Scene. You can have multiple Base Cameras in a Scene. You can use a Base Camera on its own, or you can use it in a Camera stack. For more information on working with multiple Cameras in URP, see Working with multiple cameras. When you have an active Base Camera in your Scene, this icon appears next to the Camera Gizmo in the Scene view: For information on the properties that Unity exposes in the Inspector for a Base Camera, see the Camera component reference. Overlay Camera An Overlay Camera is a Camera that renders its view on top of another Camera's output. You can use Overlay Cameras to create effects such as 3D objects in a 2D UI, or a cockpit in a vehicle. You must use Overlay Cameras in conjunction with one or more Base Cameras using the Camera Stacking system. You cannot use Overlay Cameras on their own. An Overlay Camera that is not part of a Camera Stack does not perform any steps of its render loop, and is known as an orphan Camera. Important note: In this version of URP, Overlay Cameras and Camera Stacking are supported only when using the Universal Renderer. When you have an active Overlay Camera in your Scene, this icon appears next to the Camera Gizmo in the Scene view: The Base Camera in a Camera Stack determines most of the properties of the Camera Stack. Because you can only use Overlay Cameras in a Camera Stack, URP uses only the following properties of an Overlay Camera when rendering the Scene: Projection FOV Axis Field of View Physical Camera properties Clipping plans Renderer Clear Depth Render Shadows Culling Mask Occlusion Culling Unity hides all of the other unused properties in the Inspector. You can access unused properties using a script, but any changes you make to these unused properties will not affect the visual output of any Camera Stacks that use the Overlay Camera. You cannot apply post-processing to an individual Overlay Camera. You can apply post-processing to an individual Base Camera, or to a Camera Stack. For information on the properties that Unity exposes in the Inspector of an Overlay Camera, see the Camera component reference."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras-advanced.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras-advanced.html",
    "title": "Clearing, rendering order and overdraw | FSM Unity Framework",
    "keywords": "Clearing, rendering order and overdraw Clearing In the Universal Render Pipeline (URP), Camera clearing behavior depends on the Camera's Render Type. Base Camera Color buffer At the start of its render loop, a Camera with the Base Render Type can clear its color buffer to a Skybox, clear its color buffer to a solid color, or use an uninitialized color buffer. You can define this behavior using the Background Type property in the Camera Inspector when Render Type is set to Base. Note that the contents of the uninitialized color buffer vary by platform. On some platforms, the unitialized color buffer will contain data from the previous frame. On other platforms, the unitialized color buffer will contain unintialized memory. You should choose to use an unitialized color buffer only if your Camera draws to every pixel in the color buffer, and you do not wish to incur the cost of an unnecessary clear operation. Depth buffer A Base Camera always clears its depth buffer at the start of each render loop. Overlay Camera Color buffer At the start of its render loop, an Overlay Camera receives a color buffer containing color data from the previous Cameras in the Camera stack. It does not clear the contents of the color buffer. Depth buffer At the start of its render loop, an Overlay Camera receives a depth buffer containing depth data from the previous Cameras in the Camera Stack. You can define this behavior using the Clear Depth property in the Camera Inspector when Render Type is set to Overlay. When Clear Depth is set to true, the Overlay Camera clears the depth buffer and draws its view to the color buffer on top of any existing color data. When Clear Depth is set to false, the Overlay Camera tests against the depth buffer before drawing its view to the color buffer. Camera culling and rendering order If your URP Scene contains multiple Cameras, Unity performs their culling and rendering operations in a predictable order. Once per frame, Unity performs the following operations: Unity gets the list of all active Base Cameras in the Scene. Unity organises the active Base Cameras into 2 groups: Cameras that render their view to Render Textures, and Cameras that render their view to the screen. Unity sorts the Base Cameras that render to Render Textures into Priority order, so that Cameras with a higher Priority value are drawn last. For each Base Camera that renders to a Render Texture, Unity performs the following steps: Cull the Base Camera Render the Base Camera to the Render Texture For each Overlay Camera that is part of the Base Camera's Camera Stack, in the order defined in the Camera Stack: Cull the Overlay Camera Render the Overlay Camera to the Render Texture Unity sorts the Base Cameras that render to the screen into Priority order, so that Cameras with a higher Priority value are drawn last. For each Base Camera that renders to the screen, Unity performs the following steps: Cull the Base Camera Render the Base Camera to the screen For each Overlay Camera that is part of the Base Camera's Camera Stack, in the order defined in the Camera Stack: Cull the Overlay Camera Render the Overlay Camera to the screen Unity can render an Overlay Camera’s view multiple times during a frame - either because the Overlay Camera appears in more than one Camera Stack, or because the Overlay Camera appears in the same Camera Stack more than once. When this happens, Unity does not reuse any element of the culling or rendering operation. The operations are repeated in full, in the order detailed above. Important note: In this version of URP, Overlay Cameras and Camera Stacking are supported only when using the Universal Renderer. Overlay Cameras will not perform any part of their rendering loop if using the 2D Renderer. Overdraw URP performs several optimizations within a Camera, including rendering order optimizations to reduce overdraw. However, when you use a Camera Stack, you effectively define the order in which those Cameras are rendered. You must therefore be careful not to order the Cameras in a way that causes excessive overdraw. When multiple Cameras in a Camera Stack render to the same render target, Unity draws each pixel in the render target for each Camera in the Camera Stack. Additionally, if more than one Base Camera or Camera Stack renders to the same area of the same render target, Unity draws any pixels in the overlapping area again, as many times as required by each Base Camera or Camera Stack. You can use Unity's Frame Debugger, or platform-specific frame capture and debugging tools, to understand where excessive overdraw occurs in your Scene. You can then optimize your Camera Stacks accordingly."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras-multiple.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras-multiple.html",
    "title": "Working with multiple cameras | FSM Unity Framework",
    "keywords": "Working with multiple cameras In the Universal Render Pipeline (URP), you can work with multiple Cameras to: Stack Cameras to layer the outputs of multiple Cameras into a single combined output. Camera Stacking allows you to create effects such as 3D models in a 2D UI, or the cockpit of a vehicle. Render multiple Base Cameras or Camera Stacks to the same render target. This allows you to create effects such as split screen rendering. Render a Base Camera or Camera Stack to a Render Texture. Rendering to a Render Texture allows you to create effects such as CCTV monitors. You can combine these ways of working for more complex effects. For example, you can define two Camera Stacks, and then set each of those to Camera Stacks that render to a different area of the same render target. For information on Camera rendering order when working with multiple Cameras, see Rendering order and overdraw."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/cameras.html",
    "title": "Cameras | FSM Unity Framework",
    "keywords": "Cameras A Camera in Unity works like a camera in the real world: it captures a view of objects in 3-dimensional space and flattens that view to display it on a 2-dimensional surface. Cameras in the Universal Render Pipeline (URP) are based on Unity's standard Camera functionality, but with some significant differences. The most notable ways in which Cameras in URP differ from standard Unity Cameras are: The Universal Additional Camera Data component, which extends the Camera component's functionality and allows URP to store additional Camera-related data The Render Type setting, which defines the two types of Camera in URP: Base and Overlay The Camera Stacking system, which allows you to layer the output of multiple Cameras into a single combined output The Volume system, which allows you to apply post-processing effects to a Camera based on a given Transform's position within your Scene The Camera component, which exposes URP-specific properties in the Inspector Each camera in the Scene requires resources for URP culling and rendering. To configure URP for better performance, minimize the number of cameras you use. This also reduces processing time on the GPU. For a general introduction to how Cameras work in Unity, and examples of common Camera workflows, see the Unity Manual section on Cameras."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/configure-for-better-performance.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/configure-for-better-performance.html",
    "title": "Configure for better performance | FSM Unity Framework",
    "keywords": "Configure for better performance You can disable or change Universal Render Pipeline (URP) settings and features that have a large performance impact. This helps you get better performance for your project, especially on lower-end platforms. Depending on your project or the platforms you target, one or all of the following might have the biggest effect: Which rendering path you choose How much memory URP uses Processing time on the CPU Processing time on the GPU You can use the Unity Profiler or a GPU profiler such as RenderDoc or Xcode to measure the effect of each setting on the performance of your project. You might not be able to disable some features if your project needs them. Choose a rendering path Refer to Universal Renderer for more information about the three rendering paths in URP, and the performance effects and limitations of each one. Reduce how much memory URP uses You can do the following in the URP Asset: Disable Depth Texture unless you need it (for example, if you use a shader that samples scene depth), so that URP doesn't store a depth texture unless it's needed. Disable Opaque Texture, so that URP doesn't store a snapshot of the opaques in a scene unless it needs to. If you use the Deferred rendering path, disable Use Rendering Layers so that URP doesn't create an extra render target. Disable High Dynamic Range (HDR) if you don't need it, so that URP doesn't do HDR calculations. If you need HDR, set HDR Precision to 32 Bit. Reduce Main Light > Shadow Resolution, to lower the resolution of the shadow map for the main light. If you use additional lights, reduce Additional Lights > Shadow Atlas Resolution, to lower the resolution of the shadow map for additional lights. Disable Light Cookies if you don't need them, or reduce Cookie Atlas Resolution and Cookie Atlas Format. On lower-end mobile platforms, set Store Actions to Auto or Discard, so that URP doesn't use memory bandwidth to copy the render targets from each pass into and out of memory. In the Universal Renderer asset, you can set Intermediate Texture to Auto, so that Unity only renders using an intermediate texture when necessary. This might also reduce how much GPU memory bandwidth URP uses. Use the Frame Debugger to check if URP removes the intermediate texture when you change this setting. You can also do the following: Minimize the use of the Decal Renderer Feature, because URP creates an additional render pass to render decals. This also reduces processing time on the CPU and GPU. Refer to Decal Renderer Feature for more information. Strip shader variants for features you don't use. Reduce processing time on the CPU You can do the following in the URP Asset: Set Volume Update Mode to Via Scripting, so that URP doesn't update volumes every frame. You need to update volumes manually using an API such as UpdateVolumeStack. On lower-end mobile platforms, if you use Reflection Probes, disable Probe Blending and Box Projection. In the Shadows section, reduce Max Distance so that URP processes fewer objects in the shadow pass. This also reduces processing time on the GPU. In the Shadows section, reduce Cascade Count to reduce the number of render passes. This also reduces processing time on the GPU. In the Additional Lights section, disable Cast Shadows. This also reduces processing time on the GPU and how much memory URP uses. Each camera in the Scene requires resources for URP culling and rendering. To optimize URP for better performance, minimize the number of cameras you use. This also reduces processing time on the GPU. Reduce processing time on the GPU You can do the following in the URP Asset: Reduce or disable Anti-aliasing (MSAA), so that URP doesn't use memory bandwidth to copy frame buffer attachments into and out of memory. This also reduces how much memory URP uses. Disable Terrain Holes. Enable SRP Batcher, so that URP reduces the GPU setup between draw calls and makes material data persistent in GPU memory. Check your shaders are compatible with the SRP Batcher first. This also reduces processing time on the CPU. On lower-end mobile platforms, disable LOD Cross Fade, so that URP doesn't use alpha testing to fade level of detail (LOD) meshes in and out. Set Additional Lights to Disabled, or Per Vertex if you use the Forward rendering path. This reduces the work URP does to calculate lighting. This also reduces processing time on the CPU if you set to Disabled. Disable Soft Shadows, or enable Soft Shadows but reduce Quality. You can do the following in the Universal Renderer asset: Enable Native RenderPass if you use Vulkan, Metal or DirectX 12 graphics APIs, so that URP automatically reduces how often it copies render textures into and out of memory. This also reduces how much memory URP uses. If you use the Forward or Forward+ rendering path, set Depth Priming Mode to Auto or Forced for PC and console platforms, or Disabled for mobile platforms. On PC and console platforms, this makes URP create and use depth textures to avoid running pixel shaders on obscured pixels. Set Depth Texture Mode to After Transparents, so that URP avoids switching render targets between the opaque pass and the transparent pass. You can also do the following: Avoid use of the Complex Lit shader, which has complex lighting calculations. If you use the Complex Lit shader, disable Clear Coat. On lower-end mobile platforms, use the Baked Lit shader for static objects and the Simple Lit shader for dynamic objects. If you use Screen Space Ambient Occlusion (SSAO), refer to Ambient Occlusion for more information about settings that have a large performance impact. Additional resources Understand performance in URP Optimize for better performance Introduction to the Universal Render Pipeline for advanced Unity creators Performance optimization for high-end graphics on PC and console Making Alba: How to build a performant open-world game Post-processing in URP for mobile devices. Optimizing lighting for a healthy frame rate Refer to the following for more information on the settings: Deferred Rendering Path in URP Forward+ Rendering Path Universal Render Pipeline Asset Universal Renderer"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/create-custom-renderer-feature-1.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/create-custom-renderer-feature-1.html",
    "title": "How to create a custom Renderer Feature | FSM Unity Framework",
    "keywords": "How to create a custom Renderer Feature This section describes how to create a custom Renderer Feature for a URP Renderer. This section assumes the following: The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). This article contains the following sections: Create example Scene and GameObjects. Create a scriptable Renderer Feature and add it to the Universal Renderer. Create and enqueue the scriptable Render Pass. Implement rendering commands in the Execute method. Implement the example-specific Material and rendering code. Change the order of the render passes Complete code for this example Create example Scene and GameObjects To follow the steps in this section, create a new Scene with the following GameObjects: Create a plane. Create a new Material and assign it the Universal Render Pipeline/Lit shader. Set the base color to grey (for example, #6A6A6A). Call the Material Plane. Create a Point Light and place it above the plane. Your Scene should look like the following illustration: Create a scriptable Renderer Feature and add it to the Universal Renderer This part shows how to create a scriptable Renderer Feature and implement the methods that let you configure and inject ScriptableRenderPass instances into the scriptable Renderer. Create a new C# script. Call the script LensFlareRendererFeature.cs. Open the script, remove all the code from the LensFlareRendererFeature class that Unity created. Add the following using directive. using UnityEngine.Rendering.Universal; The LensFlareRendererFeature class must inherit from the ScriptableRendererFeature class. public class LensFlareRendererFeature : ScriptableRendererFeature The class must implement the following methods: Create: Unity calls this method on the following events: When the Renderer Feature loads the first time. When you enable or disable the Renderer Feature. When you change a property in the inspector of the Renderer Feature. AddRenderPasses: Unity calls this method every frame, once for each Camera. This method lets you inject ScriptableRenderPass instances into the scriptable Renderer. Now you have the custom LensFlareRendererFeature Renderer Feature with its main methods. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { public override void Create() { } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { } } Add the Renderer Feature you created to the the Universal Renderer asset. Follow this link to read how to add a Renderer Feature to a Renderer. Add the Lens Flare Renderer Feature to the Universal Renderer. Create and enqueue the scriptable Render Pass This part shows how to create a scriptable Render Pass and and enqueue its instance into the scriptable Renderer. In the LensFlareRendererFeature class, declare the LensFlarePass class that inherits from ScriptableRenderPass. class LensFlarePass : ScriptableRenderPass { } In LensFlarePass, add the Execute method. Unity runs the Execute method every frame. In this method, you can implement your custom rendering functionality. public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { } In the LensFlareRendererFeature class, declare a private LensFlarePass field. private LensFlarePass _lensFlarePass; In the Create method, instantiate the _lensFlarePass object: _lensFlarePass = new LensFlarePass(FlareSettings); In the AddRenderPasses method, use the EnqueuePass method of the renderer object to enqueue _lensFlarePass in the rendering queue. renderer.EnqueuePass(_lensFlarePass); Now your custom LensFlareRendererFeature Renderer Feature is executing the Execute method inside the custom LensFlarePass pass. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { Debug.Log(message: \"The Execute() method runs.\"); } } private LensFlarePass _lensFlarePass; public override void Create() { _lensFlarePass = new LensFlarePass(); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { renderer.EnqueuePass(_lensFlarePass); } } Implement rendering commands in the Execute method This part shows how to implement custom logic in the Execute method. Create a CommandBuffer type object. This object holds the list of rendering commands to execute. In the Execute method, add the following line: CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); The method CommandBufferPool.Get(name: \"LensFlarePass\") gets the new command buffer and assigns a name to it. Add the line that executes the command buffer and the line that releases it. In the Execute method, add the following lines after the command buffer declaration: context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); Now the boilerplate part is ready and we can proceed to implementing the custom rendering logic. The following steps implement the custom rendering logic. In this example, the Renderer Feature draws lens flares as a texture on a Quad. The implementation requires a Material and a mesh (Quad). In the LensFlarePass class, declare two private fields: Material and Mesh: private Material _material; private Mesh _mesh; Then declare the constructor that takes those variables as arguments: public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } Now the LensFlarePass class expects two arguments. To initialize the class with the arguments, add the following public fields in the LensFlareRendererFeature class: public Material material; public Mesh mesh; And add the arguments to the LensFlarePass declaration in the Create method: _lensFlarePass = new LensFlarePass(material, mesh); In the Execute method, use the DrawMesh method of the cmd object. The method takes the _material and the _mesh fields as arguments. Add the following line between the cmd object declaration and the command context.ExecuteCommandBuffer(cmd). cmd.DrawMesh(_mesh, Matrix4x4.identity, _material); To ensure that Unity does call the DrawMesh method with null arguments, in the AddRenderPasses method, wrap the EnqueuePass call in the null check condition: if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } Now the LensFlarePass class has the following basic logic in the Execute method: Get the new command buffer and assign it the name LensFlarePass. Add rendering commands. Execute the command buffer. Release the buffer. NOTE: Unity does not enqueue the LensFlarePass pass yet, because the Material and the Mesh properties are null. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { private Material _material; private Mesh _mesh; public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); cmd.DrawMesh(_mesh, Matrix4x4.identity, _material); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } private LensFlarePass _lensFlarePass; public Material material; public Mesh mesh; public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } } } Implement the example-specific Material and rendering code This section shows how to create a Material for the lens flare effect and how to implement the code to render flares at the positions of Lights. Create a new Material, and assign it the Universal Render Pipeline/Unlit shader. Call the Material LensFlare. For demonstration purpose, change the base color of the Material to red. In the Universal Renderer, in Lens Flare Renderer Feature, select the LensFlare Material in the Material property, and the Quad mesh in the Mesh property. The Renderer Feature draws the quad in the Scene, but at this point it's just black. This is because the Universal Render Pipeline/Unlit shader has multiple passes, and one of them paints the quad black. To change this behavior, use the cmd.DrawMesh method overload that accepts the shaderPass argument, and specify shader pass 0: cmd.DrawMesh(_mesh, Matrix4x4.identity, _material, 0, 0); The following steps show the changes that are specific to the effect implementation in this example. They are for illustrative purposes. Add the following lines in the Execute method. Place them after the cmd object declaration. These lines ensure that Unity draws the quad with the flare in the following way: In the screen space. With the correct aspect ratio. For each Light, in the center of the Light. // Get the Camera data from the renderingData argument. Camera camera = renderingData.cameraData.camera; // Set the projection matrix so that Unity draws the quad in screen space cmd.SetViewProjectionMatrices(Matrix4x4.identity, Matrix4x4.identity); // Add the scale variable, use the Camera aspect ratio for the y coordinate Vector3 scale = new Vector3(1, camera.aspect, 1); // Draw a quad for each Light, at the screen space position of the Light. foreach (VisibleLight visibleLight in renderingData.lightData.visibleLights) { Light light = visibleLight.light; // Convert the position of each Light from world to viewport point. Vector3 position = camera.WorldToViewportPoint(light.transform.position) * 2 - Vector3.one; // Set the z coordinate of the quads to 0 so that Uniy draws them on the same plane. position.z = 0; // Change the Matrix4x4 argument in the cmd.DrawMesh method to use the position and // the scale variables. cmd.DrawMesh(_mesh, Matrix4x4.TRS(position, Quaternion.identity, scale), _material, 0, 0); } Now Unity draws a quad in the center of each Light. To visualize the lens flare, make the following changes to the LensFlare Material. Add the following texture to the base map: Set the color to white. Set Surface Type to Transparent. Set Blending Mode to Additive. Now Unity draws the lens flare texture on the quad, but a part of the flare is not visible: This is because Unity draws the skybox after the LensFlarePass render pass. Change the order of the render passes To see the order in which Unity draws the render passes, open the Frame Debugger (Window > Analysis > Frame Debugger). To enqueue the LensFlarePass pass after the skybox pass, use the renderPassEvent property of LensFlarePass. Assign the property the AfterRenderingSkybox event from the RenderPassEvent enum. Make the following changes in the Create method: public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); // Draw the lens flare effect after the skybox. _lensFlarePass.renderPassEvent = RenderPassEvent.AfterRenderingSkybox; } Now Unity draws the lens flare on top of the skybox. Complete code for this example Below is the complete code for this example. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { private Material _material; private Mesh _mesh; public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); // Get the Camera data from the renderingData argument. Camera camera = renderingData.cameraData.camera; // Set the projection matrix so that Unity draws the quad in screen space. cmd.SetViewProjectionMatrices(Matrix4x4.identity, Matrix4x4.identity); // Add the scale variable, use the Camera aspect ratio for the y coordinate Vector3 scale = new Vector3(1, camera.aspect, 1); // Draw a quad for each Light, at the screen space position of the Light. foreach (VisibleLight visibleLight in renderingData.lightData.visibleLights) { Light light = visibleLight.light; // Convert the position of each Light from world to viewport point. Vector3 position = camera.WorldToViewportPoint(light.transform.position) * 2 - Vector3.one; // Set the z coordinate of the quads to 0 so that Uniy draws them on the same // plane. position.z = 0; // Change the Matrix4x4 argument in the cmd.DrawMesh method to use // the position and the scale variables. cmd.DrawMesh(_mesh, Matrix4x4.TRS(position, Quaternion.identity, scale), _material, 0, 0); } context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } private LensFlarePass _lensFlarePass; public Material material; public Mesh mesh; public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); // Draw the lens flare effect after the skybox. _lensFlarePass.renderPassEvent = RenderPassEvent.AfterRenderingSkybox; } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } } }"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/how-to-custom-effect-render-objects.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/how-to-custom-effect-render-objects.html",
    "title": "Example: How to create a custom rendering effect using the Render Objects Renderer Feature | FSM Unity Framework",
    "keywords": "Example: How to create a custom rendering effect using the Render Objects Renderer Feature URP draws objects in the DrawOpaqueObjects and DrawTransparentObjects passes. You might need to draw objects at a different point in the frame rendering, or interpret and write rendering data (like depth and stencil) in alternate ways. The Render Objects Renderer Feature lets you do such customizations by letting you draw objects on a certain layer, at a certain time, with specific overrides. The example on this page describes how to create a custom rendering effect with the Render Objects Renderer Feature. Example overview The example on this page demonstrates how to implement the following effect: There is a character in the Scene. When the character goes behind GameObjects, Unity draws the character silhouette with a different Material. Prerequisites This example requires the following: A Unity project with the URP package installed. The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). Create example Scene and GameObjects To follow the steps in this example, create a new Scene with the following GameObjects: Create a Cube. Set its Scale values so that it looks like a wall. Create a Material and assign it the Universal Render Pipeline/Lit shader. Select the base color (for example, red). Call the Material Character. Create a basic character and assign it the Character Material. In this example, the character consists of three capsules: the big capsule in the center represents the body, and the two smaller capsules represent the hands. To make it easier to manipulate the character in the Scene, add the three Capsules as child GameObjects under the Character GameObject. Create a Material and assign it the Universal Render Pipeline/Unlit shader. Select the base color that you would like the character to have when it's behind GameObjects (for example, blue). Call the Material CharacterBehindObjects. Now you have the setup necessary to follow the steps in this example. Example implementation This section assumes that you created a Scene as described in section Example Scene and GameObjects. The example implementation uses two Render Objects Renderer Features: one to draw parts of the character that are behind other GameObjects, and another one to draw the parts of the character that are in front of other GameObjects. Create a Renderer Feature to draw the character behind GameObjects Follow these steps to create a Renderer Feature to draw the character behind GameObjects. Select a URP Renderer. In the Inspector, click Add Renderer Feature and select Render Objects. Select the Name field and enter the name of the new Renderer Feature, for example, DrawCharacterBehind. This example uses Layers to filter the GameObjects to render. Create a new Layer and call it Character. Select the Character GameObject and assign it to the Character Layer. To do this, open the Layer drop down and select Character. In the DrawCharacterBehind Renderer Feature, in Filters > Layer Mask, select Character. With this setting, this Renderer Feature renders GameObjects only in the Layer Character. In Overrides > Material, select the CharacterBehindObjects Material. The Renderer Feature overrides the Material of a GameObject with the selected Material. The intended behavior is that the Renderer Feature renders the character with the CharacterBehindObjects Material only when the character is behind other GameObjects. To achieve this, select the Depth check box, and set the Depth Test property to Greater. With these settings, Unity renders the character with the CharacterBehindObjects Material only when the character is behind another GameObject. However, Unity also renders parts of the character using the CharacterBehindObjects Material, because some parts of the character occlude the character itself. Create an extra Renderer Feature to avoid the self see-through effect The settings in the previous section result in the self see-through effect for the following reason: When performing the Opaque rendering pass of the URP Renderer, Unity renders all GameObjects belonging to the character with the Character Material and writes depth values to the Depth buffer. This happens before Unity starts executing the DrawCharacterBehind Renderer Feature, because, by default, new Render Objects Renderer Features have the value AfterRenderingOpaques in the Event property. The Event property defines the injection point where Unity injects Render Passes from the Render Objects Renderer Feature. The event when URP Renderer draws GameObjects in the Opaque Layer Mask is the BeforeRenderingOpaques event. When executing the DrawCharacterBehind Renderer Feature, Unity performs the depth test using the condition specified in the Depth Test property. In the following screenshot, a bigger capsule occludes part of the smaller capsule, and the depth test passes for that part of the smaller capsule. The Renderer Feature overrides the Material for that part. The following steps describe how to avoid such behavior and ensure that Unity draws all parts of the character with proper Materials. In the URP asset, in Filtering > Opaque Layer Mask, clear the check mark next to the Character Layer. Now Unity does not render the character unless it's behind a GameObject. Add a new Render Objects Renderer Feature, and call it Character. In the Character Renderer Feature, in Filters > Layer Mask, select the Character Layer. Now Unity renders the character with the Character Material even when the character is behind GameObjects. This happens because the DrawCharacterBehind Renderer Feature writes values to the depth buffer. When Unity executes the Character Renderer Feature, the pixels on the character appear to be in front of the pixels that Unity has drawn previously, and Unity draws on top of those pixels. In the DrawCharacterBehind Renderer Feature, In Overrides > Depth, clear the Write Depth check box. With this setting, the DrawCharacterBehind Renderer Feature does not make changes to the depth buffer and the Character Renderer Feature does not draw the character when it's behind GameObjects. The example is complete. When the character goes behind GameObjects, Unity draws the character silhouette with the CharacterBehindObjects Material. With the extra Character Renderer Feature, Unity renders GameObjects as follows: URP Renderer does not render the Character GameObject in the BeforeRenderingOpaques event, because the Character Layer is excluded from the Opaque Layer Mask list. The DrawCharacterBehind Renderer Feature draws parts of the character that are behind other GameObjects. This happens in the AfterRenderingOpaques event. The Character Renderer Feature draws parts of the character that are in front of other GameObjects. This happens in the AfterRenderingOpaques event, and after executing the DrawCharacterBehind Renderer Feature."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/post-processing-custom-effect-low-code.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/containers/post-processing-custom-effect-low-code.html",
    "title": "How to create a custom post-processing effect | FSM Unity Framework",
    "keywords": "How to create a custom post-processing effect The example on this page shows how to use a Full Screen Render Pass to create a grayscale custom post-processing effect. Prerequisites This example requires the following: A Unity project with the URP package installed. The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). Create a Fullscreen Shader Graph You must create a Fullscreen Shader Graph to create a custom post-processing effect. Create a new Shader Graph in your Project. To do this right-click in the Project window and select Create > Shader Graph > URP > Fullscreen Shader Graph. Add a URP Sample Buffer node. To do this right-click in the Shader Graph window, and select Create Node. Then locate and select URP Sample Buffer. In the URP Sample Buffer node's Source Buffer dropdown menu, select BlitSource. Add a Vector 3 node. Assign the Vector 3 node the following values: X = 0.2126 Y = 0.7152 Z = 0.0722 Add a Dot Product node. Connect the nodes as shown below. Node Connection URP Sample Buffer Output to Dot Product A Vector 3 Out to Dot Product B Dot Product Out to Fragment Base Color Save your Shader Graph. Create a new Material in your Project. To do this right-click in the Project window and select Create > Material. Apply the Shader Graph shader to the Material. To do this, open the Material in the Inspector and select Shader > Shader Graphs, then select the Shader Graph you created in the previous steps. Use the Material in a Full Screen Pass Renderer Feature Once you've created a compatible Shader Graph and Material, you can use the Material with a Full Screen Pass Renderer Feature to create a custom post-processing effect. In the Project window, select a URP Renderer. In the Inspector, click Add Renderer Feature and select Full Screen Pass Renderer Feature. For more information on adding Renderer Features see How to add a Renderer Feature to a Renderer. Set the Post Process Material to the Material you created with the Fullscreen Shader Graph. Set Injection Point to After Rendering Post Processing. Set Requirements to Color. You should now see the effect in both Scene view and Game view. Example scene with a grayscale custom post-processing effect."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/creating-a-new-project-with-urp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/creating-a-new-project-with-urp.html",
    "title": "Using the Universal Render Pipeline in a new Project | FSM Unity Framework",
    "keywords": "Using the Universal Render Pipeline in a new Project If you want to use the Universal Render Pipeline (URP) in a new Project, you can create a new Project using a Template. To create a URP Project using a Template: Open the Unity Hub. On the Home page, click New to start a new Project. The Templates popup appears. Select the Universal Render Pipeline Template. Click Create. Unity creates a new Project for you. The new Project has URP installed and configured, and includes some example content that demonstrates URP's functionality. In the Project window, navigate to the Assets folder, and select the Readme Asset. Unity will show information about the Project in the Inspector window."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/blit-overview.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/blit-overview.html",
    "title": "URP blit best practices | FSM Unity Framework",
    "keywords": "URP blit best practices A blit operation is a process of copying a source texture to a destination texture. This page provides an overview of different ways to perform a blit operation in URP and best practices to follow when writing custom render passes. The legacy CommandBuffer.Blit API Avoid using the CommandBuffer.Blit API in URP projects. The CommandBuffer.Blit API is the legacy API. It implicitly runs extra operations related to changing states, binding textures, and setting render targets. Those operations happen under the hood in SRP projects and are not transparent to the user. The API has compatibility issues with the URP XR integration. Using cmd.Blit might implicitly enable or disable XR shader keywords, which breaks XR SPI rendering. The CommandBuffer.Blit API is not compatible with NativeRenderPass and RenderGraph. Similar considerations apply to any utilities or wrappers relying on cmd.Blit internally, RenderingUtils.Blit is one such example. SRP Blitter API Use the Blitter API in URP projects. This API does not rely on legacy logic, and is compatible with XR, native Render Passes, and other SRP APIs. Custom full-screen blit example The How to perform a full screen blit in URP example shows how to create a custom Renderer Feature that performs a full screen blit. The example works in XR and is compatible with SRP APIs."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/custom-pass-injection-points.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/custom-pass-injection-points.html",
    "title": "Injection points | FSM Unity Framework",
    "keywords": "Injection points URP contains multiple injection points that let you inject render passes into the frame rendering loop and execute them upon different events. Injection points give a custom render pass access to URP buffers. A render pass has read and write access to all buffers at each injection point. Unity provides the following events in the rendering loop. You can use these events to inject your custom passes. Injection point Description BeforeRendering Executes a ScriptableRenderPass instance before rendering any other passes in the pipeline for the current Camera. Camera matrices and stereo rendering are not setup at this point. You can use this injection point to draw to custom input textures used later in the pipeline, for example, LUT textures. BeforeRenderingShadows Executes a ScriptableRenderPass instance before rendering shadow maps (MainLightShadow, AdditionalLightsShadow passes). Camera matrices and stereo rendering are not set up at this point. AfterRenderingShadows Executes a ScriptableRenderPass instance after rendering shadow maps (MainLightShadow, AdditionalLightsShadow passes). Camera matrices and stereo rendering are not set up this point. BeforeRenderingPrePasses Executes a ScriptableRenderPass instance before rendering prepasses (DepthPrepass, DepthNormalPrepass passes). Camera matrices and stereo rendering are already set up at this point. AfterRenderingPrePasses Executes a ScriptableRenderPass instance after rendering prepasses (DepthPrepass, DepthNormalPrepass passes). Camera matrices and stereo rendering are set up at this point. BeforeRenderingGbuffer Executes a ScriptableRenderPass instance before rendering the GBuffer pass. AfterRenderingGbuffer Executes a ScriptableRenderPass instance after rendering the GBuffer pass. BeforeRenderingDeferredLights Executes a ScriptableRenderPass instance before rendering the Deferred pass. AfterRenderingDeferredLights Executes a ScriptableRenderPass instance after rendering the Deferred pass. BeforeRenderingOpaques Executes a ScriptableRenderPass instance before rendering opaque objects (DrawOpaqueObjects pass). AfterRenderingOpaques Executes a ScriptableRenderPass instance after rendering opaque objects (DrawOpaqueObjects pass). BeforeRenderingSkybox Executes a ScriptableRenderPass instance before rendering the skybox (Camera.RenderSkybox pass). AfterRenderingSkybox Executes a ScriptableRenderPass instance after rendering the skybox (Camera.RenderSkybox pass). BeforeRenderingTransparents Executes a ScriptableRenderPass instance before rendering transparent objects (DrawTransparentObjects pass). AfterRenderingTransparents Executes a ScriptableRenderPass instance after rendering transparent objects (DrawTransparentObjects pass). BeforeRenderingPostProcessing Executes a ScriptableRenderPass instance before rendering post-processing effects (Render PostProcessing Effects pass). AfterRenderingPostProcessing Executes a ScriptableRenderPass instance after rendering post-processing effects but before the final blit, post-processing anti-aliasing effects, and color grading. AfterRendering Executes ScriptableRenderPass instance after rendering all other passes. The following diagram shows the passes and the flow of frame resources in a URP frame:"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/inject-render-pass-via-script.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customize/inject-render-pass-via-script.html",
    "title": "How to inject a custom render pass into the URP frame rendering via scripting | FSM Unity Framework",
    "keywords": "How to inject a custom render pass into the URP frame rendering via scripting This page describes how to inject a custom render pass into the URP frame rendering via scripting, without implementing a custom renderer feature. Subscribe a method to one of the events in the RenderPipelineManager class. In the subscribed method, use the EnqueuePass method of a ScriptableRenderer instance to inject a custom render pass into the URP frame rendering. Example code: public class EnqueuePass : MonoBehaviour { [SerializeField] private BlurSettings settings; private BlurRenderPass blurRenderPass; private void OnEnable() { ... blurRenderPass = new BlurRenderPass(settings); // Subscribe the OnBeginCamera method to the beginCameraRendering event. RenderPipelineManager.beginCameraRendering += OnBeginCamera; } private void OnDisable() { RenderPipelineManager.beginCameraRendering -= OnBeginCamera; blurRenderPass.Dispose(); ... } private void OnBeginCamera(ScriptableRenderContext context, Camera cam) { ... // Use the EnqueuePass method to inject a custom render pass cam.GetUniversalAdditionalCameraData() .scriptableRenderer.EnqueuePass(blurRenderPass); } }"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customizing-urp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/customizing-urp.html",
    "title": "Customizing URP | FSM Unity Framework",
    "keywords": "Customizing URP This section contains information on how to customize and extend the rendering process in URP. This section contains the following articles: Using the beginCameraRendering event"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/decal-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/decal-shader.html",
    "title": "Decal Shader Graph | FSM Unity Framework",
    "keywords": "Decal Shader Graph The Decal Projector component can project a Material as a decal if the Material uses a Shader Graph with the Decal Material type. Shader Graph with the Decal Material type URP contains the pre-built Decal Shader (Shader Graphs/Decal). Decal Material properties and advanced options. You can assign a Material that uses a Decal Shader Graph to a GameObject directly. For example, you can use a Quad as the Decal GameObject. The pre-built Decal Shader has the following properties: Base Map: the Base texture of the Material. Normal Map: the normal texture of the Material. Normal Blend: this property defines the proportion in which the the normal texture selected in the Normal Map property blends with the normal map of the Material that the decal is projected on. 0: the decal does not affect the Material it's projected on. 1: the normal map of the decal replaces the normal map of the Material it's projected on. You can create your own Shader Graphs that render decals in a way that suits your project best. The Decal Material properties above are defined in the pre-built Shader Graph. Custom decal Material properties depend on a custom Shader Graph. The following table describes the properties in the Advanced Options section. These properties are common for all decal shaders. Property Description Enable GPU Instancing Enabling this option lets URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Priority This property defines the order in which URP draws decals in the Scene. URP draws decals with lower Priority values first, and draws decals with higher Priority values on top of those with lower values. If there are multiple Decal Materials with the same Priority in the Scene, URP renders them in the order in which the Materials were created. Mesh Bias Type Select the Mesh bias type. The Mesh bias lets you prevent z-fighting between the Decal GameObject and the GameObject it overlaps. This property is only applicable for GameObjects with a Decal Material type assigned directly. View Bias A world-space bias (in meters). When drawing the Decal GameObject, Unity shifts each pixel of the GameObject by this value along the view vector. A positive value shifts pixels closer to the Camera, so that Unity draws the Decal GameObject on top of the overlapping Mesh, which prevents z-fighting. Decal Projectors ignore this property. Depth Bias When drawing the Decal GameObject, Unity changes the depth value of each pixel of the GameObject by this value. A negative value shifts pixels closer to the Camera, so that Unity draws the Decal GameObject on top of the overlapping Mesh, which prevents z-fighting. Decal Projectors ignore this property. Create custom Decal shaders The pre-built Shader Graphs/Decal shader serves as a simple example. You can create your own decal shaders that render decals in a way that suits your project best. To create a custom decal Shader Graph, select the Decal value in Material property of the shader target. Enabling one of the following properties override the equivalent Lit Shader property on the surface of the Material. Property Description Affect BaseColor When enabled, the shader affects the base color. Most decals make use of this option. An exception is a surface damage effect, were you might want to manipulate other properties, such as normals. From left to right: shader affecting only the color, affecting all properties, affecting all properties but color. Affect Normal When enabled, the shader affects normals. Use cases: adding damage effects to materials, for example, bullet holes or cracks. Use the Blend property to blend the normals of the decal with the normals of the surface it's projected to. If the Blend property is disabled, the decal overrides the normals all over the surface it's projected to. From left to right: Affect Normal is off; Affect Normal is on, Blend is off; Affect Normal and Blend are on. Affect MAOS MOAS stands for Metallic, Ambient Occlusion, and Smoothness. These properties are grouped together to save memory. You can change values of each property separately in the shader, but all properties are blended with a single common alpha value. Use cases: Override smoothness to highlight puddles or wet paint. Override metallic values with lower values to render rust. Override AO to give the decal more depth. Left: the decal does not affect MAOS. Right: the decal affects MAOS. Affect Emission Use cases: you can affect the emission values to make surfaces seem like they are emitting light, or to make surfaces seem like they are being lit by light. Left: Affect Emission is off. Right: Affect Emission is on. To improve performance, pack data for different surface properties into a single texture. This way the shader performs fewer samples and Unity stores fewer textures. For example, the following Shader Graph uses a normal map and a mask map to drive all properties in the shader. This decal is used for the damaged tarmac effect, and a hardcoded roughness value of 0 suites the use case. The shader samples the mask and uses the color for setting the Ambient Occlusion values (Red channel), smoothness values (Green channel), Emission intensity values (Blue channel), and alpha values for the entire decal. Decals are often blended using single alpha values for all properties. The following image shows the mask map for the example tarmac cracks: Example of mask map that packs Ambient Occlusion, Smoothness, Emission, and alpha values of a decal atlas into a single texture."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/faq.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/faq.html",
    "title": "Frequently asked questions (FAQ) | FSM Unity Framework",
    "keywords": "Frequently asked questions (FAQ) This section answers some frequently asked questions about the Universal Render Pipeline (URP). These questions come from the General Graphics section on our forums, from the Unity Discord channel, and from our support teams. For information about the High Definition Render Pipeline (HDRP), see the HDRP documentation. Can I use URP and HDRP at the same time? No. They're both built with the Scriptable Render Pipeline (SRP), but their render paths and light models are different. Can I convert from one pipeline to the other? You can convert from the Built-in Render Pipeline to URP. To do so, you'll have to re-write your Assets and redo the lighting in your game or app. See this upgrade guide on installing URP into an existing Project. You can use our upgrader to upgrade Built-in Shaders to the URP Shaders. For custom Shaders, you'll have to upgrade them manually. You should not swap pipeline Assets from one pipeline to another at run time, and there's no upgrader between URP and HDRP. How do I update the Universal Render Pipeline package? You should update via the Package Manager. In the Unity Editor, go to Unity > Window > Package Manager, and find the Universal RP package. If you’ve added SRP code or Shader Graph manually via Github, make sure to upgrade them to the same package version as URP in your manifest file. Where has Dynamic Batching gone? The Dynamic Batching checkbox has moved from the Player Settings to the URP Asset. How do I enable Double Sided Global Illumination in the Editor? In the Material Inspector, find Render Face, and select Both. This means that both sides of your geometry contribute to global illumination, because URP doesn’t cull either side. Is this render pipeline usable for desktop apps and games? Yes. The graphics quality and performance is scalable across platforms, so you can create apps for PCs and consoles as well as mobile devices. A certain feature from the Built-in Render Pipeline is not supported in URP. Will URP support it? To see which features URP currently supports, check the comparison table. URP will not support features marked as Not Supported. Does URP have a public roadmap? Yes. You can check it here. You can add suggestions as well. To do so, you’ll have to enter your email address, but you won’t have to make an account. I’ve found a bug. How do I report it? You can open bugs by using the bug reporter system. URP bugs go through the same process as all other Unity bugs. You can also check the active list of bugs for URP in the issue tracker. I’ve upgraded my Project from the Built-in render pipeline to URP, but it’s not running faster. Why? URP and the Built-in Render Pipeline have different quality settings. While the Built-in Render Pipeline configures many settings in different places like the Quality Settings, Graphics Settings, and Player Settings, all URP settings are stored in the URP Asset. The first thing to do is to check whether your URP Asset settings match the settings your Built-in render pipeline Project. For example, if you disabled MSAA or HDR in your Built-in render pipeline Project, make sure they are disabled in the URP Asset in your URP Project. For advice on configuring URP Assets, see documentation on the URP Asset. If, after comparing the settings, you still experience worse performance with URP, please open a bug report and attach your Project. URP doesn’t run on device X or platform Y. Is this expected? No. Please open a bug report. My Project takes a long time to build. Is this expected? We are looking into how to strip Shader keywords more aggressively. You can help the Shader stripper by disabling features you don’t require for your game in the URP Asset. For more information on settings that affect shader variants and build time, see the shader stripping documentation. How do I set Camera clear flags in URP? You can set the Background Type in the Camera Inspector to control how a Camera's color buffer is initialized. What rendering space does URP work in? By default, URP uses a linear color space while rendering. You can also use a gamma color space, which is non-linear. To do so, toggle it in the Player Settings. How do I extend URP with scriptable render pass? To create a scriptable render pass, you have to create a ScriptableRendererFeature script. This is because the scriptable render feature is a container that can have the pass in it. To create the scriptable render feature in the Editor, click on Asset > Create > Rendering > Universal Render Pipeline > Renderer Feature."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rendering-debugger.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rendering-debugger.html",
    "title": "Rendering Debugger | FSM Unity Framework",
    "keywords": "Rendering Debugger The Rendering Debugger window lets you visualize various lighting, rendering, and Material properties. The visualizations help you identify rendering issues and optimize Scenes and rendering configurations. This section contains the following topics: How to access the Rendering Debugger window. Information on how to access the Rendering Debugger window in the Editor, in the Play mode, and at runtime in Development builds. Rendering Debugger window sections Descriptions of the elements and properties in the Rendering Debugger window. Navigation at runtime How to navigate the Rendering Debugger interface at runtime. How to access the Rendering Debugger window The Rendering Debugger window is available in the following modes: The Editor. The Play mode. At runtime in the standalone Unity Player, on any device. The window is only available in Development builds. When using the Rendering Debugger window in the Development build, clear the Strip Debug Variants check box in Project Settings > Graphics > URP Global Settings. Use one of the following options to open the Rendering Debugger window. In the Editor: Select Window > Analysis > Rendering Debugger. Press Ctrl+Backspace (Ctrl+Delete on macOS). In the Play mode or at runtime in a Development build: On a desktop or laptop computer, press LeftCtrl+Backspace (LeftCtrl+Delete on macOS). On a console controller, press L3 and R3 (Left Stick and Right Stick). On a mobile device, use a three-finger double tap. You can disable the runtime UI using the enableRuntimeUI property. Rendering Debugger window sections The Rendering Debugger window contains the following sections: Display Stats Frequently Used Material Lighting Rendering The following illustration shows the Rendering Debugger window in the Scene view. Display Stats The Display Stats panel shows statistics relevant to debugging performance issues in your project. You can only view this section of the Rendering Debugger in Play mode. Use the [runtime shortcuts](#Navigation at runtime) to open the Display stats window in the scene view at runtime. Frame Stats The Frame Stats section displays the average, minimum, and maximum value of each property. HDRP calculates each Frame Stat value over the 30 most recent frames. Property Description Frame Rate The frame rate (in frames per second) for the current camera view. Frame Time The total frame time for the current camera view. CPU Main Thread Frame The total time (in milliseconds) between the start of the frame and the time when the Main Thread finished the job. CPU Render Thread Frame The time (in milliseconds) between the start of the work on the Render Thread and the time Unity waits to render the present frame (Gfx.PresentFrame). CPU Present Wait The time (in milliseconds) that the CPU spent waiting for Unity to render the present frame (Gfx.PresentFrame) during the last frame. GPU Frame The amount of time (in milliseconds) the GPU takes to render a given frame. Debug XR Layout Display debug information for XR passes. This mode is only available in editor and development builds. Bottlenecks A bottleneck is a condition that occurs when one process performs significantly slower than other components, and other components depend on it. The Bottlenecks section describes the distribution of the last 60 frames across the CPU and GPU. You can only see the Bottleneck information when you build your player on a device. Note: Vsync limits the Frame Rate based on the refresh rate of your device’s screen. This means when you enable Vsync, the Present Limited category is 100% in most cases. To turn Vsync off, go to Edit > Project settings > Quality > Current Active Quality Level and set the Vsync Count set to Don't Sync. Bottleneck categories Category Description CPU The percentage of the last 60 frames in which the CPU limited the frame time. GPU The percentage of the last 60 frames in which the GPU limited the frame time. Present limited The percentage of the last 60 frames in which the frame time was limited by the following presentation constraints: • Vertical Sync (Vsync): Vsync synchronizes rendering to the refresh rate of your display. •Target framerate: A function that you can use to manually limit the frame rate of an application. If a frame is ready before the time you specify in targetFrameRate, Unity waits before presenting the frame. Balanced This percentage of the last 60 frames in which the frame time was not limited by any of the above categories. A frame that is 100% balanced indicates the processing time for both CPU and GPU is approximately equal. Bottleneck example If Vsync limited 20 of the 60 most recent frames, the Bottleneck section might appear as follows: CPU 0.0%: This indicates that HDRP did not render any of the last 60 frames on the CPU. GPU 66.6%: This indicates that the GPU limited 66.6% of the 60 most recent frames rendered by HDRP. Present Limited 33.3%: This indicates that presentation constraints (Vsync or the target framerate) limited 33.3% of the last 60 frames. Balanced 0.0%: This indicates that in the last 60 frames, there were 0 frames where the CPU processing time and GPU processing time were the same. In this example, the bottleneck is the GPU. Detailed Stats The Detailed Stats section displays the amount of time in milliseconds that each rendering step takes on the CPU and GPU. HDRP updates these values once every frame based on the previous frame. Property Description Update every second with average Calculate average values over one second and update every second. Hide empty scopes Hide profiling scopes that use 0.00ms of processing time on the CPU and GPU. Debug XR Layout Enable to display debug information for XR passes. This mode only appears in the editor and development builds. Frequently Used This section contains a selection of properties that users use often. The properties are from the other sections in the Rendering Debugger window. For information about the properties, see the sections Material, Lighting, and Rendering. Material The properties in this section let you visualize different Material properties. Rendering Debugger, Material section Material Filters Property Description Material Override Select a Material property to visualize on every GameObject on screen. The available options are: Albedo Specular Alpha Smoothness AmbientOcclusion Emission NormalWorldSpace NormalTangentSpace LightingComplexity Metallic SpriteMask With the LightingComplexity value selected, Unity shows how many Lights affect areas of the screen space. Vertex Attribute Select a vertex attribute of GameObjects to visualize on screen. The available options are: Texcoord0 Texcoord1 Texcoord2 Texcoord3 Color Tangent Normal Material Validation Property Description Material Validation Mode Select which Material properties to visualize: Albedo, or Metallic. Selecting one of the properties shows the new context menu. Validation Mode: Albedo Selecting Albedo in the Material Validation Mode property shows the Albedo Settings section with the following properties: Validation Preset: Select a pre-configured material, or Default Luminance to visualize luminance ranges. Min Luminance: Unity draws pixels where the luminance is lower than this value with red color. Max Luminance: Unity draws pixels where the luminance is higher than this value with blue color. Hue Tolerance: available only when you select a pre-set material. Unity adds the hue tolerance to the minimum and maximum luminance values. Saturation Tolerance: available only when you select a pre-set material. Unity adds the saturation tolerance to the minimum and maximum luminance values. Validation Mode: Metallic Selecting Metallic in the Material Validation Mode property shows the Metallic Settings section with the following properties: Min Value: Unity draws pixels where the metallic value is lower than this value with red color. Max Value: Unity draws pixels where the metallic value is higher than this value with blue color. Lighting The properties in this section let you visualize different settings and elements related to the lighting system, such as shadow cascades, reflections, contributions of the Main and the Additional Lights, and so on. Lighting Debug Modes The Lighting Debug Modes subsection. Property Description Lighting Debug Mode Specifies which lighting and shadow information to overlay on-screen to debug. The options are: None: Renders the scene normally without a debug overlay. Shadow Cascades: Overlays shadow cascade information so you can see which shadow cascade each pixel uses. Use this to debug shadow cascade distances. For information on which color represents which shadow cascade, see the Shadows section of the URP Asset. Lighting Without Normal Maps: Renders the scene to visualize lighting. This mode uses neutral materials and disables normal maps. This and the Lighting With Normal Maps mode are useful for debugging lighting issues caused by normal maps. Lighting With Normal Maps: Renders the scene to visualize lighting. This mode uses neutral materials and allows normal maps. Reflections: Renders the scene to visualize reflections. This mode applies perfectly smooth, reflective materials to every Mesh Renderer. Reflections With Smoothness: Renders the scene to visualize reflections. This mode applies reflective materials without an overridden smoothness to every GameObject. Lighting Features Specifies flags for which lighting features contribute to the final lighting result. Use this to view and debug specific lighting features in your scene. The options are: Nothing: Shortcut to disable all flags. Everything: Shortcut to enable all flags. Global Illumination: Indicates whether to render global illumination. Main Light: Indicates whether the main directional Light contributes to lighting. Additional Lights: Indicates whether lights other than the main directional light contribute to lighting. Vertex Lighting: Indicates whether additional lights that use per-vertex lighting contribute to lighting. Emission: Indicates whether emissive materials contribute to lighting. Ambient Occlusion: Indicates whether ambient occlusion contributes to lighting. Rendering The properties in this section let you visualize different rendering features. Rendering Debug The Rendering Debug subsection. Property Description Map Overlays Specifies which render pipeline texture to overlay on the screen. The options are: None: Renders the scene normally without a texture overlay. Depth: Overlays the camera's depth texture on the screen. Additional Lights Shadow Map: Overlays the shadow map that contains shadows cast by lights other than the main directional light. Main Light Shadow Map: Overlays the shadow map that contains shadows cast by the main directional light. ** Map Size** The width and height of the overlay texture as a percentage of the view window URP displays it in. For example, a value of 50 fills up a quarter of the screen (50% of the width and 50% of the height). HDR Indicates whether to use high dynamic range (HDR) to render the scene. Enabling this property only has an effect if you enable HDR in your URP Asset. MSAA Indicates whether to use Multisample Anti-aliasing (MSAA) to render the scene. Enabling this property only has an effect if: You set Anti Aliasing (MSAA) to a value other than Disabled in your URP Asset. You use the Game View. MSAA has no effect in the Scene View. Post-processing Specifies how URP applies post-processing. The options are: Disabled: Disables post-processing. Auto: Unity enables or disables post-processing depending on the currently active debug modes. If color changes from post-processing would change the meaning of a debug mode's pixel, Unity disables post-processing. If no debug modes are active, or if color changes from post-processing don't change the meaning of the active debug modes' pixels, Unity enables post-processing. Enabled: Applies post-processing to the image that the camera captures. Additional Wireframe Modes Specifies whether and how to render wireframes for meshes in your scene. The options are: None: Doesn't render wireframes. Wireframe: Exclusively renders edges for meshes in your scene. In this mode, you can see the wireframe for meshes through the wireframe for closer meshes. Solid Wireframe: Exclusively renders edges and faces for meshes in your scene. In this mode, the faces of each wireframe mesh hide edges behind them. Shaded Wireframe: Renders edges for meshes as an overlay. In this mode, Unity renders the scene in color and overlays the wireframe over the top. Overdraw Indicates whether to render the overdraw debug view. This is useful to see where Unity draws pixels over one other. Pixel Validation The Pixel Validation subsection. Property Description Pixel Validation Mode Specifies which mode Unity uses to validate pixel color values. The options are: None: Renders the scene normally and doesn't validate any pixels. Highlight NaN, Inf and Negative Values: Highlights pixels that have color values that are NaN, Inf, or negative. Highlight Values Outside Range: Highlights pixels that have color values outside a particular range. Use Value Range Min and Value Range Max. ** Channels** Specifies which value to use for the pixel value range validation. The options are: RGB: Validates the pixel using the luminance value calculated from the red, green, and blue color channels. R: Validates the pixel using the value from the red color channel. G: Validates the pixel using the value from the green color channel. B: Validates the pixel using the value from the blue color channel. A: Validates the pixel using the value from the alpha channel. This property only appears if you set Pixel Validation Mode to Highlight Values Outside Range. ** Value Range Min** The minimum valid color value. Unity highlights color values that are less than this value. This property only appears if you set Pixel Validation Mode to Highlight Values Outside Range. ** Value Range Max** The maximum valid color value. Unity highlights color values that are greater than this value. This property only appears if you set Pixel Validation Mode to Highlight Values Outside Range. Navigation at runtime This section describes how to navigate the Rendering Debugger interface at runtime. To change the current active item: Keyboard: use the arrow keys. Touch screen: tap the arrows next to properties. Xbox controller: use the Directional pad (D-Pad). PlayStation controller: use the Directional buttons. To change the current tab: Keyboard: use the Page up and Page down keys (Fn + Up and Fn + Down keys for MacOS). Touch screen: tap the arrows next to tab title. Xbox controller: use the Left Bumper and Right Bumper. PlayStation controller: use the L1 button and R1 button. To display the current active item independently of the debug window: Keyboard: press the right Shift key. Xbox controller: press the X button. PlayStation controller: press the Square button."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rendering-layers.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rendering-layers.html",
    "title": "Rendering Layers | FSM Unity Framework",
    "keywords": "Rendering Layers The Rendering Layers feature lets you configure certain Lights to affect only specific GameObjects. For example, in the following illustration, Light A affects Sphere D, but not Sphere C. Light B affects Sphere C, but not Sphere D. To read how to implement this example, see section How to use Rendering Layers. Enable Rendering Layers for Lights To enable Rendering Layers for Lights in your project: In the URP Asset, in the Lighting section, click the vertical ellipsis icon (⋮) and select Show Additional Properties In the URP Asset, in the Lighting section, select Use Rendering Layers. URP Asset > Lighting > Use Rendering Layers Enable Rendering Layers for Decals To enable Rendering Layers for decals in your project: In the Decal Renderer Feature, enable Use Rendering Layers. Decal Renderer Feature, Inspector view. When you enable Rendering Layers for Decals, Unity shows the Rendering Layers property on each Decal Projector: How to edit Rendering Layer names To edit the names of Rendering Layers: Go to Project Settings > Graphics > URP Global Settings. Edit the Rendering Layer names in the Rendering Layers (3D) section. Graphics > URP Global Settings > Rendering Layers (3D) How to use Rendering Layers with Lights This section describes how to configure the following application example: The Scene contains two Point Lights (marked A and B in the illustration) and two Sphere GameObjects (C and D in the illustration). Light A affects Sphere D, but not Sphere C. Light B affects Sphere C, but not Sphere D. The following illustration shows the example: Light A affects Sphere D, but not Sphere C. Light B affects Sphere C, but not Sphere D. To implement the example: Enable Rendering Layers in your project. Create two Point Lights (call them A, and B) and two Spheres (call them C, and D). Position the objects so that both Spheres are within the emission range of Lights. Go to Project Settings > Graphics > URP Global Settings. Rename Rendering Layer 1 to Red, and Layer 2 to Green. Select Light A, change its color to green. Select Light B, change its color to red. With this setup, both Lights affect both Spheres. Make the following settings on Lights and Spheres: Light A: in the property Light > General > Rendering Layers, clear all options, and select Green. Light B: in the property Light > General > Rendering Layers, clear all options, and select Red. Sphere C: in the property Mesh Renderer > Additional Settings > Rendering Layer Mask, select all options, clear Green. Sphere D: in the property Mesh Renderer > Additional Settings > Rendering Layer Mask, select all options, clear Red. Now Point Light A affects Sphere D, but not Sphere C. Point Light B affects Sphere C, but not Sphere D. How to use Custom Shadow Layers In the illustration above, Light A does not affect Sphere C, and the Sphere does not cast shadow from Light A. The Custom Shadow Layers property lets you configure the Scene so that Sphere C casts the shadow from Light A. Select Light A. In Light > Shadows, select the Custom Shadow Layers property. Unity shows the Layer property. In the Layer property, select the Rendering Layer that Sphere C belongs to. Now Light A does not affect Sphere C, but Sphere C casts shadow from Light A. The following illustrations show the Scene with the Custom Shadow Layers property off and on. How to use Rendering Layers with Decals This section describes how to configure the following application example: The Scene contains a Decal Projector. The Decal Projector projects a decal on the wall and the ground, but not on the paint bucket. The following illustration shows the example: In image 1, the paint bucket has the Receive decals layer selected. In image 2 it does not, so the Decal Projector does not project on the bucket. To implement the example: Enable Rendering Layers for Decals in your project. Create a Decal Projector in the Scene. Go to Project Settings > Graphics > URP Global Settings. Add a Rendering Layer called Receive decals. Select the Decal Projector. In the Rendering Layers property, select Receive decals. Select the paint bucket GameObject. In the Rendering Layer Mask field, clear the Receive decals layer. Now the Decal Projector does not affect this GameObject. Performance This section contains information related to the impact of Rendering Layers on performance. Keep the Rendering Layer count as small as possible. Avoid creating Rendering Layers that you don't use in the project. When using Rendering Layers for decals, increasing the layer count increases the required memory bandwidth and decreases the performance. When using Rendering Layers only for Lights in the Forward Rendering Path, the performance impact is insignificant. Performance impact grows more significantly when the Rendering Layer count exceeds a multiple of 8. For example: increasing the layer count from 8 to 9 layers has a bigger relative impact than increasing the layer count from 9 to 10 layers."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rp-converter.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/features/rp-converter.html",
    "title": "Render Pipeline Converter | FSM Unity Framework",
    "keywords": "Render Pipeline Converter The Render Pipeline Converter converts assets made for a Built-in Render Pipeline project to assets compatible with URP. NOTE: The conversion process makes irreversible changes to the project. Back up your project before the conversion. How to use the Render Pipeline Converter To convert project assets: Select Window > Rendering > Render Pipeline Converter. Unity opens the Render Pipeline Converter window. Select the conversion type. Depending on the conversion type, the dialog shows the available converters. Select or clear the check boxes next to converter names to enable or disable the converters. For the list of available converters, see the section Converters. Click Initialize Converters. The Render Pipeline Converter preprocesses the assets in the project and shows the list of elements to convert. Select or clear check boxes next to assets to include or exclude them from the conversion process. The following illustration shows initialized converters. Click a converter to see the list of items that a converter is about to convert. Yellow icon: a yellow icon next to an element indicates that a user action might be required to run the conversion. Hover the mouse pointer over the icon to see the description of the issue. Click Convert Assets to start the conversion process. NOTE: The conversion process makes irreversible changes to the project. Back up your project before the conversion. When the conversion process finishes, the window shows the status of each converter. Green check mark: the conversion went without issues. Yellow icon: the conversion finished with warnings and might require user action. Red icon: the conversion failed. Click a converter to see the list of processed items in that converter. After reviewing the converted project, close the Render Pipeline Converter window. Conversion types and converters The Render Pipeline Converter let's you select one of the following conversion types: Built-in Render Pipeline to URP Built-in Render Pipeline 2D to URP 2D Upgrade 2D URP Assets When you select on of the conversion types, the tool shows you the available converters. The following sections describe the converters available for each conversion type. Built-in Render Pipeline to URP This conversion type converts project elements from the Built-in Render Pipeline to URP. Available converters: Rendering Settings This converter creates the URP Asset and Renderer assets. Then the converter evaluates the settings in the Built-in Render Pipeline project and converts them into equivalent properties in the URP assets. Material Upgrade This converter converts the Materials. The converter works on pre-built Materials that are supplied by Unity, it does not support Materials with custom shaders. Animation Clip Converter This converter converts the animation clips. It runs after the Material Upgrade converter finishes. NOTE: This converter is available only if the project contains animations that affect the properties of Materials, or Post-processing Stack v2 properties. Read-only Material Converter This converter converts the pre-built read-only Materials, where the Material Upgrade converter cannot replace the shader. This converter indexes the project and creates a temporary .index file, which might take a significant time. Examples of read-only Materials: Default-Diffuse, Default-Line, Dafault-Terrain-Diffuse, etc. Post-Processing Stack v2 Converter This converter converts PPv2 Volumes, Profiles, and Layers to URP Volumes, Profiles, and Cameras. This converter indexes the project and creates a temporary .index file, which might take a significant time. Built-in Render Pipeline 2D to URP 2D This conversion type converts elements of a project from Built-in Render Pipeline 2D to URP 2D. Available converters: Material and Material Reference Upgrade This converter converts all Materials and Material references from Built-in Render Pipeline 2D to URP 2D. Upgrade 2D URP Assets This conversion type upgrades assets of a 2D project from an earlier URP version to the current URP version. Available converters: Parametric to Freeform Light Upgrade This converter converts all parametric lights to freeform lights. Run conversion using API or CLI The Render Pipeline Converter implements the Converters class with RunInBatchMode methods that let you run the conversion process from a command line. For example, the following script initializes and executes the converters Material Upgrade, and Read-only Material Converter. using System.Collections; using System.Collections.Generic; using UnityEditor; using UnityEditor.Rendering.Universal; using UnityEngine; public class MyUpgradeScript : MonoBehaviour { public static void ConvertBuiltinToURPMaterials() { Converters.RunInBatchMode( ConverterContainerId.BuiltInToURP , new List<ConverterId> { ConverterId.Material, ConverterId.ReadonlyMaterial } , ConverterFilter.Inclusive ); EditorApplication.Exit(0); } } To run the example conversion from the command line, use the following command: \"<path to Unity application> -projectPath <project path> -batchmode -executeMethod MyUpgradeScript.ConvertBuiltinToURPMaterials See also: Unity Editor command line arguments."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/how-to.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/how-to.html",
    "title": "Practical how-to guides | FSM Unity Framework",
    "keywords": "Practical how-to guides This section contains practical how-to guides. The section contains the following topics: How to perform a full screen blit in URP. How to create a custom rendering effect using the Render Objects Renderer Feature How to create a custom post-processing effect"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/index.html",
    "title": "Universal Render Pipeline overview | FSM Unity Framework",
    "keywords": "Universal Render Pipeline overview The Universal Render Pipeline (URP) is a prebuilt Scriptable Render Pipeline, made by Unity. URP provides artist-friendly workflows that let you quickly and easily create optimized graphics across a range of platforms, from mobile to high-end consoles and PCs. Requirements For information about requirements and compatibility, see section Requirements. What's new in URP For information on what's new in the latest version of URP, see section What's new in URP. Getting started with URP For information on starting a new URP Project from scratch, or about installing URP in an existing Unity Project, see Getting started. Upgrading For information on upgrading from a previous version of URP to the current version, or for information about upgrading from the Lightweight Render Pipeline (LWRP) to URP, see Upgrade guides."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/inside-universalrp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/inside-universalrp.html",
    "title": "Insid the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Insid the Universal Render Pipeline In the following sections, you can read more about the technology inside the Universal Render Pipeline (URP): The URP Asset Shader stripping Built-in Render Pipeline/URP comparison Shading Models in URP"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/integration-with-post-processing.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/integration-with-post-processing.html",
    "title": "Post-processing in the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Post-processing in the Universal Render Pipeline The Universal Render Pipeline (URP) includes an integrated implementation of post-processing effects. If you use URP, it's not necessary to install an extra package for post-processing effects. URP is not compatible with the Post Processing Stack v2 package. URP uses the Volume framework for post-processing effects. The images below show a Scene with and without URP post-processing. Without post-processing: With post-processing: Note: URP does not support Post-processing on OpenGL ES 2.0. How to configure post-processing effects in URP This section describes how to configure Post-processing in URP. Using post-processing in the URP Template Scene Post-processing is preconfigured in the SampleScene Scene in URP Template. To see the preconfigured effects, select Post-process Volume in the Scene. To add extra effects, add Volume Overrides to the Volume. To configure location-based post-processing effects, see How to use Local Volumes. Configuring post-processing in a new URP Scene To configure post-processing in a new Scene: Select a Camera, and select the Post Processing check box. Add a GameObject with a Volume component in the Scene. This instruction adds a Global Volume. Select GameObject > Volume > Global Volume. Select the Global Volume GameObject. In the Volume component, create a new Profile by clicking New button on the right side of the Profile property. Add post-processing effects to the Camera by adding Volume Overrides to the Volume component. Now you can adjust post-processing effect settings in Overrides in the Volume component. To configure location-based post-processing effects, see How to use Local Volumes. Post-processing in URP for mobile devices Post-processing effects can take up a lot of frame time. If you’re using URP for mobile devices, these effects are the most “mobile-friendly” by default: Bloom (with High Quality Filtering disabled) Chromatic Aberration Color Grading Lens Distortion Vignette Note: For depth-of field, Unity recommends that you use Gaussian Depth of Field for lower-end devices. For console and desktop platforms, use Bokeh Depth of Field. Note: For anti-aliasing on mobile platforms, Unity recommends that you use FXAA. Post-processing in URP for VR In VR apps and games, certain post-processing effects can cause nausea and disorientation. To reduce motion sickness in fast-paced or high-speed apps, use the Vignette effect for VR, and avoid the effects Lens Distortion, Chromatic Aberration, and Motion Blur for VR."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/known-issues.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/known-issues.html",
    "title": "Known issues | FSM Unity Framework",
    "keywords": "Known issues This page contains information on known issues you may encounter when using URP. When importing the URP package samples, Unity does not set the necessary URP asset in Quality > Render Pipeline Asset When importing the URP package samples, Unity does not set the necessary URP asset in Quality > Render Pipeline Asset, and certain sample rendering effects do not work. To fix this issue: In Project Settings > Quality > Render Pipeline Asset, select SamplesPipelineAsset. Renaming a URP Renderer asset to a name matching one of the Renderer Feature names causes erroneous behavior If a URP Renderer asset has any Renderer Features assigned, renaming the Renderer asset to a name matching one of the Renderer Feature names causes erroneous behavior: the URP Renderer and the Renderer Feature switch places. The following scenario shows how the error occurs: Let's assume that the URP Renderer in your project is called UniversalRenderer. The Renderer has a Renderer Feature called NewRenderObjects assigned. Renaming UniversalRenderer to NewRenderObjects causes erroneous behavior: The Renderer switches places with the Renderer Feature and does not behave correctly. To avoid the issue, do not give the URP Renderer asset the same name as the Renderer Feature asset. To see updates on this issue, refer to the Unity Issue Tracker. Warning about _AdditionalLights property when upgrading the URP package In certain cases, you might see the following warning when upgrading the URP package to a newer version: Property (_AdditionalLights<...>) exceeds previous array size (256 vs 16). Cap to previous size. This warning does not cause issues with the project, the warning disappears if you restart the Editor."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/light-component.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/light-component.html",
    "title": "Light component reference | FSM Unity Framework",
    "keywords": "Light component reference Lights determine the shading of an object and the shadows it casts. This page contains information on Light components in the Universal Render Pipeline (URP). For a general introduction to lighting in Unity and examples of common lighting workflows, see the Lighting section of the Unity Manual. Properties The Light Inspector includes the following groups of properties: General Shape Emission Rendering Shadows General Property: Function: Type The current type of light. Possible values are Directional, Point, Spot and Area. Mode Specify the Light Mode used to determine if and how a light is \"baked\". Possible modes are Realtime, Mixed and Baked. Shape Property: Function: Spot Angle Define the angle (in degrees) at the base of a spot light’s cone (Spot light only). Emission Property: Function: Color Use the color picker to set the color of the emitted light. Intensity Set the brightness of the light. The default value for a Directional light is 0.5. The default value for a Point, Spot or Area light is 1. Indirect Multiplier Use this value to vary the intensity of indirect light. Indirect light is light that has bounced from one object to another. The Indirect Multiplier defines the brightness of bounced light calculated by the global illumination (GI) system. If you set Indirect Multiplier to a value lower than 1, the bounced light becomes dimmer with every bounce. A value higher than 1 makes light brighter with each bounce. This is useful, for example, when a dark surface in shadow (such as the interior of a cave) needs to be brighter in order to make detail visible. Range Define how far the light emitted from the center of the object travels (Point and Spot lights only). Cookie The RGB texture this Light projects into the scene. Use cookies to create silhouettes or patterned illumination. The texture format to use depends on the type of Light: • Directional: 2D texture • Spot: 2D texture • Point: cubemap texture If you enable cookies, URP uses more memory. Note: URP doesn't support light cookies for Area lights. For more information about light cookies, see Cookies. Cookie Size The per-axis scale Unity applies to the cookie texture. Use this property to set the size of the cookie. This property is available only if you set Type to Directional and assign a texture to Cookie. Cookie Offset The per-axis offset Unity applies to the cookie texture. Use this property to move the cookie without moving the light itself. You can also animate this property to scroll the cookie. This property is available only if you set Type to Directional and assign a texture to Cookie. Rendering Property: Function: Render Mode Use this drop-down to set the rendering priority of the selected Light. This can affect lighting fidelity and performance (see Performance Considerations, below). Auto The rendering method is determined at run time, depending on the brightness of nearby lights and the current Quality settings. Important The light is always rendered at per-pixel quality. Use Important mode only for the most noticeable visual effects (for example, the headlights of a player’s car). Not Important The light is always rendered in a faster, vertex/object light mode. Culling Mask Use this to selectively exclude groups of objects from being affected by the Light. For more information, see Layers. Shadows Property: Function: Shadow Type Determine whether this Light casts Hard Shadows, Soft Shadows, or no shadows at all. See the page Lights for information on hard and soft shadows. Baked Shadow Angle If Type is set to Directional and Shadow Type is set to Soft Shadows, this property adds some artificial softening to the edges of shadows and gives them a more natural look. Baked Shadow Radius If Type is set to Point or Spot and Shadow Type is set to Soft Shadows, this property adds some artificial softening to the edges of shadows and gives them a more natural look. Realtime Shadows These properties are available when Shadow Type is set to Hard Shadows or Soft Shadows. Use these properties to control real-time shadow rendering settings. Strength Use the slider to control how dark the shadows cast by this Light are, represented by a value between 0 and 1. This is set to 1 by default. Bias Controls whether to use shadow bias settings from the URP Asset, or whether to define custom shadow bias settings for this Light. Possible values are Use Pipeline Settings or Custom. Depth Controls the distance at which the shadows will be pushed away from the light. Useful for avoiding false self-shadowing artifacts. This property is visible only when Bias is set to Custom. Normal Controls the distance at which the shadow casting surfaces will be shrunk along the surface normal. Useful for avoiding false self-shadowing artifacts. This property is visible only when Bias is set to Custom. Near Plane Use the slider to control the value for the near clip plane when rendering shadows, defined as a value between 0.1 and 10. This value is clamped to 0.1 units or 1% of the light’s Range property, whichever is lower. This is set to 0.2 by default. Soft Shadows Quality Select the soft shadows quality. With the Use Pipeline Settings option selected Unity uses the value from the URP Asset. Options Low, Medium, and High let you specify the soft shadow quality value for this Light. For more information on the values, see the Soft Shadows section. Preset When using Preset of Light Component, only a subset of properties are supported. Unsupported properties are hidden."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lighting.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lighting.html",
    "title": "Lighting in the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Lighting in the Universal Render Pipeline Using the Universal Render Pipeline (URP), you can achieve realistic lighting that is suitable for a range of art styles. All of Unity's render pipelines share common lighting functionality, but each render pipeline has some important differences. Areas where the Universal Render Pipeline (URP) differs from Unity's common lighting functionality are: The Light component inspector, which displays some URP-specific controls. The Universal Additional Light Data component, which allows Unity to store Light-related data that is specific to URP. Enlighten Realtime Global Illumination is supported in URP from version 12. For more information, see Realtime Global Illumination using Enlighten. For a full comparison of lighting features between Unity's Built-in Render Pipeline and URP, and an up to date list of lighting features that are currently under research, see this feature comparison chart. For a general introduction to lighting in Unity and examples of common lighting workflows, see the Lighting section of the Unity Manual. Configure lighting for better performance Refer to Configure for better performance for more information about how to adjust lighting settings for better performance."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lighting/reflection-probes.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lighting/reflection-probes.html",
    "title": "Reflection probes | FSM Unity Framework",
    "keywords": "Reflection probes This page describes URP-specific behavior of reflection probes. For general information on reflection probes, see the page Reflection Probes. For examples of how to use reflection probes, see the Lighting samples in URP Package Samples. Configuring reflection probe settings To configure settings related to reflection probes, in a URP Asset, select Lighting > Reflection Probes. Reflection probe settings. The Reflection Probes section contains the following properties: Property Description Probe Blending Select this property to enable reflection probe blending. On lower-end mobile platforms, disable this property to decrease processing time on the CPU. Box Projection Select this property to enable reflection probe box projection. On lower-end mobile platforms, disable this property to decrease processing time on the CPU. Reflection probe blending Reflection probe blending lets you avoid a situation where a reflection suddenly appears on an object when it enters the probe box volume. When reflection probe blending is enabled, Unity gradually fades probe cubemaps in and out as the reflective object passes from one volume to the other. URP supports reflection probe blending in all Rendering Paths. Reflection probe volume Each reflection probe has a box volume. A reflection probe only affects parts of a GameObject that are inside the box volume. When a pixel of an object is outside of any reflection probe volume, Unity uses the skybox reflection. In URP, Unity evaluates the contribution of each probe for each individual pixel, depending on the position of the pixel relative to the boundary of the probe volume. This behavior is different from the Built-in Render Pipeline, where Unity evaluates the contribution of a probe for the whole object. Blend Distance Each reflection probe has the Blend Distance property. This property is the distance from the face of a reflection box volume towards the center of the box volume. Unity uses the Blend Distance property to determine the contribution of a reflection probe. When a pixel of an object is on the face of a reflection probe volume, that pixel gets 0% of reflections from the probe. When a pixel is inside the reflection probe volume and its distance from each face exceeds the Blend Distance value, the pixel gets 100% of reflections. If the Blend Distance value is more than half of the distance between faces of the reflection probe volume, the reflection probe cannot provide 100% contribution to any pixel within the volume. Which probes affect a GameObject When a GameObject is within multiple reflection probe volumes, maximum two of the probes can affect the GameObject. Unity selects which probes affect the GameObject using the following criteria: The Importance property of a reflection probe. Unity selects two probes with higher Importance values and ignores the others. If the Importance values are the same, Unity selects probes which have the smallest box volumes. If the Importance values and the box volumes are the same, Unity determines which two reflection probe volumes contain larger surface areas of a GameObject, and picks the probes of those volumes. When two reflection probes affect a GameObject, for each pixel, Unity calculates the weight of each probe depending on the distance of this pixel from the faces of the probe box volumes and the values of the Blend Distance properties. If the pixel is relatively close to faces of both box volumes and the sum of weights of both probes is less than 1, Unity assigns the remaining weight to the skybox reflection. If the pixel is within both box volumes and farther than the Blend Distance values from faces of both volumes: If the Importance properties of the reflection probes are the same, Unity blends reflections from the probes with equal weights. If the Importance property of one of the probes is higher, Unity applies the reflections only from that probe. Box projection For the box projection to work: Select the Box Projection check box on the URP asset. Select the Box Projection property on the reflection probe."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/lit-shader.html",
    "title": "Lit Shader | FSM Unity Framework",
    "keywords": "Lit Shader The Lit Shader lets you render real-world surfaces like stone, wood, glass, plastic, and metals in photo-realistic quality. Your light levels and reflections look lifelike and react properly across various lighting conditions, for example bright sunlight, or a dark cave. This Shader uses the most computationally heavy shading model in the Universal Render Pipeline (URP). For examples of how to use the Lit Shader, see the Shaders samples in URP Package Samples. Using the Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how URP renders the Material on a screen. Property Description Workflow Mode Use this drop-down menu to choose a workflow that fits your Textures, either Metallic and Specular. When you have made your choice, the main Texture options in the rest of the Inspector now follow your chosen workflow. For information on metallic or specular workflows, see this Manual page for the Standard built-in Shader in Unity. Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Select how Unity calculates the color of each pixel of a transparent Material when it blends the Material with the background. In the context of Blending Modes, Source refers to the transparent Material where the Blending Mode is set and Destination refers to anything that Material overlaps with. Alpha Alpha blending mode. Alpha uses the Material's alpha value to change how transparent an object is. 0 is fully transparent. 255 is fully opaque, this is translated to a value of 1 when used with the blending equations. The Material is always rendered in the Transparent render pass regardless of it's alpha value. This mode lets you use the Preserve Specular Lighting property. Alpha equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB × (1 − SourceAlpha) Premultiply Premultiply blending mode. Premultiply first multiplies the RGB values of the transparent Material by its alpha value then applies a similar effect to the Material as Alpha. The equation for Premultiply also allows areas of the transparent Material with an alpha value of 0 to have an additive blend effect. This can help reduce artifacts that may appear at the edge of the overlap between opaque and transparent pixels. Premultiply equation: OutputRGBA = SourceRGB + DestinationRGB × (1 − SourceAlpha) Additive Additive blending mode. Additive adds the color values of the Materials together to create the blend effect. The alpha value determines the strength of the source Material's color before the blend is calculated. This mode lets you use the Preserve Specular Lighting property. Additive equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB Multiply Multiply blending mode. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. This mode uses the Material’s alpha value to adjust how much the colors blend. An alpha value of 1 results in unadjusted multiplication of the colors while lower values blend the colors towards white. Multiply equation: OutputRGBA = SourceRGB × DestinationRGB Preserve Specular Lighting Indicates whether Unity preserves specular highlights on a GameObject or not. This applies even when your surface is transparent which means that only the reflected light is visible. This property only appears when the Surface Type is set to Transparent and Blending Mode is set to either Alpha or Additive. Material with Preserve Specular Lighting disabled. Material with Preserve Specular Lighting enabled. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Receive Shadows Tick this box to enable your GameObject to have shadows cast upon it by other objects. If you untick this box, the GameObject will not have shadows on it. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Note: If you are used to the Standard Shader in the built-in Unity render pipeline, these options are similar to the Main Maps settings in the Material Editor. Property Description Base Map Adds color to the surface, also known as the diffuse map. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. Metallic / Specular Map Shows a map input for your chosen Workflow Mode in the Surface Options. For the Metallic Map workflow, the map gets the color from the Base Map assigned above. Use the slider to control how metallic the surface appears. 1 is fully metallic, like silver or copper, and 0 is fully dielectric, like plastic or wood. You can generally use values in between 0 and 1 for dirty or corroded metals. For the Specular Map setting, you can assign a texture to it by clicking the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. For both workflows, you can use the Smoothness slider to control the spread of highlights on the surface. 0 gives a wide, rough highlight. 1 gives a small, sharp highlight like glass. Values in between produce semi-glossy looks. For example, 0.5 produces a plastic-like glossiness. Use the Source drop-down menu to select where the shader samples a smoothness map from. Options are: Metallic Alpha (alpha channel from the metallic map), and Albedo Alpha (alpha channel from the base map). The default value is Metallic Alpha. If the selected source has the alpha channel, the shader samples the channel and multiplies each sample by the Smoothness value. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. The float value next to the setting is a multiplier for the effect of the Normal Map. Low values decrease the effect of the normal map. High values create stronger effects. Height Map URP implements the parallax mapping technique which uses the height map to achieve surface-level occlusion effect by shifting the areas of the visible surface texture. To add the map, click the object picker next to it. The float value next to the setting is a multiplier for the effect of the Height Map. Low values decrease the effect of the height map. High values create stronger effects. Occlusion Map Select an occlusion map. This simulates shadows from ambient light and reflection, which makes lighting look more realistic as less light reaches corners and crevices of objects. To select the occlusion map, click the object picker next to it. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Tiling A 2D multiplier value that scales the Texture to fit across a mesh according to the U and V axes. This is good for surfaces like floors and walls. The default value is 1, which means no scaling. Set a higher value to make the Texture repeat across your mesh. Set a lower value to stretch the Texture. Try different values until you reach your desired effect. Offset The 2D offset that positions the Texture on the mesh. To adjust the position on your mesh, move the Texture across the U or V axes. Detail Inputs Use the Detail Inputs settings to add extra details to the surface. Requirement: GPU supporting shader model 2.5 or higher. Property Description Mask Select a texture that defines areas where Unity overlays the Detail maps over the Surface Inputs maps. The mask uses the alpha channel of the selected texture. The Tiling and Offset settings have no effect on the mask. Base Map Select the texture containing the surface details. Unity blends this map with the Surface Base Map using the overlay mode. Normal Map Select the texture containing the normal vector data. Use a normal map to add surface details like bumps, scratches and grooves. Use the slider next to the setting to change the intensity of the effect of the map. The default value is 1. Tiling Use this setting to scale the Base Map and the Normal Map on the mesh along the U and V axes, so that the maps fit the mesh best. The default value is 1. Select a value higher than one to make the maps repeat themselves across the mesh. Set a value lower than 1 to stretch the maps. Offset The offset that moves the Base Map and the Normal Map on the mesh along the U and V axes. Advanced The Advanced settings affect the underlying calculations of your rendering. They do not have a visible effect on your surface. Property Description Specular Highlights Enable this to allow your Material to have specular highlights from direct lighting, for example Directional, Point, and Spot lights. This means that your Material reflects the shine from these light sources. Disable this to leave out these highlight calculations, so your Shader renders faster. By default, this feature is enabled. Environment Reflections Sample reflections using the nearest Reflection Probe, or, if you have set one in the Lighting window, the Lighting Probe. If you disable this, you will have fewer Shader calculations, but this also means that your surface has no reflections. Enable GPU Instancing Makes URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline. Channel packing This Shader uses channel packing, so you can use a single RGBA texture for the metallic, smoothness and occlusion properties. When you use texture packing, you only have to load one texture into memory instead of three separate ones. When you write your texture maps in a program like Substance or Photoshop, you can pack the maps like this: Channel Property Red Metallic Green Occlusion Blue None Alpha Smoothness"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/materialvariant-URP.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/materialvariant-URP.html",
    "title": "Material Variants | FSM Unity Framework",
    "keywords": "Material Variants Many of the materials in a game may be variations on a source—outfits with a variety of color schemes, damaged and undamaged versions of scenery, shiny and weathered instances of props. To help you manage and maintain these materials, Material Variants address specific shortcomings of copied materials. To learn more about this functionality, see Material Variants."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/optimize-for-better-performance.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/optimize-for-better-performance.html",
    "title": "Optimize for better performance | FSM Unity Framework",
    "keywords": "Optimize for better performance If the performance of your Universal Render Pipeline (URP) project seems slow, you can analyze your project and adjust settings to increase performance. Use the Unity Profiler to analyze your project You can use the Unity Profiler to get data on the performance of your project in areas such as the CPU and memory. Profiler markers The following table lists markers that appear in the Unity Profiler for a URP frame and have a significant effect on performance. The table doesn't include a marker if it's deep in the Profiler hierarchy, or the label already describes what URP does. Marker Sub-marker Description Inl_UniversalRenderPipeline. RenderSingleCameraInternal URP builds a list of rendering commands in the ScriptableRenderContext, for a single camera. URP only records rendering commands in this marker, but doesn't yet execute them. The marker includes the camera name, for example Main Camera. Inl_ScriptableRenderer.Setup URP prepares for rendering, for example preparing render textures for the camera and shadow maps. CullScriptable URP generates a list of GameObjects and lights to render, and culls (excludes) any that are outside the camera's view. The time this takes depends on the number of GameObjects and lights in your scene. Inl_ScriptableRenderContext.Submit URP submits the list of commands in the ScriptableRenderContext to the graphics API. This marker might appear more than once if URP submits commands more than once per frame, or you call ScriptableRenderContext.Submit in your own code. MainLightShadow URP renders a shadow map for the main Directional Light. AdditionalLightsShadow URP renders shadow maps for other lights. UberPostProcess URP renders post-processing effects you enable. This marker contains separate markers for some post-processing effects. RenderLoop.DrawSRPBatcher URP uses the Scriptable Render Pipeline Batcher to render one or more batches of objects. CopyColor URP copies the color buffer from one render texture to another. You can disable Opaque Texture in the URP Asset, so that URP only copies the color buffer if it needs to. CopyDepth URP copies the depth buffer from one render texture to another. You can disable Depth Texture in the URP Asset unless you need the depth texture (for example, if you use a shader that uses scene depth). FinalBlit URP copies a render texture to the current camera render target. Use a GPU profiler to analyze your project You can use a platform GPU profiler such as Xcode to get data on the performance of the GPU during rendering. You can also use a profiler such as RenderDoc, but it might provide less accurate performance data. Data from a GPU profiler includes URP markers for rendering events, such as different render passes. Use other tools to analyze your project You can also use the following tools to analyze the performance of your project: Scene view View Options Rendering Debugger Frame Debugger Adjust settings Based on your analysis, you can adjust the following settings in the Universal Render Pipeline (URP) Asset or the Universal Renderer asset to improve the performance of your project. Depending on your project or the platforms you target, some settings might not have a significant effect. There might also be other settings that have an effect on performance in your project. Setting Where the setting is What to do for better performance Accurate G-buffer normals Universal Renderer > Rendering Disable if you use the Deferred rendering path Additional Lights > Cast Shadows URP Asset > Lighting Disable Additional Lights > Cookie Atlas Format URP Asset > Lighting Set to Color Low Additional Lights > Cookie Atlas Resolution URP Asset > Lighting Set to the lowest you can accept Additional Lights > Per Object Limit URP Asset > Lighting Set to the lowest you can accept. This setting has no effect if you use the Deferred or Forward+ rendering paths. Additional Lights > Shadow Atlas Resolution URP Asset > Lighting Set to the lowest you can accept Additional Lights > Shadow Resolution URP Asset > Lighting Set to the lowest you can accept Cascade Count URP Asset > Shadows Set to the lowest you can accept Conservative Enclosing Sphere URP Asset > Shadows Enable Technique Decal Renderer Feature Set to Screen Space, and set Normal Blend to Low or Medium Fast sRGB/Linear conversion URP Asset > Post Processing Enable Grading Mode URP Asset > Post Processing Set to Low Dynamic Range LOD Cross Fade Dither URP Asset > Quality Set to Bayer Matrix LUT size URP Asset > Post Processing Set to the lowest you can accept Main Light > Cast Shadows URP Asset > Lighting Disable Max Distance URP Asset > Shadows Reduce Opaque Downsampling URP Asset > Rendering If Opaque Texture is enabled in the URP Asset, set to 4x Bilinear Render Scale URP Asset > Quality Set to below 1.0 Soft Shadows URP Asset > Shadows Disable, or set to Low Upscaling Filter URP Asset > Quality Set to Bilinear or Nearest-Neighbor Refer to the following for more information on the settings: Deferred Rendering Path in URP Forward+ Rendering Path Decal Renderer Feature Universal Render Pipeline Asset Universal Renderer Additional resources Understand performance in URP Configure for better performance Graphics performance and profiling Best practices for profiling game performance Tools for profiling and debugging Native CPU profiling: Tips to optimize your game performance"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/package-sample-urp-package-samples.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/package-sample-urp-package-samples.html",
    "title": "URP Package Samples | FSM Unity Framework",
    "keywords": "URP Package Samples URP Package Samples is a package sample for the Universal Render Pipeline (URP). It contains example shaders, C# scripts, and other assets you can build upon, use to learn how to use a feature, or use directly in your application. For information on how to import URP Package Samples into your project, see Importing package samples. Each example uses its own URP Asset so, if you want to build an example scene, add the example's URP Asset to your Graphics settings. If you don't do this, Unity might strip shaders or render passes that the example uses. Camera Stacking The URP Package Samples/CameraStacking folder contains examples for Camera Stacking. The following table describes each Camera Stacking example in this folder. Example Description Mixed field of view The example in CameraStacking/MixedFOV demonstrates how to use Camera Stacking in a first-person application to prevent the character's equipped items from clipping into the environment. This setup also makes it possible to have different fields of view for the environment camera and the equipped items camera. Split screen The example in CameraStacking/SplitScreenPPUI demonstrates how to create a split-screen camera setup where each screen has its own Camera Stack. It also demonstrates how to apply post-processing on world-space and screen-space camera UI. 3D skybox The example in CameraStacking/3D Skybox uses Camera Stacking to transform a miniature environment into a skybox. One overlay camera renders a miniature city and another renders miniature planets. The overlay cameras render to pixels that the main camera did not draw to. With some additional scripted translation, this makes the miniature environment appear full size in the background of the main camera's view. Decals The URP Package Samples/Decals folder contains examples for decals. The following table describes each decal example in this folder. Example Description Blob shadows The example in Decals/BlobShadow uses the Decal Projector component to cast a shadow under a character. This method of shadow rendering is less resource-intensive than shadow maps and is suitable for use on low-end devices. Paint splat The example in Decals/PaintSplat uses a WorldSpaceUV Sub Graph and the Simple Noise Shader Graph node to create procedural decals. The noise in each paint splat uses the world position of the Decal Projector component. Proxy lighting The example in Decals/ProxyLighting builds on the Blob shadows example and uses Decal Projectors to add proxy spotlights. These decals modify the emission of surfaces inside the projector's volume. Note: To demonstrate the extent of its lighting simulation, this example disables normal real-time lighting. Lens Flares The URP Package Samples/LensFlares folder contains lens flare examples. The following table describes each lens flare example in this folder. Example Description Sun flare The LensFlares/SunFlare example demonstrates how to use the Lens Flare component to add a lens flare effect to the main directional light in the scene. Lens flare showroom The LensFlares/LensFlareShowroom example helps you to author lens flares. To use it: 1. In the Hierarchy window, select the Lens Flare GameObject. 2. In the Lens Flare component, assign a LensFlareDataSRP asset to the Lens Flare Data property. 3. Change the Lens Flare component and data properties and view the lens flare in the Game View. Note: If the text box is in the way, disable the Canvas in the scene. Lighting The URP Package Samples/Lighting folder contains examples for lighting. The following table describes each lighting example in this folder. Example Description Reflection probes The example in Lighting/Reflection Probes uses reflection probes to create reflection maps for a reflective sphere GameObject. This sample shows how the Probe Blending and Box Projection settings can change the reflection within a scene that uses reflection probes. Renderer Features The URP Package Samples/RendererFeatures folder contains examples for Renderer Features. The following table describes each Renderer Feature example in this folder. Example Description Ambient occlusion The example in RendererFeatures/AmbientOcclusion uses a Renderer Feature to add screen space ambient occlusion (SSAO) to URP. See the SSAO_Renderer asset for an example of how to set up this effect. Glitch effect The example in RendererFeatures/GlitchEffect uses the Render Objects Render Feature and the Scene Color Shader Graph node to draw some GameObjects with a glitchy effect. See the Glitch_Renderer asset for an example of how to set up this effect. Keep frame The example in RendererFeatures/KeepFrame uses a custom Renderer Feature to preserve frame color between frames. The example uses this to create a swirl effect from a simple particle system. Note: The effect is only visible in Play Mode. Occlusion effect The example in RendererFeatures/OcclusionEffect uses the Render Objects Renderer Feature to draw occluded geometry. The example achieves this effect without any code and sets everything up in the OcclusionEffect_Renderer asset. Trail effect The example in RendererFeatures/TrailEffect uses the Renderer Feature from the Keep frame example on an additional camera to create a trail map. To do this, the additional camera draws depth to a RenderTexture. The Sand_Graph shader samples the map and displaces vertices on the ground. Shaders The URP Package Samples/Shaders folder contains examples for shaders. The following table describes each shader example in this folder. Example Description Lit The example in Shaders/Lit demonstrates how different properties of the Lit shader affect the surface of some geometry. You can use the materials and textures as guidelines on how to set up materials in URP."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/package-samples.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/package-samples.html",
    "title": "Package samples | FSM Unity Framework",
    "keywords": "Package samples The Universal Render Pipeline (URP) comes with a set of samples to help you get started. A sample is a set of assets that you can import into your Unity project and use as a base to build upon or learn how to use a feature. A package sample can contain anything from a single C# script to multiple scenes. Importing package samples Before you import any package samples for URP, be aware that they require your project to be URP-compatible. A project is URP-compatible if you created it from a template or manually installed and set up URP in it. If the project is not URP-compatible, errors can occur when you import a package sample. To import package samples, use the Unity Package Manager window: Go to Window > Package Manager and, in the packages list view, select Universal RP. In the package details view, find the Samples section. Find the sample you want to import and click the Import button next to it. Unity imports URP package samples into Assets/Samples/Universal RP/<package version>/<sample name>. Opening package samples To open a package sample: Go to Assets/Samples/Universal RP/<package version>/. Here there is a folder for each URP package sample you have imported. Find the folder that contains the package sample you want and open it. The folder has the same name that the package sample has in the Unity Package Manager window. Package samples list The package samples that URP provides are: URP Package Samples: A collection of example shaders, C# scripts, and other assets you can build upon or use in your application. For more information, see URP Package Samples."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-lit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-lit-shader.html",
    "title": "Particles Lit Shader | FSM Unity Framework",
    "keywords": "Particles Lit Shader In the Universal Render Pipeline (URP), use this Shader to make particles appear almost photorealistic, for example for camp fire particles, rain drops or torch smoke. This Shader produces lifelike visuals but uses the most computationally heavy shading model in URP, which can impact performance. Using the Particles Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Particles > Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how URP renders the Material on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the Material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque Materials. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Select how Unity calculates the color of each pixel of a transparent Material when it blends the Material with the background. In the context of Blending Modes, Source refers to the transparent Material where the Blending Mode is set and Destination refers to anything that Material overlaps with. Alpha Alpha blending mode. Alpha uses the Material's alpha value to change how transparent an object is. 0 is fully transparent. 255 is fully opaque, this is translated to a value of 1 when used with the blending equations. The Material is always rendered in the Transparent render pass regardless of it's alpha value. This mode lets you use the Preserve Specular Lighting property. Alpha equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB × (1 − SourceAlpha) Premultiply Premultiply blending mode. Premultiply first multiplies the RGB values of the transparent Material by its alpha value then applies a similar effect to the Material as Alpha. The equation for Premultiply also allows areas of the transparent Material with an alpha value of 0 to have an additive blend effect. This can help reduce artifacts that may appear at the edge of the overlap between opaque and transparent pixels. Premultiply equation: OutputRGBA = SourceRGB + DestinationRGB × (1 − SourceAlpha) Additive Additive blending mode. Additive adds the color values of the Materials together to create the blend effect. The alpha value determines the strength of the source Material's color before the blend is calculated. This mode lets you use the Preserve Specular Lighting property. Additive equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB Multiply Multiply blending mode. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. This mode uses the Material’s alpha value to adjust how much the colors blend. An alpha value of 1 results in unadjusted multiplication of the colors while lower values blend the colors towards white. Multiply equation: OutputRGBA = SourceRGB × DestinationRGB Preserve Specular Lighting Indicates whether Unity preserves specular highlights on a GameObject or not. This applies even when your surface is transparent which means that only the reflected light is visible. This property only appears when the Surface Type is set to Transparent and Blending Mode is set to either Alpha or Additive. Material with Preserve Specular Lighting disabled. Material with Preserve Specular Lighting enabled. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Color Mode Use this drop-down to determine how the particle color and the Material color blend together. Multiply produces a darker final color by multiplying the two colors. Additive produces a brighter final colour by adding the two colours together.</br/>Subtractive subtracts the particle color from the base color of the Material. This creates an overall dark effect in the pixel itself, with less brightness. Overlay blends the particle color over the base color of the Material. This creates a brighter color at values over 0.5 and darker colors at values under 0.5. Color uses the particle color to colorize the Material color, while keeping the value and saturation of the base color of the Material. This is good for adding splashes of color to monochrome Scenes. Difference returns the difference between both color values. This is good for blending particle and Material colors that are similar to each other. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. The Base Map is also known as a diffuse map. Metallic Map Shows the map input for the metallic highlights and reflections from direct lighting, for example Directional, Point, and Spot lights. You can use the Smoothness slider to control the spread of highlights on the surface. 0 gives a wide, rough highlight. 1 gives a small, sharp highlight like glass. Values in between 0 and 1 produce semi-glossy looks. For example, 0.5 produces a plastic-like glossiness. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. The float value next to the setting is a multiplier for the effect of the Normal Map. Low values decrease the effect of the normal map. High values create stronger effects. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Advanced The Advanced settings affect behind-the-scenes rendering. They do not have a visible effect on your surface, but on underlying calculations that impact performance. Property Description Flip-Book Blending Tick this box to blend flip-book frames together. This is useful in texture sheet animations with limited frames, because it makes animations smoother. If you have performance issues, try turning this off. Vertex Streams This list shows the vertex streams that this Material requires in order to work properly. If the vertex streams aren’t correctly assigned, the Fix Now button appears. Click this button to apply the correct setup of vertex streams to the Particle System that this Material is assigned to. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline. Transparent surface type If you’ve chosen a Transparent surface type under Surface Options, these options appear: Property Description Soft Particles Tick this box to make particles fade out when they get close to intersecting with the surface of other geometry written into the depth buffer. When you enable this feature, the Surface Fade settings appear: Near sets the distance from the other surface where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the other surface where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Only usable for transparent surface types. Note: This setting uses the CameraDepthTexture that is created by URP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Camera Fading Tick this box to make particles fade out when they get close to the camera. When you enable this feature, the Distance settings appear: Near sets the distance from the camera where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the camera where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Note: This uses the CameraDepthTexture that is created by URP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Distortion Creates a distortion effect by making particles perform refraction with the objects drawn before them. This is useful for creating a heat wave effect or for warping objects behind the particles. When you enable this feature, these settings appear: Strength controls how much the Particle distorts the background. Negative values have the opposite effect of positive values. So if something was offset to the right with a positive value, the equal negative value offsets it to the left. Blend controls how visible the distortion is. At 0, there is no visible distortion. At 1, only the distortion effect is visible. Note: This uses the CameraOpaqueTexture that is created by URP. To use this setting, enable Opaque Texture in the URP Asset or for the Camera that is rendering the particles."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-simple-lit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-simple-lit-shader.html",
    "title": "Particles Simple Lit Shader | FSM Unity Framework",
    "keywords": "Particles Simple Lit Shader In the Universal Render Pipeline (URP), use this Shader for particles where performance is more important than photorealism. This Shader uses a simple approximation for lighting. Because this Shader does not calculate for physical correctness and energy conservation, it renders quickly. Using the Particles Simple Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Particles > Simple Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how URP renders the Material on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Use this drop-down to determine how URP calculates the color of each pixel of the transparent Material by blending the Material with the background pixels. Alpha uses the Material’s alpha value to change how transparent an object is. 0 is fully transparent. 1 appears fully opaque, but the Material is still rendered during the Transparent render pass. This is useful for visuals that you want to be fully visible but to also fade over time, like clouds. Premultiply applies a similar effect to the Material as Alpha, but preserves reflections and highlights, even when your surface is transparent. This means that only the reflected light is visible. For example, imagine transparent glass. Additive adds an extra layer to the Material, on top of another surface. This is good for holograms. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Color Mode Use this drop-down to determine how the particle color and the Material color blend together. Multiply produces a darker final color by multiplying the two colors. Additive produces a brighter final colour by adding the two colours together.</br/>Subtractive subtracts the particle color from the base color of the Material. This creates an overall dark effect in the pixel itself, with less brightness. Overlay blends the particle color over the base color of the Material. This creates a brighter color at values over 0.5 and darker colors at values under 0.5. Color uses the particle color to colorize the Material color, while keeping the value and saturation of the base color of the Material. This is good for adding splashes of color to monochrome Scenes. Difference returns the difference between both color values. This is good for blending Particle and Material colors that are similar to each other. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. The Base Map is also known as a diffuse map. Specular Map Controls the color of your specular highlights from direct lighting, for example Directional, Point, and Spot lights. To assign a Texture to the Specular Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. In__ Source__, you can select a Texture in your Project to act as a source for the smoothness. By default, the source is the alpha channel for this Texture. You can use the Smoothness slider to control the spread of highlights on the surface. 0 gives a wide, rough highlight. 1 gives a small, sharp highlight like glass. Values in between 0 and 1 produce semi-glossy looks. For example, 0.5 produces a plastic-like glossiness. Note: If this setting appears greyed out, check if Specular Highlights are enabled under the Advanced settings. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Advanced The Advanced settings affect behind-the-scenes rendering. They do not have a visible effect on your surface, but on underlying calculations that impact performance. Property Description Flip-Book Blending Tick this box to blend flip-book frames together. This is useful in texture sheet animations with limited frames, because it makes animations smoother. If you have performance issues, try turning this off. Specular Highlights When enabled, your particles have specular highlights from direct lighting, for example Directional, Point, and Spot lights. This means that each particle reflects the shine from these light sources. When disabled, these highlight calculations are not part of the Shader, which can make the Shader render faster. By default, this feature is enabled. Vertex Streams This list shows the vertex streams that this Material requires in order to work properly. If the vertex streams aren’t correctly assigned, the Fix Now button appears. Click this button to apply the correct setup of vertex streams to the Particle System that this Material is assigned to. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline. Transparent surface type If you’ve chosen a Transparent surface type under Surface Options, these options appear: Property Description Soft Particles Tick this box to make particles fade out when they get close to intersecting with the surface of other geometry written into the depth buffer. When you enable this feature, the Surface Fade settings appear: Near sets the distance from the other surface where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the other surface where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Only usable for transparent surface types. Note: This setting uses the CameraDepthTexture that is created by URP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Camera Fading Tick this box to make particles fade out when they get close to the camera. When you enable this feature, the Distance settings appear: Near sets the distance from the camera where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the camera where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Note: This uses the CameraDepthTexture that is created by URP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Distortion Creates a distortion effect by making particles perform refraction with the objects drawn before them. This is useful for creating a heat wave effect or for warping objects behind the particles. When you enable this feature, these settings appear: Strength controls how much the Particle distorts the background. Negative values have the opposite effect of positive values. So if something was offset to the right with a positive value, the equal negative value offsets it to the left. Blend controls how visible the distortion is. At 0, there is no visible distortion. At 1, only the distortion effect is visible. Note: This uses the CameraOpaqueTexture that is created by URP. To use this setting, enable Opaque Texture in the URP Asset or for the Camera that is rendering the particles."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-unlit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/particles-unlit-shader.html",
    "title": "Particles Unlit Shader | FSM Unity Framework",
    "keywords": "Particles Unlit Shader Use this Shader for Particles that don’t need lighting. Because there are no time-consuming lighting calculations or lookups, this Shader is optimal for lower-end hardware. The Unlit Shader uses the most simple shading model in the Universal Render Pipeline (URP). Using the Particles Unlit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Particles > Unlit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how URP renders the Material on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the Material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque Materials. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Use this drop-down to determine how URP calculates the color of each pixel of the transparent Material by blending the Material with the background pixels. Alpha uses the Material’s alpha value to change how transparent a surface is. 0 is fully transparent. 1 appears fully opaque, but the Material is still rendered during the Transparent render pass. This is useful for visuals that you want to be fully visible but to also fade over time, like clouds. Premultiply applies a similar effect to the Material as Alpha, but preserves reflections and highlights, even when your surface is transparent. This means that only the reflected light is visible. For example, imagine transparent glass. Additive adds an extra layer to the Material, on top of another surface. This is good for holograms. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you view an through tinted glass. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Color Mode Use this drop-down to determine how the particle color and the Material color blend together. Multiply produces a darker final color by multiplying the two colors. Additive produces a brighter final colour by adding the two colours together.</br/>Subtractive subtracts the particle color from the base color of the Material. This creates an overall dark effect in the pixel itself, with less brightness. Overlay blends the particle color over the base color of the Material. This creates a brighter color at values over 0.5 and darker colors at values under 0.5. Color uses the particle color to colorize the Material color, while keeping the value and saturation of the base color of the Material. This is good for adding splashes of color to monochrome Scenes. Difference returns the difference between both color values. This is good for blending particle and Material colors that are similar to each other. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. The Base Map is also known as a diffuse map. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Advanced The Advanced settings affect behind-the-scenes rendering. They do not have a visible effect on your surface, but on underlying calculations that impact performance. Property Description Flip-Book Blending Tick this box to blend flip-book frames together. This is useful in texture sheet animations with limited frames, because it makes animations smoother. If you have performance issues, try turning this off. Vertex Streams This list shows the vertex streams that this Material requires in order to work properly. If the vertex streams aren’t correctly assigned, the Fix Now button appears. Click this button to apply the correct setup of vertex streams to the Particle System that this Material is assigned to. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline. Transparent surface type If you’ve chosen a Transparent surface type under Surface Options, these options appear: Property Description Soft Particles Tick this box to make particles fade out when they get close to intersecting with the surface of other geometry written into the depth buffer. When you enable this feature, the Surface Fade settings appear: Near sets the distance from the other surface where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the other surface where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Only usable for transparent surface types. Note: This setting uses the CameraDepthTexture that is created by URP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Camera Fading Tick this box to make particles fade out when they get close to the camera. When you enable this feature, the Distance settings appear: Near sets the distance from the camera where the particle is completely transparent. This is where the particle appears to fade out completely. Far sets the distance from the camera where the particle is completely opaque. The particle appears solid here. Distances are measured in world units. Note: This uses the CameraDepthTexture that is created by Universal RP. To use this setting, enable Depth Texture in the URP Asset or for the Camera that is rendering the particles. Distortion Creates a distortion effect by making particles perform refraction with the objects drawn before them. This is useful for creating a heat wave effect or for warping objects behind the particles. When you enable this feature, these settings appear: Strength controls how much the Particle distorts the background. Negative values have the opposite effect of positive values. So if something was offset to the right with a positive value, the equal negative value offsets it to the left. Blend controls how visible the distortion is. At 0, there is no visible distortion. At 1, only the distortion effect is visible. Note: This uses the CameraOpaqueTexture that is created by URP. To use this setting, enable Opaque Texture in the URP Asset or for the Camera that is rendering the particles."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/pixel-cinemachine.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/pixel-cinemachine.html",
    "title": "Using the Cinemachine Pixel Perfect extension | FSM Unity Framework",
    "keywords": "Using the Cinemachine Pixel Perfect extension Both the Pixel Perfect Camera and Cinemachine modify the Camera’s orthographic size. Using these two systems together in a single Scene would cause them to fight for control over the Camera and produce unwanted results. The Cinemachine Pixel Perfect extension solves this incompatibility. Cinemachine Pixel Perfect is an extension for the Cinemachine Virtual Camera that alters the orthographic size of the virtual camera. The extension detects the presence of the Pixel Perfect Camera component, and uses the component settings to calculate for the correct orthographic size of the virtual camera that best retains the Sprites in a pixel-perfect resolution. To add this extension to your virtual cameras, use the Add Extension dropdown menu on the Cinemachine Virtual Camera Inspector window. Add this extension to each virtual camera in your Project. For each virtual camera attached with this extension, the Pixel Perfect Camera component then calculates a pixel-perfect orthographic size that best matches the original size of the virtual camera during __Play Mode __ or when Run In Edit Mode is enabled. This is done to match the original framing of each virtual camera as close as possible when the pixel-perfect calculations are implemented. When the Cinemachine Brain component blends between multiple virtual cameras, the rendered image is temporarily not pixel-perfect during the transition between cameras. The image becomes pixel-perfect once the view fully transitions to a single virtual camera. The following are the current limitations of the extension: When a virtual camera with the Pixel Perfect extension is set to follow a Target Group, there may be visible choppiness when the virtual camera is positioned with the Framing Transposer component. If the Upscale Render Texture option is enabled on the Pixel Perfect Camera, there are less possible pixel-perfect resolutions that match the original orthographic size of the virtual cameras. This may cause the framing of the virtual cameras to be off by quite a large margin after the pixel-perfect calculations."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-bloom.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-bloom.html",
    "title": "Bloom | FSM Unity Framework",
    "keywords": "Bloom Scene with Bloom effect turned off. Scene with Bloom effect turned on. The Bloom effect creates fringes of light extending from the borders of bright areas in an image. This creates the illusion of extremely bright light overwhelming the Camera. The Bloom effect also has a Lens Dirt feature, which you can use to apply a full-screen layer of smudges or dust to diffract the Bloom effect. Using Bloom Bloom uses the Volume system, so to enable and modify Bloom properties, you must add a Bloom override to a Volume in your Scene. To add Bloom to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing, and click on Bloom. Universal Render Pipeline applies Bloom to any Camera this Volume affects. Properties Bloom Property Description Threshold Set the gamma space brightness value at which URP applies Bloom. URP does not apply Bloom to any pixels in the Scene that have a brightness lower than this value. The minimum value is 0, where nothing is filtered. The default value is 0.9. There is no maximum value. Intensity Set the strength of the Bloom filter, in a range from 0 to 1. The default is 0, which means that the Bloom effect is disabled. Scatter Set the radius of the bloom effect in a range from 0 to 1. Higher values give a larger radius. The default value is 0.7. Tint Use the color picker to select a color for the Bloom effect to tint to. Clamp Set the maximum intensity that Unity uses to calculate Bloom. If pixels in your Scene are more intense than this, URP renders them at their current intensity, but uses this intensity value for the purposes of Bloom calculations. The default value is 65472. High Quality Filtering Enable this to use high quality sampling. This reduces flickering and improves the overall smoothness, but is more resource-intensive and can affect performance. Downscale Set the initial resolution scale for the effect. The lower this value is, the fewer system resources the initial blur effect consumes. Max Iterations The size of the rendered image determines the number of iterations. Use this setting to define the maximum number of iterations. Decreasing this value reduces processing load and increases performance, especially on mobile devices with high DPI screens. The default value is 6. Lens Dirt Property Description Texture Assign a Texture to apply the effect of dirt (such as smudges or dust) to the lens. Intensity Set the strength of the Lens Dirt effect. Troubleshooting performance issues There are multiple ways to improve the performance impact of Bloom. Listed in order of effectiveness, you can: Disable High Quality Filtering. Bloom then uses bilinear filtering instead of bicubic. This reduces the overall smoothness of the Bloom effect, but greatly improves performance, especially on lower-end hardware and platforms. In some extreme cases, you might see blocky graphical artifacts in your Scene. Set Downscale to Quarter starting resolution to make the initial cost of Bloom much lower. Use a lower resolution Lens Dirt Texture to reduce memory pressure and speed up blending across volumes."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-chromatic-aberration.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-chromatic-aberration.html",
    "title": "Chromatic Aberration | FSM Unity Framework",
    "keywords": "Chromatic Aberration Scene with Chromatic Aberration effect turned off. Scene with Chromatic Aberration effect turned on. Chromatic Aberration creates fringes of color along boundaries that separate dark and light parts of the image. It mimics the color distortion that a real-world camera produces when its lens fails to join all colors to the same point. See Wikipedia: Chromation aberration. Using Chromatic Aberration Chromatic Aberration uses the Volume system, so to enable and modify Chromatic Aberration properties, you must add a Chromatic Aberration override to a Volume in your Scene. To add Chromatic Aberration to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Chromatic Aberration. Universal Render Pipeline applies Chromatic Aberration to any Camera this Volume affects. Properties Property Description Intensity Set the strength of the Chromatic Aberration effect. Values range between 0 and 1. The higher the value, the more intense the effect is. The default value is 0, which disables the effect."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-depth-of-field.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-depth-of-field.html",
    "title": "Depth Of Field | FSM Unity Framework",
    "keywords": "Depth Of Field The Depth Of Field component applies a depth of field effect, which simulates the focus properties of a camera lens. In real life, a camera can only focus sharply on an object at a specific distance. Objects nearer or farther from the camera are out of focus. The blurring gives a visual cue about an object’s distance, and introduces “bokeh”, which refers to visual artifacts that appear around bright areas of the image as they fall out of focus. To read more about bokeh, see the Wikipedia article on Bokeh. The Universal Render Pipeline (URP) has two depth of field modes: Gaussian: this mode approximates camera-like effects, but doesn’t imitate them completely. It has a limited blur radius and only does far-field blurring. This mode is the fastest, and is the best mode for lower-end platforms. Bokeh: a slower but higher quality mode that closely imitates the effects of a real-life camera. It can do both near & far-field blurring, and generates bokeh on areas with high luminosity intensity, also known as hot spots. Using Depth Of Field Depth Of Field uses the Volume system, so to enable and modify Depth Of Field properties, you must add a Depth Of Field override to a Volume in your Scene. To add Depth Of Field to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing, and click on Depth Of Field. Universal Render Pipeline applies Depth Of Field to any Camera this Volume affects. Properties Property Description Mode Use this drop-down to select the mode that URP uses to set the focus for the depth of field effect. Off: Select this option to disable depth of field. Gaussian: Select this option to use the faster but more limited depth of field mode. Bokeh: Select this option to use the Bokeh-based depth of field mode. Gaussian Depth of Field Property Description Start Set the distance from the Camera at which the far field starts blurring. End Set the distance from the Camera at which the far field blur reaches its maximum blur radius. Max Radius Set the maximum radius the far blur can reach. The default value is 1. Note: Values above 1 can cause visual under-sampling artifacts to appear in your Scene. If your blur effects are not smooth or appear to have static noise in them, try decreasing the value back to 1 or lower. High Quality Sampling Use higher quality sampling to reduce flickering and improve the overall blur smoothness. This can cause some performance cost. Bokeh Depth of Field The Bokeh Depth of Field mode closely imitates the effect of a real-life camera. For this reason, the settings are based on real-life camera settings, and offer a number of properties to adjust the diaphragm blades on the Camera. For an introduction to diaphragm blades and how they affect the visual quality of your Camera output, see Improve Photography’s guide Aperture Blades: How many is best?. Property Description Focus Distance Set the distance from the Camera to the focus point. Focal Length Set the distance, in millimeters, between the Camera sensor and the Camera lens. The larger the value is, the shallower the depth of field. Aperture Set the ratio of aperture (known as f-stop or f-number). The smaller the value is, the shallower the depth of field is. Blade Count Use the slider to set the number of diaphragm blades the Camera uses to form the aperture. The more blades you use, the rounder the bokeh appears. Blade Count from left to right: 3, 4, 5, and 6. Blade Curvature Use the slider to set the curvature of diaphragm blades the Camera uses to form the aperture. The smaller the value is, the more visible aperture blades are. A value of 1 makes the bokeh perfectly circular. Blade Curvature value of 1 (left), and 0 (right). Blade Rotation Use the slider to set the rotation of diaphragm blades in degrees."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-ssao.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-ssao.html",
    "title": "Ambient Occlusion | FSM Unity Framework",
    "keywords": "Ambient Occlusion The Ambient Occlusion effect darkens creases, holes, intersections and surfaces that are close to each other in real-time. In the real world, such areas tend to block out or occlude ambient light, so they appear darker. URP implements the Screen Space Ambient Occlusion (SSAO) effect as a Renderer Feature. It works with every shader that the Universal Render Pipeline (URP) provides as well as any custom opaque Shader Graphs you create. Note: The SSAO effect is a Renderer Feature and works independently from the post-processing effects in URP. This effect does not depend on or interact with Volumes. The following images show a scene with the Ambient Occlusion effect turned off, on, and only the Ambient Occlusion texture. Scene with Ambient Occlusion effect turned off. Scene with Ambient Occlusion effect turned on. Scene with only the Ambient Occlusion texture. Add the SSAO Renderer Feature to a Renderer URP implements the Ambient Occlusion effect as a Renderer Feature. To use the SSAO effect in your project follow the instructions on How to add a Renderer Feature to a Renderer and add the Screen Space Ambient Occlusion Renderer Feature. This causes any Cameras that use the Renderer with the SSAO Renderer Feature to have the SSAO effect. Properties This section describes the properties of the SSAO Renderer Feature. Method This property defines the type of noise the SSAO effect uses. Available Options: Interleaved Gradient Noise: Uses interleaved gradient noise to generate static SSAO. Blue Noise: Uses a selection of blue noise textures to generate dynamic SSAO. This creates an animated effect as the texture changes with every frame, as a result the SSAO effect is more subtle when the camera is in motion. Performance impact: Insignificant. Intensity This property defines the intensity of the darkening effect. Performance impact: Insignificant. Radius When Unity calculates the Ambient Occlusion value, the SSAO effect takes samples of the normal texture within this radius from the current pixel. Performance impact: High. A lower Radius value improves performance, because the SSAO Renderer Feature samples pixels closer to the source pixel. This makes caching more efficient. Calculating the Ambient Occlusion Pass on objects closer to the Camera takes longer than on objects further from the Camera. This is because the Radius property scales with the object. Falloff Distance SSAO does not apply to objects farther than this distance from the Camera. A lower value increases performance in scenes that contain many distant objects. The performance improvement is smaller in smaller scenes with fewer objects. Performance impact: Depends on the application. Direct Lighting Strength This property defines how visible the effect is in areas exposed to direct lighting. These images show how the Direct Lighting Strength value changes the SSAO effect depending on whether they are in the shadow or not. Direct Lighting Strength: 0.2. Direct Lighting Strength: 0.9. A. Shows the effect of Direct Lighting Strength on the SSAO effect in lit areas. B. Shows the effect of Direct Lighting Strength on the SSAO effect in areas one or more shadows cover. Performance impact: Insignificant. Quality Source Select the source of the normal vector values. The SSAO Renderer Feature uses normal vectors for calculating how exposed each point on a surface is to ambient lighting. Available options: Depth Normals: SSAO uses the normal texture generated by the DepthNormals Pass. This option lets Unity make use of a more accurate normal texture. Depth: SSAO does not use the DepthNormals Pass to generate the normal texture. SSAO reconstructs the normal vectors using the depth texture instead. Use this option only if you want to avoid using the DepthNormals Pass block in your custom shaders. Selecting this option enables the Normal Quality property. Performance impact: Depends on the application. When you switch between the options Depth Normals and Depth, there might be a variation in performance, which depends on the target platform and the application. In a wide range of applications the difference in performance is small. In most cases, Depth Normals produce a better visual look. For more information on the Source property, see the section Implementation details. Normal Quality This property becomes active when you select the option Depth in the Source property. Higher quality of the normal vectors produces smoother SSAO effect. Available options: Low Medium High Performance impact: Medium. In some scenarios, the Depth option produces results comparable with the Depth Normals option. But in certain cases, the Depth Normals option provides a significant increase in quality. The following images show an example of such case. Source: Depth. Normal Quality: Low. Source: Depth. Normal Quality: Medium. Source: Depth. Normal Quality: High. Source: Depth Normals. For more information, see the section Implementation details. Downsample Selecting this check box reduces the resolution of the Pass that calculates the Ambient Occlusion effect by a factor of two. The reduction in resolution of the Ambient Occlusion Pass by a factor of two reduces the pixel count to process by a factor of four. This reduces the load on the GPU significantly, but makes the effect less detailed. Performance impact: Very high. After Opaque When you enable After Opaque, Unity calculates and applies the SSAO effect after the opaque render pass. This can increase performance when used with Depth as the Source for normal vector values as Unity does not perform the skips depth prepass to calculate SSAO and instead uses the existing depth values. After Opaque can also increase performance on mobile devices that use tile-based rendering. Performance impact: Medium. Blur Quality This property defines the quality of blur that Unity applies to the SSAO effect. Higher quality blur creates a smoother, higher fidelity effect but requires more processing power. Available options: High (Bilateral): Bilateral blur, takes three passes to process. Medium (Gaussian): Gaussian blur, takes two passes to process. Low (Kawase): Kawase blur, takes a single pass to process. Performance impact: Very high. Samples For each pixel, the SSAO Render Feature takes the selected number of samples within the specified radius to calculate the Ambient Occlusion value. A higher value makes the effect smoother and more detailed, but also reduces performance. Available options: High: 12 Samples Medium: 8 Samples Low: 4 Samples Performance impact: High. An increase in the Sample Count value from 4 to 8 doubles the computational load on the GPU. Implementation details The SSAO Renderer Feature uses normal vectors for calculating how exposed each point on a surface is to ambient lighting. URP 10.0 implements the DepthNormals Pass block that generates the the normal texture _CameraNormalsTexture for the current frame. By default, the SSAO Renderer Feature uses this texture to calculate Ambient Occlusion values. If you implement your custom SRP and if you do not want to implement the DepthNormals Pass block in your shaders, you can use the SSAO Renderer Feature and set its Source property to Depth. In this case, Unity does not use the DepthNormals Pass to generate the normal vectors, it reconstructs the normal vectors using the depth texture instead. Selecting the option Depth in the Source property enables the Normal Quality property. The options in this property (Low, Medium, and High) determine the number of samples of the depth texture that Unity takes when reconstructing the normal vector from the depth texture. The number of samples per quality level: Low: 1, Medium: 5, High: 9."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-tonemapping.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-tonemapping.html",
    "title": "Tonemapping | FSM Unity Framework",
    "keywords": "Tonemapping Tonemapping is the process of remapping the HDR values of an image to a new range of values. Its most common purpose is to make an image with a low dynamic range appear to have a higher range. See Wikipedia: Tone mapping. Using Tonemapping Tonemapping uses the Volume system, so to enable and modify Tonemapping properties, you must add a Tonemapping override to a Volume in your Scene. To add Tonemapping to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing, and click on Tonemapping. The Universal Render Pipeline applies Tonemapping to any Camera this Volume affects. Properties Property Description Mode Select a tonemapping algorithm to use for color grading. The options are: None: Use this option if you do not want to apply tonemapping. Neutral: Use this option if you only want range-remapping with minimal impact on color hue & saturation. It is generally a good starting point for extensive color grading. ACES: Use this option to apply a close approximation of the reference ACES tonemapper, for a more cinematic look. It is more contrasted than Neutral, and has an effect on actual color hue & saturation. If you use this tonemapper, Unity does all the grading operations in the ACES color spaces, for optimal precision and results. Note: ACES HDR tonemapping is not supported on Android devices with Adreno 300 series GPU."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-vignette.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing-vignette.html",
    "title": "Vignette | FSM Unity Framework",
    "keywords": "Vignette In photography, vignetting is the term for the darkening and/or desaturating towards the edges of an image compared to the center. In real life, thick or stacked filters, secondary lenses, and improper lens hoods are usually the cause of this effect. You can use vignetting to draw focus to the center of an image. Using Vignette Vignette uses the Volume system, so to enable and modify Vignette properties, you must add a Vignette override to a Volume in your Scene. To add Vignette to a Volume: In the Scene or Hierarchy view, select a GameObject that contains a Volume component to view it in the Inspector. In the Inspector, navigate to Add Override > Post-processing and click on Vignette. Universal Render Pipeline applies Vignette to any Camera this Volume affects. Properties Property Description Color Use the color picker to set the color of the vignette. Center Set the vignette center point. For reference, the screen center is [0.5, 0.5]. Intensity Set the strength of the vignette effect. Smoothness Use the slider to set the smoothness of the vignette borders. Values range between 0.01 and 1. The higher the value, the smoother the vignette border. The default value is 0.2. Rounded When enabled, the vignette is perfectly round. When disabled, the vignette matches the shape on the current aspect ratio."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/custom-post-processing.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/custom-post-processing.html",
    "title": "Custom post-processing | FSM Unity Framework",
    "keywords": "Custom post-processing The Universal Render Pipeline (URP) provides a variety of pre-built post-processing effects that you can adjust to create a particular visual effect or style. URP also lets you create custom post-processing effects using the Full Screen Pass Renderer Feature. For example, you can implement a grayscale effect to indicate when a player has run out of health. Scene with no post-processing effects. Scene with grayscale custom post-processing effect. The following page describes how to create a custom post-processing effect using the Full Screen Pass Renderer Feature. How to create a custom post-processing effect."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/hdr-output.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/hdr-output.html",
    "title": "High Dynamic Range (HDR) Output | FSM Unity Framework",
    "keywords": "High Dynamic Range (HDR) Output High Dynamic Range content has a wider color gamut and greater luminosity range than standard definition content. URP can output HDR content for displays which support that functionality. How to enable HDR Output To activate HDR output, follow these steps. Locate the URP Asset in the Project window under Assets > Settings. Navigate to Quality > HDR and enable the checkbox to enable HDR. Navigate to Edit > Project Settings > Player > Other Settings and enable the following settings: Allow HDR Display Output Use HDR Display Output Note: Only enable Use HDR Display Output if you need the main display to use HDR Output. If you switch to a URP Asset that does not have HDR enabled, URP disables HDR Output until you change to a URP Asset with HDR enabled. Note: If HDR Output is active, the grading mode falls back to HDR, even if there is a different Color Grading Mode active in the URP Asset. HDR tone mapping in URP After you enable Allow HDR Display Output, you must configure Tonemapping settings for your HDR input. In order to configure these settings effectively, you need to understand how certain values related to tone mapping determine the visual characteristics of your HDR output. Important tone mapping values To properly make use of the capabilities of HDR displays, your Tonemapping configuration must take into account the capabilities of the target display, specifically these three values (in nits): Minimum supported brightness. Maximum supported brightness. Paper White value: This value represents the brightness of a paper-white surface represented on the display, which determines the display's brightness overall. Note: Low Dynamic Range (LDR) and High Dynamic Range (HDR) content do not appear equally bright on displays with the same Paper White value. This is because displays apply extra processing to low dynamic range content that bumps its brightness levels up. For this reason, it is best practice to implement a calibration menu for your application. Usable user interfaces depend on accurate Paper White values Unlit materials do not respond to lighting changes, so it is standard practice to use an Unlit material for user interfaces. Calculations for Unlit material rendering define brightness with values between 0 and 1 when you are not specifically targeting HDR displays. In this context, a value of 1 corresponds to white, and a value of 0 corresponds to black. However, in HDR mode, URP uses Paper White values to determine the brightness of Unlit materials. This is because HDR values can exceed the 0 to 1 range. As a result, Paper White values determine the brightness of UI elements in HDR mode, especially white elements, whose brightness matches Paper White values. Configure HDR Tone Mapping settings You can select and adjust Tonemapping modes in the Volume component settings. You can also adjust some aspects of your HDR Tonemapping configuration with a script (see the HDROutputSettings API). After you enable Allow HDR Display Output, HDR Tonemapping options become visible in the Volume component. Tone mapping modes URP provides two Tonemapping modes: Neutral and ACES. Each Tonemapping mode has some unique properties. Neutral mode is especially suitable for situations where you do not want the tone mapper to color grade your content. ACES mode uses the ACES reference color space for feature films. It produces a cinematic, contrasty result. Neutral Property Description Neutral HDR Range Reduction Mode The curve that the Player uses for tone mapping. The options are: BT2390: The default. Defined by the BT.2390 broadcasting recommendations. Reinhard: A simple Tone Mapping operator. This option is only available when you enable Show Additional Properties. Hue Shift Amount The value determines the extent to which your content retains its original hue after you apply HDR settings. When this value is 0, the tonemapper attempts to preserve the hue of your content as much as possible by only tonemapping luminance. Detect Paper White Enable this property if you want URP to use the Paper White value that the display communicates to the Unity Engine. In some cases, the value the display communicates may not be accurate. Implement a calibration menu for your application so that users can display your content correctly on displays that communicate inaccurate values. Paper White The Paper White value of the display. If you do not enable Detect Paper White, you must specify a value here. Detect Brightness Limits Enable this property if you want URP to use the minimum and maximum nit values that the display communicates. In some cases, the value the display communicates may not be accurate. It is best practice to implement a calibration menu for your application to allow for these situations. Min Nits The minimum brightness value of the display. If you do not enable Detect Brightness Limits, you must specify a value here and in Max Nits. Max Nits The maximum brightness value of the display. If you do not enable Detect Brightness Limits, you must specify a value here and in Min Nits. Misuse of Hue Shift Amount Creators might author some content with the intention to use Hue Shift Amount to produce special effects. In the illustration below, the Hue Shift Amount is 0 for Image A and 1 for Image B. The flames image B appear more intense because of the hue shift effect. It is preferable not to author content in this way, because settings optimized for special effects can have undesirable effects on other content in the Scene. Image A: Output when Hue Shift Amount is 0. Image B: Output when Hue Shift Amount is 1. ACES This mode has fixed presets to target 1000, 2000, and 4000 nit displays. It is best practice to implement a calibration menu for your application to ensure that the user can select the right preset. Property Description ACES Preset The tone mapper preset to use. The options are: ACES 1000 Nits: The default. This curve targets 1000 nits displays. ACES 2000 Nits: Curve that targets 2000 nits displays. ACES 4000 Nits: Curve that targets 4000 nits displays. Detect Paper White Enable this property if you want URP to use the Paper White value that the display communicates to the Unity Engine. In some cases, the value the display communicates may not be accurate. Implement a calibration menu for your application so that users can display your content correctly on displays that communicate inaccurate values. Paper White The Paper White value of the display. If you do not enable Detect Paper White, you must specify a value here. The HDROutputSettings API The HDROutputSettings API makes it possible to enable and disable HDR mode, as well as query certain values (such as Paper White). Offscreen Rendering When using offscreen rendering techniques, not all cameras in a scene output directly to the display. For example, when Unity is rendering the output to a Render Texture. In these situations, you use the output of the camera before rendering post-processing. Unity does not apply HDR Output processing to the output of cameras which use offscreen rendering techniques. This prevents HDR Output processing being applied twice to the camera's output. SDR Rendering HDR Output relies on HDR Rendering to provide pixel values in the correct format for tone mapping and color encoding. The values after HDR tone mapping are in nits and exceed 1. This differs from SDR Rendering where the pixel values are between 0 and 1. As a result of this, the use of SDR Rendering with HDR Output can cause the rendered image to look underexposed or oversaturated. You can use SDR Rendering on a per-camera basis when you have HDR Output enabled, this can be useful for cameras that only render unlit materials, for example, for mini-map rendering. However, the use of SDR Rendering with HDR Output imposes some limitations. To ensure correct rendering when you use SDR Rendering with HDR Output, you must avoid any render passes that occur after post-processing. This includes URP's built-in effects which insert render passes after post-processing. As a result, SDR Rendering with HDR Output is incompatible with the following features: Upscaling FXAA HDR Debug Views Custom passes which occur after post-processing 2D Renderer To use SDR Rendering with HDR Output on the 2D Renderer, you must ensure post-processing is turned off. HDR Debug Views URP offers three debug views for HDR rendering. To access them, navigate to Window > Analysis > Render Pipeline Debugger > Lighting > HDR Debug Mode. Gamut View The triangles in this debug view indicate which parts of three specific color gamuts this scene covers. The small triangle displays the Rec709 gamut values, the medium triangle displays the P3-D65 gamut values, and the large triangle displays the Rec2020 gamut values. This enables you to check color plot changes while color grading. It can also help you ensure that you benefit from the wider color gamut available in HDR. Gamut Clip This debug view indicates the relationship between scene values and specific color gamuts. Areas of the screen with values within the Rec709 gamut are green, areas outside of the Rec709 gamut but inside the P3-D65 gamut are blue, and areas outside of both are red. Values exceeding Paper White This debug view uses a color coded gradient to indicate parts of the Scene that exceed the Paper White value. The gradient ranges from yellow to red. Yellow corresponds to Paper White +1, and red corresponds to Max Nits. Platform Compatibility URP only supports HDR Output on the following platforms: Windows with DirectX 11, DirectX 12 or Vulkan MacOS with Metal Consoles XR devices with HDR support Note: DirectX 11 only supports HDR Output in the Player, it does not support HDR Output in the Editor."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/post-processing-custom-effect-low-code.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/post-processing/post-processing-custom-effect-low-code.html",
    "title": "How to create a custom post-processing effect | FSM Unity Framework",
    "keywords": "How to create a custom post-processing effect The example on this page shows how to use a Full Screen Render Pass to create a grayscale custom post-processing effect. Prerequisites This example requires the following: A Unity project with the URP package installed. The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). Create a Fullscreen Shader Graph You must create a Fullscreen Shader Graph to create a custom post-processing effect. Create a new Shader Graph in your Project. To do this right-click in the Project window and select Create > Shader Graph > URP > Fullscreen Shader Graph. Add a URP Sample Buffer node. To do this right-click in the Shader Graph window, and select Create Node. Then locate and select URP Sample Buffer. In the URP Sample Buffer node's Source Buffer dropdown menu, select BlitSource. Add a Vector 3 node. Assign the Vector 3 node the following values: X = 0.2126 Y = 0.7152 Z = 0.0722 Add a Dot Product node. Connect the nodes as shown below. Node Connection URP Sample Buffer Output to Dot Product A Vector 3 Out to Dot Product B Dot Product Out to Fragment Base Color Save your Shader Graph. Create a new Material in your Project. To do this right-click in the Project window and select Create > Material. Apply the Shader Graph shader to the Material. To do this, open the Material in the Inspector and select Shader > Shader Graphs, then select the Shader Graph you created in the previous steps. Use the Material in a Full Screen Pass Renderer Feature Once you've created a compatible Shader Graph and Material, you can use the Material with a Full Screen Pass Renderer Feature to create a custom post-processing effect. In the Project window, select a URP Renderer. In the Inspector, click Add Renderer Feature and select Full Screen Pass Renderer Feature. For more information on adding Renderer Features see How to add a Renderer Feature to a Renderer. Set the Post Process Material to the Material you created with the Fullscreen Shader Graph. Set Injection Point to After Rendering Post Processing. Set Requirements to Color. You should now see the effect in both Scene view and Game view. Example scene with a grayscale custom post-processing effect."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/quality/quality-settings-through-code.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/quality/quality-settings-through-code.html",
    "title": "Control URP Quality settings through code | FSM Unity Framework",
    "keywords": "Control URP Quality settings through code Unity has several preset levels of Quality settings and you might add more to your project. To accommodate different hardware specifications, you can switch between these levels and the associated URP Asset from C# scripts. The following examples show how to use API to change Quality setting levels and the active URP Asset, and how to change specific settings in the URP Asset at runtime. Note: You should only change Quality settings and URP Asset settings at runtime at points where performance is not essential, such as during loading screens or on static menus. This is because these changes cause a temporary but significant performance impact. Change URP Asset at runtime Each quality level uses a URP Asset to control many of the specific graphics settings. You can assigning different URP Assets to each quality level and switch between them at runtime. Configure Project Quality settings To use Quality settings to switch between URP Assets, ensure that the quality levels of your project are configured to use different URP Assets. The URP 3D Sample scene has this configuration by default. Create a URP Asset for each quality level. To do this, right-click in the Project window and select Create > Rendering > URP Asset (with Universal Renderer). Note: These instructions are also valid for URP Assets that use the 2D Renderer. Configure and name the new URP Assets as necessary. Open the Quality section in the Project Settings (Edit > Project Settings > Quality). Assign each URP Asset to a quality level. To do this, select a quality level from the Levels list, then go to Rendering > Render Pipeline Asset and choose the URP Asset you created for this quality level. Do this for each quality level. The quality levels of your project are now ready to be used to change between URP Assets at runtime. Change Quality Level You can change the quality level Unity uses at runtime through the QualitySettings API. With the quality levels setup as shown previously, this enables you to switch between URP Assets as well as Quality settings presets. In the following simple example, the C# script uses the system's total Graphics Memory to determine the appropriate quality level without any input from the user when they open the built project. Create a new C# script with the name QualityControls. Open the QualityControls script and add the SwitchQualityLevel method to the QualityControls class. using System.Collections; using System.Collections.Generic; using UnityEngine; public class QualityControls : MonoBehaviour { void Start() { } private void SwitchQualityLevel() { } } Add a switch statement in the SwitchQualityLevel method to select the quality level with the QualitySettings.SetQualityLevel() method as shown below. Note: Each Quality level has an index that matches the level's position in the list in the Quality section of the Project Settings window. The quality level at the top of the list has an index of 0. This index only counts quality levels which you specified as enabled for the target platform of any built version of your project. using System.Collections; using System.Collections.Generic; using UnityEngine; public class QualityControls : MonoBehaviour { void Start() { } private void SwitchQualityLevel() { // Select Quality settings level (URP Asset) based on the size of the device's graphics memory switch (SystemInfo.graphicsMemorySize) { case <= 2048: QualitySettings.SetQualityLevel(1); break; case <= 4096: QualitySettings.SetQualityLevel(2); break; default: QualitySettings.SetQualityLevel(0); break; } } } Add a call to the SwitchQualityLevel method in the Start method. This ensures that the quality level only changes when the scene first loads. using System.Collections; using System.Collections.Generic; using UnityEngine; public class QualityControls : MonoBehaviour { void Start() { SwitchQualityLevel(); } private void SwitchQualityLevel() { // Select Quality settings level (URP Asset) based on the size of the device's graphics memory switch (SystemInfo.graphicsMemorySize) { case <= 2048: QualitySettings.SetQualityLevel(1); break; case <= 4096: QualitySettings.SetQualityLevel(2); break; default: QualitySettings.SetQualityLevel(0); break; } } } Open the first scene that your built project loads on startup. Create an empty GameObject and call it QualityController. To do this, right-click in the Hierarchy Window and select Create Empty. Open the QualityController object in the Inspector. Add the QualityControls script to the QualityController as a component. Now when this scene loads, Unity runs the SwitchQualityLevel method in the QualityControls script which detects the system's total graphics memory and sets the quality level. The quality level sets the URP Asset as the active Render Pipeline Asset. You can create more complex systems and sequences of checks to determine which quality level to use, but the fundamental process remains the same. When the project starts, run a script which uses QualitySettings.SetQualityLevel to select a quality level and through that select the URP Asset for the project to use at runtime. Change URP Asset settings You can change some properties of the URP Asset at runtime with C# scripts. This can help fine tune performance on devices with hardware that doesn't perfectly match any of the quality levels in your project. Note: To change a property of the URP Asset with a C# script, the property must have a set method. For more information on these properties see Accessible Properties. The following example uses the QualityControls script and QualityController object from the Change Quality Level through code section, and extends the functionality to locate the active URP Asset and change some of its properties to fit the performance level of the hardware. Open the QualityControls script. At the top of the script add using UnityEngine.Rendering and using UnityEngine.Rendering.Universal. Add a method with the name ChangeAssetProperties and the type void to the QualityControls class as shown below. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class QualityController : MonoBehaviour { void Start() { // Select the appropriate Quality Level first SwitchQualityLevel(); } private void SwitchQualityLevel() { // Code from previous example } private void ChangeAssetProperties() { // New code is added to this method } } Retrieve the active Render Pipeline Asset with GraphicsSettings.currentRenderPipeline as shown below. Note: You must use the as keyword to cast the Render Pipeline Asset as the UniversalRenderPipelineAsset type for the script to work correctly. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class QualityController : MonoBehaviour { void Start() { // Select the appropriate Quality Level first SwitchQualityLevel(); } private void SwitchQualityLevel() { // Code from previous example } private void ChangeAssetProperties() { // Locate the current URP Asset UniversalRenderPipelineAsset data = GraphicsSettings.currentRenderPipeline as UniversalRenderPipelineAsset; // Do nothing if Unity can't locate the URP Asset if (!data) return; } } Add a switch statement in the ChangeAssetProperties method to set the value of the URP Asset properties. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class QualityController : MonoBehaviour { void Start() { // Select the appropriate Quality Level first SwitchQualityLevel(); } private void SwitchQualityLevel() { // Code from previous example } private void ChangeAssetProperties() { // Locate the current URP Asset UniversalRenderPipelineAsset data = GraphicsSettings.currentRenderPipeline as UniversalRenderPipelineAsset; // Do nothing if Unity can't locate the URP Asset if (!data) return; // Change URP Asset settings based on the size of the device's graphics memory switch (SystemInfo.graphicsMemorySize) { case <= 1024: data.renderScale = 0.7f; data.shadowDistance = 50.0f; break; case <= 3072: data.renderScale = 0.9f; data.shadowDistance = 150.0f; break; default: data.renderScale = 0.7f; data.shadowDistance = 25.0f; break; } } } Add a call to the ChangeAssetProperties method in the Start method. This ensures that the URP Asset only changes when the scene first loads. using System.Collections; using System.Collections.Generic; using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class QualityController : MonoBehaviour { void Start() { // Select the appropriate Quality Level first SwitchQualityLevel(); // Fine tune performance with specific URP Asset properties ChangeAssetProperties(); } private void SwitchQualityLevel() { // Code from previous example } private void ChangeAssetProperties() { // Locate the current URP Asset UniversalRenderPipelineAsset data = GraphicsSettings.currentRenderPipeline as UniversalRenderPipelineAsset; // Do nothing if Unity can't locate the URP Asset if (!data) return; // Change URP Asset settings based on the size of the device's graphics memory switch (SystemInfo.graphicsMemorySize) { case <= 1024: data.renderScale = 0.7f; data.shadowDistance = 50.0f; break; case <= 3072: data.renderScale = 0.9f; data.shadowDistance = 150.0f; break; default: data.renderScale = 0.7f; data.shadowDistance = 25.0f; break; } } } Now when this scene loads, Unity detects the system's total graphics memory and sets the URP Asset properties accordingly. You can use this method of changing particular URP Asset properties in conjunction with changing quality levels to fine tune the performance of your project for different systems without the need to create a quality level for every target hardware configuration. Accessible Properties You can access and change any properties of the URP Asset which have a set method through a C# script at runtime. The following properties of the URP Asset have a set method: cascadeBorder colorGradingLutSize colorGradingMode conservativeEnclosingSphere fsrOverrideSharpness fsrSharpness hdrColorBufferPrecision maxAdditionalLightsCount msaaSampleCount numIterationsEnclosingSphere renderScale shadowCascadeCount shadowDepthBias shadowDistance shadowNormalBias storeActionsOptimization supportsCameraDepthTexture supportsCameraOpaqueTexture supportsDynamicBatching supportsHDR upscalingFilter useAdaptivePerformance useSRPBatcher For more information on these properties, see Universal Render Pipeline Asset API."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/realtime-lighting-in-universalrp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/realtime-lighting-in-universalrp.html",
    "title": "| FSM Unity Framework",
    "keywords": "Important: This page is still a work in progress. To read our most current documentation, open the TableOfContents.md file to see the linked pages."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-feature-decal.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-feature-decal.html",
    "title": "Decal Renderer Feature | FSM Unity Framework",
    "keywords": "Decal Renderer Feature With the Decal Renderer Feature, Unity can project specific Materials (decals) onto other objects in the Scene. The decals interact with the Scene's lighting and wrap around Meshes. Sample scene without decals Sample scene with decals. The decals hide the seams between materials and add artistic details. For examples of how to use Decals, see the Decals samples in URP Package Samples. How to use the feature To add decals to your Scene: Add the Decal Renderer Feature to the URP Renderer. Create a Material, and assign it the Shader Graphs/Decal shader. In the Material, select the Base Map and the Normal Map. Create a new Decal Projector GameObject, or add a Decal Projector component to an existing GameObject. The following illustration shows a Decal Projector in the Scene. For more information, see also Decal Projector component. An alternative way to add decals to a Scene: Create a Quad GameObject. Assign a Decal Material to the GameObject. Position the Quad on the surface where you want the decal to be. If necessary, adjust the mesh bias value to prevent z-fighting. Limitations This feature has the following limitations: The decal projection does not work on transparent surfaces. Decal Renderer Feature properties This section describes the properties of the Decal Renderer Feature. Decal Renderer Feature, Inspector view. Technique Select the rendering technique for the Renderer Feature. This section describes the options in this property. Automatic Unity selects the rendering technique automatically based on the build platform. The Accurate G-buffer normals option is also taken into account, as it prevents normal blending from working correctly without the D-Buffer technique. DBuffer Unity renders decals into the Decal buffer (DBuffer). Unity overlays the content of the DBuffer on top of the opaque objects during the opaque rendering. Selecting this technique reveals the Surface Data property. The Surface Data property lets you specify which surface properties of decals Unity blends with the underlying meshes. The Surface Data property has the following options: Albedo: decals affect the base color and the emission color. Albedo Normal: decals affect the base color, the emission color, and the normals. Albedo Normal MAOS: decals affect the base color, the emission color, the normals, the metallic values, the smoothness values, and the ambient occlusion values. Limitations: This technique requires the DepthNormal prepass, which makes the technique less efficient on GPUs that implement tile-based rendering. This technique does not work on particles and terrain details. Screen Space Unity renders decals after the opaque objects using normals that Unity reconstructs from the depth texture, or from the G-Buffer when using the Deferred rendering path. Unity renders decals as meshes on top of the opaque meshes. This technique supports only the normal blending. When using the Deferred rendering path with Accurate G-buffer normals, blending of normals is not supported, and will yield incorrect results. Screen space decals are recommended for mobile platforms that use tile-based rendering, because URP doesn't create a DepthNormal prepass unless you enable Use Rendering Layers. Selecting this technique reveals the following properties. Property Description Normal Blend The options in this property (Low, Medium, and High) determine the number of samples of the depth texture that Unity takes when reconstructing the normal vector from the depth texture. The higher the quality, the more accurate the reconstructed normals are, and the higher the performance impact is. Low Unity takes one depth sample when reconstructing normals. Medium Unity takes three depth samples when reconstructing normals. High Unity takes five depth samples when reconstructing normals. Max Draw Distance The maximum distance from the Camera at which Unity renders decals. Use Rendering Layers Select this check box to enable the Rendering Layers functionality. If you enable Use Rendering Layers, URP creates a DepthNormal prepass. This makes decals less efficient on GPUs that implement tile-based rendering. Decal Projector component The Decal Projector component lets Unity project decals onto other objects in the Scene. A Decal Projector component must use a Material with the Decal Shader Graph assigned (Shader Graphs/Decal). For more information on how to use the Decal Projector, see section How to use the feature. The Decal Projector component contains the Scene view editing tools and the Decal Projector properties. Decal Projector component in the Inspector. NOTE: If you assign a Decal Material to a GameObject directly (not via a Decal Projector component), then Decal Projectors do not project decals on such GameObject. Decal Scene view editing tools When you select a Decal Projector, Unity shows its bounds and the projection direction. The Decal Projector draws the decal Material on every Mesh inside the bounding box. The white arrow shows the projection direction. The base of the arrow is the pivot point. The Decal Projector component provides the following Scene view editing tools. Icon Action Description Scale Select to scale the projector box and the decal. This tool changes the UVs of the Material to match the size of the projector box. The tool does not affect the pivot point. Crop Select to crop or tile the decal with the projector box. This tool changes the size of the projector box but not the UVs of the Material. The tool does not affect the pivot point. Pivot / UV Select to move the pivot point of the decal without moving the projection box. This tool changes the transform position. This tool also affects the UV coordinates of the projected texture. Decal Projector component properties This section describes the Decal Projector component properties. Property Description Scale Mode Select whether this Decal Projector inherits the Scale values from the Transform component of the root GameObject. Options: • Scale Invariant: Unity uses the scaling values (Width, Height, etc.) only in this component, and ignores the values in the root GameObject. • Inherit from Hierarchy: Unity evaluates the scaling values for the decal by multiplying the lossy Scale values of the Transform of the root GameObject by the Decal Projector's scale values. Note: since the Decal Projector uses the orthogonal projection, if the root GameObject is skewed, the decal does not scale correctly. Width The width of the projector bounding box. The projector scales the decal to match this value along the local X axis. Height The height of the projector bounding box. The projector scales the decal to match this value along the local Y axis. Projection Depth The depth of the projector bounding box. The projector projects decals along the local Z axis. Pivot The offset position of the center of the projector bounding box relative to the origin of the root GameObject. Material The Material to project. The Material must use a Shader Graph that has the Decal Material type. For more information, see the page Decal Shader Graph. Tiling The tiling values for the decal Material along its UV axes. Offset The offset values for the decal Material along its UV axes. Opacity This property lets you specify the opacity value. A value of 0 makes the decal fully transparent, a value of 1 makes the decal as opaque as defined by the Material. Draw Distance The distance from the Camera to the Decal at which this projector stops projecting the decal and URP no longer renders the decal. Start Fade Use the slider to set the distance from the Camera at which the projector begins to fade out the decal. Values from 0 to 1 represent a fraction of the Draw Distance. With a value of 0.9, Unity starts fading the decal out at 90% of the Draw Distance and finishes fading it out at the Draw Distance. Angle Fade Use the slider to set the fade out range of the decal based on the angle between the decal's backward direction and the vertex normal of the receiving surface. Performance Decals do not support the SRP Batcher by design because they use Material property blocks. To reduce the number of draw calls, decals can be batched together using GPU instancing. If the decals in your Scene use the same Material, and if the Material has the Enable GPU Instancing property turned on, Unity instances the Materials and reduces the number of draw calls. To reduce the number of Materials necessary for decals, put multiple decal textures into one texture (atlas). Use the UV offset properties on the decal projector to determine which part of the atlas to display. The following image shows an example of a decal atlas. left: decal atlas with four decals. Right: a decal projector is projecting one of them. If the decal Material has GPU instancing enabled, any instance of the four decals is rendered in a single instanced draw call."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-feature-screen-space-shadows.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-feature-screen-space-shadows.html",
    "title": "Screen Space Shadows Renderer Feature | FSM Unity Framework",
    "keywords": "Screen Space Shadows Renderer Feature The Screen Space Shadows Renderer Feature calculates screen-space shadows for opaque objects affected by the main directional light and draws them in the scene. To render screen-space shadows, URP requires an additional render target. This increases the amount of memory your application requires, but if your project uses forward rendering, screen-space shadows can benefit the runtime resource intensity. This is because if you use screen-space shadows, URP doesn't need to access the cascade shadow maps multiple times. Screen-space shadows in a sample Scene. The screen-space shadows texture for the above image. Enabling screen-space shadows To add screen space shadows to your project, add the Screen Space Shadows Renderer Feature to the URP Renderer. Viewing screen-space shadows in the Frame Debugger After you enable this Renderer Feature, URP renders screen-space shadows in your scene. To distinguish between shadow map shadows and screen-space shadows, you can view the render passes that draws the shadows in the Frame Debugger. Screen Space Shadows pass in frame debugger. You can compare shadows cast on opaque objects from the screen-space shadow texture or the cascade shadow maps. The Frame Debugger shows the screen-space shadows texture. The Frame Debugger shows shadows from a shadow map. Requirements and compatibility This Renderer Feature uses a depth texture and invokes a depth prepass before it draws opaque objects. It calculates the shadows in screen space before the DrawOpaqueObjects render pass. URP doesn't calculate or apply screen-space shadows for transparent objects; it uses shadow maps for transparent objects instead. The Frame Debugger showing that Unity uses shadow maps for transparent objects and screen-space shadows for opaque objects."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/create-custom-renderer-feature.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/create-custom-renderer-feature.html",
    "title": "How to create a custom Renderer Feature | FSM Unity Framework",
    "keywords": "How to create a custom Renderer Feature This section describes how to create a custom Renderer Feature for a URP Renderer. This section assumes the following: The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). This article contains the following sections: Create example Scene and GameObjects. Create a scriptable Renderer Feature and add it to the Universal Renderer. Create and enqueue the scriptable Render Pass. Implement rendering commands in the Execute method. Implement the example-specific Material and rendering code. Change the order of the render passes Complete code for this example Create example Scene and GameObjects To follow the steps in this section, create a new Scene with the following GameObjects: Create a plane. Create a new Material and assign it the Universal Render Pipeline/Lit shader. Set the base color to grey (for example, #6A6A6A). Call the Material Plane. Create a Point Light and place it above the plane. Your Scene should look like the following illustration: Create a scriptable Renderer Feature and add it to the Universal Renderer This part shows how to create a scriptable Renderer Feature and implement the methods that let you configure and inject ScriptableRenderPass instances into the scriptable Renderer. Create a new C# script. Call the script LensFlareRendererFeature.cs. Open the script, remove all the code from the LensFlareRendererFeature class that Unity created. Add the following using directive. using UnityEngine.Rendering.Universal; The LensFlareRendererFeature class must inherit from the ScriptableRendererFeature class. public class LensFlareRendererFeature : ScriptableRendererFeature The class must implement the following methods: Create: Unity calls this method on the following events: When the Renderer Feature loads the first time. When you enable or disable the Renderer Feature. When you change a property in the inspector of the Renderer Feature. AddRenderPasses: Unity calls this method every frame, once for each Camera. This method lets you inject ScriptableRenderPass instances into the scriptable Renderer. Now you have the custom LensFlareRendererFeature Renderer Feature with its main methods. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { public override void Create() { } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { } } Add the Renderer Feature you created to the the Universal Renderer asset. Follow this link to read how to add a Renderer Feature to a Renderer. Add the Lens Flare Renderer Feature to the Universal Renderer. Create and enqueue the scriptable Render Pass This part shows how to create a scriptable Render Pass and and enqueue its instance into the scriptable Renderer. In the LensFlareRendererFeature class, declare the LensFlarePass class that inherits from ScriptableRenderPass. class LensFlarePass : ScriptableRenderPass { } In LensFlarePass, add the Execute method. Unity runs the Execute method every frame. In this method, you can implement your custom rendering functionality. public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { } In the LensFlareRendererFeature class, declare a private LensFlarePass field. private LensFlarePass _lensFlarePass; In the Create method, instantiate the _lensFlarePass object: _lensFlarePass = new LensFlarePass(FlareSettings); In the AddRenderPasses method, use the EnqueuePass method of the renderer object to enqueue _lensFlarePass in the rendering queue. renderer.EnqueuePass(_lensFlarePass); Now your custom LensFlareRendererFeature Renderer Feature is executing the Execute method inside the custom LensFlarePass pass. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { Debug.Log(message: \"The Execute() method runs.\"); } } private LensFlarePass _lensFlarePass; public override void Create() { _lensFlarePass = new LensFlarePass(); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { renderer.EnqueuePass(_lensFlarePass); } } Implement rendering commands in the Execute method This part shows how to implement custom logic in the Execute method. Create a CommandBuffer type object. This object holds the list of rendering commands to execute. In the Execute method, add the following line: CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); The method CommandBufferPool.Get(name: \"LensFlarePass\") gets the new command buffer and assigns a name to it. Add the line that executes the command buffer and the line that releases it. In the Execute method, add the following lines after the command buffer declaration: context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); Now the boilerplate part is ready and we can proceed to implementing the custom rendering logic. The following steps implement the custom rendering logic. In this example, the Renderer Feature draws lens flares as a texture on a Quad. The implementation requires a Material and a mesh (Quad). In the LensFlarePass class, declare two private fields: Material and Mesh: private Material _material; private Mesh _mesh; Then declare the constructor that takes those variables as arguments: public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } Now the LensFlarePass class expects two arguments. To initialize the class with the arguments, add the following public fields in the LensFlareRendererFeature class: public Material material; public Mesh mesh; And add the arguments to the LensFlarePass declaration in the Create method: _lensFlarePass = new LensFlarePass(material, mesh); In the Execute method, use the DrawMesh method of the cmd object. The method takes the _material and the _mesh fields as arguments. Add the following line between the cmd object declaration and the command context.ExecuteCommandBuffer(cmd). cmd.DrawMesh(_mesh, Matrix4x4.identity, _material); To ensure that Unity does call the DrawMesh method with null arguments, in the AddRenderPasses method, wrap the EnqueuePass call in the null check condition: if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } Now the LensFlarePass class has the following basic logic in the Execute method: Get the new command buffer and assign it the name LensFlarePass. Add rendering commands. Execute the command buffer. Release the buffer. NOTE: Unity does not enqueue the LensFlarePass pass yet, because the Material and the Mesh properties are null. Below is the complete code for this part. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { private Material _material; private Mesh _mesh; public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); cmd.DrawMesh(_mesh, Matrix4x4.identity, _material); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } private LensFlarePass _lensFlarePass; public Material material; public Mesh mesh; public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } } } Implement the example-specific Material and rendering code This section shows how to create a Material for the lens flare effect and how to implement the code to render flares at the positions of Lights. Create a new Material, and assign it the Universal Render Pipeline/Unlit shader. Call the Material LensFlare. For demonstration purpose, change the base color of the Material to red. In the Universal Renderer, in Lens Flare Renderer Feature, select the LensFlare Material in the Material property, and the Quad mesh in the Mesh property. The Renderer Feature draws the quad in the Scene, but at this point it's just black. This is because the Universal Render Pipeline/Unlit shader has multiple passes, and one of them paints the quad black. To change this behavior, use the cmd.DrawMesh method overload that accepts the shaderPass argument, and specify shader pass 0: cmd.DrawMesh(_mesh, Matrix4x4.identity, _material, 0, 0); The following steps show the changes that are specific to the effect implementation in this example. They are for illustrative purposes. Add the following lines in the Execute method. Place them after the cmd object declaration. These lines ensure that Unity draws the quad with the flare in the following way: In the screen space. With the correct aspect ratio. For each Light, in the center of the Light. // Get the Camera data from the renderingData argument. Camera camera = renderingData.cameraData.camera; // Set the projection matrix so that Unity draws the quad in screen space cmd.SetViewProjectionMatrices(Matrix4x4.identity, Matrix4x4.identity); // Add the scale variable, use the Camera aspect ratio for the y coordinate Vector3 scale = new Vector3(1, camera.aspect, 1); // Draw a quad for each Light, at the screen space position of the Light. foreach (VisibleLight visibleLight in renderingData.lightData.visibleLights) { Light light = visibleLight.light; // Convert the position of each Light from world to viewport point. Vector3 position = camera.WorldToViewportPoint(light.transform.position) * 2 - Vector3.one; // Set the z coordinate of the quads to 0 so that Uniy draws them on the same plane. position.z = 0; // Change the Matrix4x4 argument in the cmd.DrawMesh method to use the position and // the scale variables. cmd.DrawMesh(_mesh, Matrix4x4.TRS(position, Quaternion.identity, scale), _material, 0, 0); } Now Unity draws a quad in the center of each Light. To visualize the lens flare, make the following changes to the LensFlare Material. Add the following texture to the base map: Set the color to white. Set Surface Type to Transparent. Set Blending Mode to Additive. Now Unity draws the lens flare texture on the quad, but a part of the flare is not visible: This is because Unity draws the skybox after the LensFlarePass render pass. Change the order of the render passes To see the order in which Unity draws the render passes, open the Frame Debugger (Window > Analysis > Frame Debugger). To enqueue the LensFlarePass pass after the skybox pass, use the renderPassEvent property of LensFlarePass. Assign the property the AfterRenderingSkybox event from the RenderPassEvent enum. Make the following changes in the Create method: public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); // Draw the lens flare effect after the skybox. _lensFlarePass.renderPassEvent = RenderPassEvent.AfterRenderingSkybox; } Now Unity draws the lens flare on top of the skybox. Complete code for this example Below is the complete code for this example. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class LensFlareRendererFeature : ScriptableRendererFeature { class LensFlarePass : ScriptableRenderPass { private Material _material; private Mesh _mesh; public LensFlarePass(Material material, Mesh mesh) { _material = material; _mesh = mesh; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(name: \"LensFlarePass\"); // Get the Camera data from the renderingData argument. Camera camera = renderingData.cameraData.camera; // Set the projection matrix so that Unity draws the quad in screen space. cmd.SetViewProjectionMatrices(Matrix4x4.identity, Matrix4x4.identity); // Add the scale variable, use the Camera aspect ratio for the y coordinate Vector3 scale = new Vector3(1, camera.aspect, 1); // Draw a quad for each Light, at the screen space position of the Light. foreach (VisibleLight visibleLight in renderingData.lightData.visibleLights) { Light light = visibleLight.light; // Convert the position of each Light from world to viewport point. Vector3 position = camera.WorldToViewportPoint(light.transform.position) * 2 - Vector3.one; // Set the z coordinate of the quads to 0 so that Uniy draws them on the same // plane. position.z = 0; // Change the Matrix4x4 argument in the cmd.DrawMesh method to use // the position and the scale variables. cmd.DrawMesh(_mesh, Matrix4x4.TRS(position, Quaternion.identity, scale), _material, 0, 0); } context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } private LensFlarePass _lensFlarePass; public Material material; public Mesh mesh; public override void Create() { _lensFlarePass = new LensFlarePass(material, mesh); // Draw the lens flare effect after the skybox. _lensFlarePass.renderPassEvent = RenderPassEvent.AfterRenderingSkybox; } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (material != null && mesh != null) { renderer.EnqueuePass(_lensFlarePass); } } }"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/how-to-custom-effect-render-objects.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/how-to-custom-effect-render-objects.html",
    "title": "Example: How to create a custom rendering effect using the Render Objects Renderer Feature | FSM Unity Framework",
    "keywords": "Example: How to create a custom rendering effect using the Render Objects Renderer Feature URP draws objects in the DrawOpaqueObjects and DrawTransparentObjects passes. You might need to draw objects at a different point in the frame rendering, or interpret and write rendering data (like depth and stencil) in alternate ways. The Render Objects Renderer Feature lets you do such customizations by letting you draw objects on a certain layer, at a certain time, with specific overrides. The example on this page describes how to create a custom rendering effect with the Render Objects Renderer Feature. Example overview The example on this page demonstrates how to implement the following effect: There is a character in the Scene. When the character goes behind GameObjects, Unity draws the character silhouette with a different Material. Prerequisites This example requires the following: A Unity project with the URP package installed. The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). Create example Scene and GameObjects To follow the steps in this example, create a new Scene with the following GameObjects: Create a Cube. Set its Scale values so that it looks like a wall. Create a Material and assign it the Universal Render Pipeline/Lit shader. Select the base color (for example, red). Call the Material Character. Create a basic character and assign it the Character Material. In this example, the character consists of three capsules: the big capsule in the center represents the body, and the two smaller capsules represent the hands. To make it easier to manipulate the character in the Scene, add the three Capsules as child GameObjects under the Character GameObject. Create a Material and assign it the Universal Render Pipeline/Unlit shader. Select the base color that you would like the character to have when it's behind GameObjects (for example, blue). Call the Material CharacterBehindObjects. Now you have the setup necessary to follow the steps in this example. Example implementation This section assumes that you created a Scene as described in section Example Scene and GameObjects. The example implementation uses two Render Objects Renderer Features: one to draw parts of the character that are behind other GameObjects, and another one to draw the parts of the character that are in front of other GameObjects. Create a Renderer Feature to draw the character behind GameObjects Follow these steps to create a Renderer Feature to draw the character behind GameObjects. Select a URP Renderer. In the Inspector, click Add Renderer Feature and select Render Objects. Select the Name field and enter the name of the new Renderer Feature, for example, DrawCharacterBehind. This example uses Layers to filter the GameObjects to render. Create a new Layer and call it Character. Select the Character GameObject and assign it to the Character Layer. To do this, open the Layer drop down and select Character. In the DrawCharacterBehind Renderer Feature, in Filters > Layer Mask, select Character. With this setting, this Renderer Feature renders GameObjects only in the Layer Character. In Overrides > Material, select the CharacterBehindObjects Material. The Renderer Feature overrides the Material of a GameObject with the selected Material. The intended behavior is that the Renderer Feature renders the character with the CharacterBehindObjects Material only when the character is behind other GameObjects. To achieve this, select the Depth check box, and set the Depth Test property to Greater. With these settings, Unity renders the character with the CharacterBehindObjects Material only when the character is behind another GameObject. However, Unity also renders parts of the character using the CharacterBehindObjects Material, because some parts of the character occlude the character itself. Create an extra Renderer Feature to avoid the self see-through effect The settings in the previous section result in the self see-through effect for the following reason: When performing the Opaque rendering pass of the URP Renderer, Unity renders all GameObjects belonging to the character with the Character Material and writes depth values to the Depth buffer. This happens before Unity starts executing the DrawCharacterBehind Renderer Feature, because, by default, new Render Objects Renderer Features have the value AfterRenderingOpaques in the Event property. The Event property defines the injection point where Unity injects Render Passes from the Render Objects Renderer Feature. The event when URP Renderer draws GameObjects in the Opaque Layer Mask is the BeforeRenderingOpaques event. When executing the DrawCharacterBehind Renderer Feature, Unity performs the depth test using the condition specified in the Depth Test property. In the following screenshot, a bigger capsule occludes part of the smaller capsule, and the depth test passes for that part of the smaller capsule. The Renderer Feature overrides the Material for that part. The following steps describe how to avoid such behavior and ensure that Unity draws all parts of the character with proper Materials. In the URP asset, in Filtering > Opaque Layer Mask, clear the check mark next to the Character Layer. Now Unity does not render the character unless it's behind a GameObject. Add a new Render Objects Renderer Feature, and call it Character. In the Character Renderer Feature, in Filters > Layer Mask, select the Character Layer. Now Unity renders the character with the Character Material even when the character is behind GameObjects. This happens because the DrawCharacterBehind Renderer Feature writes values to the depth buffer. When Unity executes the Character Renderer Feature, the pixels on the character appear to be in front of the pixels that Unity has drawn previously, and Unity draws on top of those pixels. In the DrawCharacterBehind Renderer Feature, In Overrides > Depth, clear the Write Depth check box. With this setting, the DrawCharacterBehind Renderer Feature does not make changes to the depth buffer and the Character Renderer Feature does not draw the character when it's behind GameObjects. The example is complete. When the character goes behind GameObjects, Unity draws the character silhouette with the CharacterBehindObjects Material. With the extra Character Renderer Feature, Unity renders GameObjects as follows: URP Renderer does not render the Character GameObject in the BeforeRenderingOpaques event, because the Character Layer is excluded from the Opaque Layer Mask list. The DrawCharacterBehind Renderer Feature draws parts of the character that are behind other GameObjects. This happens in the AfterRenderingOpaques event. The Character Renderer Feature draws parts of the character that are in front of other GameObjects. This happens in the AfterRenderingOpaques event, and after executing the DrawCharacterBehind Renderer Feature."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/how-to-fullscreen-blit.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/how-to-fullscreen-blit.html",
    "title": "Perform a full screen blit in URP | FSM Unity Framework",
    "keywords": "Perform a full screen blit in URP The example on this page describes how to create a custom Renderer Feature that performs a full screen blit. Example overview This example implements the following solution: A custom Renderer Feature calls a custom Render Pass. The Render Pass blits the Opaque Texture to the the Camera color target for the current renderer. The render pass uses the command buffer to draw a full screen mesh for both eyes. The example includes the shader that performs the GPU side of the rendering. The shader samples the color buffer using XR sampler macros. Prerequisites This example requires the following: The Scriptable Render Pipeline Settings property refers to a URP asset (Project Settings > Graphics > Scriptable Render Pipeline Settings). Create example Scene and GameObjects To follow the steps in this example, create a new Scene with the following GameObjects: Create a Cube. Ensure that the Cube is clearly visible from the main Camera. Now you have the Scene necessary to follow the steps in this example. Example implementation This section assumes that you created a Scene as described in section Create example Scene and GameObjects. Follow these steps to create a custom Renderer Feature with a custom Render Pass. Create a new C# script. Call it ColorBlitRendererFeature.cs. This script implements the custom Renderer Feature. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; internal class ColorBlitRendererFeature : ScriptableRendererFeature { public Shader m_Shader; public float m_Intensity; Material m_Material; ColorBlitPass m_RenderPass = null; public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (renderingData.cameraData.cameraType == CameraType.Game) renderer.EnqueuePass(m_RenderPass); } public override void SetupRenderPasses(ScriptableRenderer renderer, in RenderingData renderingData) { if (renderingData.cameraData.cameraType == CameraType.Game) { // Calling ConfigureInput with the ScriptableRenderPassInput.Color argument // ensures that the opaque texture is available to the Render Pass. m_RenderPass.ConfigureInput(ScriptableRenderPassInput.Color); m_RenderPass.SetTarget(renderer.cameraColorTargetHandle, m_Intensity); } } public override void Create() { m_Material = CoreUtils.CreateEngineMaterial(m_Shader); m_RenderPass = new ColorBlitPass(m_Material); } protected override void Dispose(bool disposing) { CoreUtils.Destroy(m_Material); } } Create a new C# script. Call it ColorBlitPass.cs. This script implements the custom Render Pass that performs the custom blit draw call. This Render Pass uses the Blitter.BlitCameraTexture method to draw a full-screen quad and perform the blit operation. NOTE: Do not use the cmd.Blit method in URP XR projects because that method has compatibility issues with the URP XR integration. Using cmd.Blit might implicitly enable or disable XR shader keywords, which breaks XR SPI rendering. using UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; internal class ColorBlitPass : ScriptableRenderPass { ProfilingSampler m_ProfilingSampler = new ProfilingSampler(\"ColorBlit\"); Material m_Material; RTHandle m_CameraColorTarget; float m_Intensity; public ColorBlitPass(Material material) { m_Material = material; renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public void SetTarget(RTHandle colorHandle, float intensity) { m_CameraColorTarget = colorHandle; m_Intensity = intensity; } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { ConfigureTarget(m_CameraColorTarget); } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { var cameraData = renderingData.cameraData; if (cameraData.camera.cameraType != CameraType.Game) return; if (m_Material == null) return; CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, m_ProfilingSampler)) { m_Material.SetFloat(\"_Intensity\", m_Intensity); Blitter.BlitCameraTexture(cmd, m_CameraColorTarget, m_CameraColorTarget, m_Material, 0); } context.ExecuteCommandBuffer(cmd); cmd.Clear(); CommandBufferPool.Release(cmd); } } Create the shader that performs the blit operation. Call the shader file ColorBlit.shader. The vertex function outputs the full-screen quad position. The fragment function samples the color buffer and returns the color * float4(0, _Intensity, 0, 1) value to the render target. Shader \"ColorBlit\" { SubShader { Tags { \"RenderType\"=\"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\"} LOD 100 ZWrite Off Cull Off Pass { Name \"ColorBlitPass\" HLSLPROGRAM #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" // The Blit.hlsl file provides the vertex shader (Vert), // input structure (Attributes) and output strucutre (Varyings) #include \"Packages/com.unity.render-pipelines.core/Runtime/Utilities/Blit.hlsl\" #pragma vertex Vert #pragma fragment frag TEXTURE2D_X(_CameraOpaqueTexture); SAMPLER(sampler_CameraOpaqueTexture); float _Intensity; half4 frag (Varyings input) : SV_Target { UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float4 color = SAMPLE_TEXTURE2D_X(_CameraOpaqueTexture, sampler_CameraOpaqueTexture, input.texcoord); return color * float4(0, _Intensity, 0, 1); } ENDHLSL } } } Add the ColorBlitRendererFeature to the Universal Renderer asset. For information on how to add a Renderer Feature, see the page How to add a Renderer Feature to a Renderer. For this example, set the Intensity property to 1.5. Unity shows the following views: NOTE: To visualize the example in XR, configure the project to use XR SDK. Add the MockHMD XR Plugin to the project. Set the Render Mode property to Single Pass Instanced. The example is complete."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/renderer-feature-full-screen-pass.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/renderer-feature-full-screen-pass.html",
    "title": "Full Screen Pass Renderer Feature | FSM Unity Framework",
    "keywords": "Full Screen Pass Renderer Feature The Full Screen Pass Renderer Feature lets you inject full screen render passes at pre-defined injection points to create full screen effects. You can use this Renderer Feature to create custom post-processing effects. How to use the feature To add the Renderer Feature to your Scene: Add the Full Screen Pass Renderer Feature to the URP Renderer. Refer to the following page for an example of how to use this feature: How to create a custom post-processing effect. Properties The Full Screen Pass Renderer Feature contains the following properties. Property Description Name Name of the Full Screen Pass Renderer Feature. Pass Material The Material the Renderer Feature uses to render the effect. Injection Point Select when the effect is rendered: Before Rendering Transparents: Add the effect after the skybox pass and before the transparents pass. Before Rendering Post Processing: Add the effect after the transparents pass and before the post-processing pass. After Rendering Post Processing: Add the effect after the post-processing pass and before AfterRendering pass. After Rendering Post Processing is the default setting. Requirements Select one or more of the following passes for the Renderer Feature to use: None: Add no additional passes. Everything: Adds all additional passes available (Depth, Normal, Color, and Motion). Depth: Adds a depth prepass to enable the use of depth values. Normal: Enables the use of normal vector data. Color: Copies color data of a screen to the _BlitTexture texture inside the shader. Motion: Enables the use of motion vectors. Color is the default setting. Pass Index Select a specific pass inside the Pass Material's shader for the Pass Material to use. This option is hidden by default. To access this option, click ⋮ in the Renderer Feature section of the Inspector and select Show Additional Properties."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/renderer-feature-render-objects.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/renderer-features/renderer-feature-render-objects.html",
    "title": "Render Objects Renderer Feature | FSM Unity Framework",
    "keywords": "Render Objects Renderer Feature URP draws objects in the DrawOpaqueObjects and DrawTransparentObjects passes. You might need to draw objects at a different point in the frame rendering, or interpret and write rendering data (like depth and stencil) in alternate ways. The Render Objects Renderer Feature lets you do such customizations by letting you draw objects on a certain layer, at a certain time, with specific overrides. How to use the Render Objects Renderer Feature See: How to use the Render Objects Renderer Feature. Properties The Render Objects Renderer Feature contains the following properties. Property Description Name Use this field to edit the name of the feature. Event The event in the URP queue when Unity executes this Renderer Feature. Filters Settings that let you configure which objects this Renderer Feature renders. Queue Select whether the feature renders opaque or transparent objects. Layer Mask The Renderer Feature renders objects from layers you select in this property. Pass Names If a Pass in a shader has the LightMode Pass Tag, this Renderer Feature processes only the shaders where the value of the LightMode Pass Tag equals one of the values in the Pass Names property. Overrides Settings in this section let you configure overrides for certain properties when rendering with this Renderer Feature. Override Mode Specify the material override mode. Material (Override Mode is set to Material) When rendering an object, Unity replaces the Material assigned to it with this Material. This will override all material properties with this material Shader (Override Mode is set to Shader) When rendering an object, Unity replaces the material assigned to it with this shader. This maintains all material properties and allows the override shader to access these properties. This is currently not SRPBatcher compatible and less performant. Depth Selecting this option lets you specify how this Renderer Feature affects or uses the Depth buffer. This option contains the following items: Write Depth: this option defines whether the Renderer Feature updates the Depth buffer when rendering objects. Depth Test: the condition which determines when this Renderer Feature renders pixels of a given object. Stencil With this check box selected, the Renderer processes the Stencil buffer values. For more information on how Unity works with the Stencil buffer, see ShaderLab: Stencil. Camera Selecting this option lets you override the following Camera properties: Field of View: when rendering objects, the Renderer Feature uses this Field of View instead of the value specified on the Camera. Position Offset: when rendering objects, the Renderer Feature moves them by this offset. Restore: with this option selected, the Renderer Feature restores the original Camera matrices after executing the render passes in this Renderer Feature."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-in-universalrp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-in-universalrp.html",
    "title": "Rendering in the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Rendering in the Universal Render Pipeline The Universal Render Pipeline (URP) renders Scenes using the following components: URP Renderer. URP contains the following Renderers: Universal Renderer. 2D Renderer. Shading models for shaders shipped with URP Camera URP Asset The following illustration shows the frame rendering loop of the URP Universal Renderer. When the render pipeline is active in Graphics Settings, Unity uses URP to render all Cameras in your Project, including game and Scene view cameras, Reflection Probes, and the preview windows in your Inspectors. The URP renderer executes a Camera loop for each Camera, which performs the following steps: Culls rendered objects in your Scene Builds data for the renderer Executes a renderer that outputs an image to the framebuffer. For more information about each step, see Camera loop. In the RenderPipelineManager class, URP provides events that you can use to execute code before and after rendering a frame, and before and after rendering each Camera loop. The events are: beginCameraRendering beginFrameRendering endCameraRendering endFrameRendering For the example of how to use the beginCameraRendering event, see the page Using the beginCameraRendering event. Camera loop The Camera loop performs the following steps: Step Description Setup Culling Parameters Configures parameters that determine how the culling system culls Lights and shadows. You can override this part of the render pipeline with a custom renderer. Culling Uses the culling parameters from the previous step to compute a list of visible renderers, shadow casters, and Lights that are visible to the Camera. Culling parameters and Camera layer distances affect culling and rendering performance. Build Rendering Data Catches information based on the culling output, quality settings from the URP Asset, Camera, and the current running platform to build the RenderingData. The rendering data tells the renderer the amount of rendering work and quality required for the Camera and the currently chosen platform. Setup Renderer Builds a list of render passes, and queues them for execution according to the rendering data. You can override this part of the render pipeline with a custom renderer. Execute Renderer Executes each render pass in the queue. The renderer outputs the Camera image to the framebuffer."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-to-a-render-texture.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-to-a-render-texture.html",
    "title": "Rendering to a Render Texture | FSM Unity Framework",
    "keywords": "Rendering to a Render Texture In the Universal Render Pipeline (URP), a Camera can render to the screen or to a Render Texture. Rendering to a screen is the default and is the most common use case, but rendering to a Render Texture allows you to create effects such as CCTV camera monitors. If you have a Camera that is rendering to a Render Texture, you must have a second Camera that then renders that Render Texture to the screen. In URP, all Cameras that render to Render Textures perform their render loops before all Cameras that render to the screen. This ensures that the Render Textures are ready to render to the screen. For more information on Camera rendering order in URP, see Rendering order and overdraw. Rendering to a Render Texture, and then rendering that Render Texture to the screen Create a Render Texture Asset in your Project using Assets > Create > Render Texture. Create a Quad in your Scene. Create a Material in your Project, and select it. In the Inspector, drag the Render Texture to the Material's Base Map field. In the Scene view, drag the Material on to the Quad. Create a Camera in your Scene. Its Render Mode defaults to Base, making it a Base Camera. Select the Base Camera. In the Inspector, scroll to the Output section. Set the Camera’s Output Target to Texture, and drag the Render Texture on to the Texture field. Create another Camera in your Scene. Its Render Mode defaults to Base, making it a Base Camera. Place the Quad within the view of the new Base Camera. The first Camera renders its view to the Render Texture. The second Camera renders the Scene including the Render Texture to the screen. You can set the Output Target for a Camera in a script by setting the cameraOutput property of the Camera's Universal Additional Camera Data component, like this: myUniversalAdditionalCameraData.cameraOutput = CameraOutput.Texture; myCamera.targetTexture = myRenderTexture;"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-to-the-same-render-target.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering-to-the-same-render-target.html",
    "title": "Rendering from multiple Cameras to the same render target | FSM Unity Framework",
    "keywords": "Rendering from multiple Cameras to the same render target In the Universal Render Pipeline (URP), multiple Base Cameras or Camera Stacks can render to the same render target. This allows you to create effects such as split screen rendering. If more than one Base Camera or Camera Stack renders to the same area of a render target, Unity draws each pixel in the overlapping area multiple times. Unity draws the Base Camera or Camera Stack with the highest priority last, on top of the previously drawn pixels. For more information on overdraw, see Advanced information. You use the Base Camera's Output Target property to define the render target, and the Viewport Rect property to define the area of the render target to render to. For more information on viewport coordinates, see the Unity Manual and API documentation. Setting up split screen rendering Create a Camera in your Scene. Its Render Mode defaults to Base, making it a Base Camera. Select the Camera. In the Inspector, scroll to the Output section. Ensure that Output Target is set to Camera, and change the values for Viewport rect to the following: X: 0 Y: 0 W: 0.5 H: 1 Create another Camera in your Scene. Its Render Mode defaults to Base, making it a Base Camera. Select the Camera. In the Inspector, scroll to the Output section. Ensure that Output Target is set to Camera, and change the values for Viewport rect to the following: X: 0.5 Y: 0 W: 0.5 H: 1 Unity renders the first Camera to the left-hand side of the screen, and the second Camera to the right-hand side of the screen. You can change the Viewport rect for a Camera in a script by setting its rect property, like this: myUniversalAdditionalCameraData.rect = new Rect(0.5f, 0f, 0.5f, 0f);"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering/deferred-rendering-path.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering/deferred-rendering-path.html",
    "title": "Deferred Rendering Path in URP | FSM Unity Framework",
    "keywords": "Deferred Rendering Path in URP URP Universal Renderer supports the following Rendering Paths: Forward Forward+ Deferred For information on differences between the rendering paths, see section Rendering Path comparison. This section describes the Deferred Rendering Path. Sample Scene rendered with the Deferred Rendering Path. This section contains the following topics: How to select the Deferred Rendering Path Unity Player system requirements Implementation details Relevant code files ShaderLab Pass tags Limitations and performance How to select the Deferred Rendering Path To select the Rendering Path, use the property Lighting > Rendering Path in the URP Universal Renderer asset. When you select the Deferred Rendering Path, Unity shows the Accurate G-buffer normals property. The Accurate G-buffer normals property lets you configure how Unity encodes the normals when storing them in the geometry buffer (G-buffer). Accurate G-buffer normals off: This option increases performance, especially on mobile GPUs, but might lead to color banding artifacts on smooth surfaces. Accurate G-buffer normals on: Unity uses the octahedron encoding to store values of normal vectors in the RGB channel of a normal texture. With this encoding, values of normal vectors are more accurate, but the encoding and decoding operations put extra load on the GPU. This option does not support decal normal blending when used with the Screen Space decal technique. For more information about this setting, see the section Encoding of normals in G-buffer. Unity Player system requirements The Deferred Rendering Path has the following requirements and limitations on top of the general system requirements for the Unity Player. Minimum Shader Model: Shader Model 4.5. Deferred Rendering Path does not support the OpenGL and OpenGL ES API. If a project with the Deferred Rendering Path is built for platforms using those API, the application falls back to the Forward Rendering Path. Implementation details This section describes the implementation details of this feature, and technical details about how this feature functions. G-buffer layout This section describes how Unity stores material attributes in the G-buffer in the Deferred Rendering Path. The following illustration shows the data structure for each pixel of the render targets that Unity uses in the Deferred Rendering Path. The data structure consists of the following components. Albedo (sRGB) This field contains the albedo color in sRGB format, 24 bits. MaterialFlags This field is a bit field that contains Material flags: Bit 0, ReceiveShadowsOff: if set, the pixel does not receive dynamic shadows. Bit 1, SpecularHighlightsOff: if set, the pixel does not receive specular highlights. Bit 2, SubtractiveMixedLighting: if set, the pixel uses subtractive mixed lighting. Bit 3, SpecularSetup: if set, the Material uses the specular workflow. Bits 4-7 are reserved for future use. For more technical details, see the file /ShaderLibrary/UnityGBuffer.hlsl. Specular This field contains the following values: SimpleLit Material: RGB specular color stored in 24 bits. Lit Material with metallic workflow: reflectivity stored in 8 bits, 16 bits are not used. Lit Material with specular workflow: RGB specular color stored in 24 bits. Occlusion This field contains the baked occlusion value from the baked lighting. For real-time lighting, Unity calculates the ambient occlusion value by combining the baked occlusion value with the SSAO value. Normal This field contains the world space normals encoded in 24 bits. For information on the encoding of normals, see section Encoding of normals in G-buffer. Smoothness This field stores the smoothness value for the SimpleLit and Lit materials. Emissive/GI/Lighting This render target contains the Material emissive output and baked lighting. Unity fills this field during the G-buffer Pass. During the deferred shading pass, Unity stores lighting results in this render target. Render target format: B10G11R11_UFloatPack32, unless one of the following conditions is true: In URP Asset, the setting Quality > HDR is turned on, and the target Player platform does not support HDR. In Player Settings, the setting PreserveFramebufferAlpha is true. R16G6B16A16_SFloat, if Unity cannot use B10G11R11_UFloatPack32 because of the project settings. If Unity cannot use one of the other formats in the list, it uses what the following method returns: SystemInfo.GetGraphicsFormat(DefaultFormat.HDR). ShadowMask Unity adds this render target to the G-buffer layout when Lighting Mode is set to Subtractive or Shadow mask. The Subtractive and the Shadow mask modes are optimized for the Forward Rendering Path, and are less efficient in the Deferred Rendering Path. In the Deferred Rendering Path, avoid using these modes and use the Baked Indirect mode instead to improve GPU performance. Rendering Layer Mask Unity adds this render target to the G-buffer layout when the Use Rendering Layers option is enabled (URP Asset, Lighting > Use Rendering Layers). Using Rendering Layers might have an impact on the GPU performance. For more information, see the page Rendering Layers. Depth as Color Unity adds this render target to the G-buffer layout when Native Render Pass is enabled on platforms that support it. Unity renders depth as a color into this render target. This render target has the following purpose: Improves performance on Vulkan devices. Lets Unity get the depth buffer on Metal API, which does not allow fetching the depth from the DepthStencil buffer within the same render pass. The format of the Depth as Color render target is GraphicsFormat.R32_SFloat. DepthStencil Unity reserves the four highest bits of this render target to mark the Material type. See also URP Pass tags: UniversalMaterialType. For this render target, Unity selects either the D32F_S8 format, or the D24S8 format depending on the platform. Encoding of normals in G-buffer In the Deferred Rendering Path, Unity stores normals in the G-buffer. Unity encodes each normal as a 24 bit value. When you select the Deferred option in the Rendering Path property in the URP Universal Renderer asset, Unity shows the Accurate G-buffer normals property. The Accurate G-buffer normals property lets you configure how Unity encodes the normals. Accurate G-buffer normals off: Unity stores values of normal vectors in the G-buffer in the RGB channel of a normal texture, 8 bit per value (x, y, z). The values are quantized with the loss of accuracy. This option increases performance, especially on mobile GPUs, but might lead to color banding artifacts on smooth surfaces. Accurate G-buffer normals on: Unity uses the octahedron encoding to store values of normal vectors in the RGB channel of a normal texture. With this encoding, values of normal vectors are more accurate, but the encoding and decoding operations put extra load on the GPU. This option does not support decal normal blending when used with the Screen Space decal technique. The precision of the encoded normal vectors is similar to the precision of the sampled values in the Forward Rendering Path. The following illustration shows the visual difference between the two options when the Camera is very close to the GameObject: Performance considerations With Accurate G-buffer normals option turned on, there is extra load on the GPU because of the encoding and decoding operations. This load is insignificant for desktop platforms and consoles, but might be considerable for mobile GPUs. Turning the option on does not increase the memory footprint. To store the normals, Unity uses the same RGB channel in the normal texture regardless of the encoding. Deferred Rendering Path render Passes The following table shows the sequence of Render Pass events in the Deferred Rendering Path. Render Pass events Deferred Rendering Path Passes SSAO Renderer Feature Passes BeforeRendering BeforeRenderingShadows AfterRenderingShadows BeforeRenderingPrePasses Depth, or depth and normal prepass (Forward only materials) AfterRenderingPrePasses BeforeRenderingGbuffer G-buffer Pass (GBufferPass) Copy G-buffer depth texture AfterRenderingGbuffer SSAO (optional) BeforeRenderingDeferredLights Deferred rendering (stencil) AfterRenderingDeferredLights BeforeRenderingOpaques Opaque Forward-only Materials AfterRenderingOpaques SSAO and blending (optional) BeforeRenderingSkybox AfterRenderingSkybox BeforeRenderingTransparents AfterRenderingTransparents BeforeRenderingPostProcessing AfterRenderingPostProcessing AfterRendering The following sections describe the Deferred Rendering Path render Passes. Depth, or depth and normal prepass In the Deferred Rendering Path, in the depth prepass or depth and normal prepass, Unity renders only the Materials that do not support the deferred rendering model. For example, Materials using the Complex Lit shader are such Materials. In the Deferred Rendering Path, Unity does not use the depth prepass to generate a copy of the depth buffer (this behavior is different in the Forward Rendering Path). If the Universal Renderer has the SSAO Renderer Feature, Unity executes the depth and normal prepass. SSAO uses the screen-space depth and normal buffers to calculate ambient occlusion. Optional passes: SSAO, SSAO with blending If the Universal Renderer has the SSAO Renderer Feature, and the After Opaque option is disabled (the option is disabled by default), Unity executes the SSAO Pass at the AfterRenderingGbuffer event. The SSAO Renderer Feature calculates the SSAO texture. Unity samples this texture in the Deferred rendering Pass and in the Pass that renders Forward-only Materials. Using this Pass order, Unity can combine the baked occlusion with the real-time occlusion from the SSAO Renderer Feature, and avoid double-darkening from the baked and the real-time ambient occlusion. When the After Opaque option is enabled, Unity executes the SSAO and blending Pass at the AfterRenderingOpaques event, after rendering the Forward-only Materials. Unity then executes an extra full screen Pass to overlay the SSAO texture onto the Emissive/GI/Lighting buffer. This causes over-darkening of the areas that receive baked occlusion and real-time occlusion. Performance considerations On mobile platforms with TBDR architecture, with the After Opaque option is disabled, Unity requires an extra render target for load and store operations. This has a significant performance impact. Enabling the After Opaque option on mobile platforms improves GPU performance. On mobile platforms with TBDR architecture, enabling this option avoids extra render target load and store operations. Forward-only Pass Certain Unity shaders use lighting models that Unity cannot render in the Deferred Rendering Path. Examples of such shaders: Complex Lit: the Lighting model of this shader (for example, the Clear Coat effect) is too complex and extra Material properties cannot fit into the G-buffer. Baked Lit and Unlit: these shaders do not calculate real-time lighting, that's why Unity renders them into the Emissive/GI/Lighting buffer directly during the Forward-only pass. This is faster than evaluating the shaders in the Deferred rendering (stencil) pass. Custom shaders: Unity renders the shaders that do not declare the Pass tags required by the Deferred Rendering Path as Forward-only. The required Pass tags are: LightMode, and UniversalMaterialType. For more information, see URP Pass tags. Unity renders Materials with such shaders in the Forward Rendering Path. For the SSAO Renderer Feature to be able to calculate ambient occlusion for the Materials using the Complex Lit shader, Unity must render such Materials in the depth and normal prepass first. This is because Unity does not render those Materials in the G-buffer pass (GBufferPass). For more information, see URP Pass tags. General implementation notes For maximum platform compatibility, the URP Deferred Rendering Path uses the light stencil volume technique to render light volumes and apply deferred shading. Relevant code files This section contains the list of files that contain the code related to the Deferred Rendering Path. The main class that handles the Deferred Rendering Path: com.unity.render-pipelines.universal\\Runtime\\DeferredLights.cs ScriptableRenderPass for the G-Buffer pass: com.unity.render-pipelines.universal\\Runtime\\Passes\\GBufferPass.cs ScriptableRenderPass for the deferred shading pass: com.unity.render-pipelines.universal\\Runtime\\Passes\\DeferredPass.cs Shader asset for the deferred shading: com.unity.render-pipelines.universal\\Shaders\\Utils\\StencilDeferred.shader Utility functions for the deferred shading: com.unity.render-pipelines.universal\\Shaders\\Utils\\Deferred.hlsl Utility functions for storing and loading the Material properties from the G-buffer: com.unity.render-pipelines.universal\\Shaders\\Utils\\UnityGBuffer.hlsl ShaderLab Pass tags To enable Unity to render a shader in the Deferred Rendering Path, the shader must have a Pass with the following tag definition: \"LightMode\" = \"UniversalGBuffer\" Unity executes the shader with such LightMode tag during the G-buffer Pass. To indicate that Unity must render a certain Material in the Forward-only Pass in the Deferred Rendering Path, add the following tags to a shader Pass: \"LightMode\" = \"UniversalForwardOnly\" \"LightMode\" = \"DepthNormalsOnly\" To specify the shader lighting model (Lit, SimpleLit), use the UniversalMaterialType tag. For more information, see the section URP Pass tags: LightMode. Limitations and performance This section describes the limitations of the Deferred Rendering Path. Terrain blending When blending more than four Terrain layers, the Deferred Rendering Path generates slightly different results from the Forward Rendering Path. This happens because in the Forward Rendering Path, Unity processes the first four layers separately from the next four layers using multi-pass rendering. In the Forward Rendering Path, Unity merges Material properties and calculates lighting for the combined properties of four layers at once. Unity then processes the next four layers in the same way and alpha-blends the lighting results. In the Deferred Rendering Path, Unity combines Terrain layers in the G-buffer pass, four layers at a time, and then calculates lighting only once during the deferred rendering pass. This difference with the Forward Rendering Path leads to visually different outcomes. Unity combines the Material properties in the G-buffer using hardware blending (four layers at a time), which limits how correct the combination of property values is. For example, pixel normals cannot be correctly combined using the alpha blend equation alone, because one Terrain layer might contain coarse Terrain detail while another layer might contain fine detail. Averaging or summing normals results in loss of accuracy. NOTE: Turning the setting Accurate G-buffer normals on breaks Terrain blending. With this setting turned on, Unity encodes normals using octahedron encoding. Normals in different layers encoded this way cannot be blended together because of the bitwise nature of the encoding (2 x 12 bits). If your application requires more than four Terrain layers, turn the Accurate G-buffer normals setting off. The following illustration shows the visual difference when rendering Terrain layers with different Rendering Paths. Terrain layers rendered with the Forward Rendering Path Terrain layers rendered with the Deferred Rendering Path Baked Global Illumination and Lighting Modes When Baked Global Illumination is enabled, the Subtractive and the Shadowmask Lighting modes put extra load on the GPU in the Deferred Rendering Path. The Deferred Rendering Path supports the Subtractive and the Shadowmask Lighting modes for compatibility reasons, but, unlike the case with the Forward Rendering Path, these modes do not provide any improvements in performance. In the Deferred Rendering Path, Unity processes all meshes using the same Lighting algorithm and stores the extra Lighting properties required by Subtractive and the Shadowmask modes in the ShadowMask render target. In the Deferred Rendering Path, the Baked Indirect Lighting mode provides better performance, since it does not require the ShadowMask render target. Rendering layers URP implements the Rendering Layers feature that lets you configure which Lights in a Scene affect specific meshes. Lights assigned to a specific Rendering Layer only affect the meshes assigned to the same Rendering Layer. For more information on Rendering Layers, see the page Rendering Layers. Performance impact The Rendering Layers feature requires an extra G-buffer render target to store the rendering layer mask (32 bits). The extra render target is likely to have a negative impact on GPU performance. Implementation notes In the Forward Rendering Path, the Layers feature lets you tell Unity to render specific meshes with a specific set of Lights. The Layers feature uses the culling mask system. The Deferred Rendering Path cannot use the layer system with light culling masks, because the shading is deferred to a later stage in the rendering loop (see the Deferred rendering (stencil) step in the Deferred Rendering Path render Passes table.)"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering/forward-plus-rendering-path.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/rendering/forward-plus-rendering-path.html",
    "title": "Forward+ Rendering Path | FSM Unity Framework",
    "keywords": "Forward+ Rendering Path The Forward+ Rendering Path lets you avoid the per object limit of the Forward Rendering Path. The Forward+ Rendering Path has the following advantages compared with the Forward Rendering Path: There is no per-object limit for the number of Lights that affect GameObjects, the per-Camera limit still applies. This implementation lets you avoid splitting big meshes when more than 8 lights affect them. Blending of more than 2 reflection probes. Support for multiple Lights when using Unity Entity Component System (ECS). More flexibility with procedural draws. See also: Rendering Path comparison. How to select the Forward+ Rendering Path To select the Forward+ Rendering Path, use the property Rendering > Rendering Path in the URP Universal Renderer asset. When you set the Rendering Path to Forward+, Unity ignores the values in the following properties in URP Asset, Lighting section: Main Light. With Forward+ the value of this property is Per Pixel regardless of the value you select. Additional Lights. With Forward+ the value of this property is Per Pixel regardless of the value you select. Additional Lights > Per Object Limit. Unity ignores this property. Reflection Probes > Probe Blending. Reflection probe blending is always on. Limitations The Forward+ Rendering Path has no limitations compared with the Forward Rendering Path."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/requirements.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/requirements.html",
    "title": "Requirements and compatibility | FSM Unity Framework",
    "keywords": "Requirements and compatibility This page contains information on system requirements and compatibility of this package. Unity Editor compatibility The following table shows the compatibility of URP package versions with different Unity Editor versions. Package version Minimum Unity version Maximum Unity version 16.0.x 2023.2 2023.x 15.0.x 2023.1 2023.1 14.0.x 2022.2 2022.x 13.x.x 2022.1 2022.1 12.0.x 2021.2 2021.3 11.0.0 2021.1 2021.1 10.x 2020.2 2020.3 9.x-preview 2020.1 2020.2 8.x 2020.1 2020.1 7.x 2019.3 2019.4 Since the release of Unity 2021.1, graphics packages are core Unity packages. For each release of Unity (alpha, beta, patch release), the main Unity installer contains the up-to-date versions of the following graphics packages: SRP Core, URP, HDRP, Shader Graph, VFX Graph. Since the release of Unity 2021.1, the Package Manager shows only the major revisions of the graphics packages (version 11.0.0 for all Unity 2021.1.x releases, version 12.0.0 for all Unity 2021.2.x releases). You can install a different version of a graphics package from disk using the Package Manager, or by modifying the manifest.json file. Render pipeline compatibility Projects made using URP are not compatible with the High Definition Render Pipeline (HDRP) or the Built-in Render Pipeline. Before you start development, you must decide which render pipeline to use in your Project. For information on choosing a render pipeline, see the Render Pipelines section of the Unity Manual. Unity Player system requirements This package does not add any extra platform-specific requirements. General system requirements for the Unity Player apply. For more information on Unity system requirements, see System requirements for Unity."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/scene-templates.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/scene-templates.html",
    "title": "Universal Render Pipeline Scene Templates | FSM Unity Framework",
    "keywords": "Universal Render Pipeline Scene Templates You can use Scene Templates to quickly create scenes that include pre-configured URP-specific settings and post-processing effects. For information on how to create a new scene from a Scene Template, see Creating a new scene from the New Scene dialog. The New Scene dialog displaying Scene Templates. The following Scene Templates are available for URP: Basic (URP): A scene that contains a Camera and a Light. This is the URP equivalent of Unity's default scene. Standard (URP): A scene that contains a Camera, a Light, and a global Volume with various post-processing effects. Note: If you create a scene using the Standard (URP) Scene Template, Unity creates a new Volume Profile to store the post-processing effects."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-complex-lit.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-complex-lit.html",
    "title": "Complex Lit Shader | FSM Unity Framework",
    "keywords": "Complex Lit Shader The Complex Lit Shader contains all the functionality of the Lit shader and adds advanced material features. Some features in this shader might be considerably more resource-intensive and require Unity Shader Model 4.5 hardware. In the Deferred Rendering Path, URP renders objects that have the Complex Lit shader using the Forward Rendering Path. If the hardware of the target platform does not support features in the Complex Lit shader, URP uses the Lit shader instead. If you need to reduce processing time on the GPU (for example, on lower-end platforms), avoid use of the Complex Lit shader. Instead, use the Baked Lit shader for static objects and the Simple Lit shader for dynamic objects. If you use the Complex Lit shader, disable Clear Coat. Using the Complex Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Complex Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how URP renders the Material on a screen. Property Description Workflow Mode Use this drop-down menu to choose a workflow that fits your Textures, either Metallic and Specular. When you have made your choice, the main Texture options in the rest of the Inspector now follow your chosen workflow. For information on metallic or specular workflows, see this Manual page for the Standard built-in Shader in Unity. Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Select how Unity calculates the color of each pixel of a transparent Material when it blends the Material with the background. In the context of Blending Modes, Source refers to the transparent Material where the Blending Mode is set and Destination refers to anything that Material overlaps with. Alpha Alpha blending mode. Alpha uses the Material's alpha value to change how transparent an object is. 0 is fully transparent. 255 is fully opaque, this is translated to a value of 1 when used with the blending equations. The Material is always rendered in the Transparent render pass regardless of it's alpha value. This mode lets you use the Preserve Specular Lighting property. Alpha equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB × (1 − SourceAlpha) Premultiply Premultiply blending mode. Premultiply first multiplies the RGB values of the transparent Material by its alpha value then applies a similar effect to the Material as Alpha. The equation for Premultiply also allows areas of the transparent Material with an alpha value of 0 to have an additive blend effect. This can help reduce artifacts that may appear at the edge of the overlap between opaque and transparent pixels. Premultiply equation: OutputRGBA = SourceRGB + DestinationRGB × (1 − SourceAlpha) Additive Additive blending mode. Additive adds the color values of the Materials together to create the blend effect. The alpha value determines the strength of the source Material's color before the blend is calculated. This mode lets you use the Preserve Specular Lighting property. Additive equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB Multiply Multiply blending mode. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. This mode uses the Material’s alpha value to adjust how much the colors blend. An alpha value of 1 results in unadjusted multiplication of the colors while lower values blend the colors towards white. Multiply equation: OutputRGBA = SourceRGB × DestinationRGB Preserve Specular Lighting Indicates whether Unity preserves specular highlights on a GameObject or not. This applies even when your surface is transparent which means that only the reflected light is visible. This property only appears when the Surface Type is set to Transparent and Blending Mode is set to either Alpha or Additive. Material with Preserve Specular Lighting disabled. Material with Preserve Specular Lighting enabled. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Receive Shadows Tick this box to enable your GameObject to have shadows cast upon it by other objects. If you untick this box, the GameObject will not have shadows on it. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Note: If you are used to the Standard Shader in the built-in Unity render pipeline, these options are similar to the Main Maps settings in the Material Editor. Property Description Base Map Adds color to the surface, also known as the diffuse map. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. Metallic / Specular Map Shows a map input for your chosen Workflow Mode in the Surface Options. For the Metallic Map workflow, the map gets the color from the Base Map assigned above. Use the slider to control how metallic the surface appears. 1 is fully metallic, like silver or copper, and 0 is fully dielectric, like plastic or wood. You can generally use values in between 0 and 1 for dirty or corroded metals. For the Specular Map setting, you can assign a texture to it by clicking the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. For both workflows, you can use the Smoothness slider to control the spread of highlights on the surface. 0 gives a wide, rough highlight. 1 gives a small, sharp highlight like glass. Values in between produce semi-glossy looks. For example, 0.5 produces a plastic-like glossiness. Use the Source drop-down menu to select where the shader samples a smoothness map from. Options are: Metallic Alpha (alpha channel from the metallic map), and Albedo Alpha (alpha channel from the base map). The default value is Metallic Alpha. If the selected source has the alpha channel, the shader samples the channel and multiplies each sample by the Smoothness value. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. The float value next to the setting is a multiplier for the effect of the Normal Map. Low values decrease the effect of the normal map. High values create stronger effects. Height Map URP implements the parallax mapping technique which uses the height map to achieve surface-level occlusion effect by shifting the areas of the visible surface texture. To add the map, click the object picker next to it. The float value next to the setting is a multiplier for the effect of the Height Map. Low values decrease the effect of the height map. High values create stronger effects. Occlusion Map Select an occlusion map. This simulates shadows from ambient light and reflection, which makes lighting look more realistic as less light reaches corners and crevices of objects. To select the occlusion map, click the object picker next to it. Clear Coat Select this check box to enable the Clear Coat feature. The feature adds an extra Material layer which simulates a transparent and thin coating on top of the base Material. The feature affects the color and the Smoothness values of the underlying base material slightly. The index of refraction (IOR) of the Clear Coat is 1.5. Performance impact: Rendering Clear Coat has roughly twice the cost of rendering a base material, because the lighting is evaluated once per layer. Mask: This property defines the intensity of the effect: 0 - no effect, 1 - maximum effect. Setting the Mask value to 0 does not disable the feature. Smoothness: This property defines the spread of highlights on the surface. 0 gives wide, rough highlights. 1 gives sharp, glasslike highlights. There is the Clear Coat map property to the left of the Mask property. The channels have the following mapping: Red: the Mask property. Green: the Smoothness property. If a Clear Coat map is present, URP multiplies the map's pixel values by value of the Mask property. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Tiling A 2D multiplier value that scales the Texture to fit across a mesh according to the U and V axes. This is good for surfaces like floors and walls. The default value is 1, which means no scaling. Set a higher value to make the Texture repeat across your mesh. Set a lower value to stretch the Texture. Try different values until you reach your desired effect. Offset The 2D offset that positions the Texture on the mesh. To adjust the position on your mesh, move the Texture across the U or V axes. Detail Inputs Use the Detail Inputs settings to add extra details to the surface. Requirement: GPU supporting shader model 2.5 or higher. Property Description Mask Select a texture that defines areas where Unity overlays the Detail maps over the Surface Inputs maps. The mask uses the alpha channel of the selected texture. The Tiling and Offset settings have no effect on the mask. Base Map Select the texture containing the surface details. Unity blends this map with the Surface Base Map using the overlay mode. Normal Map Select the texture containing the normal vector data. Use a normal map to add surface details like bumps, scratches and grooves. Use the slider next to the setting to change the intensity of the effect of the map. The default value is 1. Tiling Use this setting to scale the Base Map and the Normal Map on the mesh along the U and V axes, so that the maps fit the mesh best. The default value is 1. Select a value higher than one to make the maps repeat themselves across the mesh. Set a value lower than 1 to stretch the maps. Offset The offset that moves the Base Map and the Normal Map on the mesh along the U and V axes. Advanced The Advanced settings affect the underlying calculations of your rendering. They do not have a visible effect on your surface. Property Description Specular Highlights Enable this to allow your Material to have specular highlights from direct lighting, for example Directional, Point, and Spot lights. This means that your Material reflects the shine from these light sources. Disable this to leave out these highlight calculations, so your Shader renders faster. By default, this feature is enabled. Environment Reflections Sample reflections using the nearest Reflection Probe, or, if you have set one in the Lighting window, the Lighting Probe. If you disable this, you will have fewer Shader calculations, but this also means that your surface has no reflections. Enable GPU Instancing Makes URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline. Channel packing This Shader uses channel packing, so you can use a single RGBA texture for the metallic, smoothness and occlusion properties. When you use texture packing, you only have to load one texture into memory instead of three separate ones. When you write your texture maps in a program like Substance or Photoshop, you can pack the maps like this: Base Map Channel Property Red Metallic Green Occlusion Blue None Alpha Smoothness Clear Coat Map Channel Property Red Mask Green Smoothness Blue None Alpha None"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-stripping.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-stripping.html",
    "title": "Shader Stripping | FSM Unity Framework",
    "keywords": "Shader Stripping The shaders in the Universal Render Pipeline (URP) use shader keywords to support many different features, which can mean Unity compiles a lot of shader variants. If you disable features in the URP Asset, URP automatically excludes ('strips') the related shader variants. This speeds up builds, and reduces memory usage and file sizes. For example, if your project doesn't use shadows for directional lights, by default Unity still includes variants that support directional light shadows in your build. If you disable Cast Shadows in the URP Asset, URP strips these variants. If you want to examine the code that strips shaders in URP, see the Editor/ShaderPreprocessor.cs file. The file uses the IPreprocessShaders API. For more information on stripping shader variants, see the following pages: Check how many shader variants you have. See the standard guidance about shader stripping, which applies to all render pipelines. Strip feature shader variants By default, URP compiles variants where a feature is enabled, and variants where a feature is disabled. To reduce the number of variants, you can enable Strip Unused Variants in the URP Global Settings and do the following: Disable a feature in all URP Assets in your build, so URP keeps only variants where the feature is disabled. Enable a feature in all URP Assets in your build, so URP keeps only variants where the feature is enabled. If you disable the Strip Unused Variants setting, URP can't strip variants where the feature is disabled. This might increase the number of variants. Disable a feature To let Unity strip variants related to a feature, make sure you disable it in all the URP Assets in your build. Unity includes the following URP Assets in your build: The URP Asset you set as the default render pipeline asset in Graphics Settings. Any URP Asset you set as a Render Pipeline Asset in a Quality Settings level you enable for the current build target. Avoid including URP Assets in your build that use different rendering paths because this causes Unity to create two sets of variants for each keyword. Feature How to disable the feature Shader keywords this turns off Rendering Path Accurate G-buffer normals Disable Accurate G-buffer normals in the URP Asset. This has no effect on platforms that use the Vulkan graphics API. _GBUFFER_NORMALS_OCT Deferred Additional lights In the URP Asset, in the Lighting section, disable Additional Lights. _ADDITIONAL_LIGHTS, _ADDITIONAL_LIGHTS_VERTEX Forward Ambient occlusion Remove the Ambient Occlusion Renderer Feature in all Renderers that URP Assets use. _SCREEN_SPACE_OCCLUSION Forward and Deferred Decals Remove the Decals Renderer Feature in all Renderers that URP Assets use. _DBUFFER_MRT1, _DBUFFER_MRT2, _DBUFFER_MRT3, _DECAL_NORMAL_BLEND_LOW, _DECAL_NORMAL_BLEND_MEDIUM, _DECAL_NORMAL_BLEND_HIGH, _DECAL_LAYERS Forward and Deferred Fast sRGB to linear conversion In the URP Asset, in the Post-processing section, disable Fast sRGB/Linear conversions. _USE_FAST_SRGB_LINEAR_CONVERSION Forward and Deferred Holes in terrain In the URP Asset, in the Rendering section, disable Terrain Holes. _ALPHATEST_ON Forward Light cookies Remove Cookie textures from all the lights in your project. _LIGHT_COOKIES Forward and Deferred Rendering Layers for lights Disable Rendering Layers for Lights. _LIGHT_LAYERS Forward and Deferred Reflection Probe blending Disable Probe Blending. _REFLECTION_PROBE_BLENDING Forward and Deferred Reflection Probe box projection Disable Box Projection. _REFLECTION_PROBE_BOX_PROJECTION Forward and Deferred Render Pass Disable Native Render in all Renderers that URP Assets use. _RENDER_PASS_ENABLED Forward and Deferred Shadows from additional lights In the URP Asset, in the Additional Lights section, disable Cast Shadows. _ADDITIONAL_LIGHT_SHADOWS Forward and Deferred Shadows from the main light In the URP Asset, in the Main Light section, disable Cast Shadows. The keywords Unity removes might depend on your settings. _MAIN_LIGHT_SHADOWS, _MAIN_LIGHT_SHADOWS_CASCADE, _MAIN_LIGHT_SHADOWS_SCREEN Forward and Deferred Soft shadows In the URP Asset, in the Shadows section, disable Soft shadows. _SHADOWS_SOFT Forward and Deferred Strip post-processing shader variants Enable Strip Unused Post Processing Variants in URP Global Settings to strip shader variants for Volume Overrides you don't use. For example if your project uses only the Bloom effect, URP keeps Bloom variants but strips all other post-processing variants. Unity checks for Volume Overrides in all scenes, so you can't strip variants by removing a Scene from your build but keeping it in your project. Volume Override removed Shader keywords this turns off Bloom _BLOOM_HQ, BLOOM_HQ_DIRT, _BLOOM_LQ, BLOOM_LQ_DIRT Chromatic Aberration _CHROMATIC_ABERRATION Film Grain _FILM_GRAIN HDR Grading _HDR_GRADING Lens Distortion _DISTORTION Tonemapping _TONEMAP_ACES, _TONEMAP_NEUTRAL, _TONEMAP_GRADING You should also enable Strip Screen Coord Override Variants in URP Global Settings, unless you override screen coordinates to support post processing on large numbers of multiple displays ('cluster' displays). Strip XR and VR shader variants If you don't use XR or VR, you can disable the XR and VR modules. This allows URP to strip XR and VR related shader variants from its standard shaders. Remove variants if you use a custom Renderer Feature If you create a custom Renderer Feature, you can use the FilterAttribute API to remove shader variants when you enable or disable settings in the URP Asset. For example, you can do the following: Use [SerializeField] to add a Boolean variable to the custom Renderer Feature and add it as a checkbox in the URP Asset Inspector. Use ShaderKeywordFilter.RemoveIf to remove shader variants when you enable the checkbox."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-terrain-lit.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shader-terrain-lit.html",
    "title": "Terrain Lit shader | FSM Unity Framework",
    "keywords": "Terrain Lit shader URP uses the Terrain Lit shader for Unity Terrain. This shader is a simpler version of the Lit shader. A Terrain can use a Terrain Lit Material with up to eight Terrain Layers. A Terrain GameObject rendered with the Terrain Lit shader. Terrain Lit Material properties A Terrain Lit shader has the following properties. Property Description Enable Height-based Blend Enable to have Unity take the height values from the blue channel of the Mask Map Texture. If you do not enable this property, Unity blends the Terrain Layers based on the weights painted in the splatmap Textures. When you disable this property and the Terrain Lit Shader Material is assigned to a Terrain, URP adds an additional option Opacity as Density Blend for each Terrain Layer that is added to that Terrain in the Paint Texture Tool Inspector. Note: Unity ignores this option when more than four Terrain Layers are on the Terrain. Height Transition Select the size in world units of the smooth transition area between Terrain Layers. Enable Per-pixel Normal Enable to have Unity sample the normal map Texture on a per-pixel level, preserving more geometry details for distant terrain parts. Unity generates a geometry normal map at runtime from the heightmap, rather than the Mesh geometry. This means you can have high-resolution Mesh normals, even if your Mesh is low resolution. Note: This option only works if you enable Draw Instanced on the Terrain. Create a Terrain Lit Material To create a Material compatible with a Terrain GameObject: Create a new Material (Assets > Create > Material). Select the new Material. In the Inspector, click the Shader drop-down, and select Universal Render Pipeline > Terrain > Lit. Assign a Terrain Lit Material to a Terrain GameObject To assign a Terrain Lit Material to a Terrain GameObject: Select a Terrain GameObject. In the Inspector, click the gear icon on the right side of the Terrain Inspector toolbar to open the Terrain Settings section. In the Material property, select a Terrain Lit Material. Either use the Object picker (circle icon), or drag and drop the Material onto the property. Using the Paint Holes Tool To use the Paint Holes tool on a Terrain, ensure that the Terrain Holes check box in your project's URP Asset is checked. Otherwise, the Terrain holes are absent when you build the application."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shaders-in-universalrp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shaders-in-universalrp.html",
    "title": "Shaders and Materials | FSM Unity Framework",
    "keywords": "Shaders and Materials URP provides the following Shaders for the most common use case scenarios: Complex Lit Lit Simple Lit Baked Lit Unlit Terrain Lit Particles Lit Particles Simple Lit Particles Unlit SpeedTree Decal Autodesk Interactive Autodesk Interactive Transparent Autodesk Interactive Masked Shader compatibility Lit and custom Lit shaders written for the Built-in Render Pipeline are not compatible with URP. Unlit shaders written for the Built-in Render Pipeline are compatible with URP. For information on converting shaders written for the Built-in Render Pipeline to URP shaders, see the page Converting your shaders. Choosing a shader The Universal Render Pipeline implements Physically Based Rendering (PBR). The pipeline provides pre-built shaders that can simulate real world materials. PBR materials provide a set of parameters that let artists achieve consistency between different material types and under different lighting conditions. The URP Lit shader is suitable for modeling most of the real world materials. The Complex Lit shader is suitable for simulating advanced materials that require more complex lighting evaluation, such as the clear coat effect. URP provides the Simple Lit shader as a helper to convert non-PBR projects made with the Built-in Render Pipeline to URP. This shader is non-PBR and is not supported by Shader Graph. If you don’t need real-time lighting, or would rather only use baked lighting and sample global illumination, choose a Baked Lit Shader. If you don’t need lighting on a Material at all, you can choose the Unlit Shader. SRP Batcher compatibility To ensure that a Shader is SRP Batcher compatible: Declare all Material properties in a single CBUFFER called UnityPerMaterial. Declare all built-in engine properties, such as unity_ObjectToWorld or unity_WorldTransformParams, in a single CBUFFER called UnityPerDraw. For more information on the SRP Batcher, see the page Scriptable Render Pipeline (SRP) Batcher."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shading-model.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shading-model.html",
    "title": "Shading models in Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Shading models in Universal Render Pipeline A shading model defines how a Material’s color varies depending on factors such as surface orientation, viewer direction, and lighting. Your choice of a shading model depends on the artistic direction and performance budget of your application. Universal Render Pipeline (URP) provides Shaders with the following shading models: Physically Based Shading Simple Shading Baked Lit Shading No lighting Physically Based Shading Physically Based Shading (PBS) simulates how objects look in real life by computing the amount of light reflected from the surface based on physics principles. This lets you create photo-realistic objects and surfaces. This PBS model follows two principles: Energy conservation - Surfaces never reflect more light than the total incoming light. The only exception to this is when an object emits light. For example, a neon sign. Microgeometry - Surfaces have geometry at a microscopic level. Some objects have smooth microgeometry, which gives them a mirror-like appearance. Other objects have rough microgeometry, which makes them look more dull. In URP, you can mimic the level of smoothness of a rendered object’s surface. When light hits a a rendered object's surface, part of the light is reflected and part is refracted. The reflected light is called specular reflection. This varies depending on the camera direction and the point at which the light hits a surface, also called the angle of incidence. In this shading model, the shape of specular highlight is approximated with a GGX function. For metal objects, the surface absorbs and changes the light. For non-metallic objects, also called dielectric objects, the surface reflects parts of the light. Light attenuation is only affected by the light intensity. This means that you don’t have to increase the range of your light to control the attenuation. The following URP Shaders use Physically Based Shading: Lit Particles Lit Note: This shading model is not suitable for low-end mobile hardware. If you’re targeting this hardware, use Shaders with a Simple Shading model. To read more about Physically Based Rendering, see this walkthrough by Joe Wilson on Marmoset. Simple shading This shading model is suitable for stylized visuals or for games that run on less powerful platforms. With this shading model, Materials are not truly photorealistic. The Shaders do not conserve energy. This shading model is based on the Blinn-Phong model. In this Simple shading model, Materials reflect diffuse and specular light, and there’s no correlation between the two. The amount of diffuse and specular light reflected from Materials depends on the properties you select for the Material and the total reflected light can therefore exceed the total incoming light. Specular reflection varies only with camera direction. Light attenuation is only affected by the light intensity. The following URP Shaders use Simple Shading: Simple Lit Particles Simple Lit Baked Lit shading The Baked Lit shading model doesn’t have real-time lighting. Materials can receive baked lighting from either lightmaps or Light Probes. This adds some depth to your Scenes at a small performance cost. Games with this shading model can run on less powerful platforms. The URP Baked Lit shader is the only shader that uses the Baked Lit shading model. Shaders with no lighting URP comes with some Shaders that are Unlit. This means that they have no directional lights and no baked lighting. Because there are no light calculations, these shaders compile faster than Shaders with lighting. If you know in advance that your GameObject or visual doesn’t need lighting, choose an Unlit shader to save calculation and build time in your final product. The following URP Shaders have no lighting: Unlit Particles Unlit"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shared/lens-flare/lens-flare-asset.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shared/lens-flare/lens-flare-asset.html",
    "title": "Lens Flare (SRP) Data Asset | FSM Unity Framework",
    "keywords": "Lens Flare (SRP) Data Asset Unity’s Scriptable Render Pipeline (SRP) includes the Lens Flare Data asset. You can use this asset to control the appearance of Lens Flares in your scene. This is the SRP equivalent of the Built-in Render Pipeline's Flare asset, which is incompatible with SRPs. For examples of how to use Lens Flares, see the Lens Flare samples in URP Package Samples. To create a Lens Flare Data asset, select Assets > Create > Lens Flare (SRP). To use this asset, assign it to the Lens Flare Data property of a Lens Flare (SRP) component. Properties The Lens Flare Element asset has the following properties: Type Image Circle Polygon Color Transform AxisTransform Distortion Multiple Elements Uniform Curve Random Type Property Description Type Select the type of Lens Flare Element this asset creates: • Image • Circle • Polygon Image Property Description Flare Texture The Texture this lens flare element uses. Preserve Aspect Ratio Fixes the width and height (aspect ratio) of the Flare Texture. You can use Distortion to change this property. Circle Property Description Gradient Controls the offset of the circular flare's gradient. This value ranges from 0 to 1. Falloff Controls the falloff of the circular flare's gradient. This value ranges from 0 to 1, where 0 has no falloff between the tones and 1 creates a falloff that is spread evenly across the circle. Inverse Enable this property to reverse the direction of the gradient. Polygon Property Description Gradient Controls the offset of the polygon flare's gradient. This value ranges from 0 to 1. Falloff Controls the falloff of the polygon flare's gradient. This value ranges from 0 to 1, where 0 has no falloff between the tones and 1 creates a falloff that is spread evenly across the polygon. Side Count Determines how many sides the polygon flare has. Roundness Defines how smooth the edges of the polygon flare are. This value ranges from 0 to 1, where 0 is a sharp polygon and 1 is a circle. Inverse Enable this property to reverse the direction of the gradient Color Property Description Tint Changes the tint of the lens flare. If this asset is attached to the light, this property is based on the light tint. Modulate By Light Color Allows light color to affect this Lens Flare Element. This only applies when the asset is used in a Lens Flare (SRP) component that is attached to a point, spot, or area light. Intensity Controls the intensity of this element. Blend Mode Select the blend mode of the Lens Flare Element this asset creates: • Additive • Screen • Premultiplied • Lerp Transform Property Description Position Offset Defines the offset of the lens flare's position in screen space, relative to its source. Auto Rotate Enable this property to automatically rotate the Lens Flare Texture relative to its angle on the screen. Unity uses the Auto Rotate angle to override the Rotation parameter. To ensure the Lens Flare can rotate, assign a value greater than 0 to the Starting Position property. Rotation Rotates the lens flare. This value operates in degrees of rotation. Size Use this to adjust the scale of this lens flare element. This property is not available when the Type is set to Image and Preserve Aspect Ratio is enabled. Scale The size of this lens flare element in world space. AxisTransform Property Description Starting Position Defines the starting position of the lens flare relative to its source. This value operates in screen space. Angular Offset Controls the angular offset of the lens flare, relative to its current position. This value operates in degrees of rotation. Translation Scale Limits the size of the lens flare offset. For example, values of (1, 0) create a horizontal lens flare, and (0, 1) create a vertical lens flare. You can also use this property to control how quickly the lens flare appears to move. For example, values of (0.5, 0.5) make the lens flare element appear to move at half the speed. Distortion Property Description Enable Set this property to True to enable distortion. Radial Edge Size Controls the size of the distortion effect from the edge of the screen. Radial Edge Curve Blends the distortion effect along a curve from the center of the screen to the edges of the screen. Relative To Center Set this value to True to make distortion relative to the center of the screen. Otherwise, distortion is relative to the screen position of the lens flare. Multiple Elements Property Description Enable Enable this to allow multiple lens flare elements in your scene. Count Determines the number of identical lens flare elements Unity generates. A value of 1 appears the same as a single lens flare element. Distribution Select the method that Unity uses to generate multiple lens flare elements: •Uniform •Curve •Random Length Spread Controls how spread out multiple lens flare elements appear. Relative To Center If true the distortion is relative to center of the screen otherwise relative to lensFlare source screen position. Uniform Property Description Colors The range of colors that this asset applies to the lens flares. Rotation The angle of rotation (in degrees) applied to each element incrementally. Curve Property Description Colors The range of colors that this asset applies to the lens flares. You can use the Position Spacing curve to determine how this range affects each lens flare. Position Variation Adjust this curve to change the placement of the lens flare elements in the Lens Spread. Rotation The uniform angle of rotation (in degrees) applied to each element distributed along the curve. This value ranges from -180° to 180°. Scale Adjust this curve to control the size range of the lens flare elements. Random Property Description Seed The base value that this asset uses to generate randomness. Intensity Variation Controls the variation of brightness across the lens flare elements. A high value can make some elements might invisible. Colors The range of colors that this asset applies to the lens flares. This property is based on the Seed value. Position Variation Controls the position of the lens flares. The X value is spread along the same axis as Length Spread. A value of 0 means there is no change in the lens flare position. The Y value is spread along the vertical screen space axis based on the Seed value. Rotation Variation Controls the rotation variation of the lens flares, based on the Seed value. The Rotation and Auto Rotate parameters inherit from this property. Scale Variation Controls the scale of the lens flares based on the Seed value."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shared/lens-flare/lens-flare-component.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/shared/lens-flare/lens-flare-component.html",
    "title": "Lens Flare (SRP) component | FSM Unity Framework",
    "keywords": "Lens Flare (SRP) component Unity’s Scriptable Render Pipeline (SRP) includes the Lens Flare (SRP) component which renders a lens flare in your scene. This is the SRP equivalent of the Built-in Render Pipeline's Lens Flare component, which is incompatible with SRPs. You can attach a Lens Flare (SRP) component to any GameObject, but some properties only appear when you attach a Lens Flare (SRP) component to a light. Creating lens flares in SRP The Lens Flare (SRP) component controls where the lens flare is as well as properties such as attenuation and whether the lens flare considers occlusion. For properties that define how the lens flare looks, SRP uses the Lens Flare (SRP) Data asset. Each Lens Flare (SRP) component must reference a Lens Flare (SRP) data asset to display a lens flare on-screen. To create a lens flare in a scene: Create or select a GameObject to attach the lens flare too. In the Inspector, click Add Component. Select Rendering > Lens Flare (SRP). Currently, the lens flare doesn't render in the scene because the component doesn't reference a Lens Flare (SRP) Data asset in its Lens Flare Data property. Create a new Lens Flare (SRP) Data asset (menu: Assets > Create > Lens Flare (SRP)). In the Lens Flare (SRP) component Inspector, assign the new Lens Flare (SRP) Data asset to the Lens Flare Data property. Select the Lens Flare (SRP) Data asset and, in the Inspector, add a new element to the Elements list. A default white lens flare now renders at the position of the Lens Flare (SRP) component. For information on how to customize how the lens flare looks, see Lens Flare (SRP) Data. Properties General Property Description Lens Flare Data Select the Lens Flare (SRP) Data asset this component controls. Intensity Multiplies the intensity of the lens flare. Scale Multiplies the scale of the lens flare. Attenuation by Light Shape Enable this property to automatically change the appearance of the lens flare based on the type of light you attached this component to. For example, if this component is attached to a spot light and the camera is looking at this light from behind, the lens flare will not be visible. This property is only available when this component is attached to a light. Attenuation Distance The distance between the start and the end of the Attenuation Distance Curve. This value operates between 0 and 1 in world space. Attenuation Distance Curve Fades out the appearance of the lens flare over the distance between the GameObject this asset is attached to, and the Camera. Scale Distance The distance between the start and the end of the Scale Distance Curve. This value operates between 0 and 1 in world space. Scale Distance Curve Changes the size of the lens flare over the distance between the GameObject this asset is attached to, and the Camera. Screen Attenuation Curve Reduces the effect of the lens flare based on its distance from the edge of the screen. You can use this to display a lens flare at the edge of your screen Occlusion Property Description Enable Enable this property to partially obscure the lens flare based on the depth buffer Occlusion Radius Defines how far from the light source Unity occludes the lens flare. This value is in world space. Sample Count The number of random samples the CPU uses to generate the Occlusion Radius. Occlusion Offset Offsets the plane that the occlusion operates on. A higher value moves this plane closer to Camera. This value is in world space. For example, if a lens flare is inside the light bulb, you can use this to sample occlusion outside the light bulb. Occlusion Remap Curve Allow the occlusion [from 0 to 1] to be remap with any desired shape. Allow Off Screen Enable this property to allow lens flares outside the Camera's view to affect the current field of view."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/simple-lit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/simple-lit-shader.html",
    "title": "Simple Lit Shader | FSM Unity Framework",
    "keywords": "Simple Lit Shader Use this Shader when performance is more important than photorealism. This Shader uses a simple approximation for lighting. Because this Shader does not calculate for physical correctness and energy conservation, it renders quickly. Using the Simple Lit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Simple Lit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how the Material is rendered on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Select how Unity calculates the color of each pixel of a transparent Material when it blends the Material with the background. In the context of Blending Modes, Source refers to the transparent Material where the Blending Mode is set and Destination refers to anything that Material overlaps with. Alpha Alpha blending mode. Alpha uses the Material's alpha value to change how transparent an object is. 0 is fully transparent. 255 is fully opaque, this is translated to a value of 1 when used with the blending equations. The Material is always rendered in the Transparent render pass regardless of it's alpha value. This mode lets you use the Preserve Specular Lighting property. Alpha equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB × (1 − SourceAlpha) Premultiply Premultiply blending mode. Premultiply first multiplies the RGB values of the transparent Material by its alpha value then applies a similar effect to the Material as Alpha. The equation for Premultiply also allows areas of the transparent Material with an alpha value of 0 to have an additive blend effect. This can help reduce artifacts that may appear at the edge of the overlap between opaque and transparent pixels. Premultiply equation: OutputRGBA = SourceRGB + DestinationRGB × (1 − SourceAlpha) Additive Additive blending mode. Additive adds the color values of the Materials together to create the blend effect. The alpha value determines the strength of the source Material's color before the blend is calculated. This mode lets you use the Preserve Specular Lighting property. Additive equation: OutputRGBA = (SourceRGB × SourceAlpha) + DestinationRGB Multiply Multiply blending mode. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you look through colored glass. This mode uses the Material’s alpha value to adjust how much the colors blend. An alpha value of 1 results in unadjusted multiplication of the colors while lower values blend the colors towards white. Multiply equation: OutputRGBA = SourceRGB × DestinationRGB Preserve Specular Lighting Indicates whether Unity preserves specular highlights on a GameObject or not. This applies even when your surface is transparent which means that only the reflected light is visible. This property only appears when the Surface Type is set to Transparent and Blending Mode is set to either Alpha or Additive. Material with Preserve Specular Lighting disabled. Material with Preserve Specular Lighting enabled. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the back face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface, also known as the diffuse map. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. Specular Map Controls the color of your specular highlights from direct lighting, for example Directional, Point, and Spot lights. To assign a Texture to the Specular Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the textures in your Project. Alternatively, you can use the color picker. In Source, you can select a Texture in your Project to act as a source for the smoothness. By default, the source is the Alpha channel for this Texture. You can use the Smoothness slider to control the spread of highlights on the surface. 0 gives a wide, rough highlight. 1 gives a small, sharp highlight like glass. Values in between produce semi-glossy looks. For example, 0.5 produces a plastic-like glossiness. Normal Map Adds a normal map to the surface. With a normal map, you can add surface details like bumps, scratches and grooves. To add the map, click the object picker next to it. The normal map picks up ambient lighting in the environment. Emission Makes the surface look like it emits lights. When enabled, the Emission Map and Emission Color settings appear. To assign an Emission Map, click the object picture next to it. This opens the Asset Browser, where you can select from the textures in your Project. For Emission Color, you can use the color picker to assign a tint on top of the color. This can be more than 100% white, which is useful for effects like lava, that shines brighter than white while still being another color. If you have not assigned an Emission Map, the Emission setting only uses the tint you’ve assigned in Emission Color. If you do not enable Emission, URP sets the emission to black and does not calculate emission. Tiling A 2D multiplier value that scales the Texture to fit across a mesh according to the U and V axes. This is good for surfaces like floors and walls. The default value is 1, which means no scaling. Set a higher value to make the Texture repeat across your mesh. Set a lower value to stretch the Texture. Try different values until you reach your desired effect. Offset The 2D offset that positions the Texture on the mesh. To adjust the position on your mesh, move the Texture across the U or V axes. Advanced The Advanced settings affect the underlying calculations of your rendering. They do not have a visible effect on your surface. Property Description Specular Highlights Enable this to allow your Material to have specular highlights from direct lighting, for example Directional, Point, and Spot lights. This means that your Material reflects the shine from these light sources. Disable this to leave out these highlight calculations, so your Shader renders faster. By default, this feature is enabled. Enable GPU Instancing Makes URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Sorting Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/speedtree.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/speedtree.html",
    "title": "SpeedTree Shaders | FSM Unity Framework",
    "keywords": "SpeedTree Shaders The Universal Render Pipeline uses the SpeedTree system for tree Shaders. To read more about that, read the SpeedTree documentation in the Unity main manual. When you use SpeedTree Shaders in URP, keep the following in mind: There is no Global Illumination on trees in URP. Trees cannot receive shadows in URP. In URP, you can configure whether lights should be per vertex of per pixel in the URP Asset."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/understand-performance.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/understand-performance.html",
    "title": "Understand performance | FSM Unity Framework",
    "keywords": "Understand performance The performance of your project depends on the Universal Render Pipeline (URP) features you use or enable, what your scenes contain, and which platforms you target. You can use the Unity Profiler or a GPU profiler such as RenderDoc or Xcode to check how much URP uses memory, the CPU and the GPU in your project. You can then use the information to enable or disable the features and settings that have the largest performance impact. URP usually performs better if you change settings that reduce the following: How much URP uses the CPU. For example, you can disable URP updating volumes every frame. How much memory URP uses to store textures. For example, you can disable High Dynamic Range (HDR) if you don't need it, to reduce the size of the color buffer. How many render textures URP copies to and from memory, which has a large impact on mobile platforms. For example, you can disable URP creating a depth texture if you don't need it. The number of passes in the render pipeline. For example, you can disable the opaque texture if you don't need it, or disable additional lights casting shadows. The number of draw calls URP sends to the GPU. For example, you can enable the SRP Batcher. The number of pixels URP renders to the screen, which has a big effect on mobile platforms where the GPU is less powerful. For example, you can reduce the render scale. Refer to the following for more information about which settings to disable or change to improve performance: Configure for better performance Optimize for better performance"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universal-additional-camera-data.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universal-additional-camera-data.html",
    "title": "The Universal Additional Camera Data component | FSM Unity Framework",
    "keywords": "The Universal Additional Camera Data component The Universal Additional Camera Data component is a component that the Universal Render Pipeline (URP) uses for internal data storage. The Universal Additional Camera Data component allows URP to extend and override the functionality and appearance of Unity's standard Camera component. In URP, a GameObject that has a Camera component must also have a Universal Additional Camera Data component. If your Project uses URP, Unity automatically adds the Universal Additional Camera Data component when you create a Camera GameObject. You cannot remove the Universal Additional Camera Data component from a Camera GameObject. If you are not using scripts to control and customise URP, you do not need to do anything with the Universal Additiona Camera Data component. If you are using scripts to control and customise URP, you can access a Camera's Universal Additional Camera Data component in a script like this: var cameraData = camera.GetUniversalAdditionalCameraData(); For more information, see the UniversalAdditionalCameraData API documentation. If you need to access the Universal Additional Camera Data component frequently in a script, you should cache the reference to it to avoid unnecessary CPU work. Preset When using Preset of a Camera, only a subset of properties are supported. Unsupported properties are hidden."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universal-additional-light-data.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universal-additional-light-data.html",
    "title": "The Universal Additional Light Data component | FSM Unity Framework",
    "keywords": "The Universal Additional Light Data component Universal Additional Light Data is a component that the Universal Render Pipeline (URP) uses for internal data storage. The Universal Additional Light Data component allows URP to extend and override the functionality of Unity's standard Light component. In URP, a GameObject that has a Light component must also have a Universal Additional Light Data component. If your Project uses URP, Unity automatically adds the Universal Additional Light Data component when you create a Light GameObject. You cannot remove the Universal Additional Light Data component from a Light GameObject."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universalrp-asset.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universalrp-asset.html",
    "title": "Universal Render Pipeline Asset | FSM Unity Framework",
    "keywords": "Universal Render Pipeline Asset Any Unity project that uses the Universal Render Pipeline (URP) must have a URP Asset to configure the settings. When you create a project using the URP template, Unity creates the URP Assets in the Settings project folder and assigns them in Project Settings. If you are migrating an existing project to URP, you need to create a URP Asset and assign the asset in the Graphics settings. The URP Asset controls several graphical features and quality settings for the Universal Render Pipeline. It is a scriptable object that inherits from ‘RenderPipelineAsset’. When you assign the asset in the Graphics settings, Unity switches from the built-in render pipeline to the URP. You can then adjust the corresponding settings directly in the URP, instead of looking for them elsewhere. You can have multiple URP assets and switch between them. For example, you can have one with Shadows on and one with Shadows off. If you switch between the assets to see the effects, you don’t have to manually toggle the corresponding settings for shadows every time. You cannot, however, switch between HDRP/SRP and URP assets, as the render pipelines are incompatible. UI overview In the URP, you can configure settings for: Rendering Quality Lighting Shadows Post-processing Adaptive Performance Note: If you have the experimental 2D Renderer enabled (menu: Graphics Settings > add the 2D Renderer Asset under Scriptable Render Pipeline Settings), some of the options related to 3D rendering in the URP Asset don't have any impact on your final app or game. How to show Additional Properties Unity does not show certain advanced properties in the URP Asset by default. To see all available properties: In the URP Asset, in any section, click the vertical ellipsis icon (⋮) and select Show Additional Properties Unity shows all available properties in the current section. To show all additional properties in all sections: Click the vertical ellipsis icon (⋮) and select Show All Additional Properties. Unity opens the Core Render Pipeline section in the Preferences window. In the property Additional Properties > Visibility, select All Visible. Rendering The Rendering settings control the core part of the pipeline rendered frame. Property Description Depth Texture Enables URP to create a _CameraDepthTexture. URP then uses this depth texture by default for all Cameras in your Scene. You can override this for individual cameras in the Camera Inspector. If you enable this setting, URP uses more memory. Opaque Texture Enable this to create a _CameraOpaqueTexture as default for all cameras in your Scene. This works like the GrabPass in the built-in render pipeline. The Opaque Texture provides a snapshot of the scene right before URP renders any transparent meshes. You can use this in transparent Shaders to create effects like frosted glass, water refraction, or heat waves. You can override this for individual cameras in the Camera Inspector. If you enable this setting, URP uses more memory. Opaque Downsampling Set the sampling mode on the opaque texture to one of the following: None: Produces a copy of the opaque pass in the same resolution as the camera. 2x Bilinear: Produces a half-resolution image with bilinear filtering. 4x Box: Produces a quarter-resolution image with box filtering. This produces a softly blurred copy. 4x Bilinear: Produces a quarter-resolution image with bi-linear filtering. Terrain Holes If you disable this option, URP removes all Terrain hole Shader variants when you build for the Unity Player, which decreases build time and reduces processing time on the GPU. SRP Batcher Check this box to enable the SRP Batcher. This is useful if you have many different Materials that use the same Shader. The SRP Batcher is an inner loop that speeds up CPU rendering without affecting GPU performance. When you use the SRP Batcher, it replaces the SRP rendering code inner loop. Dynamic Batching Enable Dynamic Batching, to make the render pipeline automatically batch small dynamic objects that share the same Material. This is useful for platforms and graphics APIs that do not support GPU instancing. If your targeted hardware does support GPU instancing, disable Dynamic Batching. You can change this at run time. Debug Level Set the level of debug information that the render pipeline generates. The values are: Disabled: Debugging is disabled. This is the default. Profiling: Makes the render pipeline provide detailed information tags, which you can see in the FrameDebugger. Shader Variant Log Level Set the level of information about Shader Stripping and Shader Variants you want to display when Unity finishes a build. Values are: Disabled: Unity doesn’t log anything. Only Universal: Unity logs information for all of the URP Shaders. All: Unity logs information for all Shaders in your build. You can see the information in Console panel when your build has finished. Store Actions Defines if Unity discards or stores the render targets of the DrawObjects Passes. Selecting the Store option significantly increases the memory bandwidth on mobile and tile-based GPUs. Auto: Unity uses the Discard option by default, and falls back to the Store option if it detects any injected Passes. Discard: Unity discards the render targets of render Passes that are not reused later (lower memory bandwidth). Store: Unity stores all render targets of each Pass (higher memory bandwidth). Quality These settings control the quality level of the URP. This is where you can make performance better on lower-end hardware or make graphics look better on higher-end hardware. Tip: If you want to have different settings for different hardware, you can configure these settings across multiple Universal Render Pipeline assets, and switch them out as needed. Property Description HDR Enable this to allow rendering in High Dynamic Range (HDR) by default for every camera in your Scene. With HDR, the brightest part of the image can be greater than 1. This gives you a wider range of light intensities, so your lighting looks more realistic. With it, you can still see details and experience less saturation even with bright light. This is useful if you want a wide range of lighting or to use bloom effects. If you’re targeting lower-end hardware, you can disable this to skip HDR calculations and get better performance. You can override this for individual cameras in the Camera Inspector. HDR Precision The precision of the Camera color buffer in HDR rendering. The 64 bit precision lets you avoid banding artifacts, but requires higher bandwidth and might make sampling slower. Default value: 32 bit. Anti Aliasing (MSAA) Use Multisample Anti-aliasing by default for every Camera in your Scene while rendering. This softens edges of your geometry, so they’re not jagged or flickering. In the drop-down menu, select how many samples to use per pixel: 2x, 4x, or 8x. The more samples you choose, the smoother your object edges are. If you want to skip MSAA calculations, or you don’t need them in a 2D game, select Disabled. You can override this for individual cameras in the Camera Inspector. Note: On mobile platforms that do not support the StoreAndResolve store action, if Opaque Texture is selected in the URP asset, Unity ignores the Anti Aliasing (MSAA) property at runtime (as if Anti Aliasing (MSAA) is set to Disabled). Render Scale This slider scales the render target resolution (not the resolution of your current device). Use this when you want to render at a smaller resolution for performance reasons or to upscale rendering to improve quality. This only scales the game rendering. UI rendering is left at the native resolution for the device. Upscaling Filter Select which image filter Unity uses when performing the upscaling. Unity performs upscaling when the Render Scale value is less than 1.0. Automatic Unity selects one of the filtering options based on the Render Scale value and the current screen resolution. If integer scaling is possible, Unity selects the Nearest-Neighbor option, otherwise Unity selects the Bilinear option. Bilinear Unity uses the bilinear or linear filtering provided by the graphics API. Nearest-Neighbor Unity uses the nearest-neighbor or point sampling filtering provided by the graphics API. FidelityFX Super Resolution 1.0 Unity uses the AMD FidelityFX Super Resolution 1.0 (FSR) technique to perform upscaling. Unlike the other Upscaling Filter options, this filter remains active even at a Render Scale value of 1.0. This filter can still improve image quality even when no scaling is occurring. This also makes the transition between scale values 0.99 and 1.0 less noticeable in cases where dynamic resolution scaling is active. Note: This filter is only supported on devices that support Unity shader model 4.5 or higher. On devices that do not support Unity shader model 4.5, Unity uses the Automatic option instead. Override FSR Sharpness Unity shows this check box when you select the FSR filter. Selecting this check box lets you specify the intensity of the FSR sharpening pass. FSR Sharpness Specify the intensity of the FSR sharpening pass. A value of 0.0 provides no sharpening, a value of 1.0 provides maximum sharpness. This option has no effect when FSR is not the active upscaling filter. LOD Cross Fade Use this property to enable or disable the LOD cross-fade. If you disable this option, URP removes all LOD cross-fade shader variants when you build the Unity Player, which decreases the build time . If you target a low-end mobile platform, disable this setting so URP doesn't use alpha testing to fade level of detail (LOD) meshes in and out, which reduces processing time on the GPU. LOD Cross Fade Dithering Type When an LOD group has Fade Mode set to Cross Fade, Unity renders the Renderer's LOD meshes with cross-fade blending between them using alpha testing. This property defines the type of LOD cross-fade. Options: Bayer Matrix: better performance than the Blue Noise option, but has a repetitive pattern. Blue Noise: uses a precomputed blue noise texture and provides a better look than the Bayer Matrix option, but has a slightly higher performance cost. Lighting These settings affect the lights in your Scene. If you disable some of these settings, the relevant keywords are stripped from the Shader variables. If there are settings that you know for certain you won’t use in your game or app, you can disable them to improve performance and reduce build time. Property Description Main Light These settings affect the main Directional Light in your Scene. You can select this by assigning it as a Sun Source in the Lighting Inspector. If you don’t assign a sun source, the URP treats the brightest directional light in the Scene as the main light. You can choose between Pixel Lighting and None. If you choose None, URP doesn’t render a main light, even if you’ve set a sun source. Cast Shadows Check this box to make the main light cast shadows in your Scene. On lower-end platforms, you can disable this setting to reduce how much memory URP uses, and reduce processing time on the CPU and the GPU. Shadow Resolution This controls how large the shadow map texture for the main light is. High resolutions give sharper, more detailed shadows. If memory or rendering time is an issue, try a lower resolution. Mixed Lighting When Mixed Lighting is enabled, Unity includes mixed lighting shader variants in the build. Use Rendering Layers With this option selected, you can configure certain Lights to affect only specific GameObjects. For more information on Rendering Layers and how to use them, see the page Rendering Layers. If you enable this setting and you use the Deferred rendering path, URP uses more memory. Light Cookies When enabled, URP includes light cookie shader variants in the build. If you enable this setting, URP uses more memory. Refer to Light component reference for more information. Additional Lights Choose whether to add additional lights that supplement your main light. Choose between Per Vertex, Per Pixel, or Disabled. To reduce processing time on the GPU, set to Disabled, or Per Vertex if you use the Forward or Forward+ rendering path. Per Object Limit This slider sets the limit for how many additional lights can affect each GameObject. Cast Shadows Check this box to make the additional lights cast shadows in your Scene. Shadow Atlas Resolution Control the size of the textures that cast directional shadows for the additional lights. This is a sprite atlas that packs up to 16 shadow maps. High resolutions give sharper, more detailed shadows. If memory or rendering time is an issue, try a lower resolution. Mixed Lighting Enable Mixed Lighting to configure the pipeline to include mixed lighting shader variants in the build. Shadows These settings let you configure how shadows look and behave, and find a good balance between the visual quality and performance. The Shadows section has the following properties. Property Description Max Distance The maximum distance from the Camera at which Unity renders the shadows. Unity does not render shadows farther than this distance. Increasing the distance reduces the performance. Note: This property is in metric units regardless of the value in the Working Unit property. Working Unit The unit in which Unity measures the shadow cascade distances. Cascade Count The number of shadow cascades. With shadow cascades, you can avoid crude shadows close to the Camera and keep the Shadow Resolution reasonably low. For more information, see the page Shadow Cascades. Increasing the number of cascades reduces the performance. Cascade settings only affects the main light. Split 1 The distance where cascade 1 ends and cascade 2 starts. Split 2 The distance where cascade 2 ends and cascade 3 starts. Split 3 The distance where cascade 3 ends and cascade 4 starts. Last Border The size of the area where Unity fades out the shadows. Unity starts fading out shadows at the distance Max Distance - Last Border, at Max Distance the shadows fade to zero. Depth Bias Use this setting to reduce shadow acne. Normal Bias Use this setting to reduce shadow acne. Soft Shadows Select this check box to enable extra processing of the shadow maps to give them a smoother look. Performance impact: high. When this option is disabled, Unity samples the shadow map once with the default hardware filtering, which reduces processing time on the GPU. Quality Select the quality level of soft shadow processing. Available options: Low: good balance of quality and performance for mobile platforms. Filtering method: 4 PCF taps. Medium: good balance of quality and performance for desktop platforms. Filtering method: 5x5 tent filter. This is the default value. High: best quality, higher performance impact. Filtering method: 7x7 tent filter. Conservative Enclosing Sphere Enable this option to improve shadow frustum culling and prevent Unity from excessively culling shadows in the corners of the shadow cascades. Disable this option only for compatibility purposes of existing projects created in previous Unity versions. If you enable this option in an existing project, you might need to adjust the shadows cascade distances because the shadow culling enclosing spheres change their size and position. Performance impact: enabling this option is likely to improve performance, because the option minimizes the overlap of shadow cascades, which reduces the number of redundant static shadow casters. Post-processing This section allows you to fine-tune global post-processing settings. Property Description Post Processing This check box turns post-processing on (check box selected) or off (check box cleared) for the current URP asset. If you clear this check box, Unity excludes post-processing shaders and textures from the build, unless one of the following conditions is true: Other assets in the build refer to the assets related to post-processing. A different URP asset has the Post Processing property enabled. Post Process Data The asset containing references to shaders and Textures that the Renderer uses for post-processing. Note: Changes to this property are necessary only for advanced customization use cases. Grading Mode Select the color grading mode to use for the Project. High Dynamic Range: This mode works best for high precision grading similar to movie production workflows. Unity applies color grading before tonemapping. Low Dynamic Range: This mode follows a more classic workflow. Unity applies a limited range of color grading after tonemapping. LUT Size Set the size of the internal and external look-up textures (LUTs) that the Universal Render Pipeline uses for color grading. Higher sizes provide more precision, but have a potential cost of performance and memory use. You cannot mix and match LUT sizes, so decide on a size before you start the color grading process. The default value, 32, provides a good balance of speed and quality. Fast sRGB/Linear Conversions Select this option to use faster, but less accurate approximation functions when converting between the sRGB and Linear color spaces. Volume Update Mode Select how Unity updates Volumes: every frame or when triggered via scripting. If you select Every Frame, URP requires more processing time on the CPU. In the Editor, Unity updates Volumes every frame when not in the Play mode. Adaptive Performance This section is available if the Adaptive Performance package is installed in the project. The Use Adaptive Performance property lets you enable the Adaptive Performance functionality. Property Description Use Adaptive Performance Select this check box to enable the Adaptive Performance functionality, which adjusts the rendering quality at runtime."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universalrp-builtin-feature-comparison.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/universalrp-builtin-feature-comparison.html",
    "title": "Feature comparison table | FSM Unity Framework",
    "keywords": "Feature comparison table For an overview of the features supported in the Universal Render Pipeline (URP) compared to the other Unity render pipelines, refer to the page Render pipeline feature comparison."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/unlit-shader.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/unlit-shader.html",
    "title": "Unlit Shader | FSM Unity Framework",
    "keywords": "Unlit Shader Use this Shader for effects or unique objects in your visuals that don’t need lighting. Because there are no time-consuming lighting calculations or lookups, this Shader is optimal for lower-end hardware. The Unlit Shader uses the most simple shading model in URP. Using the Unlit Shader in the Editor To select and use this Shader: In your Project, create or find the Material you want to use the Shader on. Select the Material. A Material Inspector window opens. Click Shader, and select Universal Render Pipeline > Unlit. UI overview The Inspector window for this Shader contains these elements: Surface Options Surface Inputs Advanced Surface Options The Surface Options control how the Material is rendered on a screen. Property Description Surface Type Use this drop-down to apply an Opaque or Transparent surface type to the Material. This determines which render pass URP renders the material in. Opaque surface types are always fully visible, regardless of what’s behind them. URP renders opaque Materials first. Transparent surface types are affected by their background, and they can vary according to which type of transparent surface type you choose. URP renders transparent Materials in a separate pass after opaque objects. If you select Transparent, the Blending Mode drop-down appears. Blending Mode Use this drop-down to determine how URP calculates the color of each pixel of the transparent Material by blending the Material with the background pixels. Alpha uses the Material’s alpha value to change how transparent an object is. 0 is fully transparent. 1 appears fully opaque, but the Material is still rendered during the Transparent render pass. This is useful for visuals that you want to be fully visible but to also fade over time, like clouds. Premultiply applies a similar effect to the Material as Alpha, but preserves reflections and highlights, even when your surface is transparent. This means that only the reflected light is visible. For example, imagine transparent glass. Additive adds an extra layer to the Material, on top of another surface. This is good for holograms. Multiply multiplies the color of the Material with the color behind the surface. This creates a darker effect, like when you view an through tinted glass. Render Face Use this drop-down to determine which sides of your geometry to render. Front Face renders the front face of your geometry and culls the back face. This is the default setting. Back Face renders the front face of your geometry and culls the front face. Both makes URP render both faces of the geometry. This is good for small, flat objects, like leaves, where you might want both sides visible. Alpha Clipping Makes your Material act like a Cutout Shader. Use this to create a transparent effect with hard edges between the opaque and transparent areas. For example, to create blades of grass. To achieve this effect, URP does not render alpha values below the specified Threshold, which appears when you enable Alpha Clipping. You can set the Threshold by moving the slider, which accepts values from 0 to 1. All values above your threshold are fully opaque, and all values below your threshold are invisible. For example, a threshold of 0.1 means that URP doesn't render alpha values below 0.1. The default value is 0.5. Surface Inputs The Surface Inputs describe the surface itself. For example, you can use these properties to make your surface look wet, dry, rough, or smooth. Property Description Base Map Adds color to the surface, also known as the diffuse map. To assign a Texture to the Base Map setting, click the object picker next to it. This opens the Asset Browser, where you can select from the Textures in your Project. Alternatively, you can use the color picker. The color next to the setting shows the tint on top of your assigned Texture. To assign another tint, you can click this color swatch. If you select Transparent or Alpha Clipping under Surface Options, your Material uses the Texture’s alpha channel or color. Tiling A 2D multiplier value that scales the Texture to fit across a mesh according to the U and V axes. This is good for surfaces like floors and walls. The default value is 1, which means no scaling. Set a higher value to make the Texture repeat across your mesh. Set a lower value to stretch the Texture. Try different values until you reach your desired effect. Offset The 2D offset that positions the Texture on the mesh. To adjust the position on your mesh, move the Texture across the U or V axes. Advanced The Advanced settings affect the underlying calculations of your rendering. They do not have a visible effect on your surface. Property Description Enable GPU Instancing Makes URP render meshes with the same geometry and Material in one batch, when possible. This makes rendering faster. URP cannot render Meshes in one batch if they have different Materials or if the hardware does not support GPU instancing. Priority Use this slider to determine the chronological rendering order for a Material. URP renders Materials with lower values first. You can use this to reduce overdraw on devices by making the pipeline render Materials in front of other Materials first, so it doesn't have to render overlapping areas twice. This works similarly to the render queue in the built-in Unity render pipeline."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-10-0-x.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-10-0-x.html",
    "title": "Upgrading to version 10.0.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 10.0.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 10.0.x. Upgrading from URP 7.2.x and later releases URP 10.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-10-1-x.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-10-1-x.html",
    "title": "Upgrading to version 10.1.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 10.1.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 10.1.x. Upgrading from URP 10.0.x URP 10.1.x does not have breaking changes compared with URP 10.0.x. To upgrade URP to version 10.1.x, install the new version of the package. Upgrading from URP 7.2.x and later releases URP 10.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-11-0-x.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-11-0-x.html",
    "title": "Upgrading to version 11.0.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 11.0.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 11.0.x. Upgrading from URP 10.0.x–10.2.x The file names of the following Shader Graph shaders were renamed. The new file names do not have spaces: Autodesk Interactive Autodesk Interactive Masked Autodesk Interactive Transparent If your code uses the Shader.Find() method to search for the shaders, remove spaces from the shader names, for example, Shader.Find(\"AutodeskInteractive). Upgrading from URP 7.2.x and later releases URP 11.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Shadow Normal Bias In 11.0.x the formula used to apply Shadow Normal Bias has been slightly fix in order to work better with punctual lights. As a result, to match exactly shadow outlines from earlier revisions, the parameter might to be adjusted in some scenes. Typically, using 1.4 instead of 1.0 for a Directional light is usually enough. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2021-2.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2021-2.html",
    "title": "Upgrading to version 12 of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 12 of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 12.0.x. For information on converting assets made for a Built-in Render Pipeline project to assets compatible with URP, see the page Render Pipeline Converter. Upgrading from URP 11.x.x The Forward Renderer asset is renamed to the Universal Renderer asset. When you open an existing project in the Unity Editor containing URP 12, Unity updates the existing Forward Renderer assets to Universal Renderer assets. The Universal Renderer asset contains the property Rendering Path that lets you select the Forward or the Deferred Rendering Path. The method ClearFlag.Depth does not implicitly clear the Stencil buffer anymore. Use the new method ClearFlag.Stencil. URP 12 implements the Render Pipeline Converter feature. This feature replaces the asset upgrade functions that were previously available at Edit > Render Pipeline > Universal Render Pipeline > Upgrade... Upgrading from URP 10.0.x–10.2.x The file names of the following Shader Graph shaders were renamed. The new file names do not have spaces: Autodesk Interactive Autodesk Interactive Masked Autodesk Interactive Transparent If your code uses the Shader.Find() method to search for the shaders, remove spaces from the shader names, for example, Shader.Find(\"AutodeskInteractive). Upgrading from URP 7.2.x and later releases URP 12.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Shadow Normal Bias In 11.0.x the formula used to apply Shadow Normal Bias has been slightly fix in order to work better with punctual lights. As a result, to match exactly shadow outlines from earlier revisions, the parameter might to be adjusted in some scenes. Typically, using 1.4 instead of 1.0 for a Directional light is usually enough. Intermediate Texture In previous URP versions, URP performed the rendering via an intermediate Renderer if the Renderer had any active Renderer Features. On some platforms, this had significant performance implications. In this release, URP mitigates the issue in the following way: URP expects Renderer Features to declare their inputs using the ScriptableRenderPass.ConfigureInput method. The method provides the information that URP uses to determine automatically whether rendering via an intermediate texture is necessary. For compatibility purpose, there is a new property Intermediate Texture in the Universal Renderer. If you select Always in the property, URP uses an intermediate texture. Selecting Auto enables the new behavior. Use the Always option only if a Renderer Feature does not declare its inputs using the ScriptableRenderPass.ConfigureInput method. To ensure that existing projects work correctly, all existing Universal Renderer assets that were using any Renderer Features (excluding those included with URP) have the option Always selected in the Intermediate Texture property. Any newly created Universal Renderer assets have the option Auto selected. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. Upgrading from LWRP to 12.x.x There is no direct upgrade path from LWRP to URP 12.x.x. Follow the steps to upgrade LWRP to URP 11.x.x first, and then upgrade from URP 11.x.x to URP 12.x.x."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2022-1.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2022-1.html",
    "title": "Upgrading to URP 13 (Unity 2022.1) | FSM Unity Framework",
    "keywords": "Upgrading to URP 13 (Unity 2022.1) This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to URP 13 (Unity 2022.1). For information on converting assets made for a Built-in Render Pipeline project to assets compatible with URP, see the page Render Pipeline Converter. Upgrading from URP 12 (Unity 2021.2) Changes to ScriptableRenderer API behavior Unity now issues an error when instances of ScriptableRendererFeature attempt to access render targets before they are allocated by the ScriptableRenderer class. The ScriptableRendererFeature class has a new virtual function SetupRenderPasses which is called when render targets are allocated and ready to be used. If your code uses the ScriptableRenderer.cameraColorTarget or the ScriptableRenderer.cameraDepthTarget property inside of the AddRenderPasses method override, you should move that implementation to the ScriptableRendererFeature.SetupRenderPasses method. The calls to the ScriptableRenderer.EnqueuePass method should still happen in the AddRenderPasses method. The following example shows how to change the code to use the new API. Code with the old API: public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { // The target is used before allocation m_CustomPass.Setup(renderer.cameraColorTarget); // Letting the renderer know which passes are used before allocation renderer.EnqueuePass(m_ScriptablePass); } Code with the new API: public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { // Letting the renderer know which passes are used before allocation renderer.EnqueuePass(m_ScriptablePass); } public override void SetupRenderPasses(ScriptableRenderer renderer, in RenderingData renderingData) { // The target is used after allocation m_CustomPass.Setup(renderer.cameraColorTarget); } The Universal Renderer is now using the RTHandle system The Universal Renderer is now using the RTHandle system for its internal targets and in its internal passes. All usages of the RenderTargetHandle struct are set as obsolete and the struct will be removed in the future. The public interfaces ScriptableRenderer.cameraColorTarget and ScriptableRenderer.cameraDepthTarget are marked as obsolete. Replace them with ScriptableRenderer.cameraColorTargetHandle and ScriptableRenderer.cameraDepthTargetHandle respectively. RTHandle targets do not use the CommandBuffer.GetTemporaryRT method and persist for more frames than the RenderTargetIdentifier structs. You cannot allocate RTHandle targets with the properties GraphicsFormat and DepthBufferBits set to any value except for 0. The cameraDepthTarget properties must be separate from the cameraColorTarget properties. The following helper functions let you create and use temporary render target with the RTHandle system in a similar way as with the GetTemporaryRT method previously: RenderingUtils.ReAllocateIfNeeded ShadowUtils.ShadowRTReAllocateIfNeeded If the render target does not change within the lifetime of the application, use the RTHandles.Alloc method to allocate an RTHandle target. This method is efficient since the code does not have to check if a render target should be allocated on each frame. If the render target is a full screen texture, which means that its resolution matches or is a fraction of the resolution of the screen, use a scaling factor such as Vector2D.one to support dynamic scaling. The following example shows how to change the code using the RenderTargetHandle API to use the new API. Code with the old API: public class CustomPass : ScriptableRenderPass { RenderTargetHandle m_Handle; // With the old API, RenderTargetIdentifier might combine color and depth RenderTargetIdentifier m_Destination; public CustomPass() { m_Handle.Init(\"_CustomPassHandle\"); } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { var desc = renderingData.cameraData.cameraTargetDescriptor; cmd.GetTemporaryRT(m_Handle.id, desc, FilterMode.Point); } public override void OnCameraCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(m_Handle.id); } public void Setup(RenderTargetIdentifier destination) { m_Destination = destination; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); // Set the same target for color and depth ScriptableRenderer.SetRenderTarget(cmd, m_Destination, m_Destination, clearFlag, clearColor); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } Code with the new API: public class CustomPass : ScriptableRenderPass { RTHandle m_Handle; // Then using RTHandles, the color and the depth properties must be separate RTHandle m_DestinationColor; RTHandle m_DestinationDepth; void Dispose() { m_Handle?.Release(); } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { var desc = renderingData.cameraData.cameraTargetDescriptor; // Then using RTHandles, the color and the depth properties must be separate desc.depthBufferBits = 0; RenderingUtils.ReAllocateIfNeeded(ref m_Handle, desc, FilterMode.Point, TextureWrapMode.Clamp, name: \"_CustomPassHandle\"); } public override void OnCameraCleanup(CommandBuffer cmd) { m_DestinationColor = null; m_DestinationDepth = null; } public void Setup(RTHandle destinationColor, RTHandle destinationDepth) { m_DestinationColor = destinationColor; m_DestinationDepth = destinationDepth; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); CoreUtils.SetRenderTarget(cmd, m_DestinationColor, m_DestinationDepth, clearFlag, clearColor); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } Upgrading from URP 11.x.x The Forward Renderer asset is renamed to the Universal Renderer asset. When you open an existing project in the Unity Editor containing URP 12, Unity updates the existing Forward Renderer assets to Universal Renderer assets. The Universal Renderer asset contains the property Rendering Path that lets you select the Forward or the Deferred Rendering Path. The method ClearFlag.Depth does not implicitly clear the Stencil buffer anymore. Use the new method ClearFlag.Stencil. URP 12 and later implements the Render Pipeline Converter feature. This feature replaces the asset upgrade functions that were previously available at Edit > Render Pipeline > Universal Render Pipeline > Upgrade... Upgrading from URP 10.0.x–10.2.x The file names of the following Shader Graph shaders were renamed. The new file names do not have spaces: Autodesk Interactive Autodesk Interactive Masked Autodesk Interactive Transparent If your code uses the Shader.Find() method to search for the shaders, remove spaces from the shader names, for example, Shader.Find(\"AutodeskInteractive). Upgrading from URP 7.2.x and later releases URP 12.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Shadow Normal Bias In 11.0.x the formula used to apply Shadow Normal Bias has been slightly fix in order to work better with punctual lights. As a result, to match exactly shadow outlines from earlier revisions, the parameter might to be adjusted in some scenes. Typically, using 1.4 instead of 1.0 for a Directional light is usually enough. Intermediate Texture In previous URP versions, URP performed the rendering via an intermediate Renderer if the Renderer had any active Renderer Features. On some platforms, this had significant performance implications. In this release, URP mitigates the issue in the following way: URP expects Renderer Features to declare their inputs using the ScriptableRenderPass.ConfigureInput method. The method provides the information that URP uses to determine automatically whether rendering via an intermediate texture is necessary. For compatibility purpose, there is a new property Intermediate Texture in the Universal Renderer. If you select Always in the property, URP uses an intermediate texture. Selecting Auto enables the new behavior. Use the Always option only if a Renderer Feature does not declare its inputs using the ScriptableRenderPass.ConfigureInput method. To ensure that existing projects work correctly, all existing Universal Renderer assets that were using any Renderer Features (excluding those included with URP) have the option Always selected in the Intermediate Texture property. Any newly created Universal Renderer assets have the option Auto selected. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. Upgrading from LWRP to 12.x.x There is no direct upgrade path from LWRP to URP 12.x.x. Follow the steps to upgrade LWRP to URP 11.x.x first, and then upgrade from URP 11.x.x to URP 12.x.x."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2022-2.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-2022-2.html",
    "title": "Upgrading to URP 14 (Unity 2022.2) | FSM Unity Framework",
    "keywords": "Upgrading to URP 14 (Unity 2022.2) This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to URP 14 (Unity 2022.2). For information on converting assets made for a Built-in Render Pipeline project to assets compatible with URP, see the page Render Pipeline Converter. Upgrading from URP 13 (Unity 2022.1) Two shader defines were removed SHADER_QUALITY_LOW/MEDIUM/HIGH and SHADER_HINT_NICE_QUALITY shader defines were removed. If you used those defines in custom shaders, consider using SHADER_API_MOBILE or SHADER_API_GLES defines to replace SHADER_QUALITY_LOW/MEDIUM/HIGH. Upgrading from URP 12 (Unity 2021.2) Changes to ScriptableRenderer API behavior Unity now issues an error when instances of ScriptableRendererFeature attempt to access render targets before they are allocated by the ScriptableRenderer class. The ScriptableRendererFeature class has a new virtual function SetupRenderPasses which is called when render targets are allocated and ready to be used. If your code uses the ScriptableRenderer.cameraColorTarget or the ScriptableRenderer.cameraDepthTarget property inside of the AddRenderPasses method override, you should move that implementation to the ScriptableRendererFeature.SetupRenderPasses method. The calls to the ScriptableRenderer.EnqueuePass method should still happen in the AddRenderPasses method. The following example shows how to change the code to use the new API. Code with the old API: public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { // The target is used before allocation m_CustomPass.Setup(renderer.cameraColorTarget); // Letting the renderer know which passes are used before allocation renderer.EnqueuePass(m_ScriptablePass); } Code with the new API: public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { // Letting the renderer know which passes are used before allocation renderer.EnqueuePass(m_ScriptablePass); } public override void SetupRenderPasses(ScriptableRenderer renderer, in RenderingData renderingData) { // The target is used after allocation m_CustomPass.Setup(renderer.cameraColorTarget); } The Universal Renderer is now using the RTHandle system The Universal Renderer is now using the RTHandle system for its internal targets and in its internal passes. All usages of the RenderTargetHandle struct are set as obsolete and the struct will be removed in the future. The public interfaces ScriptableRenderer.cameraColorTarget and ScriptableRenderer.cameraDepthTarget are marked as obsolete. Replace them with ScriptableRenderer.cameraColorTargetHandle and ScriptableRenderer.cameraDepthTargetHandle respectively. RTHandle targets do not use the CommandBuffer.GetTemporaryRT method and persist for more frames than the RenderTargetIdentifier structs. You cannot allocate RTHandle targets with the properties GraphicsFormat and DepthBufferBits set to any value except for 0. The cameraDepthTarget properties must be separate from the cameraColorTarget properties. The following helper functions let you create and use temporary render target with the RTHandle system in a similar way as with the GetTemporaryRT method previously: RenderingUtils.ReAllocateIfNeeded ShadowUtils.ShadowRTReAllocateIfNeeded If the render target does not change within the lifetime of the application, use the RTHandles.Alloc method to allocate an RTHandle target. This method is efficient since the code does not have to check if a render target should be allocated on each frame. If the render target is a full screen texture, which means that its resolution matches or is a fraction of the resolution of the screen, use a scaling factor such as Vector2D.one to support dynamic scaling. The following example shows how to change the code using the RenderTargetHandle API to use the new API. Code with the old API: public class CustomPass : ScriptableRenderPass { RenderTargetHandle m_Handle; // With the old API, RenderTargetIdentifier might combine color and depth RenderTargetIdentifier m_Destination; public CustomPass() { m_Handle.Init(\"_CustomPassHandle\"); } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { var desc = renderingData.cameraData.cameraTargetDescriptor; cmd.GetTemporaryRT(m_Handle.id, desc, FilterMode.Point); } public override void OnCameraCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(m_Handle.id); } public void Setup(RenderTargetIdentifier destination) { m_Destination = destination; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); // Set the same target for color and depth ScriptableRenderer.SetRenderTarget(cmd, m_Destination, m_Destination, clearFlag, clearColor); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } Code with the new API: public class CustomPass : ScriptableRenderPass { RTHandle m_Handle; // Then using RTHandles, the color and the depth properties must be separate RTHandle m_DestinationColor; RTHandle m_DestinationDepth; void Dispose() { m_Handle?.Release(); } public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData) { var desc = renderingData.cameraData.cameraTargetDescriptor; // Then using RTHandles, the color and the depth properties must be separate desc.depthBufferBits = 0; RenderingUtils.ReAllocateIfNeeded(ref m_Handle, desc, FilterMode.Point, TextureWrapMode.Clamp, name: \"_CustomPassHandle\"); } public override void OnCameraCleanup(CommandBuffer cmd) { m_DestinationColor = null; m_DestinationDepth = null; } public void Setup(RTHandle destinationColor, RTHandle destinationDepth) { m_DestinationColor = destinationColor; m_DestinationDepth = destinationDepth; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { CommandBuffer cmd = CommandBufferPool.Get(); CoreUtils.SetRenderTarget(cmd, m_DestinationColor, m_DestinationDepth, clearFlag, clearColor); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); } } Upgrading from URP 11.x.x The Forward Renderer asset is renamed to the Universal Renderer asset. When you open an existing project in the Unity Editor containing URP 12, Unity updates the existing Forward Renderer assets to Universal Renderer assets. The Universal Renderer asset contains the property Rendering Path that lets you select the Forward or the Deferred Rendering Path. The method ClearFlag.Depth does not implicitly clear the Stencil buffer anymore. Use the new method ClearFlag.Stencil. URP 12 and later implements the Render Pipeline Converter feature. This feature replaces the asset upgrade functions that were previously available at Edit > Render Pipeline > Universal Render Pipeline > Upgrade... Upgrading from URP 10.0.x–10.2.x The file names of the following Shader Graph shaders were renamed. The new file names do not have spaces: Autodesk Interactive Autodesk Interactive Masked Autodesk Interactive Transparent If your code uses the Shader.Find() method to search for the shaders, remove spaces from the shader names, for example, Shader.Find(\"AutodeskInteractive). Upgrading from URP 7.2.x and later releases URP 12.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. DepthNormals Pass Starting from version 10.0.x, URP can generate a normal texture called _CameraNormalsTexture. To render to this texture in your custom shader, add a Pass with the name DepthNormals. For example, see the implementation in Lit.shader. Screen Space Ambient Occlusion (SSAO) URP 10.0.x implements the Screen Space Ambient Occlusion (SSAO) effect. If you intend to use the SSAO effect with your custom shaders, consider the following entities related to SSAO: The _SCREEN_SPACE_OCCLUSION keyword. Input.hlsl contains the new declaration float2 normalizedScreenSpaceUV in the InputData struct. Lighting.hlsl contains the AmbientOcclusionFactor struct with the variables for calculating indirect and direct occlusion: struct AmbientOcclusionFactor { half indirectAmbientOcclusion; half directAmbientOcclusion; }; Lighting.hlsl contains the following function for sampling the SSAO texture: half SampleAmbientOcclusion(float2 normalizedScreenSpaceUV) Lighting.hlsl contains the following function: AmbientOcclusionFactor GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) To support SSAO in custom shader, add the DepthNormals Pass and the _SCREEN_SPACE_OCCLUSION keyword the the shader. For example, see Lit.shader. If your custom shader implements custom lighting functions, use the function GetScreenSpaceAmbientOcclusion(float2 normalizedScreenSpaceUV) to get the AmbientOcclusionFactor value for your lighting calculations. Shadow Normal Bias In 11.0.x the formula used to apply Shadow Normal Bias has been slightly fix in order to work better with punctual lights. As a result, to match exactly shadow outlines from earlier revisions, the parameter might to be adjusted in some scenes. Typically, using 1.4 instead of 1.0 for a Directional light is usually enough. Intermediate Texture In previous URP versions, URP performed the rendering via an intermediate Renderer if the Renderer had any active Renderer Features. On some platforms, this had significant performance implications. In this release, URP mitigates the issue in the following way: URP expects Renderer Features to declare their inputs using the ScriptableRenderPass.ConfigureInput method. The method provides the information that URP uses to determine automatically whether rendering via an intermediate texture is necessary. For compatibility purpose, there is a new property Intermediate Texture in the Universal Renderer. If you select Always in the property, URP uses an intermediate texture. Selecting Auto enables the new behavior. Use the Always option only if a Renderer Feature does not declare its inputs using the ScriptableRenderPass.ConfigureInput method. To ensure that existing projects work correctly, all existing Universal Renderer assets that were using any Renderer Features (excluding those included with URP) have the option Always selected in the Intermediate Texture property. Any newly created Universal Renderer assets have the option Auto selected. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. Upgrading from LWRP to 12.x.x There is no direct upgrade path from LWRP to URP 12.x.x. Follow the steps to upgrade LWRP to URP 11.x.x first, and then upgrade from URP 11.x.x to URP 12.x.x."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-2-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-2-0.html",
    "title": "Upgrading to version 7.2.0 of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 7.2.0 of the Universal Render Pipeline On this page, you will find information about upgrading from an older version of the Universal Render Pipeline (URP) to the current version. Building your Project for consoles To build a Project for a console, you need to install an additional package for each platform you want to support. For more information, see the documentation on Building for Consoles. Require Depth Texture In previous versions of URP, if post-processing was enabled it would cause the pipeline to always require depth. We have improved the post-processing integration to only require depth from the pipeline when Depth of Field, Motion Blur or SMAA effects are enabled. This improves performance in many cases. Because Cameras that use post-processing no longer require depth by default, you must now manually indicate that Cameras require depth if you are using it for other effects, such as soft particles. To make all Cameras require depth, enable the the Depth Texture option in the Pipeline Asset. To make an individual Camera require depth, set Depth Texture option to On in the Camera Inspector. Sampling shadows from the Main Light In previous versions of URP, if shadow cascades were enabled for the main Light, shadows would be resolved in a screen space pass. The pipeline now always resolves shadows while rendering opaque or transparent objects. This allows for consistency and solved many issues regarding shadows. If have custom HLSL shaders and sample _ScreenSpaceShadowmapTexture texture, you must upgrade them to sample shadows by using the GetMainLight function instead. For example: float4 shadowCoord = TransformWorldToShadowCoord(positionWorldSpace); Light mainLight = GetMainLight(inputData.shadowCoord); // now you can use shadow to apply realtime occlusion half shadow = mainLight.shadowAttenuation; You must also define the following in your .shader file to make sure your custom shader can receive shadows correctly: #pragma multi_compile _ _MAIN_LIGHT_SHADOWS #pragma multi_compile _ _MAIN_LIGHT_SHADOWS_CASCADE Transparent receiving shadows Transparent objects can now receive shadows when using shadow cascades. You can also optionally disable shadow receiving for transparent to improve performance. To do so, disable Transparent Receive Shadows in the Forward Renderer asset."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-3-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-3-0.html",
    "title": "Upgrading to version 7.3.0 of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 7.3.0 of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 7.3.0. Upgrading from URP 7.2.x URP 7.3.0 does not have breaking changes compared with URP 7.2.x. To upgrade URP to version 7.3.0, install the new version of the package. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. Perform the procedure Upgrading from URP 7.2.x."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-4-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-7-4-0.html",
    "title": "Upgrading to version 7.4.0 of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 7.4.0 of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 7.4.0. Upgrading from URP 7.2.x and later URP 7.4.0 does not have breaking changes compared with URP 7.2.x and later. To upgrade URP to version 7.4.0, install the new version of the package. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. Perform the procedure Upgrading from URP 7.2.x."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-0-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-0-0.html",
    "title": "Upgrading to version 8.0.0 of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 8.0.0 of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 8.0.0. Upgrading from URP 7.2.x and later URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-1-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-1-0.html",
    "title": "Upgrading to version 8.1.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 8.1.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 8.1.x. Upgrading from URP 8.0.x URP 8.1.x does not have breaking changes compared with URP 8.0.x. To upgrade URP to version 8.1.x, install the new version of the package. Upgrading from URP 7.2.x and later 7.x releases URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-2-0.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-8-2-0.html",
    "title": "Upgrading to version 8.2.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 8.2.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 8.2.x. Upgrading from URP 8.0.x-8.1.x URP 8.2.x does not have breaking changes compared with URP 8.0.x. To upgrade URP to version 8.2.x, install the new version of the package. Upgrading from URP 7.2.x and later 7.x releases URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use this package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-9-0-x.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guide-9-0-x.html",
    "title": "Upgrading to version 9.0.x of the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading to version 9.0.x of the Universal Render Pipeline This page describes how to upgrade from an older version of the Universal Render Pipeline (URP) to version 9.0.x. Upgrading from URP 8.0.x and later 8.x releases URP 9.0.x does not have breaking changes compared with URP 8.x.x. To upgrade URP to version 9.0.x, install the new version of the package. Upgrading from URP 7.2.x and later 7.x releases URP 9.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first. Upgrading from URP 7.0.x-7.1.x Upgrade to URP 7.2.0 first. Refer to Upgrading to version 7.2.0 of the Universal Render Pipeline. URP 8.x.x does not support the package Post-Processing Stack v2. If your Project uses the package Post-Processing Stack v2, migrate the effects that use that package first."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guides.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-guides.html",
    "title": "Upgrade guides | FSM Unity Framework",
    "keywords": "Upgrade guides This section contains information about upgrading from an older version of the Universal Render Pipeline (URP) to a more recent version, and about upgrading from the Lightweight Render Pipeline (LWRP) to URP. For information on converting assets made for a Built-in Render Pipeline project to assets compatible with URP, see the page Render Pipeline Converter. Upgrading to URP 2022.2 Upgrading to URP 2022.1 Upgrading to URP 2021.2 Upgrading to URP 11.0.x Upgrading to URP 10.1.x Upgrading to URP 10.0.x Upgrading to URP 9.0.x Upgrading to URP 8.1.0 Upgrading to URP 8.0.0 Upgrading to URP 7.4.0 Upgrading to URP 7.3.0 Upgrading to URP 7.2.0 Upgrading from LWRP to URP"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-lwrp-to-urp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrade-lwrp-to-urp.html",
    "title": "Upgrading from the Lightweight Render Pipeline to the Universal Render Pipeline | FSM Unity Framework",
    "keywords": "Upgrading from the Lightweight Render Pipeline to the Universal Render Pipeline The Universal Render Pipeline (URP) replaces the Lightweight Render Pipeline (LWRP) in Unity 2019.3. If your Project uses LWRP, you must upgrade it to use URP to use Unity 2019.3. Unity upgrades some things automatically, and you must make some manual changes. Follow the steps in this guide to transition from using LWRP to using URP. Before upgrading Update Assembly Definition Assets URP uses GUIDs instead of Assembly Definition string names. If you are using Assembly Definition Assets (ASMDefs) in your Project, you should ensure that Use GUIDs is enabled on each of them. Unity upgrades any existing string references to LWRP automatically as part of the upgrade process, but it is best practice to use GUIDs on your Assembly Definition Assets for future proofing. For each Assembly Definition Asset in your Project: Select the Assembly Defintion Asset In the Inspector, enable Use GUIDs For information on using Assembly Definition files, see the documentation on Assembly Definitions. Upgrade process Upgrading your version of LWRP To start the upgrade process: Open the Project you want to upgrade in Unity 2019.3 Unity automatically updates LWRP to a 7.x.x version, and pulls in the URP package as a dependency of the updated LWRP package. The Unity script updater automatically upgrades your script files. When the script updater has finished, all of your scripts should compile properly. Upgrading the Shader search path If your LWRP Project uses Shader.Find to search for LWRP Shaders, you need to change the search path. To do this: Change all instances of Shader.Find that search for Lightweight to search for Universal. Upgrading custom shaders Upgrading tags URP uses its own scripting tags. If your Shaders use the LWRP LightMode tags, they will work in your URP Project, because Unity uses an internal alias for this. However, you should change the tags manually to future-proof your Project. To do this: Change all instances of Lightweight2D tag to Universal2D. Change all instances of LightweightForward tag to UniversalForward. In addition to this, URP also uses a different RenderPipeline tag to LWRP. If your own Shaders include this tag, you need to change it manually for the Shaders to work: Change all instances of LightweightPipeline tag to UniversalPipeline. Upgrading Shader names The following Shader names have been changed for URP, so you need to manually update your Shader files: Change all instances of UsePass 'Lightweight Render Pipeline/...' to UsePass 'Universal Render Pipeline/...' Upgrading include paths URP uses different include paths to LWRP. LWRP 7.x.x contains forwarding includes, so your custom Shaders will upgrade from LWRP to URP. However, URP 7.x.x does not contain forwarding includes, so you must then manually update the include paths. Change all instances of #include 'Packages/com.unity.render-pipelines.lightweight/xxx' to #include 'Packages/com.unity.render-pipelines.universal/xxx' Upgrading namespaces In the .cs files in your Project, find and replace references to the LWRP namespace with the new Universal namespace. Change all instances of UnityEditor.Rendering.LWRP.xxx to now UnityEditor.Rendering.Universal.xxx Upgrading post-processing effects URP version 7.x supports both Post Processing Stack v2 (PPv2) and its own integrated post-processing solution. If you have the Post Processing Version 2 package installed in your Project and you want to use URP's integrated post-processing solution, you need to delete the Post Processing Stack v2 package before you install URP into your Project. When you have installed URP, you can then recreate your post-processing effects. Upgrading post-processing effects from LWRP to URP is a manual process. You must manually recreate each Post-Processing Profile in your Project, using URP's post-processing implementation. URP's integrated post-processing solution does not currently support custom post-processing effects. If your Project uses custom post-processing effects, these cannot currently be recreated in URP's integrated post-processing solution. Custom post-processing effects will be supported in a forthcoming release of URP. Installing URP and removing LWRP As part of the automatic upgrade process, Unity installed URP as a dependency of LWRP. You must now install URP as a dependency of the Project itself, so that when you remove LWRP, Unity does not automatically remove URP. To install URP as a dependency of the Project: Go to menu: Window > Package Manager. Locate the Universal RP package, and note the version number to the right of its name. This is the version of URP that has been added to your Project. Close Unity. In your file explorer, open the root folder of your Unity Project. Open the Packages folder, and locate manifest.json. This is your Project's Project Manifest file. Open the Project Manifest file using a text editor. At the top of the dependencies section, add the following entry: \"com.unity.render-pipelines.universal\": \"[Version number you noted earlier]\" So, for example, if the version of URP was 7.1.1, your dependencies section would look like this: \"dependencies\": { \"com.unity.render-pipelines.universal\": \"7.1.1\", ... } This marks the version of URP that you have installed as a dependency of the Project. You can now safely remove LWRP. Open your Project in Unity. Open the Package Manager Window. Locate Lightweight RP and select it. In the bottom right of the Package Manager window, click Remove. Unity completely removes the LWRP package from the Project."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrading-your-shaders.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/upgrading-your-shaders.html",
    "title": "Converting your shaders | FSM Unity Framework",
    "keywords": "Converting your shaders Shaders written for the Built-in Render Pipeline are not compatible with the URP shaders. For an overview of the mapping between built-in shaders and URP shaders, see Shader mappings. Use the Render Pipeline Converter to apply the shader mappings automatically. Note: The Render Pipeline Converter makes irreversible changes to the project. Back up your project before the conversion. Tip: If the preview thumbnails in the Project view are not shown correctly after the conversion, try right-clicking anywhere in the Project view and selecting Reimport All. For SpeedTree Shaders, Unity does not re-generate Materials when you re-import them, unless you click the Generate Materials or Apply & Generate Materials button. Custom shaders You cannot upgrade Custom Unity shaders written for the Built-in Render Pipeline. Instead, custom shaders must be rewritten to work with URP or recreated in ShaderGraph. Any Materials in a Scene that use a custom shader when you upgrade a project to use URP turn pink to indicate the Material no longer works. To fix this, upgrade or change the Material's shader to one that is compatible with URP. Note: URP does not support Surface Shaders. Shader mappings The following table shows which URP shaders the Built-in Render Pipeline shaders convert to when you use the Render Pipeline Converter. Unity built-in shader Universal Render Pipeline shader Standard Universal Render Pipeline/Lit Standard (Specular Setup) Universal Render Pipeline/Lit Standard Terrain Universal Render Pipeline/Terrain/Lit Particles/Standard Surface Universal Render Pipeline/Particles/Lit Particles/Standard Unlit Universal Render Pipeline/Particles/Unlit Mobile/Diffuse Universal Render Pipeline/Simple Lit Mobile/Bumped Specular Universal Render Pipeline/Simple Lit Mobile/Bumped Specular(1 Directional Light) Universal Render Pipeline/Simple Lit Mobile/Unlit (Supports Lightmap) Universal Render Pipeline/Simple Lit Mobile/VertexLit Universal Render Pipeline/Simple Lit Legacy Shaders/Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Bumped Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Bumped Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Self-Illumin/Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Self-Illumin/Bumped Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Self-Illumin/Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Self-Illumin/Bumped Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Bumped Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Bumped Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Cutout/Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Cutout/Specular Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Cutout/Bumped Diffuse Universal Render Pipeline/Simple Lit Legacy Shaders/Transparent/Cutout/Bumped Specular Universal Render Pipeline/Simple Lit"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-concepts.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-concepts.html",
    "title": "URP Concepts | FSM Unity Framework",
    "keywords": "URP Concepts This section describes the concepts and settings that let you configure the Universal Render Pipeline. This section contains the following topics: The Universal Render Pipeline Asset Universal Renderer Renderer Feature How to add a Renderer Feature"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-global-settings.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-global-settings.html",
    "title": "URP Global Settings | FSM Unity Framework",
    "keywords": "URP Global Settings If a project has the URP package installed, Unity shows the URP Global Settings section in the Graphics tab in the Project Settings window. The URP Global Settings section lets you define project-wide settings for URP. The section contains the following settings. Rendering Layers (3D) Use this section to define the names of Rendering Layers. Rendering Layers only work with 3D Renderers. Shader Stripping The check boxes in this section define which shader variants Unity strips when you build the Player. Property Description Shader Variant Log Level Select what information about Shader variants Unity saves in logs when you build your Unity Project. Options: • Disabled: Unity doesn't save any shader variant information. • Only SRP Shaders: Unity saves only shader variant information for URP shaders. • All Shaders: Unity saves shader variant information for every shader type. Strip Debug Variants When enabled, Unity strips all debug view shader variants when you build the Player. This decreases build time, but prevents the use of Rendering Debugger in Player builds. Strip Unused Post Processing Variants When enabled, Unity assumes that the Player does not create new Volume Profiles at runtime. With this assumption, Unity only keeps the shader variants that the existing Volume Profiles use, and strips all the other variants. Unity keeps shader variants used in Volume Profiles even if the Scenes in the project do not use the Profiles. Strip Unused Variants When enabled, Unity performs shader stripping in a more efficient way. This option reduces the amount of shader variants in the Player by a factor of 2 if the project uses the following URP features: Rendering Layers Native Render Pass Reflection Probe Blending Reflection Probe Box Projection SSAO Renderer Feature Decal Renderer Feature Certain post-processing effects Disable this option only if you see issues in the Player. Strip Screen Coord Override Variants When enabled, Unity strips Screen Coordinates Override shader variants in Player builds."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-lighting-mode.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-lighting-mode.html",
    "title": "Lighting Mode | FSM Unity Framework",
    "keywords": "Lighting Mode The Lighting Mode that you choose in the Lighting window determines the behavior of all Mixed Lights in the current Scene. URP supports the following Lighting Modes: Baked Indirect Subtractive Shadowmask"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-optimization.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-optimization.html",
    "title": "Optimization and debugging | FSM Unity Framework",
    "keywords": "Optimization and debugging This section contains information related to optimization and debugging. This section contains the following topics: Rendering Debugger"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-renderer-feature-how-to-add.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-renderer-feature-how-to-add.html",
    "title": "How to add a Renderer Feature to a Renderer | FSM Unity Framework",
    "keywords": "How to add a Renderer Feature to a Renderer To add a Renderer Feature to a Renderer: In the Project window, select a Renderer. The Inspector window shows the Renderer properties. In the Inspector window, select Add Renderer Feature. In the list, select a Renderer Feature. Unity adds the selected Renderer Feature to the Renderer. Unity shows Renderer Features as child items of the Renderer in the Project Window:"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-renderer-feature.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-renderer-feature.html",
    "title": "URP Renderer Feature | FSM Unity Framework",
    "keywords": "URP Renderer Feature A Renderer Feature is an asset that lets you add extra Render passes to a URP Renderer and configure their behavior. For examples of how to use Renderer Features, see the Renderer Features samples in URP Package Samples. How to add a Renderer Feature For information on how to add a Renderer Feature to a Renderer, see the page How to add a Renderer Feature to a Renderer. Available Renderer Features The following Renderer Features are available in URP: Render Objects Screen Space Ambient Occlusion Decal Screen Space Shadows Full Screen Pass"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-shaders/urp-shaderlab-pass-tags.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-shaders/urp-shaderlab-pass-tags.html",
    "title": "URP ShaderLab Pass tags | FSM Unity Framework",
    "keywords": "URP ShaderLab Pass tags This section contains descriptions of URP-specific ShaderLab Pass tags. NOTE: URP does not support the following LightMode tags: Always, ForwardAdd, PrepassBase, PrepassFinal, Vertex, VertexLMRGBM, VertexLM. LightMode The value of this tag lets the pipeline determine which Pass to use when executing different parts of the Render Pipeline. If you do not set the LightMode tag in a Pass, URP uses the SRPDefaultUnlit tag value for that Pass. The LightMode tag can have the following values. Property Description UniversalForward The Pass renders object geometry and evaluates all light contributions. URP uses this tag value in the Forward Rendering Path. UniversalGBuffer The Pass renders object geometry without evaluating any light contribution. Use this tag value in Passes that Unity must execute in the Deferred Rendering Path. UniversalForwardOnly The Pass renders object geometry and evaluates all light contributions, similarly to when LightMode has the UniversalForward value. The difference from UniversalForward is that URP can use the Pass for both the Forward and the Deferred Rendering Paths. Use this value if a certain Pass must render objects with the Forward Rendering Path when URP is using the Deferred Rendering Path. For example, use this tag if URP renders a Scene using the Deferred Rendering Path and the Scene contains objects with shader data that does not fit the GBuffer, such as Clear Coat normals. If a shader must render in both the Forward and the Deferred Rendering Paths, declare two Passes with the UniversalForward and UniversalGBuffer tag values. If a shader must render using the Forward Rendering Path regardless of the Rendering Path that the URP Renderer uses, declare only a Pass with the LightMode tag set to UniversalForwardOnly. If you use the SSAO Renderer Feature, add a Pass with the LightMode tag set to DepthNormalsOnly. For more information, see the DepthNormalsOnly value. DepthNormalsOnly Use this value in combination with UniversalForwardOnly in the Deferred Rendering Path. This value lets Unity render the shader in the Depth and normal prepass. In the Deferred Rendering Path, if the Pass with the DepthNormalsOnly tag value is missing, Unity does not generate the ambient occlusion around the Mesh. Universal2D The Pass renders objects and evaluates 2D light contributions. URP uses this tag value in the 2D Renderer. ShadowCaster The Pass renders object depth from the perspective of lights into the Shadow map or a depth texture. DepthOnly The Pass renders only depth information from the perspective of a Camera into a depth texture. Meta Unity executes this Pass only when baking lightmaps in the Unity Editor. Unity strips this Pass from shaders when building a Player. SRPDefaultUnlit Use this LightMode tag value to draw an extra Pass when rendering objects. Application example: draw an object outline. This tag value is valid for both the Forward and the Deferred Rendering Paths. URP uses this tag value as the default value when a Pass does not have a LightMode tag. UniversalMaterialType Unity uses this tag in the Deferred Rendering Path. The UniversalMaterialType tag can have the following values. If this tag is not set in a Pass, Unity uses the Lit value. Property Description Lit This value indicates that the shader type is Lit. During the G-buffer Pass, Unity uses stencil to mark the pixels that use the Lit shader type (specular model is PBR). Unity uses this value by default, if the tag is not set in a Pass. SimpleLit This value indicates that the shader type is SimpleLit. During the G-buffer Pass, Unity uses stencil to mark the pixels that use the SimpleLit shader type (specular model is Blinn-Phong)."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-universal-renderer.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/urp-universal-renderer.html",
    "title": "Universal Renderer | FSM Unity Framework",
    "keywords": "Universal Renderer This page describes the URP Universal Renderer settings. For more information on rendering in URP, see also Rendering in the Universal Render Pipeline. Rendering Paths The URP Universal Renderer implements two Rendering Paths: Forward Rendering Path. Forward+ Rendering Path. Deferred Rendering Path. Rendering Path comparison The following table shows the differences between the Forward and the Deferred Rendering Paths in URP. Feature Forward Forward+ Deferred Maximum number of real-time lights per object. 9 Unlimited. The per-Camera limit applies. Unlimited Per-pixel normal encoding No encoding (accurate normal values). No encoding (accurate normal values). Two options: Quantization of normals in G-buffer (loss of accuracy, better performance). Octahedron encoding (accurate normals, might have significant performance impact on mobile GPUs). For more information, see the section Encoding of normals in G-buffer. MSAA Yes Yes No Vertex lighting Yes No No Camera stacking Yes Yes Supported with a limitation: Unity renders only the base Camera using the Deferred Rendering Path. Unity renders all overlay Cameras using the Forward Rendering Path. How to find the Universal Renderer asset To find the Universal Renderer asset that a URP asset is using: Select a URP asset. In the Renderer List section, click a renderer item or the vertical ellipsis icon (⋮) next to a renderer. Universal Renderer asset reference This section describes the properties of the Universal Renderer asset. Filtering This section contains properties that define which layers the renderer draws. Property Description Opaque Layer Mask Select which opaque layers this Renderer draws Transparent Layer Mask Select which transparent layers this Renderer draws Rendering This section contains properties related to rendering. Property Description Rendering Path Select the Rendering Path. Options: Forward: The Forward Rendering Path. Forward+: The Forward+ Rendering Path. Deferred: The Deferred Rendering Path. Depth Priming Mode This property determines when Unity performs depth priming. Depth Priming can improve GPU frame timings by reducing the number of pixel shader executions. The performance improvement depends on the amount of overlapping pixels in the opaque pass and the complexity of the pixel shaders that Unity can skip by using depth priming. The feature has an upfront memory and performance cost. The feature uses a depth prepass to determine which pixel shader invocations Unity can skip, and the feature adds the depth prepass if it's not available yet. The options are: Disabled: Unity does not perform depth priming. Auto: If there is a Render Pass that requires a depth prepass, Unity performs the depth prepass and depth priming. Forced: Unity always performs depth priming. To do this, Unity also performs a depth prepass for every render pass. Note: Depth priming is disabled at runtime on certain hardware (Tile Based Deferred Rendering) regardless of this setting. On Android, iOS, and Apple TV, Unity performs depth priming only in the Forced mode. On tiled GPUs, which are common to those platforms, depth priming might reduce performance when combined with MSAA. This property is available only if Rendering Path is set to Forward Accurate G-buffer normals Indicates whether to use a more resource-intensive normal encoding/decoding method to improve visual quality. This property is available only if Rendering Path is set to Deferred. Depth Texture Mode Specifies at which stage in the render pipeline URP should copy the scene depth to a depth texture. The options are: After Opaques: URP copies the scene depth after the opaques render pass. After Transparents: URP copies the scene depth after the transparents render pass. Force Prepass: URP does a depth prepass to generate the scene depth texture. Note: On mobile devices, the After Transparents option can lead to a significant improvement in memory bandwidth. This is because the Copy Depth pass causes a switch in render target between the Opaque pass and the Transparents pass. When this occurs, Unity stores the contents of the Color Buffer in the main memory, and then loads it again once the Copy Depth pass is complete. The impact increases significantly when MSAA is enabled, because Unity must also store and load the MSAA data alongside the Color Buffer. Native RenderPass This section contains properties related to URP's Native RenderPass API. Property Description Native RenderPass Indicates whether to use URP's Native RenderPass API. When enabled, URP uses this API to structure render passes. As a result, you can use programmable blending in custom URP shaders. Enable Native RenderPass if you use Vulkan, Metal or DirectX 12 graphics APIs, so URP automatically reduces how often it copies render textures into and out of memory. For more information about the RenderPass API, see ScriptableRenderContext.BeginRenderPass. Note: Enabling this property has no effect on OpenGL ES. Shadows This section contains properties related to rendering shadows. Property Description Transparent Receive Shadows When this option is on, Unity draws shadows on transparent objects. Overrides This section contains Render Pipeline properties that this Renderer overrides. Stencil With this check box selected, the Renderer processes the Stencil buffer values. For more information on how Unity works with the Stencil buffer, see ShaderLab: Stencil. Compatibility This section contains settings related to backwards compatibility. Property Description Intermediate Texture This property lets you force URP to renders via an intermediate texture. Options: Auto: URP uses the information provided by the ScriptableRenderPass.ConfigureInput method to determine automatically whether rendering via an intermediate texture is necessary. Always: forces rendering via an intermediate texture. Use this option only for compatibility with Renderer Features that do not declare their inputs with ScriptableRenderPass.ConfigureInput. Using this option might have a significant performance impact on some platforms. Renderer Features This section contains the list of Renderer Features assigned to the selected Renderer. For information on how to add a Renderer Feature, see How to add a Renderer Feature to a Renderer. URP contains the pre-built Renderer Feature called Render Objects."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/using-begincamerarendering.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/using-begincamerarendering.html",
    "title": "Using the beginCameraRendering event | FSM Unity Framework",
    "keywords": "Using the beginCameraRendering event The example on this page shows how to use the beginCameraRendering event to run a custom method. beginCameraRendering event overview Unity raises a beginCameraRendering event before it renders each active Camera in every frame. If a Camera is inactive (for example, if the Camera component checkbox is cleared on a Camera GameObject), Unity does not raise a beginCameraRendering event for this Camera. When you subscribe a method to this event, you can execute custom logic before Unity renders the Camera. Examples of custom logic include rendering extra Cameras to Render Textures, and using those Textures for effects like planar reflections or surveillance camera views. Other events in the RenderPipelineManager class provide more ways to customize URP. You can also use the principles described in this article with those events. beginCameraRendering event example This example demonstrates how to subscribe a method to the beginCameraRendering event. To follow the steps in this example, create a new Unity project using the Universal Project Template. In the Scene, create a Cube. Name it Example Cube. In your Project, create a C# script. Call it URPCallbackExample. Copy and paste the following code into the script. using UnityEngine; using UnityEngine.Rendering; public class URPCallbackExample : MonoBehaviour { // Unity calls this method automatically when it enables this component private void OnEnable() { // Add WriteLogMessage as a delegate of the RenderPipelineManager.beginCameraRendering event RenderPipelineManager.beginCameraRendering += WriteLogMessage; } // Unity calls this method automatically when it disables this component private void OnDisable() { // Remove WriteLogMessage as a delegate of the RenderPipelineManager.beginCameraRendering event RenderPipelineManager.beginCameraRendering -= WriteLogMessage; } // When this method is a delegate of RenderPipeline.beginCameraRendering event, Unity calls this method every time it raises the beginCameraRendering event void WriteLogMessage(ScriptableRenderContext context, Camera camera) { // Write text to the console Debug.Log($\"Beginning rendering the camera: {camera.name}\"); } } NOTE: When you subscribe to an event, your handler method (in this example, WriteLogMessage) must accept the parameters defined in the event delegate. In this example, the event delegate is RenderPipeline.BeginCameraRendering, which expects the following parameters: <ScriptableRenderContext, Camera>. Attach the URPCallbackExample script to Example Cube. Select Play. Unity prints the message from the script in the Console window each time Unity raises the beginCameraRendering event. To raise a call to the OnDisable() method: In the Play mode, select Example Cube and clear the checkbox next to the script component title. Unity unsubscribes WriteLogMessage from the RenderPipelineManager.beginCameraRendering event and stops printing the message in the Console window."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/whats-new/urp-whats-new.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/whats-new/urp-whats-new.html",
    "title": "What's new in URP 14 (Unity 2022.2) | FSM Unity Framework",
    "keywords": "What's new in URP 14 (Unity 2022.2) This section contains information about new features, improvements, and issues fixed in URP 14. For a complete list of changes made in URP 14, refer to the Changelog. Features This section contains the overview of the new features in this release. Full Screen Pass Renderer Feature This Renderer Feature lets you inject full screen render passes at pre-defined injection points to create full screen effects. To read more about the feature, refer to page Full Screen Pass Renderer Feature. Full Screen Pass with a custom Grayscale Material. Custom post-processing effects The Full Screen Pass Renderer Feature lets you create custom post-processing effects with minimum coding effort. To read how to create a simple post-processing effect, see the page How to create a custom post-processing effect. The following images show a fog effect implemented with a Full Screen Render Pass Renderer Feature. The scene without the fog effect: The scene with the custom fog effect implemented as a Full Screen Render Pass Renderer Feature: Implementing a custom effect lets you overcome the limitation of the default Unity fog effect that does not affect the skybox: Rendering layers The Rendering Layers feature lets you configure certain Lights to affect only specific GameObjects. With the Custom Shadow Layers property, you can configure certain GameObjects to cast shadows only from specific Lights (even if those Lights do not affect the GameObjects). In this URP version, Rendering Layers work not only with Lights, but also with Decals. Refer to the following pages to learn more about the feature: Rendering Layers How to use Rendering Layers with Decals Forward+ Rendering Path The Forward+ Rendering Path lets you avoid the per object limit of the Forward Rendering Path. The Forward+ Rendering Path has the following advantages compared with the Forward Rendering Path: There is no per-object limit for the number of Lights that affect GameObjects, the per-Camera limit still applies. The per-Camera limits for different platforms are: Desktop and console platforms: 256 Lights Mobile platforms: 32 Lights. OpenGL ES 3.0 and earlier: 16 Lights. This implementation lets you avoid splitting big meshes when more than 8 lights affect them. Blending of more than 2 reflection probes. Support for multiple Lights when using Unity Entity Component System (ECS). More flexibility with procedural draws. For more information, see the page Forward+ Rendering Path. LOD Cross-fade The LOD cross-fade lets you achieve a smoother transition blending between the current mesh LOD and the next LOD based on the object's distance to the Camera. As the Camera moves, Unity shows different LODs to provide a good balance between quality and processing cost. Cross-fading lets you avoid harsh LOD snapping and popping. 1: LOD cross-fade off. 2: LOD cross-fade on. For more information, see the LOD Cross Fade property. Temporal anti-aliasing (TAA) Temporal anti-aliasing (TAA) is a spatial multi-frame anti-aliasing technique that uses results from current and previous rendered frames to remove jaggies in the current frame and reduce temporal judder between frames. TAA uses Motion Vectors to reduce or avoid shimmer and ghosting artifacts caused by moving objects that end up being in different pixel locations in different frames. To enable TAA for a Camera: Select the Camera. In the Inspector, in the Rendering section, select Temporal Anti-aliasing (TAA) in the Anti-aliasing property. The following image shows a frame with TAA off: The following image shows a frame with TAA on: Improvements This section contains the overview of the major improvements in this release. Screen space ambient occlusion (SSAO) Improvements This release implements multiple performance and quality improvements to the SSAO feature. The Falloff Distance property lets you improve performance for Scenes with a lot of distant objects Performance improvements: New Blur Quality property with three blur options: High, Medium, Low. The Downsample check box now not only affects the Ambient Occlusion pass but also the following blur passes. The Falloff Distance property lets you reduce computational work on objects far away from the Camera. The last Blur pass and the After Opaque pass are now merged into one when the After Opaque option is enabled. The Samples property now has three options with pre-defined sample counts that provide a good balance of visual quality and performance. Quality improvements: The Method property lets you choose the algorithm that Unity uses to calculate the ambient occlusion values. The Blue Noise algorithm is added in this release. A new depth test was added to avoid adding SSAO to objects far away from one another. For more information, see the page Screen Space Ambient Occlusion. 64 bit high precision HDR render target format URP can now render into 64-bit high precision HDR render target format. Compared to the default 32-bit HDR render target, the 64-bit option provides the following benefits: Better precision for all color channels which reduces banding. Even precision for color channels eliminates the subtle blue-yellow banding caused by the lower blue channel precision of the 32-bit target. Support for the alpha channel. The alpha channel enables the alpha output with some limitations. The 64-bit format uses twice more memory compared to the 32-bit format. It can also have a significant performance impact on mobile platforms. To select the 64-bit render target option: in URP Asset, navigate to Quality > HDR > HDR Precision. This setting controls only the internal HDR rendering precision, not HDR output. New bloom quality settings URP 14 adds two new properties to the Bloom post-processing effect: Downscale: set the bloom texture scale to either half size or quarter size. Max Iterations: set the maximum number of scale iterations (down and up) the bloom effect does. This property replaces the Skip Iterations property, which skipped a number of last iterations, but did not limit the maximum number. For more information, see the page Bloom. Improvements to the Render Pipeline Converter This release contains multiple usability improvements of the Render Pipeline Converter: The Render Pipeline Converter window now has the Initialize and Convert button. Certain dialogs now show the number of selected elements and the total number of elements. You can click each converter to see more information about the elements it converter. Improvements to the visual appearance and usability of the Render Pipeline Converter dialogs. Material converter section improvements: Items in the list are sorted alphabetically now. The converter handles Materials in packages better. The converter ignores Shader Graph shaders. Performance improvement: Indexing is significantly faster in this release. This improves the performance of converters that use an .index file. Full screen draws in URP now use the SRP Core Bliter API All the calls to cmd.Blit method are replaced with the Blitter API. This ensures a correct and consistent way to perform full screen draws. In the current URP version, using cmd.Blit might implicitly enable or disable XR shader keywords, which breaks XR SPI rendering. Refer to the Perform a full screen blit in URP page to read how to use the Blitter API. More consistent lighting behavior on different platforms Removed implicit mobile shader optimizations to keep the visual appearance of lighting consistent on all platforms. Changes are mostly related to light fade quality and shadow filtering. Light fade calculations on mobile platforms are now the same as on desktop platforms. In this release, URP implements full quality spherical harmonics and always normalized normals for lighting. SHADER_QUALITY_LOW/MEDIUM/HIGH and SHADER_HINT_NICE_QUALITY shader defines were removed. If you used those defines in custom shaders, consider using SHADER_API_MOBILE or SHADER_API_GLES defines to replace SHADER_QUALITY_LOW/MEDIUM/HIGH. URP uses the SHADER_API_MOBILE define only for platform-specific functions now, and not for implicit quality changes. XRSystem API URP can now use the XRSystem API override the Built-In Render Pipeline stereo matrices. This lets you inject modifications to the projection matrix and the view matrix in URP. Shader stripping improvement: Light cookie stripping The URP Asset now has the option Lighting > Light Cookies that lets you enable or disable Light Cookies. When Light Cookies are disabled in all URP Assets in Graphics and Quality settings, Unity strips all shader variants with Light Cookies. This can reduce the shader variant count up to two times. CPU performance improvements for light-heavy scenes This release contains optimizations to avoid unnecessary copies of Light and Camera data. The changes mostly affect scenes containing tens or hundreds of Lights with Light cookies, with shadows enabled, and when using the Deferred Rendering Path. In scenes with hundreds of Lights that use all the mentioned features, the rendering performance can be up to 25% faster in the Editor. Soft Shadows Quality property on Lights Point Lights and Spot Lights now have the Soft Shadows Quality property. Options Low, Medium, and High let you specify the soft shadow quality value for the Light. Issues resolved For a complete list of issues resolved in URP 14, see the Changelog. Known issues For information on the known issues in URP 14, see the section Known issues."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-custom-shaders-urp.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-custom-shaders-urp.html",
    "title": "Writing custom shaders | FSM Unity Framework",
    "keywords": "Writing custom shaders This section contains guidelines that help you to get started with writing shaders for Universal Render Pipeline (URP). The section contains the following topics: Creating a sample scene URP basic unlit shader Basic ShaderLab structure URP unlit shader with color input Drawing a texture Visualizing normal vectors Reconstruct the world space positions Each example covers some extra information compared to the basic shader example. If you are new to writing shaders using Unity's ShaderLab language, consider going through the sections in the order of appearance on this page."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-basic-prerequisites.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-basic-prerequisites.html",
    "title": "Creating a sample scene | FSM Unity Framework",
    "keywords": "Creating a sample scene To follow the examples in this section: Install URP into an existing Unity project, or create a new project using the Universal Project Template. In the sample Scene, create a GameObject to test the shaders on; for example, a capsule. Create a new Material and assign it to the capsule. Create a new Shader asset and assign it to the Material of the capsule. When following an example, open the shader asset to edit the Unity shader source file. Replace the code in the source file with the code in the example. To start writing URP shaders, continue to section URP unlit basic shader."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-basic-unlit-structure.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-basic-unlit-structure.html",
    "title": "URP unlit basic shader | FSM Unity Framework",
    "keywords": "URP unlit basic shader This example shows a basic URP-compatible shader. This shader fills the mesh shape with a color predefined in the shader code. To see the shader in action, copy and paste the following ShaderLab code into the Shader asset. // This shader fills the mesh shape with a color predefined in the code. Shader \"Example/URPUnlitShaderBasic\" { // The properties block of the Unity shader. In this example this block is empty // because the output color is predefined in the fragment shader code. Properties { } // The SubShader block containing the Shader code. SubShader { // SubShader Tags define when and under which conditions a SubShader block or // a pass is executed. Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } Pass { // The HLSL code block. Unity SRP uses the HLSL language. HLSLPROGRAM // This line defines the name of the vertex shader. #pragma vertex vert // This line defines the name of the fragment shader. #pragma fragment frag // The Core.hlsl file contains definitions of frequently used HLSL // macros and functions, and also contains #include references to other // HLSL files (for example, Common.hlsl, SpaceTransforms.hlsl, etc.). #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" // The structure definition defines which variables it contains. // This example uses the Attributes structure as an input structure in // the vertex shader. struct Attributes { // The positionOS variable contains the vertex positions in object // space. float4 positionOS : POSITION; }; struct Varyings { // The positions in this struct must have the SV_POSITION semantic. float4 positionHCS : SV_POSITION; }; // The vertex shader definition with properties defined in the Varyings // structure. The type of the vert function must match the type (struct) // that it returns. Varyings vert(Attributes IN) { // Declaring the output object (OUT) with the Varyings struct. Varyings OUT; // The TransformObjectToHClip function transforms vertex positions // from object space to homogenous clip space. OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); // Returning the output. return OUT; } // The fragment shader definition. half4 frag() : SV_Target { // Defining the color variable and returning it. half4 customColor = half4(0.5, 0, 0, 1); return customColor; } ENDHLSL } } } The fragment shader colors the GameObject dark red (RGB value (0.5, 0, 0)). The following section introduces you to the structure of this basic Unity shader. Basic ShaderLab structure Unity shaders are written in a Unity-specific language called ShaderLab. The Unity shader in this example has the following blocks: Shader Properties SubShader Pass HLSLPROGRAM Shader block ShaderLab code starts with the Shader declaration. Shader \"Example/URPUnlitShaderBasic\" The path in this declaration determines the display name and location of the Unity shader in the Shader menu on a Material. The method Shader.Find also uses this path. Properties block The Properties block contains the declarations of properties that users can set in the Inspector window on a Material. In this example, the Properties block is empty, because this Unity shader does not expose any Material properties that a user can define. SubShader block A Unity shader source file contains one or more SubShader blocks. When rendering a mesh, Unity selects the first SubShader that is compatible with the GPU on the target device. A SubShader block can optionally contain a SubShader Tags block. Use the Tags keyword to declare a SubShader Tags block. Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } A SubShader Tag with a name of RenderPipeline tells Unity which render pipelines to use this SubShader with, and the value of UniversalPipeline indicates that Unity should use this SubShader with URP. To execute the same shader in different render pipelines, create multiple SubShader blocks with different RenderPipeline tag values. To execute a SubShader block in HDRP, set the RenderPipeline tag to HDRenderPipeline, to execute it in the Built-in Render Pipeline, set RenderPipeline to an empty value. For more information on SubShader Tags, see ShaderLab: SubShader Tags. Pass block In this example, there is one Pass block that contains the HLSL program code. For more information on Pass blocks, see ShaderLab: Pass. A Pass block can optionally contain a Pass tags block. For more information, see URP ShaderLab Pass tags. HLSLPROGRAM block This block contains the HLSL program code. NOTE: HLSL language is the preferred language for URP shaders. NOTE: URP supports the CG language. If you add the CGPROGRAM/ENDCGPROGRAM block in a shader, Unity includes shaders from the Built-in Render Pipeline library automatically. If you include shaders from the SRP shader library, some SRP shader macros and functions might conflict with the Built-in Render Pipeline shader functions. Shaders with the CGPROGRAM block are not SRP Batcher compatible. This block contains the #include declaration with the reference to the Core.hlsl file. #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" The Core.hlsl file contains definitions of frequently used HLSL macros and functions, and also contains #include references to other HLSL files (for example, Common.hlsl and SpaceTransforms.hlsl). For example, the vertex shader in the HLSL code uses the TransformObjectToHClip function from the SpaceTransforms.hlsl file. The function transforms vertex positions from object space to homogenous space: Varyings vert(Attributes IN) { Varyings OUT; OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); return OUT; } The fragment shader in this basic HLSL code outputs the single color predefined in the code: half4 frag() : SV_Target { half4 customColor; customColor = half4(0.5, 0, 0, 1); return customColor; } Section URP unlit shader with color input shows how to add the editable color property in the Inspector window on the Material."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-reconstruct-world-position.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-reconstruct-world-position.html",
    "title": "Reconstruct the world space positions of pixels from the depth texture | FSM Unity Framework",
    "keywords": "Reconstruct the world space positions of pixels from the depth texture The Unity shader in this example reconstructs the world space positions for pixels using a depth texture and screen space UV coordinates. The shader draws a checkerboard pattern on a mesh to visualize the positions. The following illustration shows the end result: This page contains the following sections: Create the sample scene Edit the ShaderLab code The complete ShaderLab code Create the sample scene Create the sample scene to follow the steps in this section: Install URP into an existing Unity project, or create a new project using the Universal Project Template. In the sample Scene, create a plane GameObject and place it so that it occludes some of the GameObjects. Create a new Material and assign it to the plane. Create a new shader and assign it to the material. Copy and paste the Unity shader source code from the page URP unlit basic shader. Select the URP Asset. If you created the project using the Universal Render Pipeline template, the URP Asset path is Assets/Settings/UniversalRP-HighQuality. In the URP Asset, in the General section, enable Depth Texture. Open the shader you created on step 4. Edit the ShaderLab code This section assumes that you copied the source code from the page URP unlit basic shader. Make the following changes to the ShaderLab code: In the HLSLPROGRAM block, add the include declaration for the depth texture shader header. For example, place it under the existing include declaration for Core.hlsl. #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" // The DeclareDepthTexture.hlsl file contains utilities for sampling the Camera // depth texture. #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl\" The DeclareDepthTexture.hlsl file contains functions for sampling the Camera depth texture. This example uses the SampleSceneDepth function for sampling the Z coordinate for pixels. In the fragment shader definition, add Varyings IN as input. half4 frag(Varyings IN) : SV_Target In this example, the fragment shader uses the positionHCS property from the Varyings struct to get locations of pixels. In the fragment shader, to calculate the UV coordinates for sampling the depth buffer, divide the pixel location by the render target resolution _ScaledScreenParams. The property _ScaledScreenParams.xy takes into account any scaling of the render target, such as Dynamic Resolution. float2 UV = IN.positionHCS.xy / _ScaledScreenParams.xy; In the fragment shader, use the SampleSceneDepth functions to sample the depth buffer. #if UNITY_REVERSED_Z real depth = SampleSceneDepth(UV); #else // Adjust z to match NDC for OpenGL real depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(UV)); #endif The SampleSceneDepth function comes from the DeclareDepthTexture.hlsl file. It returns the Z value in the range [0, 1]. For the reconstruction function (ComputeWorldSpacePosition) to work, the depth value must be in the normalized device coordinate (NDC) space. In D3D, Z is in range [0,1], in OpenGL, Z is in range [-1, 1]. This example uses the UNITY_REVERSED_Z constant to determine the platform and adjust the Z value range. See step 6 in this example for more explanations. The UNITY_NEAR_CLIP_VALUE variable is a platform independent near clipping plane value for the clip space. For more information, see Platform-specific rendering differences. Reconstruct world space positions from the UV and Z coordinates of pixels. float3 worldPos = ComputeWorldSpacePosition(UV, depth, UNITY_MATRIX_I_VP); ComputeWorldSpacePosition is a utility function that calculates the world space position from the UV and the depth (Z) values. This function is defined in the Common.hlsl file of the SRP Core package. UNITY_MATRIX_I_VP is an inverse view projection matrix which transforms points from the clip space to the world space. To visualize the world space positions of pixels, create the checkboard effect. uint scale = 10; uint3 worldIntPos = uint3(abs(worldPos.xyz * scale)); bool white = (worldIntPos.x & 1) ^ (worldIntPos.y & 1) ^ (worldIntPos.z & 1); half4 color = white ? half4(1,1,1,1) : half4(0,0,0,1); The scale is the inverse scale of the checkboard pattern size. The abs function mirrors the pattern to the negative coordinate side. The uint3 declaration for the worldIntPos variable snaps the coordinate positions to integers. The AND operator in the expresion <integer value> & 1 checks if the value is even (0) or odd (1). The expression lets the code divide the surface into squares. The XOR operator in the expresion <integer value> ^ <integer value> flips the square color. The depth buffer might not have valid values for areas where no geometry is rendered. The following code draws black color in such areas. #if UNITY_REVERSED_Z if(depth < 0.0001) return half4(0,0,0,1); #else if(depth > 0.9999) return half4(0,0,0,1); #endif Different platforms use different Z values for far clipping planes (0 == far, or 1 == far). The UNITY_REVERSED_Z constant lets the code handle all platforms correctly. Save the shader code, the example is ready. The following illustration shows the end result: The complete ShaderLab code Below is the complete ShaderLab code for this example. // This Unity shader reconstructs the world space positions for pixels using a depth // texture and screen space UV coordinates. The shader draws a checkerboard pattern // on a mesh to visualize the positions. Shader \"Example/URPReconstructWorldPos\" { Properties { } // The SubShader block containing the Shader code. SubShader { // SubShader Tags define when and under which conditions a SubShader block or // a pass is executed. Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } Pass { HLSLPROGRAM // This line defines the name of the vertex shader. #pragma vertex vert // This line defines the name of the fragment shader. #pragma fragment frag // The Core.hlsl file contains definitions of frequently used HLSL // macros and functions, and also contains #include references to other // HLSL files (for example, Common.hlsl, SpaceTransforms.hlsl, etc.). #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" // The DeclareDepthTexture.hlsl file contains utilities for sampling the // Camera depth texture. #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl\" // This example uses the Attributes structure as an input structure in // the vertex shader. struct Attributes { // The positionOS variable contains the vertex positions in object // space. float4 positionOS : POSITION; }; struct Varyings { // The positions in this struct must have the SV_POSITION semantic. float4 positionHCS : SV_POSITION; }; // The vertex shader definition with properties defined in the Varyings // structure. The type of the vert function must match the type (struct) // that it returns. Varyings vert(Attributes IN) { // Declaring the output object (OUT) with the Varyings struct. Varyings OUT; // The TransformObjectToHClip function transforms vertex positions // from object space to homogenous clip space. OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); // Returning the output. return OUT; } // The fragment shader definition. // The Varyings input structure contains interpolated values from the // vertex shader. The fragment shader uses the `positionHCS` property // from the `Varyings` struct to get locations of pixels. half4 frag(Varyings IN) : SV_Target { // To calculate the UV coordinates for sampling the depth buffer, // divide the pixel location by the render target resolution // _ScaledScreenParams. float2 UV = IN.positionHCS.xy / _ScaledScreenParams.xy; // Sample the depth from the Camera depth texture. #if UNITY_REVERSED_Z real depth = SampleSceneDepth(UV); #else // Adjust Z to match NDC for OpenGL ([-1, 1]) real depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(UV)); #endif // Reconstruct the world space positions. float3 worldPos = ComputeWorldSpacePosition(UV, depth, UNITY_MATRIX_I_VP); // The following part creates the checkerboard effect. // Scale is the inverse size of the squares. uint scale = 10; // Scale, mirror and snap the coordinates. uint3 worldIntPos = uint3(abs(worldPos.xyz * scale)); // Divide the surface into squares. Calculate the color ID value. bool white = ((worldIntPos.x) & 1) ^ (worldIntPos.y & 1) ^ (worldIntPos.z & 1); // Color the square based on the ID value (black or white). half4 color = white ? half4(1,1,1,1) : half4(0,0,0,1); // Set the color to black in the proximity to the far clipping // plane. #if UNITY_REVERSED_Z // Case for platforms with REVERSED_Z, such as D3D. if(depth < 0.0001) return half4(0,0,0,1); #else // Case for platforms without REVERSED_Z, such as OpenGL. if(depth > 0.9999) return half4(0,0,0,1); #endif return color; } ENDHLSL } } }"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-color.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-color.html",
    "title": "URP unlit shader with color input | FSM Unity Framework",
    "keywords": "URP unlit shader with color input The Unity shader in this example adds the Base Color property to the Material. You can select the color using that property and the shader fills the mesh shape with the color. Use the Unity shader source file from section URP unlit basic shader and make the following changes to the ShaderLab code: Add the _BaseColor property definition to the Properties block: Properties { [MainColor] _BaseColor(\"Base Color\", Color) = (1, 1, 1, 1) } This declaration adds the _BaseColor property with the label Base Color to the Material: When you declare a property with the [MainColor] attribute, Unity uses this property as the main color of the Material. Note: For compatibility reasons, the _Color property name is a reserved name. Unity uses a property with the name _Color as the main color even it does not have the [MainColor] attribute. When you declare a property in the Properties block, you also need to declare it in the HLSL code. NOTE: To ensure that the Unity shader is SRP Batcher compatible, declare all Material properties inside a single CBUFFER block with the name UnityPerMaterial. For more information on the SRP Batcher, see the page Scriptable Render Pipeline (SRP) Batcher. Add the following code before the vertex shader: CBUFFER_START(UnityPerMaterial) half4 _BaseColor; CBUFFER_END Change the code in the fragment shader so that it returns the _BaseColor property. half4 frag() : SV_Target { return _BaseColor; } Now you can select the color in the Base Color field in the Inspector window. The fragment shader fills the mesh with the color you select. Below is the complete ShaderLab code for this example. // This shader fills the mesh shape with a color that a user can change using the // Inspector window on a Material. Shader \"Example/URPUnlitShaderColor\" { // The _BaseColor variable is visible in the Material's Inspector, as a field // called Base Color. You can use it to select a custom color. This variable // has the default value (1, 1, 1, 1). Properties { [MainColor] _BaseColor(\"Base Color\", Color) = (1, 1, 1, 1) } SubShader { Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" struct Attributes { float4 positionOS : POSITION; }; struct Varyings { float4 positionHCS : SV_POSITION; }; // To make the Unity shader SRP Batcher compatible, declare all // properties related to a Material in a a single CBUFFER block with // the name UnityPerMaterial. CBUFFER_START(UnityPerMaterial) // The following line declares the _BaseColor variable, so that you // can use it in the fragment shader. half4 _BaseColor; CBUFFER_END Varyings vert(Attributes IN) { Varyings OUT; OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); return OUT; } half4 frag() : SV_Target { // Returning the _BaseColor value. return _BaseColor; } ENDHLSL } } } Section Drawing a texture shows how to draw a texture on the mesh."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-normals.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-normals.html",
    "title": "Visualizing normal vectors | FSM Unity Framework",
    "keywords": "Visualizing normal vectors The Unity shader in this example visualizes the normal vector values on the mesh. Use the Unity shader source file from section URP unlit basic shader and make the following changes to the ShaderLab code: In struct Attributes, which is the input structure for the vertex shader in this example, declare the variable containing the normal vector for each vertex. struct Attributes { float4 positionOS : POSITION; // Declaring the variable containing the normal vector for each vertex. half3 normal : NORMAL; }; In struct Varyings, which is the input structure for the fragment shader in this example, declare the variable for storing the normal vector values for each fragment: struct Varyings { float4 positionHCS : SV_POSITION; // The variable for storing the normal vector values. half3 normal : TEXCOORD0; }; This example uses the three components of the normal vector as RGB color values for each fragment. To render the normal vector values on the mesh, use the following code as the fragment shader: half4 frag(Varyings IN) : SV_Target { half4 color = 0; color.rgb = IN.normal; return color; } Unity renders the normal vector values on the mesh: A part of the capsule is black. This is because in those points, all three components of the normal vector are negative. The next step shows how to render values in those areas as well. To render negative normal vector components, use the compression technique. To compress the range of normal component values (-1..1) to color value range (0..1), change the following line: color.rgb = IN.normal; to this line: color.rgb = IN.normal * 0.5 + 0.5; Now Unity renders the normal vector values as colors on the mesh. Below is the complete ShaderLab code for this example. // This shader visuzlizes the normal vector values on the mesh. Shader \"Example/URPUnlitShaderNormal\" { Properties { } SubShader { Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" struct Attributes { float4 positionOS : POSITION; // Declaring the variable containing the normal vector for each // vertex. half3 normal : NORMAL; }; struct Varyings { float4 positionHCS : SV_POSITION; half3 normal : TEXCOORD0; }; Varyings vert(Attributes IN) { Varyings OUT; OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); // Use the TransformObjectToWorldNormal function to transform the // normals from object to world space. This function is from the // SpaceTransforms.hlsl file, which is referenced in Core.hlsl. OUT.normal = TransformObjectToWorldNormal(IN.normal); return OUT; } half4 frag(Varyings IN) : SV_Target { half4 color = 0; // IN.normal is a 3D vector. Each vector component has the range // -1..1. To show all vector elements as color, including the // negative values, compress each value into the range 0..1. color.rgb = IN.normal * 0.5 + 0.5; return color; } ENDHLSL } } }"
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-texture.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/Documentation~/writing-shaders-urp-unlit-texture.html",
    "title": "Drawing a texture | FSM Unity Framework",
    "keywords": "Drawing a texture The Unity shader in this example draws a texture on the mesh. Use the Unity shader source file from section URP unlit shader with color input and make the following changes to the ShaderLab code: In the Properties block, replace the existing code with the _BaseMap property definition. Properties { [MainTexture] _BaseMap(\"Base Map\", 2D) = \"white\" } When you declare a texture property in the Properties block, Unity adds the _BaseMap property with the label Base Map to the Material, and adds the Tiling and the Offset controls. When you declare a property with the [MainTexture] attribute, Unity uses this property as the main texture of the Material. Note: For compatibility reasons, the _MainTex property name is a reserved name. Unity uses a property with the name _MainTex as the main texture even it does not have the [MainTexture] attribute. In struct Attributes and struct Varyings, add the uv variable for the UV coordinates on the texture: float2 uv : TEXCOORD0; Define the texture as a 2D texture and specify a sampler for it. Add the following lines before the CBUFFER block: TEXTURE2D(_BaseMap); SAMPLER(sampler_BaseMap); The TEXTURE2D and the SAMPLER macros are defined in one of the files referenced in Core.hlsl. For tiling and offset to work, it's necessary to declare the texture property with the _ST suffix in the 'CBUFFER' block. The _ST suffix is necessary because some macros (for example, TRANSFORM_TEX) use it. NOTE: To ensure that the Unity shader is SRP Batcher compatible, declare all Material properties inside a single CBUFFER block with the name UnityPerMaterial. For more information on the SRP Batcher, see the page Scriptable Render Pipeline (SRP) Batcher. CBUFFER_START(UnityPerMaterial) float4 _BaseMap_ST; CBUFFER_END To apply the tiling and offset transformation, add the following line in the vertex shader: OUT.uv = TRANSFORM_TEX(IN.uv, _BaseMap); The TRANSFORM_TEX macro is defined in the Macros.hlsl file. The #include declaration contains a reference to that file. In the fragment shader, use the SAMPLE_TEXTURE2D macro to sample the texture: half4 frag(Varyings IN) : SV_Target { half4 color = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, IN.uv); return color; } Now you can select a texture in the Base Map field in the Inspector window. The shader draws that texture on the mesh. Below is the complete ShaderLab code for this example. // This shader draws a texture on the mesh. Shader \"Example/URPUnlitShaderTexture\" { // The _BaseMap variable is visible in the Material's Inspector, as a field // called Base Map. Properties { [MainTexture] _BaseMap(\"Base Map\", 2D) = \"white\" } SubShader { Tags { \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" } Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" struct Attributes { float4 positionOS : POSITION; // The uv variable contains the UV coordinate on the texture for the // given vertex. float2 uv : TEXCOORD0; }; struct Varyings { float4 positionHCS : SV_POSITION; // The uv variable contains the UV coordinate on the texture for the // given vertex. float2 uv : TEXCOORD0; }; // This macro declares _BaseMap as a Texture2D object. TEXTURE2D(_BaseMap); // This macro declares the sampler for the _BaseMap texture. SAMPLER(sampler_BaseMap); CBUFFER_START(UnityPerMaterial) // The following line declares the _BaseMap_ST variable, so that you // can use the _BaseMap variable in the fragment shader. The _ST // suffix is necessary for the tiling and offset function to work. float4 _BaseMap_ST; CBUFFER_END Varyings vert(Attributes IN) { Varyings OUT; OUT.positionHCS = TransformObjectToHClip(IN.positionOS.xyz); // The TRANSFORM_TEX macro performs the tiling and offset // transformation. OUT.uv = TRANSFORM_TEX(IN.uv, _BaseMap); return OUT; } half4 frag(Varyings IN) : SV_Target { // The SAMPLE_TEXTURE2D marco samples the texture with the given // sampler. half4 color = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, IN.uv); return color; } ENDHLSL } } } Section Visualizing normal vectors shows how to visualize normal vectors on the mesh."
  },
  "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.render-pipelines.universal@14.0.9/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.render-pipelines.universal copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.searcher@4.9.2/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.searcher@4.9.2/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [4.9.2] - 2022-02-22 Fixed a bug that stopped keyboard keys from affecting navigation and interaction with search results list, unless user explicitly focused/click on list using the mouse [1396759] [4.9.1] - 2021-10-05 Fixed a usability issue where in some cases searcher would suggest one collapsed category of results that user would have to manually expand anyway Fixed bug that caused incorrect search results with non whitespaced queries for nodes with spaces in their name and for subgraphs [1359158] Fixed bug that causes search results to not be visible sometimes in the searcher window [1366061] Fixed bug that causes exceptions to be thrown when using the up/down arrow keys with search list focused [1358016] Fixed bug that causes some searcher items to be irreversibly collapsed due to expand icon disappearing on collapsing those items [1366074] [4.9.0] - 2021-09-07 Remove Lucene API and dlls [4.8.0] - 2021-02-17 Added ability for clients of searcher window to filter and prioritize search results as they need Fixed bug that causes searcher window to prioritize categories over node entries of the same name [case 1304055] Fixed bug that causes searcher window to close when double-clicking a category [case 1302267] Fixed bug that causes searcher window to be offset too far when accounting for host window boundaries [4.7.1] - 2020-10-15 Fix Regex error during highlighting when the query contains a backslash Fix serialization depth warning caused by a property's backing field getting serialized [4.7.0] - 2020-08-11 Added Lucene.Net DLLs and first version of the LuceneDatabase [4.6.0] - 2020-07-27 Added support for multi-select, enabled via MultiSelectEnabled in the SearcherAdapter. [4.5.0] - 2020-07-15 Add support for displaying icons in SearcherItem. [4.4.0] - 2020-07-13 Add custom UserData field to SearcherItem. Add SearcherTreeUtility to create a SearcherItem tree from a flat list of paths. [4.3.1] - 2020-06-08 Fix bug that cause keyboard navigation to fail. [case 1253544] [4.3.0] - 2020-05-27 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.3.0-preview.tgz Bump minor version for synonyms. [4.2.0] - 2020-04-30 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.2.0-preview.tgz Bump to minor version for API validation. [4.1.1] - 2020-04-30 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.1.1-preview.tgz Add all children is now an adapter override. [4.1.0] - 2020-03-20 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.1.0-preview.tgz Improve matching algorithm Add a splitter between searcher and details panel Fix adding all children of matching expanded categories [4.0.9] - 2019-10-22 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.9-preview.tgz Update ListView API [4.0.8] - 2019-09-16 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.8-preview.tgz Made SearcherItem Name property virtual [4.0.7] - 2019-08-29 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.7-preview.tgz Fix bold fonts (case 1178374) case 1178373 and 1071573014: minor examples tweaks [4.0.6] - 2019-08-01 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.6-preview.tgz Fix bug where items were selected twice when using keyboard inputs [4.0.5] - 2019-07-26 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.5-preview.tgz Fix searcher look to match Northstar changes [4.0.4] - 2019-07-23 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.4-preview.tgz Change the default size when the searcher has a details panel [4.0.3] - 2019-06-11 Package: https://artifactory.prd.cds.internal.unity3d.com/artifactory/upm-candidates/com.unity.searcher/-/com.unity.searcher-4.0.3-preview.tgz Added ability to use capital letters in a search bar Bugfix: Search bar focus after the escape button pressed [4.0.2] - 2019-05-24 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/4.0.2-preview API Make Match() virtual again in SearcherDatabase [4.0.1] - 2019-04-30 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/4.0.1-preview Bugfix: [MacOs] Fix issue where the searcher moves on the top left corner while resizing/moving [4.0.0] - 2019-04-24 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/4.0.0-preview Cleanup for promotion to production [3.0.12] - 2019-04-17 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.12 API: Make SearcherField public again [3.0.11] - 2019-04-15 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.11 Fix all issues flagged by ReSharper [3.0.10] - 2019-03-26 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.10 fix CI [3.0.9] - 2019-03-24 Package: none Add Yamato CI config [3.0.8] - 2019-03-24 Package: none Bugfix: Autocomplete text was misaligned. [3.0.7] - 2019-02-28 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.7 API: Remove Experimental API reference. [3.0.6] - 2019-09-27 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.6 UI: Restyling API: Add public ctor to SearcherDatabase [3.0.5] - 2018-12-18 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.5 Bugfix: Focus search text field when window is displayed [3.0.4] - 2018-11-30 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.4 Trigger callback when an item is selected instead of when the details panel is displayed [3.0.3] - 2018-11-28 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.3 Add alignments [3.0.2] - 2018-11-22 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.2 Bugfix: Searcher autocomplete label now bold to match text input style [3.0.1] - 2018-11-20 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.1 Bugfix [3.0.0] - 2018-11-20 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/3.0.0 Restyling and move + resize [2.1.1] - 2018-11-12 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.1.1 Fix text input filtering [2.1.0] - 2018-11-05 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.1.0 UIElements compatibility update [2.0.6] - 2018-08-15 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.6-preview Add possibility to sort items [2.0.5] - 2018-08-15 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.5-preview Filtering fix [2.0.4] - 2018-08-08 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.4-preview Added hooks for analytics [2.0.3] - 2018-08-07 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.3-preview The matchFilter is now also applied at database initial setup time [2.0.2] - 2018-08-02 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.2-preview Added matchFilter delegate on SearcherDatabase to further control the match criteria [2.0.1] - 2018-07-13 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.1-preview Fixed Exception when a whitespace query is entered [2.0.0] - 2018-07-12 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/2.0.0-preview Created a base class for Databases, renamed SearcherDatabase to LuceneDatabase, add a brand new SearcherDatabase written from scratch [1.0.6] - 2018-06-21 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.6-preview hotfix for left arrow on a child that cannot be collapsed will select the parent feature [1.0.5] - 2018-06-21 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.5-preview fixed an draw issue when expanding and collapsing an item on a small list - issue #25 pressing left arrow on a child that cannot be collapsed will select the parent [1.0.4] - 2018-05-16 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.4-preview fixed compilation error with latest trunk (around styles.flex) added third party notices file [1.0.3] - 2018-05-03 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.3-preview window close due to focus loss will now trigger the selection callback with null fixed potential null ref exception in sample code [1.0.2] - 2018-04-30 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.2-preview removed AutoCompleter in favor of a more robust top-result based approach [1.0.1] - 2018-04-26 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.1-preview now showing children of matching items - issue #19 fixed completion scoring with multiple databases search results in general have been improved [1.0.0] - 2018-04-25 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/1.0.0-preview added basic tests - issue #18 added a README and documentation fixed Searcher.Search() not returning anything if query contained capital letters - issue #22 [0.1.3] - 2018-04-23 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/0.1.3-preview added ability to add a title to the Searcher window - feature #3 removed Searcher arrow and moved default display point to top-right corner - related issues #2, #12, #16 fixed lingering arrow when bring Searcher window up from Inspector - issue #2 fixed SearcherWindow.Show() to always take world space display location - issue #17 [0.1.2] - 2018-04-18 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/0.1.2-experimental fixed Searcher's list is visually cut off when closing a parent SearcherItem - issue #9 scroll to selected item/best result add parents field, do not autocomplete it, search using a multi phrase query, auto create the parents path in overwritePath() fixed window arrow being removed AFTER the target window repaint, leaving remnant arrwos sometimes - issue #6 fixed Null Ref Exception when getting the selected item of an empty listview. only get it if relevant fixed bug where child was not under parent in Searcher [0.1.1] - 2018-03-21 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/0.1.1-experimental Minor fixes for VisualScripting [0.1.0] - 2018-03-05 Package: https://bintray.com/unity/unity-staging/com.unity.searcher/0.1.0-experimental This is the first release of Unity Package Searcher. General search window for use in the Editor. First target use is for GraphView node search."
  },
  "Library/PackageCache/com.unity.searcher@4.9.2/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.searcher@4.9.2/Documentation~/index.html",
    "title": "Searcher | FSM Unity Framework",
    "keywords": "Searcher Disclaimer Currently, the API for the Searcher is not intended for public use. Using the API in external custom scripts or modifying the API is not recommended or supported at this time. The Searcher package adds a powerful search window to Unity's Graph View tools which lets you quickly find, select, and place items into the graph window. Features The Searcher window uses a tree view to display the available options and the categories they belong to. As you type, the Searcher makes predictions and provides auto-complete options for the item you might be trying to select. The Searcher also highlights the matching text in the tree view to help you quickly find what you're looking for. Some instances of the Searcher also provide inline documentation or notes in an extra window to the right of the item tree. Navigating the Searcher You can navigate through the Searcher with either your mouse or keyboard. Use your mouse to scroll through tree menu items. To expand or collapse tree menu items click on them, and double-click the item to make your selection. To navigate up and down the list of items with your keyboard, press the Up arrow key or Down arrow key. To expand tree menus, press the Right arrow key, and press the Left arrow key to collapse the tree menus. To make a selection of the highlighted item, press the Enter key."
  },
  "Library/PackageCache/com.unity.searcher@4.9.2/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.searcher@4.9.2/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Unity Companion Package License v1.0 (\"License\") Copyright © 2019 Unity Technologies ApS (\"Unity\") Unity hereby grants to you a worldwide, non-exclusive, no-charge, and royalty-free copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute the software that is made available with this License (\"Software\"), subject to the following terms and conditions: Unity Companion Use Only. Exercise of the license granted herein is limited to exercise for the creation, use, and/or distribution of applications, software, or other content pursuant to a valid Unity development engine software license (\"Engine License\"). That means while use of the Software is not limited to use in the software licensed under the Engine License, the Software may not be used for any purpose other than the creation, use, and/or distribution of Engine License-dependent applications, software, or other content. No other exercise of the license granted herein is permitted. No Modification of Engine License. Neither this License nor any exercise of the license granted herein modifies the Engine License in any way. Ownership & Grant Back to You. 3.1. You own your content. In this License, \"derivative works\" means derivatives of the Software itself--works derived only from the Software by you under this License (for example, modifying the code of the Software itself to improve its efficacy); “derivative works” of the Software do not include, for example, games, apps, or content that you create using the Software. You keep all right, title, and interest to your own content. 3.2. Unity owns its content. While you keep all right, title, and interest to your own content per the above, as between Unity and you, Unity will own all right, title, and interest to all intellectual property rights (including patent, trademark, and copyright) in the Software and derivative works of the Software, and you hereby assign and agree to assign all such rights in those derivative works to Unity. 3.3. You have a license to those derivative works. Subject to this License, Unity grants to you the same worldwide, non-exclusive, no-charge, and royalty-free copyright license to derivative works of the Software you create as is granted to you for the Software under this License. Trademarks. You are not granted any right or license under this License to use any trademarks, service marks, trade names, products names, or branding of Unity or its affiliates (\"Trademarks\"). Descriptive uses of Trademarks are permitted; see, for example, Unity’s Branding Usage Guidelines at https://unity3d.com/public-relations/brand. Notices & Third-Party Rights. This License, including the copyright notice above, must be provided in all substantial portions of the Software and derivative works thereof (or, if that is impracticable, in any other location where such notices are customarily placed). Further, if the Software is accompanied by a Unity \"third-party notices\" or similar file, you acknowledge and agree that software identified in that file is governed by those separate license terms. DISCLAIMER, LIMITATION OF LIABILITY. THE SOFTWARE AND ANY DERIVATIVE WORKS THEREOF IS PROVIDED ON AN \"AS IS\" BASIS, AND IS PROVIDED WITHOUT WARRANTY OF ANY KIND, WHETHER EXPRESS OR IMPLIED, INCLUDING ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND/OR NONINFRINGEMENT. IN NO EVENT SHALL ANY COPYRIGHT HOLDER OR AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES (WHETHER DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL, INCLUDING PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, LOSS OF USE, DATA, OR PROFITS, AND BUSINESS INTERRUPTION), OR OTHER LIABILITY WHATSOEVER, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM OR OUT OF, OR IN CONNECTION WITH, THE SOFTWARE OR ANY DERIVATIVE WORKS THEREOF OR THE USE OF OR OTHER DEALINGS IN SAME, EVEN WHERE ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. USE IS ACCEPTANCE and License Versions. Your receipt and use of the Software constitutes your acceptance of this License and its terms and conditions. Software released by Unity under this License may be modified or updated and the License with it; upon any such modification or update, you will comply with the terms of the updated License for any use of any of the Software under the updated License. Use in Compliance with Law and Termination. Your exercise of the license granted herein will at all times be in compliance with applicable law and will not infringe any proprietary rights (including intellectual property rights); this License will terminate immediately on any breach by you of this License. Severability. If any provision of this License is held to be unenforceable or invalid, that provision will be enforced to the maximum extent possible and the other provisions will remain in full force and effect. Governing Law and Venue. This License is governed by and construed in accordance with the laws of Denmark, except for its conflict of laws rules; the United Nations Convention on Contracts for the International Sale of Goods will not apply. If you reside (or your principal place of business is) within the United States, you and Unity agree to submit to the personal and exclusive jurisdiction of and venue in the state and federal courts located in San Francisco County, California concerning any dispute arising out of this License (\"Dispute\"). If you reside (or your principal place of business is) outside the United States, you and Unity agree to submit to the personal and exclusive jurisdiction of and venue in the courts located in Copenhagen, Denmark concerning any Dispute."
  },
  "Library/PackageCache/com.unity.searcher@4.9.2/README.html": {
    "href": "Library/PackageCache/com.unity.searcher@4.9.2/README.html",
    "title": "Searcher | FSM Unity Framework",
    "keywords": "Searcher Use the Searcher package to quickly search a large list of items via a popup window. For example, use Searcher to find, select, and put down a new node in a graph. The Searcher package also includes samples and tests. Features Popup Window Placement Tree View Keyboard Navigation Quick Search Auto-Complete Match Highlighting Multiple Databases Quick Usage Example void OnMouseDown( MouseDownEvent evt ) { var items = new List<SearcherItem> { new SearcherItem( \"Books\", \"Description\", new List<SearcherItem>() { new SearcherItem( \"Dune\" ), } ) }; items[0].AddChild( new SearcherItem( \"Ender's Game\" ) ); SearcherWindow.Show( this, // this EditorWindow items, \"Optional Title\", item => { Debug.Log( item.name ); return /*close window?*/ true; }, evt.mousePosition ); } Installing the Package Open this file in your project: Packages/manifest.json Add this to the dependencies array (makes sure to change the version string to your current version): \"com.unity.searcher\": \"4.0.0-preview\" For example, if this it he only package you depend on, you should have something like this (makes sure to change the version string to your current version): { \"dependencies\": { \"com.unity.searcher\": \"4.0.0-preview\" } } Enabling the Samples and Tests Right now, it seems Samples and Tests only show for local packages, meaning you cloned this repo inside your Packages folder. Given you've done that, open this file in your project: Packages/manifest.json Add a testables list with the package name so you get something like this (makes sure to change the version string to your current version): { \"dependencies\": { \"com.unity.searcher\": \"4.0.0-preview\" }, \"testables\" : [ \"com.unity.searcher\" ] } You should see a new top-level menu called Searcher and you should see Searcher tests in Test Runner. Searcher Creation from Database var bookItems = new List<SearcherItem> { new SearcherItem( \"Books\" ) }; var foodItems = new List<SearcherItem> { new SearcherItem( \"Foods\" ) }; // Create databases. var databaseDir = Application.dataPath + \"/../Library/Searcher\"; var bookDatabase = SearcherDatabase.Create( bookItems, databaseDir + \"/Books\" ); var foodDatabase = SearcherDatabase.Create( foodItems, databaseDir + \"/Foods\" ); // At a later time, load database from disk. bookDatabase = SearcherDatabase.Load( databaseDir + \"/Books\" ); var searcher = new Searcher( new SearcherDatabase[]{ foodDatabase, bookDatabase }, \"Optional Title\" ); Popup Window or Create Control Searcher m_Searcher; void OnMouseDown( MouseDownEvent evt ) { // Popup window... SearcherWindow.Show( this, m_Searcher, item => { Debug.Log( item.name ); return /*close window?*/ true; }, evt.mousePosition ); } // ...or create SearcherControl VisualElement void OnEnable() { // ...or create SearcherControl VisualElement var searcherControl = new SearcherControl(); searcherControl.Setup( m_Searcher, item => Debug.Log( item.name ) ); this.GetRootVisualContainer().Add( searcherControl ); } Customize the UI via ISearcherAdapter public interface ISearcherAdapter { VisualElement MakeItem(); VisualElement Bind( VisualElement target, SearcherItem item, ItemExpanderState expanderState, string text ); string title { get; } bool hasDetailsPanel { get; } void DisplaySelectionDetails( VisualElement detailsPanel, SearcherItem o ); void DisplayNoSelectionDetails( VisualElement detailsPanel ); void InitDetailsPanel( VisualElement detailsPanel ); } var bookDatabase = SearcherDatabase.Load( Application.dataPath + \"/Books\" ); var myAdapter = new MyAdapter(); // class MyAdapter : ISearcherAdapter var searcher = new Searcher( bookDatabase, myAdapter ); Technical details Requirements This version of Searcher is compatible with the following versions of the Unity Editor: 2019.1 and later (recommended) Known limitations Searcher version 1.0 includes the following known limitations: Only works with .Net 4.0 Package contents The following table indicates the main folders of the package: Location Description Editor/Resources Contains images used in the UI. Editor/Searcher Contains Searcher source files. Samples Contains the samples. Tests Contains the tests."
  },
  "Library/PackageCache/com.unity.settings-manager@2.0.1/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.settings-manager@2.0.1/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [2.0.1] - 2021-11-02 Bug Fixes [case:1289586] Fix performance problem when using Perforce and Polybrush due to AssetDatabase.IsOpenForEdit. [case:1347042] Fix performance problem when using Perforce and Polybrush due to AssetDatabase.IsOpenForEdit. [1.0.3] - 2020-06-21 Bug Fixes Fixed PackageSettingsRepository dirtying the settings file when no changes are present. [1.0.2] - 2020-02-26 Bug Fixes Fixed obsolete API use in Unity 2019.3. Changes Update Yamato configuration. [1.0.1] - 2019-11-25 Changes Make sure version control integration grants write access before trying to save package settings. Bug Fixes Fixed samples not compiling with Unity 2019.3. Fix package settings repo potentially initializing with a null dictionary. [1.0.0] - 2019-04-03 Bug Fixes Fixed compile errors on Unity 2018.4. [0.1.0-preview.8] - 2019-03-29 Features Support saving multiple settings repositories within a project Changes Rename ProjectSettingsRepository -> PackageSettingsRepository. Update readme with a complete code example. Add additional documentation and unit tests. Setting repositories now have names. Bug Fixes Fixed missing gear icon in Settings Provider implementation. [0.1.0-preview.4] - 2019-02-28 Package configuration update. [0.1.0-preview.3] - 2019-02-27 Small code update in sample. [0.1.0-preview.2] - 2019-02-22 Rebuild meta files. [0.1.0-preview.1] - 2019-02-01 Move samples outside of main package. [0.1.0-preview.0] - 2018-10-08 This is the first release of Unity Package Settings Manager."
  },
  "Library/PackageCache/com.unity.settings-manager@2.0.1/Documentation~/settings-manager.html": {
    "href": "Library/PackageCache/com.unity.settings-manager@2.0.1/Documentation~/settings-manager.html",
    "title": "Settings Manager | FSM Unity Framework",
    "keywords": "Settings Manager The Settings Manager is a framework that lets you convert any serializable field into a setting, including a pre-built settings interface. Installation To install this package, follow the instructions in the Package Manager documentation. This package provides a sample that demonstrates how to implement custom user settings. To install them, follow these instructions: Make sure the Settings Manager package is installed in your Unity project. In the Package Manager window, locate the Settings Manager package select it from the list. The Details view displays information about the Settings Manager package. From the Details view, click the Import button under the Samples section. Requirements This version of the Settings Manager package is compatible with the following versions of the Unity Editor: 2018.4 and later Using the Settings Manager The Settings class is responsible for setting and retrieving serialized values from a settings repository. Use settings repositories to save and load settings for a specific scope. This package provides two settings repositories: The UserSettingsRepository, backed by the EditorPrefs class, lets you save user preferences. The FileSettingsRepository saves a JSON file to the ProjectSettings directory in order to save project settings. You can create and manage all settings from a singleton Settings instance. For example: using UnityEditor.SettingsManagement; namespace UnityEditor.SettingsManagement.Examples { static class MySettingsManager { internal const string k_PackageName = \"com.example.my-settings-example\"; static Settings s_Instance; internal static Settings instance { get { if (s_Instance == null) s_Instance = new Settings(k_PackageName); return s_Instance; } } } } Getting and setting values Your Settings instance should implement generic methods to set and retrieve values: MySettingsManager.instance.Get<float>(\"myFloatValue\", SettingsScope.Project); There are two arguments: key, and scope. The Settings class finds an appropriate ISettingsRepository for the scope, while key and T are used to find the value. Keys are unique among types: you can re-use keys as long as its type is different. Alternatively, you can use the UserSetting<T> class to manage settings. This is a wrapper class around the Settings get/set properties, which makes it easy to make any field a saved setting. // UserSetting<T>(Settings instance, string key, T defaultValue, SettingsScope scope = SettingsScope.Project) Setting<int> myIntValue = new Setting<int>(MySettingsManager.instance, \"int.key\", 42, SettingsScope.User); UserSetting<T> caches the current value, and keeps a copy of the default value so that it may be reset. You can also use UserSetting<T> fields with the [UserSettingAttribute] attribute, which lets the SettingsManagerProvider automatically add it to a settings inspector. Settings Provider To register your settings so they appear in the Project Settings window, you can either write your own SettingsProvider implementation, or use the UserSettingsProvider and let it automatically create your interface. Making use of UserSettingsProvider comes with many benefits, including a uniform look for your settings UI, support for search, and per-field or mass reset support. using UnityEngine; namespace UnityEditor.SettingsManagement.Examples { static class MySettingsProvider { const string k_PreferencesPath = \"Preferences/My Settings\"; [SettingsProvider] static SettingsProvider CreateSettingsProvider() { // The last parameter tells the provider where to search for settings. var provider = new SettingsManagerProvider(k_PreferencesPath, MySettingsManager.instance, new [] { typeof(MySettingsProvider).Assembly }); return provider; } } } To register a field with the UserSettingsProvider, decorate it with [UserSettingAttribute(string displayCategory, string key)]. Note The [UserSettingAttribute] decoration is only valid for static fields. For more complex settings that require additional UI (or that don't have a built-in Editor), use UserSettingBlockAttribute to access the settings provider GUI. For more information, look at the sample source file SettingsExamples.cs under the Assets/Samples/Settings Manager/<version>/User Settings Example/PackageWithProjectAndUserSettings folder in your Unity project root. Tip If you don't see this path or file, follow the steps under the Installation section to import it."
  },
  "Library/PackageCache/com.unity.settings-manager@2.0.1/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.settings-manager@2.0.1/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.settings-manager copyright © 2021 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.settings-manager@2.0.1/README.html": {
    "href": "Library/PackageCache/com.unity.settings-manager@2.0.1/README.html",
    "title": "Settings Manager | FSM Unity Framework",
    "keywords": "Settings Manager A framework for making any serializable field a setting, complete with an procedurally populated Settings Provider. Quick Start Settings are saved in ISettingsRepository instances. Two default implementations are provided, one for saving user settings (UserSettingsRepository) and one for per-project settings (ProjectSettingsRepository). Settings repositories are responsible for saving and loading preferences. You may work directly with ISettingsRepository, or create a Settings instance to manage them. Creating a Settings is convenient because it allows you to make use of the UserSetting class and attribute. The most common case will be for packages to create a Settings manager with two repositories, one for user settings (SettingsScope.User) and one for per-project settings (SettingsScope.Project). Below is an example of how most packages will use this api. using UnityEditor; using UnityEditor.SettingsManagement; using UnityEngine; public class MySettings { static Settings s_SettingsInstance; public static Settings instance { get { if(s_SettingsInstance == null) s_SettingsInstance = new Settings(\"com.unity.my-package\"); return s_SettingsInstance; } } // Register a new SettingsProvider that will scrape the owning assembly for [UserSetting] marked fields. [SettingsProvider] static SettingsProvider CreateSettingsProvider() { var provider = new UserSettingsProvider(\"Preferences/My Settings\", instance, new [] { typeof(MySettings).Assembly }); return provider; } } public class Test { [UserSetting(\"User Settings\", \"My User Int Value\")] static UserSetting<int> userSetting = new UserSetting<int>(MySettings.instance, \"MyInteger\", 42, SettingsScope.User); [UserSetting(\"Project Settings\", \"My Project Int Value\")] static UserSetting<int> projectSetting = new UserSetting<int>(MySettings.instance, \"MyInteger\", 42, SettingsScope.Project); [MenuItem(\"Debug/Print Settings Values\")] static void PrintValues() { Debug.Log($\"User integer is: {(int) userSetting}, and project integer is {(int) projectSetting}\"); } } Values are set and retrieved using generic methods on on your Settings instance: MySettingsManager.instance.Get<float>(\"myFloatValue\", SettingsScopes.Project); The Settings class will handle finding an appropriate ISettingsRepository for the scope (and optional repository name), while key and T are used to find the value. Setting keys are unique among types, meaning you may re-use keys as long as the setting type is different. // UserSetting<T> is a wrapper class that handles saving and loading serializable values. It is compatible with the `[UserSetting]` attribute, which is used to automatically populate a settings provider. UserSetting<int> myIntValue = new UserSetting<int>(MySettingsManager.instance, \"MyIntegerKey\", 42, SettingsScopes.User); UserSetting<T> caches the current value, and keeps a copy of the default value so that it may be reset. UserSetting<T> fields are also eligible for use with the [UserSetting] attribute, which lets the UserSettingsProvider automatically add it to a settings inspector. Settings Provider To register your settings in the Settings Window you can either write your own SettingsProvider implementation, or use the provided UserSettingsProvider and let it automatically create your interface. Making use of UserSettingsProvider comes with many benefits, including a uniform look for your settings UI, support for search, and per-field or mass reset support. using UnityEngine; namespace UnityEditor.SettingsManagement.Examples { static class MySettingsProvider { [SettingsProvider] static SettingsProvider CreateSettingsProvider() { var provider = new UserSettingsProvider(\"Preferences/My Settings\", MySettingsManager.instance, new [] { typeof(MySettingsProvider).Assembly }); return provider; } } } To register a field with UserSettingsProvider, simply decorate it with [UserSetting(string displayCategory, string key)]. [SettingAttribute] is only valid for static fields. For more complex settings that require additional UI (or simply don't have a built-in editor), you can use UserSettingBlockAttribute. This provides access to the settings provider GUI. See SettingsExamples.cs for more on this."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package are documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [Unreleased] Version Updated The version number for this package has increased due to a version update of a related graphics package. [14.0.8] - 2023-09-27 This version is compatible with Unity 2022.3.11f1. Added Added the new Node Reference sample pack which adds 146 reference assets to help users learn more about available nodes Changed [SGB-613][SGB-594] Addressed an issue where certain operations were taking too long as a result of graph concretization. Fixed Fixed Texture Size node causing compilation error in the Fullscreen ShaderGraph target. [SGB-561] Addressed issue where save/save as hotkeys weren't being caught by shadergraph editor window. [SGB-581][SGB-531] Addressed minor usability issues with Custom Function Nodes. [SGB-605] Addressed issue where adding dropdown property type to newly created subgraphs did not propagate to other open shadergraph editor windows. [SGB-597] Removed invalid character from imported material sub asset. [SGB-592][SGB-596] Addressed issue where docs links from editor were incorrect. Fixed a regression where adding nodes in large graphs would cause a major slowdown. [14.0.7] - 2023-05-23 This version is compatible with Unity 2022.2.22f1. Changed Improved blackboard property drag speed when reordering the blackboard. [SGB-383]. Made adjustments to flipbook node to avoid dropping frames on AMD GPU. [SGB-280]. Fixed parallax nodes so that they use the default UV Input Slot correctly. [SGB-511]. Fixed Addressed issue where missing targets were not handled on import. [SGB-1] Addressed various issues with the Swizzle node. [SGB-159] Addressed issue where duplicate serialized blackboard category children would in subgraphs would brick the ShaderGraph editor. [SGB-378] Addressed error feedback on import of graphs with invalid or missing targets, allowing them to be modified and saved. [SGB-166][SGB-167] Fixed issue where the Gradient Noise Node was causing implicit truncation warnings. [SGB-469] Fixed issue where custom interpolator previews would provide erroneous results when connecting through a reroute node. [SGB-89] Fixed issue where reroute node would sometimes show the wrong color for its appropriate inputs. [SGB-17] Fixed issue where subgraph gradient blackboard properties could have naming conflicts with parent graphs. [SGB-310] Fixed issue where the view position of the graph editor would sometimes be forgotten when swapping between two open shadergraph editor windows. [SGB-377] Fixed issue where node searcher would fail to populate when shadergraph was undocked after domain reload. [SGB-439][IN-30581] Fixed issue where custom mesh selector for master preview would fail to initialize. [SGB-445][IN-30614] Fixed issue where nodes with dynamic vectors would not correctly cache properties for previews. [SGB-359] Fixed for [SGB-466] and related issues where nodes with warning or error badges would fail to clean up their resources properly and leave the shader graph editor in an error state. Improved performance of disconnecting nodes in large graphs. Corrected a regression in float preview properties not updating previews. [SGB-526]. ShaderGraph styles were not applied correctly when the system locale was set in Turks. [14.0.6] - 2023-03-24 This version is compatible with Unity 2022.2.13f1. Changed Sped up rename operations on properties/keywords/dropdowns in large graph. Sped up setting blackboard values in large graphs. The asset postprocessor for shader graph now only performs the majority of its work when a shader-related asset has been changed. Fixed Fixed SRP Batcher compatibility issue with instanced properties. Fixed NullReferenceException when entering Play Mode with an unfocused Shader Graph window/on closing the Shader Graph Window. [14.0.5] - 2022-12-12 This version is compatible with Unity 2022.2.4f1. Fixed Fixed unity_StereoEyeIndex error when building XR project with URP Fullscreen master node containing Shader. Fixed a number of memory leaks in ShaderGraph where windows and view elements were not disposing of resources properly. [14.0.4] - 2022-11-04 This version is compatible with Unity 2022.2.2f1. Changed Reduced time taken by code generation when a shader graph asset is imported. Fixed Fixed a compilation bug in BiRP Target in some variants with lightmaps. Fixed shader graph incorrectly stripping variants for BiRP shaders that weren't built with shader graph. [14.0.3] - 2021-05-09 This version is compatible with Unity 2022.2.0b15. Fixed Fixed the sample buffer nodes in ShaderGraph. Set the default value of Normalize Output toggle in Transform Node to true to make different node versions consistent. [14.0.2] - 2021-02-04 This version is compatible with Unity 2022.2.0a14. Fixed Fixed ShaderGraph pixel and screen coordinates to work correctly with render scale [1387468] [14.0.1] - 2021-12-07 Added Added mip sampling modes for 2d textures, 2d texture arrays and 3d textures Fixed Fixed broken documentation URL for block nodes. 1381488 Fixed SRP-batching when PVT stacks are bound per material by properly declaring properties for PVT stacks [1372152] Fixed custom editor GUI support for the BuiltIn Target 1380485 [14.0.0] - 2021-11-17 Fixed Fixed issue where Duplicating/Copy-Pasting last keyword in the blackboard throws an exception [1394378] Fixed an issue where some graphs with incorrectly formatted data would not display their shader inputs in the blackboard [1384315] Fixed bug with Shader Graph subwindows having their header text overflow when the window is resized smaller than the title [1378203] Gradient field doesn't support HDR values Case 1381867 Fixed the behavior of checkerboard node with raytracing Fixed broken documentation URL for block nodes. 1381488 Fixed an issue where edges connected to SubGraphNodes would sometimes get lost on upgrading a pre-targets graphs 1379996 Added Added mip sampling modes for 2d textures, 2d texture arrays and 3d textures [13.1.2] - 2021-11-05 Added Added ability to set \"Global\" or \"Per Material\" shader declaration in PVT node settings [1372152] Show PVT stack names (needed for binding) under the Properties in the Shader Inspector Fixed Fixed a recent regression in ShaderGraph Screen Position behavior on some platforms in Built-in, Universal and HDRP [1369450] [13.1.1] - 2021-10-04 Added Adding ability to automatically cast Bools to Vector types in ShaderGraph [1359160] Added ShaderGraph import warning to old nodes and properties, and ability to dismiss the warning if old behavior is desired. Added normal transforms to the Transform node Added an automatically generated material subasset on ShaderGraphs. Changed Changed the title suffix on old nodes and properties rom \"Deprecated\" to \"Legacy\". Updated searcher package dependency version to 4.9.1 Renamed the Shader Graph Texel Size node to Texture Size and added two additional output ports that actually output the texel size in addition to the texture size. Fixed Fixed a usability issue where in some cases searcher would suggest one collapsed category of results that user would have to manually expand anyway Fixed bug that causes search results to not be visible sometimes in the searcher window [1366061] Fixed bug that causes exceptions to be thrown when using the up/down arrow keys with search list focused [1358016] Fixed bug that causes some searcher items to be irreversibly collapsed due to expand icon disappearing on collapsing those items [1366074] Fixed bug that caused incorrect search results with non whitespaced queries for nodes with spaces in their name and for subgraphs [1359158] Fixed Triplanar ShaderGraph node to handle arbitrary input and output coordinate spaces [1346477] (https://issuetracker.unity3d.com/issues/shader-graph-rotating-gameobject-get-material-stretched-when-using-triplanar-node) Fixed a bug that Parallax Mapping and Parallax Occlusion Mapping nodes don't use the same channel to sample heightmap by adding drop-downs for channel selecting to both of the nodes. [1347270] (https://fogbugz.unity3d.com/f/cases/1347270/) Fixed errors in the ShaderGraph Transform node [1368082] Fixed the Scene Depth node so it returns proper results in Eye space when using an orthographic camera [1311272] Fixed a bug where node preview doesn't update when a texture is changed in the explorer 1363784 Fixed missing shader keyword stage during keyword copying. Fixed a ShaderGraph warning when connecting a node using Object Space BiTangent to the vertex stage [1361512] (https://issuetracker.unity3d.com/issues/shader-graph-cross-implicit-truncation-of-vector-type-errors-are-thrown-when-connecting-transform-node-to-vertex-block) Fixed upgrade warnings on SpeedTree8 subgraphs. [13.1.0] - 2021-09-24 Fixed Fixed bug where an exception was thrown on undo operation after adding properties to a category [1348910] (https://fogbugz.unity3d.com/f/cases/1348910/) Fixed the sticky-note editable title text size in shader graph not matching the set font size [1357657]. Fixed unhandled exception when loading a subgraph with duplicate slots [1366200] (https://issuetracker.unity3d.com/product/unity/issues/guid/1366200/) [13.0.0] - 2021-09-01 Changed Remove use of deprecated UNITY_USE_NATIVE_HDR keyword in shaders. Added Adding control of anisotropic settings on inline Sampler state nodes in ShaderGraph. Fixed Fixed bug where it was not possible to switch to Graph Settings tab in Inspector if multiple nodes and an edge was selected [1357648] (https://fogbugz.unity3d.com/f/cases/1357648/) Fixed an incorrect direction transform from view to world space [1362034] (https://issuetracker.unity3d.com/product/unity/issues/guid/1362034/) Fixed ShaderGraph HDRP master preview disappearing for a few seconds when graph is modified [1330289] (https://issuetracker.unity3d.com/issues/shadergraph-hdrp-main-preview-is-invisible-until-moved) Fixed noise nodes to use a deterministic integer hash, instead of platform dependent floating point hashes [1156544] Fixed the appearance (wrong text color, and not wrapped) of a warning in Node Settings [1356725] (https://issuetracker.unity3d.com/product/unity/issues/guid/1356725/) Fixed the ordering of inputs on a SubGraph node to match the properties on the blackboard of the subgraph itself [1354463] Added more inputs to the Parallax Occlusion Mapping node to handle non-uniformly scaled UVs such as HDRP/Lit POM [1347008]. Fixed the wrong scaling of the main preview window [1356719] (https://issuetracker.unity3d.com/product/unity/issues/guid/1356719/) Fixed an issue where ShaderGraph \"view shader\" commands were opening in individual windows, and blocking Unity from closing [1367188] Improved screenspace position accuracy in the fragment shader by using VPOS [1352662] (https://issuetracker.unity3d.com/issues/shadergraph-dither-node-results-in-artifacts-when-far-from-origin-caused-by-screen-position-breaking-down) Fixed the node searcher results to prefer names over synonyms [1366058] Fixed the sticky-note editable title text size in shader graph not matching the set font size [1357657]. Fixed how graph errors were displayed when variant limits were reached [1355815] [12.0.0] - 2021-01-11 Added Added categories to the blackboard, enabling more control over the organization of shader properties and keywords in the Shader Graph tool. These categories are also reflected in the Material Inspector for URP + HDRP, for materials created from shader graphs. Added ability to define custom vertex-to-fragment interpolators. Support for the XboxSeries platform has been added. Stereo Eye Index, Instance ID, and Vertex ID nodes added to the shadergraph library. Added information about selecting and unselecting items to the Blackboard article. Added View Vector Node documentation Added custom interpolator thresholds on shadergraph project settings page. Added custom interpolator documentation Added subshadergraphs for SpeedTree 8 shadergraph support: SpeedTree8Wind, SpeedTree8ColorAlpha, SpeedTree8Billboard. Added an HLSL file implementing a version of the Unity core LODDitheringTransition function which can be used in a Shader Graph Added a new target for the built-in render pipeline, including Lit and Unlit sub-targets. Added stage control to ShaderGraph Keywords, to allow fragment or vertex-only keywords. For Texture2D properties, added linearGrey and red as options for default texture mode. For Texture2D properties, changed the \"bump\" option to be called \"Normal Map\", and will now tag these properties with the [NormalMap] tag. Added Branch On Input Connection node. This node can be used inside a subgraph to branch on the connection state of an exposed property. Added Use Custom Binding option to properties. When this option is enabled, a property can be connected to a Branch On Input Connection node. The user provides a custom label that will be displayed on the exposed property, when it is disconnected in a graph. Added new dropdown property type for subgraphs, to allow compile time branching that can be controlled from the parent graph, via the subgraph instance node. Added Dropdown node per dropdown property, that can be used to configure the desired branch control. Added selection highlight and picking shader passes for URP target. Added the ability to mark textures / colors as [MainTexture] and [MainColor]. Added the ability to enable tiling and offset controls for a Texture2D input. Added the Split Texture Transform node to allow using/overriding the provided tiling and offset from a texture input. Added Calculate Level Of Detail Texture 2D node, for calculating a Texture2D LOD level. Added Gather Texture 2D node, for retrieving the four samples (red component only) that would be used for bilinear interpolation when sampling a Texture2D. Added toggle \"Disable Global Mip Bias\" in Sample Texture 2D and Sample Texture 2D array node. This checkbox disables the runtimes automatic Mip Bias, which for instance can be activated during dynamic resolution scaling. Added Sprite option to Main Preview, which is similar to Quad but does not allow rotation. Sprite is used as the default preview for URP Sprite shaders. Added Tessellation Option to PositionNode settings, to provide access to the pre-displaced tessellated position. Added visible errors for invalid stage capability connections to shader graph. Added a ShaderGraph animated preview framerate throttle. Added many node synonyms for the Create Node search so that it's easier to find nodes. Changed Properties and Keywords are no longer separated by type on the blackboard. Categories allow for any combination of properties and keywords to be grouped together as the user defines. Vector2/Vector3/Vector4 property types will now be properly represented by a matching Vector2/Vector3/Vector4 UI control in the URP + HDRP Material Inspector as opposed to the fallback Vector4 field that was used for any multi-dimensional vector type in the past. Updated/corrected View Direction documentation Change Asset/Create/Shader/Blank Shader Graph to Asset/Create/Shader Graph/Blank Shader Graph Change Asset/Create/Shader/Sub Graph to Asset/Create/Shader Graph/Sub Graph Change Asset/Create/Shader/VFX Shader Graph to Asset/Create/Shader Graph/VFX Shader Graph Adjusted Blackboard article to clarify multi-select functionality Limited max number of inspectable items in the Inspector View to 20 items Added borders to inspector items styling, to better differentiate between separate items Updated Custom Function Node to use new ShaderInclude asset type instead of TextAsset (.hlsl and .cginc softcheck remains). Change BranchOnInputNode to choose NotConnected branch when generating Preview Only ShaderGraph keywords count towards the shader permutation variant limit, SubGraph keywords do not. ShaderGraph SubGraphs will now report errors and warnings in a condensed single error. Changed \"Create Node\" action in ShaderGraph stack separator context menu to \"Add Block Node\" and added it to main stack context menu GatherTexture2D and TexelSize nodes now support all shader stages. Fixed Fixed an issue where fog node density was incorrectly calculated. Fixed inspector property header styling Added padding to the blackboard window to prevent overlapping of resize region and scrollbars interfering with user interaction Blackboard now properly handles selection persistence of items between undo and redos Fixed the Custom Editor GUI field in the Graph settings that was ignored. Node included HLSL files are now tracked more robustly, so they work after file moves and renames [1301915] (https://issuetracker.unity3d.com/product/unity/issues/guid/1301915/) Prevent users from setting enum keywords with duplicate reference names and invalid characters [1287335] Fixed a bug where old preview property values would be used for node previews after an undo operation. Clean up console error reporting from node shader compilation so errors are reported in the graph rather than the Editor console [1296291] (https://issuetracker.unity3d.com/product/unity/issues/guid/1296291/) Fixed treatment of node precision in subgraphs, now allows subgraphs to switch precisions based on the subgraph node [1304050] (https://issuetracker.unity3d.com/issues/precision-errors-when-theres-a-precision-discrepancy-between-subgraphs-and-parent-graphs) Fixed an issue where the Rectangle Node could lose detail at a distance. New control offers additional method that preserves detail better [1156801] Fixed virtual texture layer reference names allowing invalid characters [1304146] Fixed issue with SRP Batcher compatibility [1310624] Fixed issue with Hybrid renderer compatibility [1296776] Fixed ParallaxOcclusionMapping node to clamp very large step counts that could crash GPUs (max set to 256). [1329025] (https://issuetracker.unity3d.com/issues/shadergraph-typing-infinity-into-the-steps-input-for-the-parallax-occlusion-mapping-node-crashes-unity) Fixed an issue where the shader variant limit exceeded message was not getting passed [1304168] (https://issuetracker.unity3d.com/product/unity/issues/guid/1304168) Fixed a bug in master node preview generation that failed compilation when a block was deleted [1319066] (https://issuetracker.unity3d.com/issues/shadergraph-deleting-stack-blocks-of-universal-rp-targeted-shadergraph-causes-the-main-preview-to-fail-to-compile) Fixed issue where vertex generation was incorrect when only custom blocks were present [1320695]. Fixed a bug where property deduplication was failing and spamming errors [1317809] (https://issuetracker.unity3d.com/issues/console-error-when-adding-a-sample-texture-operator-when-a-sampler-state-property-is-present-in-blackboard) Fixed a bug where big input values to the SimpleNoise node caused precision issues, especially noticeable on Mali GPUs. [1322891] (https://issuetracker.unity3d.com/issues/urp-mali-missing-glitch-effect-on-mali-gpu-devices) Fixed a bug where synchronously compiling an unencountered shader variant for preview was causing long delays in graph updates [1323744] Fixed a regression where custom function node file-included functions could not access shadergraph properties [1322467] Fixed an issue where a requirement was placed on a fixed-function emission property [1319637] Fixed default shadergraph precision so it matches what is displayed in the graph settings UI (single) [1325934] Fixed an unhelpful error message when custom function nodes didn't have a valid file [1323493]. Fixed an issue with how the transform node handled direction transforms from absolute world space in camera relative SRPs [1323726] Fixed a bug where changing a Target setting would switch the inspector view to the Node Settings tab if any nodes were selected. Fixed \"Disconnect All\" option being grayed out on stack blocks [1313201]. Fixed how shadergraph's prompt for \"unsaved changes\" was handled to fix double messages and incorrect window sizes [1319623]. Fixed an issue where users can't create multiple Boolean or Enum keywords on the blackboard. 1329021 Fixed an issue where generated property reference names could conflict with Shader Graph reserved keywords [1328762] (https://issuetracker.unity3d.com/product/unity/issues/guid/1328762/) Fixed a ShaderGraph issue where ObjectField focus and Node selections would both capture deletion commands [1313943]. Fixed a ShaderGraph issue where the right click menu doesn't work when a stack block node is selected [1320212]. Fixed a bug when a node was both vertex and fragment exclusive but could still be used causing a shader compiler error [1316128]. Fixed a ShaderGraph issue where a warning about an uninitialized value was being displayed on newly created graphs [1331377]. Fixed divide by zero warnings when using the Sample Gradient Node Fixed the default dimension (1) for vector material slots so that it is consistent with other nodes. (https://issuetracker.unity3d.com/product/unity/issues/guid/1328756/) Fixed reordering when renaming enum keywords. (https://issuetracker.unity3d.com/product/unity/issues/guid/1328761/) Fixed an issue where an integer property would be exposed in the material inspector as a float 1330302 Fixed a bug in ShaderGraph where sticky notes couldn't be copied and pasted [1221042]. Fixed an issue where upgrading from an older version of ShaderGraph would cause Enum keywords to be not exposed [1332510] Fixed an issue where a missing subgraph with a \"Use Custom Binding\" property would cause the parent graph to fail to load [1334621] (https://issuetracker.unity3d.com/issues/shadergraph-shadergraph-cannot-be-opened-if-containing-subgraph-with-custom-binding-that-has-been-deleted) Fixed a ShaderGraph issue where unused blocks get removed on edge replacement [1334341]. Fixed an issue where the ShaderGraph transform node would generate incorrect results when transforming a direction from view space to object space [1333781] (https://issuetracker.unity3d.com/product/unity/issues/guid/1333781/) Fixed a ShaderGraph issue where keyword properties could get stuck highlighted when deleted [1333738]. Fixed issue with ShaderGraph custom interpolator node dependency ordering [1332553]. Fixed SubGraph SamplerState property defaults not being respected [1336119] Fixed an issue where nested subgraphs with identical SamplerState property settings could cause compile failures [1336089] Fixed an issue where SamplerState properties could not be renamed after creation [1336126] Fixed loading all materials from project when saving a ShaderGraph. Fixed issues with double prompts for \"do you want to save\" when closing Shader Graph windows [1316104]. Fixed a ShaderGraph issue where resize handles on blackboard and graph inspector were too small [1329247] (https://issuetracker.unity3d.com/issues/shadergraph-resize-bounds-for-blackboard-and-graph-inspector-are-too-small) Fixed a ShaderGraph issue where a material inspector could contain an extra set of render queue, GPU instancing, and double-sided GI controls. Fixed a Shader Graph issue where property auto generated reference names were not consistent across all property types [1336937]. Fixed a warning in ShaderGraph about BuiltIn Shader Library assembly having no scripts. Fixed ShaderGraph BuiltIn target not having collapsible foldouts in the material inspector [1339256]. Fixed GPU instancing support in Shadergraph [1319655] (https://issuetracker.unity3d.com/issues/shader-graph-errors-are-thrown-when-a-propertys-shader-declaration-is-set-to-hybrid-per-instance-and-exposed-is-disabled). Fixed indent level in shader graph target foldout (case 1339025). Fixed ShaderGraph BuiltIn target shader GUI to allow the same render queue control available on URP with the changes for case 1335795. Fixed ShaderGraph BuiltIn target not to apply emission in the ForwardAdd pass to match surface shader results [1345574]. (https://issuetracker.unity3d.com/product/unity/issues/guid/1345574/) Fixed Procedural Virtual Texture compatibility with SRP Batcher [1329336] (https://issuetracker.unity3d.com/issues/procedural-virtual-texture-node-will-make-a-shadergraph-incompatible-with-srp-batcher) Fixed an issue where SubGraph keywords would not deduplicate before counting towards the permutation limit [1343528] (https://issuetracker.unity3d.com/issues/shader-graph-graph-is-generating-too-many-variants-error-is-thrown-when-using-subgraphs-with-keywords) Fixed an issue where an informational message could cause some UI controls on the graph inspector to be pushed outside the window [1343124] (https://issuetracker.unity3d.com/product/unity/issues/guid/1343124/) Fixed a ShaderGraph issue where selecting a keyword property in the blackboard would invalidate all previews, causing them to recompile [1347666] (https://issuetracker.unity3d.com/product/unity/issues/guid/1347666/) Fixed the incorrect value written to the VT feedback buffer when VT is not used. Fixed ShaderGraph isNaN node, which was always returning false on Vulkan and Metal platforms. Fixed ShaderGraph sub-graph stage limitations to be per slot instead of per sub-graph node [1337137]. Disconnected nodes with errors in ShaderGraph no longer cause the imports to fail [1349311] (https://issuetracker.unity3d.com/issues/shadergraph-erroring-unconnected-node-causes-material-to-become-invalid-slash-pink) ShaderGraph SubGraphs now report node warnings in the same way ShaderGraphs do [1350282]. Fixed ShaderGraph exception when trying to set a texture to \"main texture\" [1350573]. Fixed a ShaderGraph issue where Float properties in Integer mode would not be cast properly in graph previews 1330302 Fixed a ShaderGraph issue where hovering over a context block but not its node stack would not bring up the incorrect add menu 1351733 Fixed the BuiltIn Target to perform shader variant stripping [1345580] (https://issuetracker.unity3d.com/product/unity/issues/guid/1345580/) Fixed incorrect warning while using VFXTarget Fixed a bug with Sprite Targets in ShaderGraph not rendering correctly in game view [1352225] Fixed compilation problems on preview shader when using hybrid renderer v2 and property desc override Hybrid Per Instance Fixed a serialization bug wrt PVT property flags when using subgraphs. This fixes SRP batcher compatibility. [11.0.0] - 2020-10-21 Added Changed Fixed Fixed an issue where nodes with ports on one side would appear incorrectly on creation [1262050] Fixed a broken link in the TOC to Main Preview Fixed an issue with the Gradient color picker displaying different values than the selected color. Fixed an issue where blackboard properties when dragged wouldn't scroll the list of properties to show the user more of the property list [1293632] Fixed an issue where, when blackboard properties were dragged and then the user hit the \"Escape\" key, the drag indicator would still be visible Fixed an issue where renaming blackboard properties through the Blackboard wouldn't actually change the underlying property name Fixed an issue where blackboard wasn't resizable from all directions like the Inspector and Main Preview Fixed an issue where deleting a property node while your mouse is over it leaves the property highlighted in the blackboard [1238635] Fixed an issue where Float/Vector1 properties did not have the ability to be edited using a slider in the Inspector like the other Vector types Fixed an issue with inactive node deletion throwing a superfluous exception. Fixed an issue where interpolators with preprocessors were being packed incorrectly. Fixed rounded rectangle shape not rendering correctly on some platforms. Fixed an issue where generated BuildVertexDescriptionInputs() produced an HLSL warning, \"implicit truncation of vector type\" 1299179 Fixed an issue on upgrading graphs with inactive Master Nodes causing null ref errors. 1298867 Fixed an issue with duplicating a node with the blackboard closed 1294430 Fixed an issue where ShaderGraph stopped responding after selecting a node after opening the graph with the inspector window hidden 1304501 Fixed the InputNodes tests that were never correct. These were incorrect tests, no nodes needed tochange. Fixed the ViewDirection Node in Tangent space's calculation to match how the transform node works [1296788] Fixed an issue where SampleRawCubemapNode were requiring the Normal in Object space instead of World space [1307962] Boolean keywords now have no longer require their reference name to end in _ON to show up in the Material inspector [1306820] (https://issuetracker.unity3d.com/product/unity/issues/guid/1306820/) Newly created properties and keywords will no longer use obfuscated GUID-based reference names in the shader code [1300484] Fixed ParallaxMapping node compile issue on GLES2 Fixed a selection bug with block nodes after changing tabs [1312222] Fixed some shader graph compiler errors not being logged [1304162]. Fixed a shader graph bug where the Hue node would have a large seam with negative values [1340849]. Fixed an error when using camera direction with sample reflected cube map [1340538]. Fixed ShaderGraph's FogNode returning an incorrect density when the fog setting was disabled [1347235]. [10.3.0] - 2020-11-03 Added Users can now manually control the preview mode of nodes in the graph, and subgraphs Changed Adjusted and expanded Swizzle Node article as reviewed by docs editorial.(DOC-2695) Adjusted docs for SampleTexture2D, SampleTexture2DLOD, SampleTexture2DArray, SampleTexture3D, SampleCubemap, SampleReflectedCubemap, TexelSize, NormalFromTexture, ParallaxMapping, ParallaxOcclusionMapping, Triplanar, Sub Graphs, and Custom Function Nodes to reflect changes to texture wire data structures. (DOC-2568) Texture and SamplerState types are now HLSL structures (defined in com.unity.render-pipelines.core/ShaderLibrary/Texture.hlsl). CustomFunctionNode use of the old plain types is supported, but the user should upgrade to structures to avoid bugs. The shader graph inspector window will now switch to the \"Node Settings\" tab whenever a property/node/other selectable item in the graph is clicked on to save the user a click Fixed Fixed an issue where shaders could be generated with CR/LF (\"\\r\\n\") instead of just LF (\"\\n\") line endings [1286430] Fixed Custom Function Node to display the name of the custom function. [1293575] Addressed C# warning 0649 generated by unassigned structure members Fixed using TexelSize or reading sampler states from Textures output from a Subgraph or Custom Function Node [1284036] Shaders using SamplerState types now compile with GLES2 (SamplerStates are ignored, falls back to Texture-associated sampler state) [1292031] Fixed an issue where the horizontal scrollbar at the bottom of the shader graph inspector window could not be used due to the resizing widget always taking priority over it Fixed an issue where the shader graph inspector window could be resized past the edges of the shader graph view Fixed an issue where resizing the shader graph inspector window sometimes had unexpected results Fixed Graph Inspector scaling that was allocating too much space to the labels [1268134] Fixed some issues with our Convert To Subgraph contextual menu to allow passthrough and fix inputs/outputs getting lost. Fixed issue where a NullReferenceException would be thrown on resetting reference name for a Shader Graph property Fixed an upgrade issue where old ShaderGraph files with a weird/bugged state would break on update to master stack [1255011] Fixed a bug where non-word characters in an enum keyword reference name would break the graph. 1270168 Fixed issue where a NullReferenceException would be thrown on resetting reference name for a Shader Graph property [10.2.0] - 2020-10-19 Added Changed Renamed the existing Sample Cubemap Node to Sample Reflected Cubemap Node, and created a new Sample Cubemap Node that samples cubemaps with a direction. Removed unnecessary HDRP constant declarations used by Material inspector from the UnityPerMaterial cbuffer [1285701] Virtual Texture properties are now forced to be Exposed, as they do not work otherwise [1256374] Fixed Fixed an issue where old ShaderGraphs would import non-deterministically, changing their embedded property names each import [1283800] Using the TexelSize node on a ShaderGraph texture property is now SRP batchable [1284029] Fixed an issue where Mesh Deformation nodes did not have a category color. 1227081 Fixed SampleTexture2DLOD node to return opaque black on unsupported platforms [1241602] ShaderGraph now detects when a SubGraph is deleted while being used by a SubGraph node, and displays appropriate errors [1206438] Fixed an issue where the Main Preview window rendered too large on small monitors during first open. [1254392] Fixed an issue where Block nodes using Color slots would not be automatically removed from the Master Stack. [1259794] Fixed an issue where the Create Node menu would not close when pressing the Escape key. [1263667] Fixed an issue with the Preview Manager not updating correctly when deleting an edge that was created with a node (dragging off an existing node slot) Fixed an issue where ShaderGraph could not read matrices from a Material or MaterialPropertyBlock while rendering with SRP batcher [1256374] Fixed an issue where user setting a property to not Exposed, Hybrid-Instanced would result in a non-Hybrid Global property [1285700] Fixed an issue with Gradient when it is used as expose parameters. Generated code was failing [1285640 ] Fixed the subgraph slot sorting function [1286805] Fixed Parallax Occlusion Mapping not working in sub graphs. 1221317 All textures in a ShaderGraph, even those not used, will now be pulled into an Exported Package [1283902] Fixed an issue where the presence of an HDRP DiffusionProfile property or node would cause the graph to fail to load when HDRP package was not present [1287904] Fixed an issue where unknown type Nodes (i.e. HDRP-only nodes used without HDRP package) could be copied, resulting in an unloadable graph [1288475] Fixed an issue where dropping HDRP-only properties from the blackboard field into the graph would soft-lock the graph [1288887] Fixed an issue using the sample gradient macros in custom function nodes, which was using a scalar value instead of a vector value for the gradients [1299830] [10.1.0] - 2020-10-12 Added Added parallax mapping node and parallax occlusion mapping node. Added the possibility to have multiple POM node in a single graph. Added better error feedback when SampleVirtualTexture nodes run into issues with the VirtualTexture property inputs Added ability for Shader Graph to change node behavior without impacting existing graphs via the “Allow Deprecated Nodes” Changed Added method chaining support to shadergraph collection API. Optimized ShaderSubGraph import dependencies to minimize unnecessary reimports when using CustomFunctionNode Changed UI names from Vector1 to Float Renamed Float precision to Single Cleaned up the UI to add/remove Targets The * in the ShaderGraph title bar now indicates that the graph has been modified when compared to the state it was loaded, instead of compared to what is on disk Cancelling a \"Save changes on Close?\" will now cancel the Close as well When attempting to Save and encountering a Read Only file or other exception, ShaderGraph will allow the user to retry as many times as they like Fixed Fixed a bug where ShaderGraph subgraph nodes would not update their slot names or order Fixed an issue where very old ShaderGraphs would fail to load because of uninitialized data 1269616 Fixed an issue where ShaderGraph previews didn't display correctly when setting a texture to \"None\" [1264932] Fixed an issue with the SampleVirtualTexture node in ShaderGraph, where toggling Automatic Streaming would cause the node to incorrectly display four output slots [1271618] Fixed an issue in ShaderGraph with integer-mode Vector1 properties throwing errors when the value is changed [1264930] Fixed a bug where ShaderGraph would not load graphs using Procedural VT nodes when the nodes were the project had them disabled [1271598] Fixed an issue where the ProceduralVT node was not updating any connected SampleVT nodes when the number of layers was changed [1274288] Fixed an issue with how unknown nodes were treated during validation Fixed an issue where ShaderGraph shaders did not reimport automatically when some of the included files changed [1269634] Fixed an issue where building a context menu on a dragging block node would leave it floating and undo/redo would result in a soft-lock Fixed an issue where ShaderGraph was logging error when edited in play mode [1274148]. Fixed a bug where properties copied over with their graph inputs would not hook up correctly in a new graph [1274306] Fixed an issue where renaming a property in the blackboard at creation would trigger an error. Fixed an issue where ShaderGraph shaders did not reimport automatically when missing dependencies were reintroduced [1182895] Fixed an issue where ShaderGraph previews would not show error shaders when the active render pipeline is incompatible with the shader [1257015] ShaderGraph DDX, DDY, DDXY, and NormalFromHeight nodes do not allow themselves to be connected to vertex shader, as the derivative instructions can't be used [1209087] When ShaderGraph detects no active SRP, it will still continue to render the master preview, but it will use the error shader [1264642] VirtualTexture is no longer allowed as a SubGraph output (it is not supported by current system) [1254483] ShaderGraph Custom Function Node will now correctly convert function and slot names to valid HLSL identifiers [1258832] Fixed an issue where ShaderGraph Custom Function Node would reorder slots when you modified them [1280106] Fixed Undo handling when adding or removing Targets from a ShaderGraph [1257028] Fixed an issue with detection of circular subgraph dependencies [1269841] Fixed an issue where subgraph nodes were constantly changing their serialized data [1281975] Modifying a subgraph will no longer cause ShaderGraphs that use them to \"reload from disk?\" [1198885] Fixed issues with ShaderGraph title bar not correctly displaying the modified status * [1282031] Fixed issues where ShaderGraph could discard modified data without user approval when closed [1170503] Fixed an issue where ShaderGraph file dependency gathering would fail to include any files that didn't exist Fixed issues with ShaderGraph detection and handling of deleted graph files Fixed an issue where the ShaderGraph was corrupting the translation cache Fixed an issue where ShaderGraph would not prompt the user to save unsaved changes after an assembly reload Fixed an issue with Position Node not automatically upgrading Fixed an issue where failing SubGraphs would block saving graph files using them (recursion check would throw exceptions) [1283425] Fixed an issue where choosing \"None\" as the default texture for a texture property would not correctly preview the correct default color [1283782] Fixed some bugs with Color Nodes and properties that would cause incorrect collorspace conversions [10.0.0] - 2019-06-10 Added Added the Internal Inspector which allows the user to view data contained in selected nodes and properties in a new floating graph sub-window. Also added support for custom property drawers to let you visualize any data type you like and expose it to the inspector. Added samples for Procedural Patterns to the package. You can now use the right-click context menu to delete Sticky Notes. You can now save your graph as a new Asset. Added support for vertex skinning when you use the DOTS animation package. You can now use the right-click context menu to set the precision on multiple selected nodes. You can now select unused nodes in your graph. When you start the Editor, Shader Graph now displays Properties in the Blackboard as collapsed. Updated the zoom level to let you zoom in further. Blackboard properties now have a Duplicate menu option. When you duplicate properties, Shader Graph maintains the order, and inserts duplicates below the current selection. When you convert a node to a Sub Graph, the dialog now opens up in the directory of the original graph that contained the node. If the new Sub Graph is outside this directory, it also remembers that path for the next dialog to ease folder navigation. If Unity Editor Analytics are enabled, Shader Graph collects anonymous data about which nodes you use in your graphs. This helps the Shader Graph team focus our efforts on the most common graph scenarios, and better understand the needs of our customers. We don't track edge data and cannot recreate your graphs in any form. The Create Node Menu now has a tree view and support for fuzzy field searching. When a Shader Graph or Sub Graph Asset associated with a open window has been deleted, Unity now displays a dialog that asks whether you would like to save the graph as a new Asset or close the window. Added a drop-down menu to the PBR Master Node that lets you select the final coordinate space of normals delivered from the fragment function. Added support for users to drag and drop Blackboard Properties from one graph to another. Breaking out GraphData validation into clearer steps. Added AlphaToMask render state. Added a field to the Master Nodes that overrides the generated shader's ShaderGUI, which determines how a Material that uses a Shader Graph looks. Added Redirect Nodes. You can now double-click an edge to add a control point that allows you to route edges around other nodes and connect multiple output edges. Added Compute Deformation Node to read deformed vertex data from Dots Deformations. Added new graph nodes that allow sampling Virtual Textures Shader Graph now uses a new file format that is much friendlier towards version control systems and humans. Existing Shader Graphs and will use the new format next time they are saved. Added 'Allow Material Override' option to the built-in target for shader graph. Changed Changed the Branch node so that it uses a ternary operator (Out = bool ? a : B) instead of a linear interpolate function. Copied nodes are now pasted at the cursor location instead of slightly offset from their original location. Error messages reported on Sub Graph output nodes for invalid previews now present clearer information, with documentation support. Updated legacy COLOR output semantic to SV_Target in pixel shader for compatibility with DXC. Updated the functions in the Normal From Height node to avoid NaN outputs. Changed the Voronoi Node algorithm to increase the useful range of the input values and to always use float values internally to avoid clipping. Changed the Reference Suffix of Keyword Enum entries so that you cannot edit them, which ensures that material keywords compile properly. Updated the dependent version of Searcher to 4.2.0. Added support for Linear Blend Skinning Node to Universal Render Pipeline. Moved all code to be under Unity specific namespaces. Changed ShaderGraphImporter and ShaderSubgraphImporter so that graphs are imported before Models. Remove VFXTarget if VisualEffect Graph package isn't included. VFXTarget doesn't overwrite the shader export anymore, VFXTarget can be active with another target. Fixed Edges no longer produce errors when you save a Shader Graph. Shader Graph no longer references the NUnit package. Fixed a shader compatibility issue in the SRP Batcher when you use a hybrid instancing custom variable. Fixed an issue where Unity would crash when you imported a Shader Graph Asset with invalid formatting. Fixed an issue with the animated preview when there is no Camera with animated Materials in the Editor. Triplanar nodes no longer use Camera-relative world space by default in HDRP. Errors no longer occur when you activate Enable GPU Instancing on Shader Graph Materials. 1184870 Errors no longer occur when there are multiple tangent transform nodes on a graph. 1185752 The Main Preview for Sprite Lit and Sprite Unlit master nodes now displays the correct color. 1184656 Shader Graph shaders in Always Include Shaders no longer crash builds. 1191757 The Transform node now correctly transforms Absolute World to Object. Errors no longer occur when you change the precision of Sub Graphs. 1158413 Fixed an error where the UV channel drop-down menu on nodes had clipped text. 1188710 Added StencilOverride support. Sticky Notes can now be grouped properly. Fixed an issue where nodes couldn't be copied from a group. Fixed a bug that occurred when you duplicated multiple Blackboard properties or keywords simultaneously, where Shader Graph stopped working, potentially causing data loss. Fixed a bug where you couldn't reorder Blackboard properties. Shader Graph now properly duplicates the Exposed status for Shader properties and keywords. Fixed a bug where the Save Graph As dialog for a Shader or Sub Graph sometimes appeared in the wrong Project when you had multiple Unity Projects open simultaneously. Fixed an issue where adding the first output to a Sub Graph without any outputs prior caused Shader Graphs containing the Sub Graph to break. Fixed an issue where Shader Graph shaders using the CameraNode failed to build on PS4 with \"incompatible argument list for call to 'mul'\". Fixed a bug that caused problems with Blackboard property ordering. Fixed a bug where the redo functionality in Shader Graph often didn't work. Fixed a bug where using the Save As command on a Sub Graph raised an exception. Fixed a bug where the input fields sometimes didn't render properly. 1176268 Fixed a bug where the Gradient property didn't work with all system locales. 1140924 Fixed a bug where Properties in the Blackboard could have duplicate names. Fixed a bug where you could drag the Blackboard into a graph even when you disabled the Blackboard. Fixed a bug where the Vertex Normal slot on master nodes needed vertex normal data input to compile. 1193348 Fixed a bug where GetWorldSpaceNormalizeViewDir() could cause undeclared indentifier errors. 1190606 Fixed a bug where Emission on PBR Shader Graphs in the Universal RP would not bake to lightmaps. 1190225 Fixed a bug where Shader Graph shaders were writing to POSITION instead of SV_POSITION, which caused PS4 builds to fail. Fixed a bug where Object to Tangent transforms in the Transform node used the wrong matrix. 1162203 Fixed an issue where boolean keywords in a Shader Graph caused HDRP Material features to fail. 1204827 Fixed a bug where Object space normals scaled with Object Scale. Documentation links on nodes now point to the correct URLs and package versions. Fixed an issue where Sub Graphs sometimes had duplicate names when you converted nodes into Sub Graphs. Fixed an issue where the number of ports on Keyword nodes didn't update when you added or removed Enum Keyword entries. Fixed an issue where colors in graphs didn't update when you changed a Blackboard Property's precision while the Color Mode is set to Precision. Fixed a bug where custom mesh in the Master Preview didn't work. Fixed a number of memory leaks that caused Shader Graph assets to stay in memory after closing the Shader Graph window. You can now smoothly edit controls on the Dielectric Specular node. Fixed Blackboard Properties to support scientific notation. Fixed a bug where warnings in the Shader Graph or Sub Graph were treated as errors. Fixed a bug where the error Output value 'vert' is not initialized displayed on all PBR graphs in Universal. 1210710 Fixed a bug where PBR and Unlit master nodes in Universal had Alpha Clipping enabled by default. Fixed an issue in where analytics wasn't always working. Fixed a bug where if a user had a Blackboard Property Reference start with a digit the generated shader would be broken. Avoid unintended behavior by removing the ability to create presets from Shader Graph (and Sub Graph) assets. 1220914 Fixed a bug where undo would make the Master Preview visible regardless of its toggle status. Fixed a bug where any change to the PBR master node settings would lose connection to the normal slot. Fixed a bug where the user couldn't open up HDRP Master Node Shader Graphs without the Render Pipeline set to HDRP. Fixed a bug where adding a HDRP Master Node to a Shader Graph would softlock the Shader Graph. Fixed a bug where shaders fail to compile due to #pragma target generation when your system locale uses commas instead of periods. Fixed a compilation error when using Hybrid Renderer due to incorrect positioning of macros. Fixed a bug where the Create Node Menu lagged on load. Entries are now only generated when property, keyword, or subgraph changes are detected. 1209567. Fixed a bug with the Transform node where converting from Absolute World space in a sub graph causes invalid subscript errors. 1190813 Fixed a bug where depndencies were not getting included when exporting a shadergraph and subgraphs Fixed a bug where adding a \" to a property display name would cause shader compilation errors and show all nodes as broken Fixed a bug where the Position node would change coordinate spaces from World to Absolute World when shaders recompile. 1184617 Fixed a bug where instanced shaders wouldn't compile on PS4. Fixed a bug where switching a Color Nodes' Mode between Default and HDR would cause the Color to be altered incorrectly. Fixed a bug where nodes dealing with matricies would sometimes display a preview, sometimes not. Optimized loading a large Shader Graph. 1209047 Fixed NaN issue in triplanar SG node when blend goes to 0. Fixed a recurring bug where node inputs would get misaligned from their ports. [1224480] Fixed an issue where Blackboard properties would not duplicate with Precision or Hybrid Instancing options. Fixed an issue where Texture properties on the Blackboard would not duplicate with the same Mode settings. Fixed an issue where Keywords on the Blackboard would not duplicate with the same Default value. Shader Graph now requests preview shader compilation asynchronously. 1209047 Fixed an issue where Shader Graph would not compile master previews after an assembly reload. Fixed issue where Linear Blend Skinning node could not be converted to Sub Graph 1227087 Fixed a compilation error in preview shaders for nodes requiring view direction. Fixed undo not being recorded properly for setting active master node, graph precision, and node defaults. Fixed an issue where Custum Function nodes and Sub Graph Output nodes could no longer rename slots. Fixed a bug where searcher entries would not repopulate correctly after an undo was perfromed (https://fogbugz.unity3d.com/f/cases/1241018/) Fixed a bug where Redirect Nodes did not work as inputs to Custom Function Nodes. 1235999 Fixed a bug where changeing the default value on a keyword would reset the node input type to vec4 (https://fogbugz.unity3d.com/f/cases/1216760/) Fixed a soft lock when you open a graph when the blackboard hidden. Fixed an issue where keyboard navigation in the Create Node menu no longer worked. [1253544] Preview correctly shows unassigned VT texture result, no longer ignores null textures Don't allow duplicate VT layer names when renaming layers Moved VT layer TextureType to the VTProperty from the SampleVT node Fixed the squished UI of VT property layers Disallow Save As and Convert to Subgraph that would create recursive dependencies Fixed an issue where the user would not get a save prompt on application close 1262044 Fixed bug where output port type would not visually update when input type changed (for example from Vec1 to Vec3) 1259501 Fixed an issue with how we collected/filtered nodes for targets. Applied the work to the SearchWindowProvider as well Fixed a bug where the object selector for Custom Function Nodes did not update correctly. 1176129 Fixed a bug where whitespaces were allowed in keyword reference names Fixed a bug where the Create Node menu would override the Object Field selection window. 1176125 Fixed a bug where the Main Preview window was no longer a square aspect ratio. 1257053 Fixed a bug where the size of the Graph Inspector would not save properly. 1257084 Replace toggle by an enumField for lit/unlit with VFXTarget Alpha Clipping option in Graph inspector now correctly hides and indents dependent options. (https://fogbugz.unity3d.com/f/cases/1257041/) Fixed a bug where changing the name of a property did not update nodes on the graph. 1249164 Fixed a crash issue when ShaderGraph included in a project along with DOTS assemblies Added missing SampleVirtualTextureNode address mode control in ShaderGraph Fixed a badly named control on SampleVirtualTextureNode in ShaderGraph Fixed an issue where multiple SampleVirtualTextureNodes created functions with names that may collide in ShaderGraph Made sub graph importer deterministic to avoid cascading shader recompiles when no change was present. Adjusted style sheet for Blackboard to prevent ui conflicts. Fixed a bug where the SampleVirtualTexture node would delete slots when changing its LOD mode Use preview of the other target if VFXTarget is active. [7.1.1] - 2019-09-05 Added You can now define shader keywords on the Blackboard. Use these keywords on the graph to create static branches in the generated shader. The tab now shows whether you are working in a Sub Graph or a Shader Graph file. The Shader Graph importer now bakes the output node type name into a meta-data object. Fixed The Shader Graph preview no longer breaks when you create new PBR Graphs. Fixed an issue where deleting a group and a property at the same time would cause an error. Fixed the epsilon that the Hue Node uses to avoid NaN on platforms that support half precision. Emission nodes no longer produce errors when you use them in Sub Graphs. Exposure nodes no longer produce errors when you use them in Sub Graphs. Unlit master nodes no longer define unnecessary properties in the Universal Render Pipeline. Errors no longer occur when you convert a selection to a Sub Graph. Color nodes now handle Gamma and Linear conversions correctly. Sub Graph Output nodes now link to the correct documentation page. When you use Keywords, PBR and Unlit master nodes no longer produce errors. PBR master nodes now calculate Global Illumination (GI) correctly. PBR master nodes now apply surface normals. PBR master nodes now apply fog. The Editor now displays correct errors for missing or deleted Sub Graph Assets. You can no longer drag and drop recursive nodes onto Sub Graph Assets. [7.0.1] - 2019-07-25 Changed New Shader Graph windows are now docked to either existing Shader Graph windows, or to the Scene View. Fixed Fixed various dependency tracking issues with Sub Graphs and HLSL files from Custom Function Nodes. Fixed an error that previously occurred when you used Sampler State input ports on Sub Graphs. Normal Reconstruct Z node is now compatible with both fragment and vertex stages. Position node now draws the correct label for Absolute World. Node previews now inherit preview type correctly. Normal maps now unpack correctly for mobile platforms. Fixed an error that previously occurred when you used the Gradient Sample node and your system locale uses commas instead of periods. Fixed an issue where you couldn't group several nodes. [7.0.0] - 2019-07-10 Added You can now use the SHADERGRAPH_PREVIEW keyword in Custom Function Node to generate different code for preview Shaders. Color Mode improves node visibility by coloring the title bar by Category, Precision, or custom colors. You can now set the precision of a Shader Graph and individual nodes. Added the _TimeParameters variable which contains Time, Sin(Time), and Cosine(Time) Absolute World space on Position Node now provides absolute world space coordinates regardless of the active render pipeline. You can now add sticky notes to graphs. Changed The Custom Function Node now uses an object field to reference its source when using File mode. To enable master nodes to generate correct motion vectors for time-based vertex modification, time is now implemented as an input to the graph rather than as a global uniform. World space on Position Node now uses the default world space coordinates of the active render pipeline. Fixed Fixed an error in Custom Function Node port naming. Sampler State properties and nodes now serialize correctly. Labels in the Custom Port menu now use the correct coloring when using the Personal skin. Fixed an error that occured when creating a Sub Graph from a selection containing a Group Node. When you change a Sub Graph, Shader Graph windows now correctly reload. When you save a Shader Graph, all other Shader Graph windows no longer re-compile their preview Shaders. Shader Graph UI now draws with correct styling for 2019.3. When deleting edge connections to nodes with a preview error, input ports no longer draw in the wrong position. Fixed an error involving deprecated components from VisualElements. When you convert nodes to a Sub Graph, the nodes are now placed correctly in the Sub Graph. The Bitangent Vector Node now generates all necessary shader requirements. [6.7.0-preview] - 2019-05-16 Added Added a hidden path namespace for Sub Graphs to prevent certain Sub Graphs from populating the Create Node menu. Changed Anti-aliasing (4x) is now enabled on Shader Graph windows. Fixed When you click on the gear icon, Shader Graph now focuses on the selected node, and brings the settings menu to front view. Sub Graph Output and Custom Function Node now validate slot names, and display an appropriate error badge when needed. Remaining outdated documentation has been removed. When you perform an undo or redo to an inactive Shader Graph window, the window no longer breaks. When you rapidly perform an undo or redo, Shader Graph windows no longer break. Sub Graphs that contain references to non-existing Sub Graphs no longer break the Sub Graph Importer. You can now reference sub-assets such as Textures. You can now reference Scene Color and Scene Depth correctly from within a Sub Graph. When you create a new empty Sub Graph, it no longer shows a warning about a missing output. When you create outputs that start with a digit, Shader generation no longer fails. You can no longer add nodes that are not allowed into Sub Graphs. A graph must now always contain at least one Master Node. Duplicate output names are now allowed. Fixed an issue where the main preview was always redrawing. When you set a Master Node as active, the Main Preview now shows the correct result. When you save a graph that contains a Sub Graph node, the Shader Graph window no longer freezes. Fixed an error that occured when using multiple Sampler State nodes with different parameters. Fixed an issue causing default inputs to be misaligned in certain cases. You can no longer directly connect slots with invalid types. When the graph detects that situation, it now doesn't break and gives an error instead. [6.6.0] - 2019-04-01 Added You can now add Matrix, Sampler State and Gradient properties to the Blackboard. Added Custom Function node. Use this node to define a custom HLSL function either via string directly in the graph, or via a path to an HLSL file. You can now group nodes by pressing Ctrl + G. Added \"Delete Group and Contents\" and removed \"Ungroup All Nodes\" from the context menu for groups. You can now use Sub Graphs in other Sub Graphs. Preview shaders now compile in the background, and only redraw when necessary. Changed Removed Blackboard fields, which had no effect on Sub Graph input ports, from the Sub Graph Blackboard. Subgraph Output node is now called Outputs. Subgraph Output node now supports renaming of ports. Subgraph Output node now supports all port types. Subgraph Output node now supports reordering ports. When you convert nodes to a Sub Graph, Shader Graph generates properties and output ports in the Sub Graph, and now by default, names those resulting properties and output ports based on their types. When you delete a group, Shader Graph now deletes the Group UI, but doesn't delete the nodes inside. Fixed You can now undo edits to Vector port default input fields. You can now undo edits to Gradient port default input fields. Boolean port input fields now display correct values when you undo changes. Vector type properties now behave as expected when you undo changes. Fixed an error that previously occurred when you opened saved Shader Graphs containing one or more Voronoi nodes. You can now drag normal map type textures on to a Shader Graph to create Sample Texture 2D nodes with the correct type set. Fixed the Multiply node so default input values are applied correctly. Added padding on input values for Blend node to prevent NaN outputs. Fixed an issue where IsFaceSign would not compile within Sub Graph Nodes. Null reference errors no longer occur when you remove ports with connected edges. Default input fields now correctly hide and show when connections change. [6.5.0] - 2019-03-07 Fixed Fixed master preview for HDRP master nodes when alpha clip is enabled. [6.4.0] - 2019-02-21 Fixed Fixed the Transform node, so going from Tangent Space to any other space now works as expected. [6.3.0] - 2019-02-18 Fixed Fixed an issue where the Normal Reconstruct Z Node sometimes caused Not a Number (NaN) errors when using negative values. [6.2.0] - 2019-02-15 Fixed Fixed the property blackboard so it no longer goes missing or turns very small. Changed Code refactor: all macros with ARGS have been swapped with macros with PARAM. This is because the ARGS macros were incorrectly named. [6.1.0] - 2019-02-13 [6.0.0] - 2019-02-23 Added When you hover your cursor over a property in the blackboard, this now highlights the corresponding property elements in your Shader Graph. Similarly, if you hover over a property in the Shader Graph itself, this highlights the corresponding property in the blackboard. Property nodes in your Shader Graph now have a similar look and styling as the properties in the blackboard. Changed Errors in the compiled shader are now displayed as badges on the appropriate node. In the Scene Depth node you can now choose the depth sampling mode: Linear01, Raw or Eye. Fixed When you convert an inline node to a Property node, this no longer allows duplicate property names. When you move a node, you'll now be asked to save the Graph file. You can now Undo edits to Property parameters on the Blackboard. You can now Undo conversions between Property nodes and inline nodes. You can now Undo moving a node. You can no longer select the Texture2D Property type Mode, if the Property is not exposed. The Vector1 Property type now handles default values more intuitively when switching Mode dropdown. The Color node control is now a consistent width. Function declarations no longer contain double delimiters. The Slider node control now functions correctly. Fixed an issue where the Editor automatically re-imported Shader Graphs when there were changes to the asset database. Reverted the visual styling of various graph elements to their previous correct states. Previews now repaint correctly when Unity does not have focus. Code generation now works correctly for exposed Vector1 shader properties where the decimal separator is not a dot. The Rotate About Axis node's Modes now use the correct function versions. Shader Graph now preserves grouping when you convert nodes between property and inline. The Flip node now greys out labels for inactive controls. The Boolean property type now uses the ToggleUI property attribute, so as to not generate keywords. The Normal Unpack node no longer generates errors in Object space. The Split node now uses values from its default Port input fields. The Channel Mask node now allows multiple node instances, and no longer generates any errors. Serialized the Alpha control value on the Flip node. The Is Infinite and Is NaN nodes now use Vector 1 input ports, but the output remains the same. You can no longer convert a node inside a Sub Graph into a Sub Graph, which previously caused errors. The Transformation Matrix node's Inverse Projection and Inverse View Projection modes no longer produce errors. The term Shader Graph is now captilized correctly in the Save Graph prompt. [5.2.0] - 2018-11-27 Added Shader Graph now has Group Node, where you can group together several nodes. You can use this to keep your Graphs organized and nice. Fixed The expanded state of blackboard properties are now remembered during a Unity session. [5.1.0] - 2018-11-19 Added You can now show and hide the Main Preview and the Blackboard from the toolbar. Changed The Shader Graph package is no longer in preview. Moved NormalBlendRNM node to a dropdown option on Normal Blend node. Sample Cubemap node now has a SamplerState slot. New Sub Graph assets now default to the \"Sub Graphs\" path in the Create Node menu. New Shader Graph assets now default to the \"Shader Graphs\" path in the Shader menu. The Light Probe node is now a Baked GI node. When you use LWRP with lightmaps, this node now returns the correct lightmap data. This node is supported in HDRP. Reflection Probe nodes now only work with LWRP. This solves compilation errors in HDRP. Ambient nodes now only work with LWRP. This solves compilation errors in HDRP. Fog nodes now only work with LWRP. This solves compilation errors in HDRP. In HDRP, the Position port for the Object node now returns the absolute world position. The Baked GI, Reflection Probe, and Ambient nodes are now in the Input/Lighting category. The master node no longer has its own preview, because it was redundant. You can see the results for the master node in the Main Preview. Fixed Shadow projection is now correct when using the Unlit master node with HD Render Pipeline. Removed all direct references to matrices Matrix Construction nodes with different Mode values now evaluate correctly. Is Front Face node now works correctly when connected to Alpha and AlphaThreshold slots on the PBR master node. Corrected some instances of incorrect port dimensions on several nodes. Scene Depth and Scene Color nodes now work in single pass stereo in Lightweight Render Pipeline. Channel Mask node controls are now aligned correctly. In Lightweight Render Pipeline, Pre-multiply surface type now matches the Lit shader. Non-exposed properties in the blackboard no longer have a green dot next to them. Default reference name for shader properties are now serialized. You cannot change them after initial creation. When you save Shader Graph and Sub Graph files, they're now automatically checked out on version control. Shader Graph no longer throws an exception when you double-click a folder in the Project window. Gradient Node no longer throws an error when you undo a deletion. [5.0.0-preview] - 2018-09-28 [4.0.0-preview] - 2018-09-28 Added Shader Graph now supports the High Definition Render Pipeline with both PBR and Unlit Master nodes. Shaders built with Shader Graph work with both the Lightweight and HD render pipelines. You can now modify vertex position via the Position slot on the PBR and Unlit Master nodes. By default, the input to this node is object space position. Custom inputs to this slot should specify the absolute local position of a given vertex. Certain nodes (such as Procedural Shapes) are not viable in the vertex shader. Such nodes are incompatible with this slot. You can now edit the Reference name for a property. To do so, select the property and type a new name next to Reference. If you want to reset to the default name, right-click Reference, and select Reset reference. In the expanded property window, you can now toggle whether the property is exposed. You can now change the path of Shader Graphs and Sub Graphs. When you change the path of a Shader Graph, this modifies the location it has in the shader selection list. When you change the path of Sub Graph, it will have a different location in the node creation menu. Added Is Front Face node. With this node, you can change graph output depending on the face sign of a given fragment. If the current fragment is part of a front face, the node returns true. For a back face, the node returns false. Note: This functionality requires that you have enabled two sided on the Master node. Gradient functionality is now available via two new nodes: Sample Gradient and Gradient Asset. The Sample Gradient node samples a gradient given a Time parameter. You can define this gradient on the Gradient slot control view. The Gradient Asset node defines a gradient that can be sampled by multiple Sample Gradient nodes using different Time parameters. Math nodes now have a Waves category. The category has four different nodes: Triangle wave, Sawtooth wave, Square wave, and Noise Sine wave. The Triangle, Sawtooth, and Square wave nodes output a waveform with a range of -1 to 1 over a period of 1. The Noise Sine wave outputs a standard Sine wave with a range of -1 to 1 over a period of 2 * pi. For variance, random noise is added to the amplitude of the Sine wave, within a determined range. Added Sphere Mask node for which you can indicate the starting coordinate and center point. The sphere mask uses these with the Radius and Hardness parameters. Sphere mask functionality works in both 2D and 3D spaces, and is based on the vector coordinates in the Coords and Center input. Added support for Texture 3D and Texture 2D Array via two new property types and four new nodes. A new node Texture 2D LOD has been added for LOD functionality on a Texture 2D Sample. Sample Texture 2D LOD uses the exact same input and output slots as Sample Texture 2D, but also includes an input for level of detail adjustments via a Vector1 slot. Added Texel Size node, which allows you to get the special texture properties of a Texture 2D Asset via the {texturename}_TexelSize variable. Based on input from the Texture 2D Asset, the node outputs the width and height of the texel size in Vector1 format. Added Rotate About Axis node. This allows you to rotate a 3D vector space around an axis. For the rotation, you can specify an amount of degrees or a radian value. Unpacking normal maps in object space. Unpacking derivative maps option on sample texture nodes. Added Uint type for instancing support. Added HDR option for color material slots. Added definitions used by new HD Lit Master node. Added a popup control for a string list. Added conversion type (position/direction) to TransformNode. In your preview for nodes that are not master nodes, pixels now display as pink if they are not finite. Changed The settings for master nodes now live in a small window that you can toggle on and off. Here, you can change various rendering settings for your shader. There are two Normal Derive Nodes: Normal From Height and Normal Reconstruct Z. Normal From Height uses Vector1 input to derive a normal map. Normal Reconstruct Z uses the X and Y components in Vector2 input to derive the proper Z value for a normal map. The Texture type default input now accepts render textures. HD PBR subshader no longer duplicates surface description code into vertex shader. If the current render pipeline is not compatible, master nodes now display an error badge. The preview shader now only considers the current render pipeline. Because of this there is less code to compile, so the preview shader compiles faster. When you rename a shader graph or sub shader graph locally on your disk, the title of the Shader Graph window, black board, and preview also updates. Removed legacy matrices from Transfomation Matrix node. Texture 2D Array and Texture 3D nodes can no longer be used in the vertex shader. Normal Create node has been renamed to Normal From Texture. When you close the Shader Graph after you have modified a file, the prompt about saving your changes now shows the file name as well. Blend node now supports Overwrite mode. Simple Noise node no longer has a loop. The Polygon node now calculates radius based on apothem. Normal Strength node now calculates Z value more accurately. You can now connect Sub Graphs to vertex shader slots. If a node in the Sub Graph specifies a shader stage, that specific Sub Graph node is locked to that stage. When an instance of a Sub Graph node is connected to a slot that specifies a shader stage, all slots on that instance are locked to the stage. Separated material options and tags. Master node settings are now recreated when a topological modification occurs. Fixed Vector 1 nodes now evaluate correctly. (#334 and #337) Properties can now be copied and pasted. Pasting a property node into another graph will now convert it to a concrete node. (#300 and #307) Nodes that are copied from one graph to another now spawn in the center of the current view. (#333) When you edit sub graph paths, the search window no longer yields a null reference exception. The blackboard is now within view when deserialized. Your system locale can no longer cause incorrect commands due to full stops being converted to commas. Deserialization of subgraphs now works correctly. Sub graphs are now suffixed with (sub), so you can tell them apart from other nodes. Boolean and Texture type properties now function correctly in sub-graphs. The preview of a node does not obstruct the selection outliner anymore. The Dielectric Specular node no longer resets its control values. You can now copy, paste, and duplicate sub-graph nodes with vector type input ports. The Lightweight PBR subshader now normalizes normal, tangent, and view direction correctly. Shader graphs using alpha clip now generate correct depth and shadow passes. Normal Create node has been renamed to Normal From Texture. The preview of nodes now updates correctly. Your system locale can no longer cause incorrect commands due to full stops being converted to commas. Show Generated Code no longer throws an \"Argument cannot be null\" error. Sub Graphs now use the correct generation mode when they generate preview shaders. The CodeFunctionNode API now generates correct function headers when you use DynamicMatrix type slots. Texture type input slots now set correct default values for 'Normal' texture type. SpaceMaterialSlot now reads correct slot. Slider node control now functions correctly. Shader Graphs no longer display an error message intended for Sub Graphs when you delete properties. The Shader Graph and Sub Shader Graph file extensions are no longer case-sensitive. The dynamic value slot type now uses the correct decimal separator during HLSL generation. Fixed an issue where Show Generated Code could fail when external editor was not set. In the High Definition Render Pipeline, Shader Graph now supports 4-channel UVs. The Lightweight PBR subshader now generates the correct meta pass. Both PBR subshaders can now generate indirect light from emission. Shader graphs now support the SRP batcher. Fixed an issue where floatfield would be parsed according to OS locale settings with .NET 4.6"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Absolute-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Absolute-Node.html",
    "title": "Absolute Node | FSM Unity Framework",
    "keywords": "Absolute Node Description Returns the absolute value of the input In. Components of the input Dynamic Vector that are positive will remain positive and components that are negative will be inverted and become positive. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Absolute_float4(float4 In, out float4 Out) { Out = abs(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Add-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Add-Node.html",
    "title": "Add Node | FSM Unity Framework",
    "keywords": "Add Node Description Returns the sum of the two input values A and B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Add_float4(float4 A, float4 B, out float4 Out) { Out = A + B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/All-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/All-Node.html",
    "title": "All Node | FSM Unity Framework",
    "keywords": "All Node Description Returns true if all components of the input In are non-zero. This is useful for Branching. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_All_float4(float4 In, out float Out) { Out = all(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ambient-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ambient-Node.html",
    "title": "Ambient Node | FSM Unity Framework",
    "keywords": "Ambient Node Description Provides access to the Scene's Ambient color values. When Environment Lighting Source is set to Gradient Port Color/Sky returns the value Sky Color. When Environment Lighting Source is set to Color Port Color/Sky returns the value Ambient Color. Ports Equator and Ground always return the values Equator Color and Ground Color regardless of the current Environment Lighting Source. Note: Values of this Node are only updated when entering Play mode or saving the current Scene/Project. Note: The behavior of this Node is undefined globally. Shader Graph does not define the function of the node. Instead, each Render Pipeline defines what HLSL code to execute for this Node. Different Render Pipelines may produce different results. If you're building a shader in one Render Pipeline that you want to use in both, try checking it in both pipelines before production. A Node might be defined in one Render Pipeline and undefined in the other. If this Node is undefined, it returns 0 (black). Unity Render Pipelines Support Universal Render Pipeline The High Definition Render Pipeline does not support this Node. Ports Name Direction Type Binding Description Color/Sky Output Vector 3 None Color (Color) or Sky (Gradient) color value Equator Output Vector 3 None Equator (Gradient) color value Ground Output Vector 3 None Ground (Gradient) color value Generated Code Example The following example code represents one possible outcome of this node. float3 _Ambient_ColorSky = SHADERGRAPH_AMBIENT_SKY; float3 _Ambient_Equator = SHADERGRAPH_AMBIENT_EQUATOR; float3 _Ambient_Ground = SHADERGRAPH_AMBIENT_GROUND;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/And-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/And-Node.html",
    "title": "And Node | FSM Unity Framework",
    "keywords": "And Node Description Returns true if both the inputs A and B are true. This is useful for Branching. Ports Name Direction Type Binding Description A Input Boolean None First input value B Input Boolean None Second input value Out Output Boolean None Output value Generated Code Example void Unity_And(float A, float B, out float Out) { Out = A && B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Any-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Any-Node.html",
    "title": "Any Node | FSM Unity Framework",
    "keywords": "Any Node Description Returns true if any of the components of the input In are non-zero. This is useful for Branching. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Any_float4(float4 In, out float Out) { Out = any(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arccosine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arccosine-Node.html",
    "title": "Arccosine Node | FSM Unity Framework",
    "keywords": "Arccosine Node Description Returns the arccosine of each component of the input In as a vector of the same dimension and equal length. Each component should be within the range of -1 to 1. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Arccosine_float4(float4 In, out float4 Out) { Out = acos(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arcsine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arcsine-Node.html",
    "title": "Arcsine Node | FSM Unity Framework",
    "keywords": "Arcsine Node Description Returns the arcsine of each component of the input In as a vector of the same dimension and equal length. Each component should be within the range of -Pi/2 to Pi/2. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Arcsine_float4(float4 In, out float4 Out) { Out = asin(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arctangent-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arctangent-Node.html",
    "title": "Arctangent Node | FSM Unity Framework",
    "keywords": "Arctangent Node Description Returns the arctangent of the value of input In. Each component should be within the range of -Pi/2 to Pi/2. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Arctangent_float4(float4 In, out float4 Out) { Out = atan(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arctangent2-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Arctangent2-Node.html",
    "title": "Arctangent2 Node | FSM Unity Framework",
    "keywords": "Arctangent2 Node Description Returns the arctangent of the values of both input A and input B. The signs (whether they are positive or negative values) of the input values are used to determine whether the output components, or channels, are positive or negative within a range of -Pi to Pi. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Arctangent2_float4(float4 A, float4 B, out float4 Out) { Out = atan2(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Artistic-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Artistic-Nodes.html",
    "title": "Artistic Nodes | FSM Unity Framework",
    "keywords": "Artistic Nodes Adjustment Channel Mixer Contrast Controls the amount each of the channels of input In contribute to each of the output channels. Adjusts the contrast of input In by the amount of input Contrast. Hue Invert Colors Offsets the hue of input In by the amount of input Offset. Inverts the colors of input In on a per channel basis. Replace Color Saturation Replaces values in input In equal to input From to the value of input To. Adjusts the saturation of input In by the amount of input Saturation. White Balance Adjusts the temperature and tint of input In by the amount of inputs Temperature and Tint respectively. Blend Blend Blends the value of input Blend onto input Base using the blending mode defined by parameter Mode. Filter Dither Dither is an intentional form of noise used to randomize quantization error. It is used to prevent large-scale patterns such as color banding in images.. Mask Channel Mask Color Mask Masks values of input In on channels selected in dropdown Channels. Creates a mask from values in input In equal to input Mask Color. Normal Normal Blend Normal From Height Blends two normal maps defined by inputs A and B together. Creates a normal map from a height map defined by input Texture. Normal Strength Normal Unpack Adjusts the strength of the normal map defined by input In by the amount of input Strength. Unpacks a normal map defined by input In. Utility Colorspace Conversion Returns the result of converting the value of input In from one colorspace space to another."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Baked-GI-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Baked-GI-Node.html",
    "title": "Baked GI Node | FSM Unity Framework",
    "keywords": "Baked GI Node Description Provides access to the Baked GI values at the vertex or fragment's position. Requires Position and Normal input for light probe sampling, and lightmap coordinates Static UV and Dynamic UV for all potential lightmap sampling cases. Note: The behavior of this Node is undefined globally. Shader Graph does not define the function of the node. Instead, each Render Pipeline defines what HLSL code to execute for this Node. Different Render Pipelines may produce different results. If you're building a shader in one Render Pipeline that you want to use in both, try checking it in both pipelines before production. A Node might be defined in one Render Pipeline and undefined in the other. If this Node is undefined, it returns 0 (black). Unity Render Pipelines Support High Definition Render Pipeline. Although, this Node does not work in a Shader Graph that targets HDRP's Unlit Master Node. Universal Render Pipeline Ports Name Direction Type Binding Description Position Input Vector 3 Position (world space) Mesh vertex/fragment's Position Normal Input Vector 3 Normal (world space) Mesh vertex/fragment's Normal Static UV Input Vector 2 UV1 Lightmap coordinates for the static lightmap Dynamic UV Input Vector 2 UV2 Lightmap coordinates for the dynamic lightmap Out Output Vector 3 None Output color value Controls Name Type Options Description Apply Lightmap Scaling Toggle True, False If enabled lightmaps are automatically scaled and offset. Generated Code Example The following example code represents one possible outcome of this node. void Unity_BakedGI_float(float3 Position, float3 Normal, float2 StaticUV, float2 DynamicUV, out float Out) { Out = SHADERGRAPH_BAKED_GI(Position, Normal, StaticUV, DynamicUV, false); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Bitangent-Vector-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Bitangent-Vector-Node.html",
    "title": "Bitangent Node | FSM Unity Framework",
    "keywords": "Bitangent Node Description Provides access to the mesh vertex or fragment's Bitangent Vector, depending on the effective Shader Stage of the graph section the Node is part of. The coordinate space of the output value can be selected with the Space dropdown parameter. Ports Name Direction Type Binding Description Out Output Vector 3 None Bitangent Vector for the Mesh Vertex/Fragment. Controls Name Type Options Description Space Dropdown Object, View, World, Tangent Selects coordinate space of Bitangent Vector to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blackboard.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blackboard.html",
    "title": "Blackboard | FSM Unity Framework",
    "keywords": "Blackboard Description You can use the Blackboard to define, order, and categorize the Properties and Keywords in a graph. From the Blackboard, you can also edit the path for the selected Shader Graph Asset or Sub Graph. Accessing the Blackboard The Blackboard is visible by default, and you cannot drag it off the graph and lose it. However, you are able to position it anywhere in the Shader Graph Window. It always maintains the same distance from the nearest corner, even if you resize the window. Adding properties and keywords to the Blackboard To create a new property or keyword, click the Add (+) button on the Blackboard's title bar and select a type. For a full list of property types, see Property Types. Editing properties and keywords Select a property or keyword in the Blackboard or graph to modify its settings in the Node Settings Menu. Setting Description Name The property's display name. The Editor strips quotation marks from display names and replaces them with underscores. Rename an item via the Blackboard by double-clicking on its name. Reference The name that Shader Graph uses internally for this property. Although the Editor populates this value by default, you can modify it. To revert to the original reference name, right-click on the word Reference (not the entry field) and select Reset Reference in the context menu. If the Reference Name contains any characters that HLSL does not support, the Editor replaces those characters with underscores. Default The default value of this property in any Material based on this Shader Graph. For example, if you have a Shader Graph for grass and expose the grass color as a property, you might set the default to Green. Precision Set the precision mode for the property. See Precision Modes. Exposed Enable this setting to make the property available for you to edit via the C# API. Enabled by default. Modifying and selecting keywords and properties To reorder items listed on the Blackboard, drag and drop them. To delete items, use the Delete key on Windows or Command + Backspace keys on macOS. To select multiple items, hold down the Ctrl key while making your selections. To cancel the selection of one or multiple items, hold down the Ctrl key while clicking on the items you want to remove from the selection. Using Blackboard categories To make the properties in your shader more discoverable, organize them into categories. Expand and collapse categories to make the Blackboard easier to navigate. Creating, renaming, moving, and deleting categories To add a category, use + on the Blackboard. To rename a category, double-click on the category name, or right-click and select Rename. To move a category within the Blackboard, select and drag it. To remove a category, select it and press Delete, or right-click and select Delete. Deleting a category also deletes the properties within it, so move those you wish to keep. Adding, removing, and reordering properties and keywords To add a property or keyword to a category, expand the category with the foldout (⌄) symbol, then drag and drop the property or keyword onto the expanded category. To remove a property or keyword, select it and press Delete, or right-click and select Delete. To re-order properties or keywords, drag and drop them within a category or move them into other categories. Creating a category for specific properties and keywords Select multiple properties or keywords and use + on the Blackboard to create a category that contains all of the items you have selected. Copying and pasting categories, with or without properties You can paste empty categories, categories with all of their properties, and categories with some of their properties into one or more graphs. To copy a category with all of its properties: Select the property. Copy it with Ctrl+C. Paste it into your target graph with Ctrl+V. To copy a specific set of properties: Select the category. Hold down the Ctrl key. Click the properties you do not want to include to remove them from the selection. Copy the property with Ctrl+C. Paste it into your target graph with Ctrl+V. Using categories in the Material Inspector To modify a material you have created with a Shader Graph, you can adjust specific property or keyword values in the Material Inspector, or edit the graph itself. Working with Streaming Virtual Textures Streaming Virtual Texture Properties sample texture layers. To access these layers in the Material Inspector, expand the relevant Virtual Texture section with the ⌄ symbol next to its name. You can add and remove layers via the Inspector. Exposing properties and keywords Unity exposes properties and keywords by default. This enables write access from scripts, so that you can edit them via the C# API, in addition to the graph. Exposed items have a green dot in their label. Enable or disable this feature in the Node Settings menu. Creating nodes Drag a property or keyword from the Blackboard into the graph to create a node of that kind. Settings for a node in the graph are identical to those for the related property or keyword in the Blackboard. Expand these nodes to use a sub-member of the property value. Property node names include a green dot if the property is exposed."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blackbody-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blackbody-Node.html",
    "title": "Blackbody Node | FSM Unity Framework",
    "keywords": "Blackbody Node Description Samples a Gradient that simulates the effect of black body radiation. The calculations in this node are based on data gathered by Mitchell Charity. This node outputs color in linear RGB space and preforms the conversion using a D65 whitepoint and a CIE 1964 10 degree color space. For more information, see What color is a blackbody? Ports Name Direction Type Binding Description Temperature Input Float None Temperature or temperature map in Kelvin to sample. Out Output Vector 3 None Intensity represented by color in Vector 3. Generated Code Example The following example code represents one possible outcome of this node. void Unity_Blackbody_float(float Temperature, out float3 Out) { float3 color = float3(255.0, 255.0, 255.0); color.x = 56100000. * pow(Temperature,(-3.0 / 2.0)) + 148.0; color.y = 100.04 * log(Temperature) - 623.6; if (Temperature > 6500.0) color.y = 35200000.0 * pow(Temperature,(-3.0 / 2.0)) + 184.0; color.z = 194.18 * log(Temperature) - 1448.6; color = clamp(color, 0.0, 255.0)/255.0; if (Temperature < 1000.0) color *= Temperature/1000.0; Out = color; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blend-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Blend-Node.html",
    "title": "Blend Node | FSM Unity Framework",
    "keywords": "Blend Node Description Blends the value of input Blend onto input Base using the blending mode defined by the Mode parameter. The strength of the blend is defined by input Opacity. An Opacity value of 0 will return the input Base, unaltered. Ports Name Direction Type Binding Description Base Input Dynamic Vector None Base layer value Blend Input Dynamic Vector None Blend layer value Opacity Input Float None Strength of blend Out Output Dynamic Vector None Output value Controls Name Type Options Description Mode Dropdown Burn, Darken, Difference, Dodge, Divide, Exclusion, HardLight, HardMix, Lighten, LinearBurn, LinearDodge, LinearLight, LinearLightAddSub, Multiply, Negation, Overlay, PinLight, Screen, SoftLight, Subtract, VividLight, Overwrite Blend mode to apply Generated Code Example The following example code represents one possible outcome of this node per blend mode. Burn void Unity_Blend_Burn_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = 1.0 - (1.0 - Blend)/Base; Out = lerp(Base, Out, Opacity); } Darken void Unity_Blend_Darken_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = min(Blend, Base); Out = lerp(Base, Out, Opacity); } Difference void Unity_Blend_Difference_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = abs(Blend - Base); Out = lerp(Base, Out, Opacity); } Dodge void Unity_Blend_Dodge_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base / (1.0 - Blend); Out = lerp(Base, Out, Opacity); } Divide void Unity_Blend_Divide_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base / (Blend + 0.000000000001); Out = lerp(Base, Out, Opacity); } Exclusion void Unity_Blend_Exclusion_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Blend + Base - (2.0 * Blend * Base); Out = lerp(Base, Out, Opacity); } HardLight void Unity_Blend_HardLight_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { float4 result1 = 1.0 - 2.0 * (1.0 - Base) * (1.0 - Blend); float4 result2 = 2.0 * Base * Blend; float4 zeroOrOne = step(Blend, 0.5); Out = result2 * zeroOrOne + (1 - zeroOrOne) * result1; Out = lerp(Base, Out, Opacity); } HardMix void Unity_Blend_HardMix_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = step(1 - Base, Blend); Out = lerp(Base, Out, Opacity); } Lighten void Unity_Blend_Lighten_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = max(Blend, Base); Out = lerp(Base, Out, Opacity); } LinearBurn void Unity_Blend_LinearBurn_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base + Blend - 1.0; Out = lerp(Base, Out, Opacity); } LinearDodge void Unity_Blend_LinearDodge_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base + Blend; Out = lerp(Base, Out, Opacity); } LinearLight void Unity_Blend_LinearLight_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Blend < 0.5 ? max(Base + (2 * Blend) - 1, 0) : min(Base + 2 * (Blend - 0.5), 1); Out = lerp(Base, Out, Opacity); } LinearLightAddSub void Unity_Blend_LinearLightAddSub_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Blend + 2.0 * Base - 1.0; Out = lerp(Base, Out, Opacity); } Multiply void Unity_Blend_Multiply_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base * Blend; Out = lerp(Base, Out, Opacity); } Negation void Unity_Blend_Negation_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = 1.0 - abs(1.0 - Blend - Base); Out = lerp(Base, Out, Opacity); } Overlay void Unity_Blend_Overlay_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { float4 result1 = 1.0 - 2.0 * (1.0 - Base) * (1.0 - Blend); float4 result2 = 2.0 * Base * Blend; float4 zeroOrOne = step(Base, 0.5); Out = result2 * zeroOrOne + (1 - zeroOrOne) * result1; Out = lerp(Base, Out, Opacity); } PinLight void Unity_Blend_PinLight_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { float4 check = step (0.5, Blend); float4 result1 = check * max(2.0 * (Base - 0.5), Blend); Out = result1 + (1.0 - check) * min(2.0 * Base, Blend); Out = lerp(Base, Out, Opacity); } Screen void Unity_Blend_Screen_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = 1.0 - (1.0 - Blend) * (1.0 - Base); Out = lerp(Base, Out, Opacity); } SoftLight void Unity_Blend_SoftLight_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { float4 result1 = 2.0 * Base * Blend + Base * Base * (1.0 - 2.0 * Blend); float4 result2 = sqrt(Base) * (2.0 * Blend - 1.0) + 2.0 * Base * (1.0 - Blend); float4 zeroOrOne = step(0.5, Blend); Out = result2 * zeroOrOne + (1 - zeroOrOne) * result1; Out = lerp(Base, Out, Opacity); } Subtract void Unity_Blend_Subtract_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = Base - Blend; Out = lerp(Base, Out, Opacity); } VividLight void Unity_Blend_VividLight_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { float4 result1 = 1.0 - (1.0 - Blend) / (2.0 * Base); float4 result2 = Blend / (2.0 * (1.0 - Base)); float4 zeroOrOne = step(0.5, Base); Out = result2 * zeroOrOne + (1 - zeroOrOne) * result1; Out = lerp(Base, Out, Opacity); } Overwrite void Unity_Blend_Overwrite_float4(float4 Base, float4 Blend, float Opacity, out float4 Out) { Out = lerp(Base, Blend, Opacity); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Block-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Block-Node.html",
    "title": "Block Node | FSM Unity Framework",
    "keywords": "Block Node Description A Block is a specific type of node for the Master Stack. A Block represents a single piece of the surface (or vertex) description data that Shader Graph uses in the final shader output. Built In Block nodes are always available, but nodes that are specific to a certain render pipeline are only available for that pipeline. For example, Universal Block nodes are only available for the Universal Render Pipeline (URP), and High Definition Block nodes are only available for the High Definition Render Pipeline (HDRP). Some blocks are only compatible with specific Graph Settings, and might become active or inactive based on the graph settings you select. You can't cut, copy, or paste Blocks. Add and Remove Block Nodes To add a new Block node to a Context in the Master Stack, place the cursor over an empty area in the Context, then press the Spacebar or right-click and select Create Node. This brings up the Create Node menu, which displays only Block nodes that are valid for the Context. For example, Vertex Blocks don't appear in the Create Node menu of a Fragment Context. Select a Block node from the menu to add it to the Context. To remove a Block from the Context, select the Block node in the Context, then press the Delete key or right-click and select Delete. Automatically Add or Remove Blocks You can also enable or disable an option in the Shader Graph Preferences to automatically add and remove Blocks from a Context. If you enable Automatically Add or Remove Blocks, Shader Graph automatically adds the required Block nodes for that particular asset's Target or material type. It automatically removes any incompatible Block nodes that have no connections and default values. If you disable Automatically Add or Remove Blocks, Shader Graph doesn't automatically add and remove Block nodes. You must manually add and remove all Block nodes. Active and Inactive Blocks Active Block nodes are Blocks that contribute to the final shader. Inactive Block nodes are Blocks that are present in the Shader Graph, but don't contribute to the final shader. When you change the graph settings, certain Blocks might become active or inactive. Inactive Block nodes and any node streams that are connected only to Inactive Block nodes appear grayed out."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Boolean-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Boolean-Node.html",
    "title": "Boolean Node | FSM Unity Framework",
    "keywords": "Boolean Node Description Defines a constant Boolean value in the Shader Graph, although internally to the shader this is treated as a constant float value that is ether 0 or 1, similar to Shaderlab's Toggle property. Can be converted to a Boolean type Property via the Node's context menu. Ports Name Direction Type Binding Description Out Output Boolean None Output value Controls Name Type Options Description Toggle Defines the output value. Generated Code Example The following example code represents one possible outcome of this node. float _Boolean = 1;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Branch-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Branch-Node.html",
    "title": "Branch Node | FSM Unity Framework",
    "keywords": "Branch Node Description Provides a dynamic branch to the shader. If input Predicate is true the return output will be equal to input True, otherwise it will be equal to input False. This is determined per vertex or per pixel depending on shader stage. Both sides of the branch will be evaluated in the shader, and the branch not used will be discarded. Ports Name Direction Type Binding Description Predicate Input Boolean None Determines which input to returned True Input Dynamic Vector None Returned if Predicate is true False Input Dynamic Vector None Returned if Predicate is false Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Branch_float4(float Predicate, float4 True, float4 False, out float4 Out) { Out = Predicate ? True : False; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Branch-On-Input-Connection-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Branch-On-Input-Connection-Node.html",
    "title": "Branch On Input Connection node | FSM Unity Framework",
    "keywords": "Branch On Input Connection node The Branch On Input Connection node allows you to change the behavior of a Subgraph based on the connected state of an input property in the parent Shader Graph. You should use the Branch On Input Connection node when you want to create a default input for a port Shader Graph determines whether the property in the parent Shader Graph is connected, or not connected, and chooses a value to use as an output based on that connection state. Shader Graph uses two ports when it determines the node's connection state: The Branch On Input Connection node's Input port. The Subgraph node's matching Property port in the parent Shader Graph. For more information on Subgraph nodes, see Subgraph node. The Branch On Input Connection node's functionality is based on the Branch node. Note You can't use the Branch On Input Connection node with a Streaming Virtual Texture Property. For more information on Streaming Virtual Texturing, see Using Streaming Virtual Texturing in Shader Graph. The Branch On Input Connection node generates branching HLSL source code, but during compilation, the branch is optimized out of your shader. Create Node menu category The Branch On Input Connection node is under the Utility > Logic category in the Create Node menu. You can only use it in a Shader Subgraph. To use the Branch On Input Connection node in a Subgraph: Open the Subgraph where you want to add a Branch On Input Connection node. In the Blackboard, do one of the following: To add a new property, select Add (+), then select a property type from the menu. Enter a name for your new property and press Enter. Then, select your property in the Blackboard and drag it onto your graph to create a Property node. Select an existing property in the Blackboard and drag it onto your graph to create a Property node. With your Property node selected, in the Graph Inspector, enable Use Custom Binding. Note If you disable Use Custom Binding, you can't connect your Property node to the Branch On Input Connection node. If you've already made a connection, the Unity Editor breaks the connection and displays a warning on the node. In the Label field, enter the label for the default value that displays on your Subgraph node's port binding in its parent Shader Graph. For more information on port bindings, see Port Bindings. Press Spacebar or right-click and select Create Node. Find the Branch On Input Connection node in the Create Node Menu, then double-click or press Enter with the node selected to add it to your Subgraph. On your Property node, select the output port and drag its new connection to the Branch On Connection node's Input port. To specify the value Shader Graph uses when the Input port is connected on the Subgraph node in the parent Shader Graph, connect a node to the Connected port. To specify the value that Shader Graph uses when the Input port isn't connected, connect another node to the NotConnected port. To specify how Shader Graph uses your Connected or NotConnected values in your shader, connect any valid node to the Output port on the Branch On Input Connection node. Compatibility The Branch On Input Connection node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes Inputs The Branch On Input Connection node has the following input ports: Name Type Description Input Property The property that determines the branching logic in the node, based on its connection state in the parent Shader Graph. Connected Dynamic Vector The value to send to the Out port when Input is connected in the parent Shader Graph. NotConnected Dynamic Vector The value to send to the Out port when Input isn't connected in the parent Shader Graph. Outputs The Branch On Input Connection node has one output port: Name Type Description Out Dynamic Vector Outputs the value of either Connected or NotConnected, based on the Input property's connection state in the parent Shader Graph. Example Subgraph usage In the following example, a Branch On Input Connection node specifies the default behavior for a UV Subgraph input property. When a value for the UV property is connected in the parent graph, then the value from that property is passed to the Checkerboard node to determine the UV coordinates for the checkerboard pattern. When the UV property isn't connected, then the Branch On Input Connection node uses the UV0 channel from the UV node for the Checkerboard node's UV coordinates: Note When you preview a Subgraph, the Branch On Input Connection node always uses its NotConnected value. Related nodes The following nodes are related or similar to the Branch On Input Connection node: Branch node Subgraph node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Built-In-Blocks.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Built-In-Blocks.html",
    "title": "Built In Blocks | FSM Unity Framework",
    "keywords": "Built In Blocks Vertex Blocks Name Type Binding Description Position Vector 3 Object Space Position Defines the absolute object space vertex position per vertex. Normal Vector 3 Object Space Normal Defines the absolute object space vertex normal per vertex. Tangent Vector 3 Object Space Tangent Defines the absolute object space vertex tangent per vertex. Color Vector 4 Vertex Color Defines vertex color. Expected range 0 - 1. Fragment Blocks Name Type Binding Description Base Color Vector 3 None Defines material's base color value. Expected range 0 - 1. Normal (Tangent Space) Vector 3 Tangent Space Normal Defines material's normal value in tangent space. Normal (Object Space) Vector 3 Object Space Normal Defines material's normal value in object space. Normal (World Space) Vector 3 World Space Normal Defines material's normal value in world space. Emission Vector 3 None Defines material's emission color value. Expects positive values. Metallic Float None Defines material's metallic value, where 0 is non-metallic and 1 is metallic. Specular Vector 3 None Defines material's specular color value. Expected range 0 - 1. Smoothness Float None Defines material's smoothness value. Expected range 0 - 1. Ambient Occlusion Float None Defines material's ambient occlusion value. Expected range 0 - 1. Alpha Float None Defines material's alpha value. Used for transparency and/or alpha clip. Expected range 0 - 1. Alpha Clip Threshold Float None Fragments with an alpha below this value are discarded. Expected range 0 - 1."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Calculate-Level-Of-Detail-Texture-2D-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Calculate-Level-Of-Detail-Texture-2D-Node.html",
    "title": "Calculate Level Of Detail Texture 2D node | FSM Unity Framework",
    "keywords": "Calculate Level Of Detail Texture 2D node The Calculate Level of Detail Texture 2D node takes an input Texture 2D and outputs the mip level of a Texture sample. This node is useful in situations where you need to know the mip level of a Texture, such as when you might want to modify the mip level before sampling in your shader. The Calculate Level of Detail Texture 2D node also has a clamped and unclamped mode: Clamped: The node clamps the returned mip level to the actual mips available on the Texture. The node uses the CalculateLevelOfDetail HLSL intrinsic function. Use this mode when you want to know which mip to sample your Texture from and restrict the result to an existing mip. Unclamped: The node returns the ideal mip level, based on an idealized Texture with all its mips present. The node uses the CalculateLevelOfDetailUnclamped HLSL intrinsic function. Use this mode when you need a more generic value for your mip level. For example, a Texture might only have 3 mips: a 64×64 mip, a 32×32 mip, and a 16×16 mip. When you use the Calculate Level Of Detail Texture 2D node in its Clamped mode, the node restricts the LOD output to one of the 3 mips on the Texture, even if the ideal mip level might be a smaller resolution, such as an 8×8 version. In its Unclamped mode, the node outputs the ideal 8×8 mip level, even though it doesn't exist on the Texture. Note On platforms where these HLSL functions don't exist, Shader Graph determines an appropriate approximation to use, instead. Create Node menu category The Calculate Level of Detail Texture 2D node is under the Input > Texture category in the Create Node menu. Compatibility The Calculate Level of Detail Texture 2D node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes The Calculate Level of Detail Texture 2D node can only be connected to a Block node in the Fragment Context. For more information on Block nodes and Contexts, refer to Master Stack. Inputs The Calculate Level of Detail Texture 2D node has the following input ports: Name Type Binding Description Texture Texture 2D None The Texture to use in the mip level calculation. UV Vector 2 UV The UV coordinate to use to calculate the Texture's mip level. Sampler SamplerState None The Sampler State and corresponding settings to use to calculate the Texture's mip level. Controls The Calculate Level of Detail Texture 2D node has one control: Name Type Options Description Clamp Toggle True, False When enabled, Shader Graph clamps the output mip level to the actual mips present on the provided Texture input. When disabled, Shader Graph returns an ideal mip level, based on an idealized Texture with all its mips present. Outputs The Calculate Level of Detail Texture 2D node has one output port: Name Type Description LOD Float The final calculated mip level of the Texture. Example graph usage In the following example, a Calculate Level of Detail Texture 2D node calculates the mip level of the Leaves_Albedo Texture for a set of UV coordinates and a specific Sampler State. It sends the calculated mip level for the Texture to the LOD input port on a Sample Texture 2D LOD node, which samples the same Texture: Related nodes The following nodes are related or similar to the Calculate Level of Detail Texture 2D node: Sample Texture 2D LOD node Sampler State node Gather Texture 2D node Texture 2D Asset node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Camera-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Camera-Node.html",
    "title": "Camera Node | FSM Unity Framework",
    "keywords": "Camera Node Description Provides access to various parameters of the Camera currently being used for rendering. This is comprised of values the Camera's GameObject, such as Position and Direction, as well as various projection parameters. Unity Render Pipelines Support Universal Render Pipeline The High Definition Render Pipeline does not support this Node. Ports Name Direction Type Binding Description Position Output Vector 3 None Position of the Camera's GameObject in world space Direction Output Vector 3 None The Camera's forward vector direction Orthographic Output Float None Returns 1 if the Camera is orthographic, otherwise 0 Near Plane Output Float None The Camera's near plane distance Far Plane Output Float None The Camera's far plane distance Z Buffer Sign Output Float None Returns -1 when using a reversed Z Buffer, otherwise 1 Width Output Float None The Camera's width if orthographic Height Output Float None The Camera's height if orthographic Generated Code Example The following example code represents one possible outcome of this node. float3 _Camera_Position = _WorldSpaceCameraPos; float3 _Camera_Direction = -1 * mul(UNITY_MATRIX_M, transpose(mul(UNITY_MATRIX_I_M, UNITY_MATRIX_I_V)) [2].xyz); float _Camera_Orthographic = unity_OrthoParams.w; float _Camera_NearPlane = _ProjectionParams.y; float _Camera_FarPlane = _ProjectionParams.z; float _Camera_ZBufferSign = _ProjectionParams.x; float _Camera_Width = unity_OrthoParams.x; float _Camera_Height = unity_OrthoParams.y;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ceiling-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ceiling-Node.html",
    "title": "Ceiling Node | FSM Unity Framework",
    "keywords": "Ceiling Node Description Returns the smallest integer value, or whole number, that is greater than or equal to the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Ceiling_float4(float4 In, out float4 Out) { Out = ceil(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Mask-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Mask-Node.html",
    "title": "Channel Mask Node | FSM Unity Framework",
    "keywords": "Channel Mask Node Description Masks values of input In on channels selected in dropdown Channels. Outputs a vector of the same length as the input vector but with the selected channels set to 0. Channels available in the dropdown Channels will represent the amount of channels present in input In. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Dynamic Vector None Output value Controls Name Type Options Description Channels Mask Dropdown Dynamic Selects any number of channels to mask Generated Code Example The following example code represents one possible outcome of this node. void Unity_ChannelMask_RedGreen_float4(float4 In, out float4 Out) { Out = float4(0, 0, In.b, In.a); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Mixer-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Mixer-Node.html",
    "title": "Channel Mixer Node | FSM Unity Framework",
    "keywords": "Channel Mixer Node Description Controls the amount each of the channels of input In contribute to each of the channels of output Out. The slider parameters on the node control the contribution of each of the input channels. The toggle button parameters control which of the output channels is currently being edited. Slider controls for editing the contribution of each input channnel range between -2 and 2. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Out Output Vector 3 None Output value Controls Name Type Options Description Toggle Button Array R, G, B Selects the output channel to edit. R Slider Controls contribution of input red channel to selected output channel. G Slider Controls contribution of input green channel to selected output channel. B Slider Controls contribution of input blue channel to selected output channel. Shader Function Generated Code Example The following example code represents one possible outcome of this node. _ChannelMixer_Red = float3 (OutRedInRed, OutRedInGreen, OutRedInBlue); _ChannelMixer_Green = float3 (OutGreenInRed, OutGreenInGreen, OutGreenInBlue); _ChannelMixer_Blue = float3 (OutBlueInRed, OutBlueInGreen, OutBlueInBlue); void Unity_ChannelMixer_float(float3 In, float3 _ChannelMixer_Red, float3 _ChannelMixer_Green, float3 _ChannelMixer_Blue, out float3 Out) { Out = float3(dot(In, _ChannelMixer_Red), dot(In, _ChannelMixer_Green), dot(In, _ChannelMixer_Blue)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Channel-Nodes.html",
    "title": "Channel Nodes | FSM Unity Framework",
    "keywords": "Channel Nodes Combine Flip Controls the amount each of the channels of input In contribute to each of the output channels. Adjusts the contrast of input In by the amount of input Contrast. Split Swizzle Offsets the hue of input In by the amount of input Offset. Inverts the colors of input In on a per channel basis."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Checkerboard-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Checkerboard-Node.html",
    "title": "Checkerboard Node | FSM Unity Framework",
    "keywords": "Checkerboard Node Description Generates a checkerboard of alternating colors between inputs Color A and Color B based on input UV. The checkerboard scale is defined by input Frequency. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Color A Input Color RGB None First checker color Color B Input Color RGB None Second checker color Frequency Input Vector 2 None Scale of checkerboard per axis Out Output Vector 2 None Output UV value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Checkerboard_float(float2 UV, float3 ColorA, float3 ColorB, float2 Frequency, out float3 Out) { UV = (UV.xy + 0.5) * Frequency; float4 derivatives = float4(ddx(UV), ddy(UV)); float2 duv_length = sqrt(float2(dot(derivatives.xz, derivatives.xz), dot(derivatives.yw, derivatives.yw))); float width = 1.0; float2 distance3 = 4.0 * abs(frac(UV + 0.25) - 0.5) - width; float2 scale = 0.35 / duv_length.xy; float freqLimiter = sqrt(clamp(1.1f - max(duv_length.x, duv_length.y), 0.0, 1.0)); float2 vector_alpha = clamp(distance3 * scale.xy, -1.0, 1.0); float alpha = saturate(0.5f + 0.5f * vector_alpha.x * vector_alpha.y * freqLimiter); Out = lerp(ColorA, ColorB, alpha.xxx); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Circle-Pupil-Animation-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Circle-Pupil-Animation-Node.html",
    "title": "Circle Pupil Animation Node | FSM Unity Framework",
    "keywords": "Circle Pupil Animation Node This node applies a deformation to a normalized IrisUV coordinate to simulate the opening and closure of the pupil. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Circle Pupil Animation Node No Yes Ports name Direction type description IrisUV Input Vector2 Position of the fragment to shade in object space. Pupil Radius Input float Direction of the incident ray in object space. Either from the camera in rasterization or from the previous bounce in ray tracing. Maximal Pupil Aperture Input float The normal of the eye surface in object space. Minimal Pupil Aperture Input float The index of refraction of the eye (1.333 by default). Pupil Apertur Input float Distance between the end of the cornea and the iris plane. For the default model, this value should be 0.02 IrisUV Output Vector2 Position of the refracted point on the iris plane in object space."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Clamp-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Clamp-Node.html",
    "title": "Clamp Node | FSM Unity Framework",
    "keywords": "Clamp Node Description Returns the input In clamped between the minimum and maximum values defined by inputs Min and Max respectively. Ports Name Direction Type Description In Input Dynamic Vector Unclamped input value Min Input Dynamic Vector Minimum value Max Input Dynamic Vector Maximum value Out Output Dynamic Vector Clamped output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Clamp_float4(float4 In, float4 Min, float4 Max, out float4 Out) { Out = clamp(In, Min, Max); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Mask-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Mask-Node.html",
    "title": "Color Mask Node | FSM Unity Framework",
    "keywords": "Color Mask Node Description Creates a mask from values in input In equal to input Mask Color. Input Range can be used to define a wider range of values around input Mask Color to create the mask. Colors within this range will return 1, otherwise the node will return 0. Input Fuzziness can be used to soften the edges around the selection similar to anti-aliasing. Ports Name Direction Type Binding Description In Input Vector 3 None Input value. Mask Color Input Vector 3 Color Color to use for mask. Range Input Float None Select colors within this range from input Mask Color. Fuzziness Input Float None Feather edges around selection. Higher values result in a softer selection mask. Out Output Float None Output mask value. Generated Code Example The following example code represents one possible outcome of this node. void Unity_ColorMask_float(float3 In, float3 MaskColor, float Range, float Fuzziness, out float4 Out) { float Distance = distance(MaskColor, In); Out = saturate(1 - (Distance - Range) / max(Fuzziness, 1e-5)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Modes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Modes.html",
    "title": "Color Modes | FSM Unity Framework",
    "keywords": "Color Modes Description Shader Graph can display colors on nodes in your graph to improve readability. This feature uses Color Modes to change which colors to display in the graph. Use the Color Mode: drop-down menu in the top right corner of the Shader Graph Window to change the Color Modes. Modes Name Description None Does not display colors on the nodes. All nodes use the default gray. Category Displays colors on the nodes based on their assigned category. See Category Colors below. Precision Displays colors on the nodes based on the current Precision Mode in use. User Defined Lets you set the display colors on a per-node basis. These are custom colors for your graph. See User Defined Colors below. Category Colors This mode displays colors on the nodes based on their category. See the Node Library to learn about the different categories available. The table below lists current categories and their corresponding colors. Name Color Hex Value Artistic #DB773B Channel #97D13D Input #CB3022 Math #4B92F3 Procedural #9C4FFF Utility #AEAEAE UV #08D78B Note: Sub Graph nodes in a main Shader Graph fall in the Utility category. If you select Category mode, all Sub Graphs use the Utility color. Precision Colors This mode displays colors on the nodes based on their current precision. If you set a node to Inherit Precision, the display color reflects the currently active precision. See Precision Modes for more information about inheritance. User Defined Colors This mode displays colors on the nodes based on user preferences. In this mode, the user defines colors for each node. If a custom color is not set, the node displays in the default gray. To set a custom color for a node, right-click on the target node to bring up the the context menu, and select Color. Option Description Change... Brings up a color picker menu and lets you set your own custom color on the node. Reset Removes the currently selected color and sets it to the default gray. Overriding Default Colors For each project, you can override preset colors in the Category and Precision modes. Unity uses a .uss style sheet and Hex color codes to set colors. The default style sheet in your project is Packages/com.unity.shadergraph/Editor/Resources/Styles/ColorMode.uss. The best practice is to create a copy of this file to override the presets. Under your project's Assets folder, create a new Editor/Resources/Styles folder structure, and place a copy of ColorMode.uss in the Styles folder. Change the Hex color codes in this .uss file to override the presets and use your own custom colors for the Category and Precision modes."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Color-Node.html",
    "title": "Color Node | FSM Unity Framework",
    "keywords": "Color Node Description Defines a constant Vector 4 value in the shader using a Color field. Can be converted to a Color Property Type via the Node's context menu. The value of the Mode parameter will also respected when generating the Property. NOTE: In versions prior to 10.0, Shader Graph assumed that HDR colors from the Color Node were in gamma space. Version 10.0 corrected this behavior, and Shader Graph now interprets HDR colors in linear space. HDR Color nodes that you created with older versions maintain the old behavior, but you can use the Graph Inspector to upgrade them. To mimic the old behavior on a new HDR Color node, you can use a Colorspace Conversion Node to convert the HDR color from RGB to Linear. Ports Name Direction Type Binding Description Out Output Vector 4 None Output value Controls Name Type Options Description Color Defines the output value. Mode Dropdown Default, HDR Sets properties of the Color field Generated Code Example The following example code represents one possible outcome of this node. float4 _Color = IsGammaSpace() ? float4(1, 2, 3, 4) : float4(SRGBToLinear(float3(1, 2, 3)), 4);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Colorspace-Conversion-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Colorspace-Conversion-Node.html",
    "title": "Colorspace Conversion Node | FSM Unity Framework",
    "keywords": "Colorspace Conversion Node Description Returns the result of converting the value of input In from one colorspace space to another. The spaces to transform from and to are defined by the values of the dropdowns on the node. Ports Name Direction Type Description In Input Vector 3 Input value Out Output Vector 3 Output value Controls Name Type Options Description From Dropdown RGB, Linear, HSV Selects the colorspace to convert from To Dropdown RGB, Linear, HSV Selects the colorspace to convert to Generated Code Example The following example code represents one possible outcome of this node per from/to permutation. RGB > RGB void Unity_ColorspaceConversion_RGB_RGB_float(float3 In, out float3 Out) { Out = In; } RGB > Linear void Unity_ColorspaceConversion_RGB_Linear_float(float3 In, out float3 Out) { float3 linearRGBLo = In / 12.92;; float3 linearRGBHi = pow(max(abs((In + 0.055) / 1.055), 1.192092896e-07), float3(2.4, 2.4, 2.4)); Out = float3(In <= 0.04045) ? linearRGBLo : linearRGBHi; } RGB > HSV void Unity_ColorspaceConversion_RGB_HSV_float(float3 In, out float3 Out) { float4 K = float4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); float4 P = lerp(float4(In.bg, K.wz), float4(In.gb, K.xy), step(In.b, In.g)); float4 Q = lerp(float4(P.xyw, In.r), float4(In.r, P.yzx), step(P.x, In.r)); float D = Q.x - min(Q.w, Q.y); float E = 1e-10; Out = float3(abs(Q.z + (Q.w - Q.y)/(6.0 * D + E)), D / (Q.x + E), Q.x); } Linear > RGB void Unity_ColorspaceConversion_Linear_RGB_float(float3 In, out float3 Out) { float3 sRGBLo = In * 12.92; float3 sRGBHi = (pow(max(abs(In), 1.192092896e-07), float3(1.0 / 2.4, 1.0 / 2.4, 1.0 / 2.4)) * 1.055) - 0.055; Out = float3(In <= 0.0031308) ? sRGBLo : sRGBHi; } Linear > Linear void Unity_ColorspaceConversion_Linear_Linear_float(float3 In, out float3 Out) { Out = In; } Linear > HSV void Unity_ColorspaceConversion_Linear_HSV_float(float3 In, out float3 Out) { float3 sRGBLo = In * 12.92; float3 sRGBHi = (pow(max(abs(In), 1.192092896e-07), float3(1.0 / 2.4, 1.0 / 2.4, 1.0 / 2.4)) * 1.055) - 0.055; float3 Linear = float3(In <= 0.0031308) ? sRGBLo : sRGBHi; float4 K = float4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); float4 P = lerp(float4(Linear.bg, K.wz), float4(Linear.gb, K.xy), step(Linear.b, Linear.g)); float4 Q = lerp(float4(P.xyw, Linear.r), float4(Linear.r, P.yzx), step(P.x, Linear.r)); float D = Q.x - min(Q.w, Q.y); float E = 1e-10; Out = float3(abs(Q.z + (Q.w - Q.y)/(6.0 * D + E)), D / (Q.x + E), Q.x); } HSV > RGB void Unity_ColorspaceConversion_HSV_RGB_float(float3 In, out float3 Out) { float4 K = float4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0); float3 P = abs(frac(In.xxx + K.xyz) * 6.0 - K.www); Out = In.z * lerp(K.xxx, saturate(P - K.xxx), In.y); } HSV > Linear void Unity_ColorspaceConversion_HSV_Linear_float(float3 In, out float3 Out) { float4 K = float4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0); float3 P = abs(frac(In.xxx + K.xyz) * 6.0 - K.www); float3 RGB = In.z * lerp(K.xxx, saturate(P - K.xxx), In.y); float3 linearRGBLo = RGB / 12.92; float3 linearRGBHi = pow(max(abs((RGB + 0.055) / 1.055), 1.192092896e-07), float3(2.4, 2.4, 2.4)); Out = float3(RGB <= 0.04045) ? linearRGBLo : linearRGBHi; } HSV > HSV void Unity_ColorspaceConversion_HSV_HSV_float(float3 In, out float3 Out) { Out = In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Combine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Combine-Node.html",
    "title": "Combine Node | FSM Unity Framework",
    "keywords": "Combine Node Description Creates new vectors from the four inputs R, G, B and A. Output RGBA is a Vector 4 composed of inputs R, G, B and A. Output RGB is a Vector 3 composed of inputs R, G and B. Output RG is a Vector 2 composed of inputs R and G. Ports Name Direction Type Binding Description R Input Float None Defines red channel of output G Input Float None Defines green channel of output B Input Float None Defines blue channel of output A Input Float None Defines alpha channel of output RGBA Output Vector 4 None Output value as Vector 4 RGB Output Vector 3 None Output value as Vector 3 RG Output Vector 2 None Output value as Vector 2 Generated Code Example The following example code represents one possible outcome of this node. void Unity_Combine_float(float R, float G, float B, float A, out float4 RGBA, out float3 RGB, out float2 RG) { RGBA = float4(R, G, B, A); RGB = float3(R, G, B); RG = float2(R, G); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Comparison-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Comparison-Node.html",
    "title": "Comparison Node | FSM Unity Framework",
    "keywords": "Comparison Node Description Compares the two input values A and B based on the condition selected on the dropdown. This is often used as an input to the Branch Node. Ports Name Direction Type Binding Description A Input Float None First input value B Input Float None Second input value Out Output Boolean None Output value Controls Name Type Options Description Dropdown Equal, NotEqual, Less, LessOrEqual, Greater, GreaterOrEqual Condition for comparison Generated Code Example The following example code represents one possible outcome of this node per comparison type. Equal void Unity_Comparison_Equal_float(float A, float B, out float Out) { Out = A == B ? 1 : 0; } NotEqual void Unity_Comparison_NotEqual_float(float A, float B, out float Out) { Out = A != B ? 1 : 0; } Less void Unity_Comparison_Less_float(float A, float B, out float Out) { Out = A < B ? 1 : 0; } LessOrEqual void Unity_Comparison_LessOrEqual_float(float A, float B, out float Out) { Out = A <= B ? 1 : 0; } Greater void Unity_Comparison_Greater_float(float A, float B, out float Out) { Out = A > B ? 1 : 0; } GreaterOrEqual void Unity_Comparison_GreaterOrEqual_float(float A, float B, out float Out) { Out = A >= B ? 1 : 0; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Compute-Deformation-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Compute-Deformation-Node.html",
    "title": "Compute Deformation Node | FSM Unity Framework",
    "keywords": "Compute Deformation Node Description This node lets you pass compute deformed vertex data to a vertex shader, and only works with the DOTS Hybrid Renderer. You must provide DeformedVertexData in the _DeformedMeshData buffer. The node uses the _ComputeMeshIndex property to calculate where the DeformedVertexData associated with the current mesh are located in the _DeformedMeshData buffer. To output data, you must either install both the DOTS Hybrid Renderer and DOTS Animation packages, or use a custom solution. Ports Name Direction Type Stage Description Position Output Vector3 Vertex Outputs the deformed vertex position. Normal Output Vector3 Vertex Outputs the deformed vertex normal. Tangent Output Vector3 Vertex Outputs the deformed vertex tangent."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Compute-Vertex-Position-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Compute-Vertex-Position-Water-Node.html",
    "title": "Compute Water Vertex Position | FSM Unity Framework",
    "keywords": "Compute Water Vertex Position This node provides access to the water mesh vertex position. It's used in water instead of the Position node. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. Don't modify the settings of this node. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Compute Water Vertex Position No Yes Ports Name Direction Type Description PositionWS Output Vector3 The position of the water surface vertex in world space."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Constant-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Constant-Node.html",
    "title": "Constant Node | FSM Unity Framework",
    "keywords": "Constant Node Description Defines a Float of a mathematical constant value in the shader. Ports Name Direction Type Binding Description Out Output Float None Output value Controls Name Type Options Description Mode Dropdown PI, TAU, PHI, E, SQRT2 Sets output constant value Generated Code Example The following example code represents one possible outcome of this node per constant type. PI float _Constant_PI = 3.1415926; TAU float _Constant_TAU = 6.28318530; PHI float _Constant_PHI = 1.618034; E float _Constant_E = 2.718282; SQRT2 float _Constant_SQRT2 = 1.414214;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Contrast-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Contrast-Node.html",
    "title": "Contrast Node | FSM Unity Framework",
    "keywords": "Contrast Node Description Adjusts the contrast of input In by the amount of input Contrast. A Contrast value of 1 will return the input unaltered. A Contrast value of 0 will return the midpoint of the input. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Contrast Input Float None Contrast value Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Contrast_float(float3 In, float Contrast, out float3 Out) { float midpoint = pow(0.5, 2.2); Out = (In - midpoint) * Contrast + midpoint; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cornea-Refraction-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cornea-Refraction-Node.html",
    "title": "Cornea Refraction Node | FSM Unity Framework",
    "keywords": "Cornea Refraction Node This node performs the refraction of the view ray in object space and returns the object space position that results. This is used to simulate the refraction that can be seen when looking at an eye. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Cornea Refraction Node No Yes Ports name Direction type description Position OS Input Vector3 Position of the fragment to shade in object space. View Direction OS Input Vector3 Direction of the incident ray in object space. Either from the camera in rasterization or from the previous bounce in ray tracing. Cornea Normal OS Input Vector3 The normal of the eye surface in object space. Cornea IOR Input float The index of refraction of the eye (1.333 by default). Iris Plane Offset Input float Distance between the end of the cornea and the iris plane. For the default model, this value should be 0.02 RefractedPositionOS Output Vector3 Position of the refracted point on the iris plane in object space."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cosine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cosine-Node.html",
    "title": "Cosine Node | FSM Unity Framework",
    "keywords": "Cosine Node Description Returns the cosine of the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Cosine_float4(float4 In, out float4 Out) { Out = cos(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Create-Node-Menu.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Create-Node-Menu.html",
    "title": "Create Node Menu | FSM Unity Framework",
    "keywords": "Create Node Menu Description Use the Create Node Menu to create nodes in Shader Graph. To open the Create Node Menu, either right-click on the workspace in the Shader Graph Window and select Create Node, or press the spacebar. At the top of the Create Node Menu is a search bar. To search for a node, type any part of its name in the search field. The search box gives you autocomplete options, and you can press Tab to accept the predictive text. It highlights matching text in yellow. The Create Node Menu lists all nodes that are available in Shader Graph, categorized by their function. User-created Sub Graphs are also available in the Create Node Menu under Sub Graph Assets, or in a custom category that you define in the Sub Graph Asset. To add a node to the workspace, double-click it in the Create Node Menu. Contextual Create Node Menu A contextual Create Node Menu filters the available nodes, and only shows those that use the Data Type of a selected edge. It lists every available Port on nodes that match that Data Type. To open a contextual Create Node Menu, click and drag an Edge from a Port, and then release it in an empty area of the workspace. Master Stack Create Node Menu To add a new Block Node to the Master Stack, either right click and select Create Node or press spacebar with the stack selected. The Create Node Menu will display all available blocks for the master stack based on the render pipelines in your project. Any block can be added to the master stack via the Create Node Menu. If the block added is not compatible with the current Graph settings, the block will be disabled until the settings are configured to support it."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Create-Shader-Graph.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Create-Shader-Graph.html",
    "title": "Creating a new Shader Graph Asset | FSM Unity Framework",
    "keywords": "Creating a new Shader Graph Asset After you configure an SRP, you can create a new Shader Graph Asset. Right-click the Project window, locate Create > Shader Graph in the context menu, then select your desired type of Shader Graph. The type of Shader Graph available is dependent on the render pipelines present in your project. Some options may or may not be present based on the render pipelines. The following options are always available: Blank Shader Graph A completely blank shader graph. No target is selected and no blocks are added to the Master Stack. Sub Graph A blank sub graph asset. A sub menu for each installed render pipeline may be present containing template stacks for standard shading models ( Lit, Unlit, etc ). For a full list of provided options, refer to the Universal Render Pipeline and High Definition Render Pipeline documentation. For this example, Universal is installed so a Unversal Lit Shader Graph has been created. Double-click your newly created Shader Graph Asset to open it in the Shader Graph window. Shader Graph window The Shader Graph window consists of the Master Stack, the Preview Window, the Blackboard, and the Graph Inspector. Master Stack The final connection that determines your shader output. Refer to Master Stack for more information. Preview window An area to preview the current shader output. Here, you can rotate the object, and zoom in and out. You can also change the basic mesh on which the shader is previewed. Refer to Main Preview for more information. Blackboard An area that contains all of the shader's properties in a single, collected view. Use the Blackboard to add, remove, rename, and reorder properties. Refer to Blackboard for more information. After you've set up a project, and become familiar with the Shader Graph window, refer to My first Shader Graph for more information on how to get started. Internal Inspector An area that contains information contextual to whatever the user is currently clicking on. It's a window that automatically is hidden by default and only appears when something is selected that can be edited by the user. Use the Internal Inspector to display and modify properties, node options, and the graph settings. Refer to Internal Inspector for more information."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cross-Product-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cross-Product-Node.html",
    "title": "Cross Product Node | FSM Unity Framework",
    "keywords": "Cross Product Node Description Returns the cross product of the values of the inputs A and B. The cross product of two vectors results in a third vector which is perpendicular to the two input vectors. The result's magnitude is equal to the magnitudes of the two inputs multiplied together and then multiplied by the sine of the angle between the inputs. You can determine the direction of the result vector using the \"left hand rule\". Ports Name Direction Type Description A Input Vector 3 First input value B Input Vector 3 Second input value Out Output Vector 3 Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_CrossProduct_float(float3 A, float3 B, out float3 Out) { Out = cross(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cubemap-Asset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Cubemap-Asset-Node.html",
    "title": "Cubemap Asset Node | FSM Unity Framework",
    "keywords": "Cubemap Asset Node Description Defines a constant Cubemap Asset for use in the shader. To sample the Cubemap Asset it should be used in conjunction with a Sample Cubemap Node. When using a separate Cubemap Asset Node you can sample a Cubemap twice, with different parameters, without defining the Cubemap itself twice. Ports Name Direction Type Binding Description Out Output Cubemap None Output value Controls Name Type Options Description Object Field (Cubemap) Defines the cubemap asset from the project."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Function-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Function-Node.html",
    "title": "Custom Function Node | FSM Unity Framework",
    "keywords": "Custom Function Node Description The Custom Function Node enables you to inject your own custom HLSL code in Shader Graphs. This provides you with an extra level of control when you need it (for example, to do some fine-grained optimization). You can either write small functions directly into graphs by using the string mode, or reference external HLSL include files. Use the Custom Port Menu to define your own input and output ports on the node itself. How to Use Use the Create Node Menu to create Custom Function nodes. By default, new Custom Function nodes don't have any input or output ports. In the Graph Inspector, open the Node Settings to access the Custom Function and Custom Port Menu menus. Custom Function menu Menu Item Description Inputs A Custom Port Menu that defines the node's input ports. Outputs A Custom Port Menu that defines the node's input ports. Type A function type selector. Choose File to reference an external file or string to directly input functions to the node. Name Part of the name this custom function has in the final generated code. Suffixed by the function type _half or _float. Source An asset field to reference the external HLSL include file. Only available in File mode. Body A text box where you enter HLSL code. Only available in String mode. Defining the Function via string If you select String mode, the graph generates the shader function. The Name field defines the name of the generated function, and the Body field defines the contents of the generated function. Unity handles the arguments, braces, and indent scope automatically. In String mode you may use the token $precision instead of half or float in the Body field. Unity replaces this with the correct type, based on that node's precision, when the node is processed. The example in the image above generates the following function: void MyFunction_float(float3 A, float B, out float3 Out) { Out = A + B + 1/2; } Defining the Function via file If you select File mode, the graph does not automatically generate the shader function. This mode injects an include reference in the final generated shader, and uses a function from within the referenced file. The Name field must match the name of the function you wish to call. The Source field contains a reference to the HLSL file that includes the function. When you use File mode for the Custom Function node, you must manually format the functions properly. One thing to note when creating custom functions for Shader Graph is the precision suffixes. The generated code appends a precision suffix to function names. Your include file function must also append your desired precision suffix (shown below with _float), or contain multiple functions with both _float and _half suffixes, but your Name field must not include the precision suffix. //UNITY_SHADER_NO_UPGRADE #ifndef MYHLSLINCLUDE_INCLUDED #define MYHLSLINCLUDE_INCLUDED void MyFunction_float(float3 A, float B, out float3 Out) { Out = A + B; } #endif //MYHLSLINCLUDE_INCLUDED File mode allows for more flexbility with custom functions in a graph. You can define uniform variables outside of the function scope, as shown here with a matrix. //UNITY_SHADER_NO_UPGRADE #ifndef MYHLSLINCLUDE_INCLUDED #define MYHLSLINCLUDE_INCLUDED float4x4 _MyMatrix; void MyFunction_float(float3 A, float B, out float3 Out) { A = mul(float4(A, 0.0), _MyMatrix).rgb; Out = A + B; } #endif //MYHLSLINCLUDE_INCLUDED You can define multiple functions in the same file, and call them from your referenced function. Alternatively, you can reference the same file, but use different functions from different Custom Function nodes. //UNITY_SHADER_NO_UPGRADE #ifndef MYHLSLINCLUDE_INCLUDED #define MYHLSLINCLUDE_INCLUDED float3 MyOtherFunction_float(float3 In) { return In * In; } void MyFunction_float(float3 A, float B, out float3 Out) { A = MyOtherFunction_float(A); Out = A + B; } #endif //MYHLSLINCLUDE_INCLUDED You can even include other files that contain other functions. //UNITY_SHADER_NO_UPGRADE #ifndef MYHLSLINCLUDE_INCLUDED #define MYHLSLINCLUDE_INCLUDED #include \"Assets/MyOtherInclude.hlsl\" void MyFunction_float(float3 A, float B, out float3 Out) { A = MyOtherFunction_float(A); Out = A + B; } #endif //MYHLSLINCLUDE_INCLUDED Reusing Custom Function Nodes The Custom Function node, on its own, is a single node instance. If you wish to re-use the same custom functions without re-creating the inputs, outputs, and function referencing, use Sub Graphs. Sub Graphs appear in the Create Node Menu, and they enable you to share or re-use your custom functions. Create your custom function either directly in a Sub Graph, or right-click the existing Custom Function node and select Convert to Sub Graph. To add the appropriate input and output ports, use the Graph Inspector and Custom Port Menu. After this, you can reuse your custom function as many times as needed, even within other Sub Graphs. Working with texture wires From version 10.3, Shader Graph has five new data structures to ensure that Custom Function Nodes (CFNs) and SubGraphs input and output data from texture wires in a consistent way. The new structures also make it possible for SamplerState to compile on GLES2 platforms and access data associated with textures via myInputTex.samplerstate and myInputTex.texelSize. Four structures are for the texture types, and one is for the sampler state: UnityTexture2D UnityTexture2DArray UnityTexture3D UnityTextureCube UnitySamplerState CFNs you create with earlier versions of Shader Graph continue to work after this change. As part of the automatic update, Unity transitions them to the new Bare node type. This type replicates the old input and output behavior. All other types pass the new structs. However, you should manually upgrade CFNs that produce texture or samplerstate types as output to ensure that they behave consistently—and to gain the benefits of the new design. Unity flags this type of outdated Custom Function Nodes with a warning when you open your Shader Graph in 10.3 or later. How to upgrade Change all of the input and output types from Bare to non-Bare. String type: Ensure that your HLSL string already uses Unity's texture access macros (such as SAMPLE_TEXTURE2D). File type: Replace Bare types (such as Texture2D) with the new struct types (such as UnityTexture2D) in your function parameters. If your HLSL code is using platform-specific or non-standard texture operations, you'll need to convert the way you access textures to take that structure into account. For example, myInputTex.GetDimensions(...) would become myInputTex.tex.GetDimensions(...) From version 10.3, you can access data associated with textures via myInputTex.samplerstate and myInputTex.texelSize."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Interpolators.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Interpolators.html",
    "title": "Custom Interpolators | FSM Unity Framework",
    "keywords": "Custom Interpolators Description The Custom Interpolator feature provides fine-grained control over the specific calculations Shader Graph uses to bring data from the vertex stage to the pixel stage. There are two target audiences for Custom Interpolators: Technical Directors and Lead Technical Artists setting up environments for their teams. Graphics programmers helping artists to optimize content performance. Supported data types Custom interpolators support float, vec2, vec3, and vec4 options. Channel limits The Custom Interpolator feature supports a maximum of 32 channels. A channel is equivalent to four floats. Each float is an interpolator variable. Different platforms and GPUs have different interpolator variable limits. Exceeding the interpolator limitations of your target platform prevents your shaders from compiling. For detailed information about the number of interpolators supported by common interfaces, see the Unity documentation on Shader semantics, and view the section Interpolator count limits. Test your Custom Interpolators on your target configuration to ensure that your content compiles properly. Technical directors can set warnings and errors to help their team members avoid creating graphs with too many channels to be compatible with their target pipeline, platform, or GPU. See Creating channel warnings and errors below. How to use To use this feature, create a Custom Interpolator block in the Vertex context of the Master Stack and set a name and a data type. Create a vertex node to write data to that interpolator. Use the interpolator in your graph, then connect your graph to the relevant block in the Fragment context. These instructions include a contextual example illustrating the process of using a Custom Interpolator to fetch per-vertex data from a texture. To read the HLSL you use to replicate this behavior with the Built In Render Pipeline, see the Unity documentation on Shader semantics and view the section Vertex ID: SV_VertexID. Creating channel warnings and errors It is not possible to limit the number of channels a user can create in a Shader Graph. However, it is possible to create alerts to let users know when they are close to or exceeding a certain number of channels. The Warning Threshold lets users know that they are approaching the channel limit, and the Error Threshold informs them if they have reached or surpassed that limit. The Warning Threshold value must be between 8 and 32 channels. The Error Threshold value must be higher than the Warning Threshold, and has a minimum value of 8 channels. To configure these parameters, go to the Unity Editor Project Settings menu and open the Custom Interpolator Channel Settings. Adding a Custom Interpolator block to the Master Stack Right-click in the Vertex contex to create a block node. Select Custom Interpolator. Select a data type. Enter a name for this interpolator. In the illustrated example, you use the Vector 4 (vec4) data type. Writing data to the interpolator Right-click in your graph to create a node. Select the type Vertex ID. Connect this node to the Custom Interpolator block. In the example, you write Vertex ID values from your graph into the Custom Interpolator. Reading data from the interpolator Right-click in your graph to create a node. Select Custom Interpolator. Connect the Custom Interpolator node to the relevant block in the Fragment context. In this example, you connect to the Base Color block in order to pass the Vertex ID from the vertex shader to the fragment shader and use it as color output. Deleting the block from the Master Stack If you delete a Custom Interpolator which is associated with nodes that are still in your graph, Unity displays an alert. If you want to keep using these nodes, you can create a new Custom Interpolator and associate them with it. This prevents the alert from appearing."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Port-Menu.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Custom-Port-Menu.html",
    "title": "Custom Port Menu | FSM Unity Framework",
    "keywords": "Custom Port Menu Description The Custom Port Menu is displayed in the Node Settings tab of the Graph Inspector by clicking on the Custom Function Node and Sub Graph output node. This menu allows you to add, remove, rename, reorder, and define the types of your custom input and output ports. How to Use Select the Custom Function Node or the Sub Graph output node to view the Custom Port Menu in the Inspector. To close the menu, click anywhere in the graph or on another graph-element. Adding and Removing Ports To add ports, click the + icon at the bottom right corner of the port list. To remove ports, select a port using the hamburger icon on the left, and click the - icon at the bottom right corner of the port list. Renaming Ports To rename a port, double-click its text field and enter the new name. Currently, only the following characters are valid for port names: A-Z, a-z, 0-9, _, ( ), and whitespace. If the name contains an invalid character, an error badge appears. Reordering Ports To reorder ports, click and hold the hamburger icon on the left, and drag the port to your desired place in the list. Changing Port Types To change a port type, use the Type drop-down menu on the right. See the Data Types page for a list of currently valid port types."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDX-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDX-Node.html",
    "title": "DDX Node | FSM Unity Framework",
    "keywords": "DDX Node Description Returns the partial derivative of the input In with respect to the screen-space x-coordinate. This node can only be used in the pixel shader stage. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output partial derivative value Generated Code Example The following example code represents one possible outcome of this node. void Unity_DDX_float4(float4 In, out float4 Out) { Out = ddx(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDXY-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDXY-Node.html",
    "title": "DDXY Node | FSM Unity Framework",
    "keywords": "DDXY Node Description Returns the sum of both partial derivatives of input In, with respect to the screen-space x-coordinate and screen-space y-coordinate respectively. This node can only be used in the pixel shader stage. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output partial derivative value Generated Code Example The following example code represents one possible outcome of this node. void Unity_DDXY_float4(float4 In, out float4 Out) { Out = ddxy(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDY-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/DDY-Node.html",
    "title": "DDY Node | FSM Unity Framework",
    "keywords": "DDY Node Description Returns the partial derivative of the input In with respect to the screen-space y-coordinate. This node can only be used in the pixel shader stage. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output partial derivative value Generated Code Example The following example code represents one possible outcome of this node. void Unity_DDY_float4(float4 In, out float4 Out) { Out = ddy(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Data-Types.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Data-Types.html",
    "title": "Data Types | FSM Unity Framework",
    "keywords": "Data Types Description There are a number of Data Types in Shader Graph. Each Port on a Node has an associated Data Type that defines what edges can be connected to it. The Data Types have colors for usability, these colors are applied to ports and edges of that Data Type. Some Data Types have associated Property Types for exposing these values to the Inspector for Materials that use the shader. Data Types Name Color Description Float Light Blue A Float or scalar value Vector 2 Green A Vector 2 value Vector 3 Yellow A Vector 3 value Vector 4 Pink A Vector 4 value Dynamic Vector Light Blue See Dynamic Data Types below Matrix 2 Blue A Matrix 2x2 value Matrix 3 Blue A Matrix 3x3 value Matrix 4 Blue A Matrix 4x4 value Dynamic Matrix Blue See Dynamic Data Types below Dynamic Blue See Dynamic Data Types below Boolean Purple A Boolean value. Defined as a float in the generated shader Texture 2D Red A Texture 2D asset Texture 2D Array Red A Texture 2D Array asset Texture 3D Red A Texture 3D asset Cubemap Red A Cubemap asset Virtual Texture Gray A Texture Stack Gradient Gray A Gradient value. Defined as a struct in the generated shader SamplerState Gray A state used for sampling a texture Promoting/Truncating All Vector types can be promoted or truncated to match any Vector type Port. This behaviour occurs only when the Port in question is not of type Dynamic Vector. When truncating, excess channels are simply removed. When promoting, the extra required channels are filled by default values. These values are (0, 0, 0, 1). Dynamic Data Types Some Data Types are dynamic. This means a port using these Data Types can change their underlying Concrete Data Type based on what Data Type is connected to it. By default, Nodes using dynamic Data Types can only have one Concrete Data Type, meaning that once a connected edge has applied its Data Type to that port, all other Dynamic Data Type slots of that Node will apply the same Data Type. One notable exception to this is the Multiply Node which allows both Dynamic Matrix and Vector types. Dynamic Vector The Dynamic Vector type allows connected edges of any Vector type. All connected edges are automatically truncated to the type with the lowest dimension, unless the lowest dimension is 1, in which case the Float is promoted. Dynamic Matrix The Dynamic Matrix type allows connected edges of any Matrix type. All connected edges are automatically truncated to the type with the lowest dimension. Dynamic The Dynamic type is a special case. Nodes that support it must define how it is validated. In the case of the Multiply Node, it allows connections of any Vector or Matrix type, ensuring the correct multiplication is applied depending on the mix of Data Types."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Degrees-To-Radians-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Degrees-To-Radians-Node.html",
    "title": "Degrees To Radians Node | FSM Unity Framework",
    "keywords": "Degrees To Radians Node Description Returns the value of input In converted from degrees to radians. One degree is equivalent to approximately 0.0174533 radians and a full rotation of 360 degrees is equal to 2 Pi radians. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_DegreesToRadians_float4(float4 In, out float4 Out) { Out = radians(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dielectric-Specular-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dielectric-Specular-Node.html",
    "title": "Dielectric Specular Node | FSM Unity Framework",
    "keywords": "Dielectric Specular Node Description Returns a Dielectric Specular F0 value for a physically based material. The material to use can be selected with the Material dropdown parameter on the Node. A Common Material type defines a range between 0.034 and 0.048 sRGB values. The value between this range can be selected with the Range parameter. This Material type should be used for various materials such as plastics and fabrics. You can use Custom material type to define your own physically based material value. The output value in this case is defined by its index of refraction. This can be set by the parameter IOR. Ports Name Direction Type Binding Description Out Output Float None Output value Controls Name Type Options Description Material Dropdown Common, RustedMetal, Water, Ice, Glass, Custom Selects the material value to output. Range Slider Controls output value for Common material type. IOR Slider Controls index of refraction for Custom material type. Generated Code Example The following example code represents one possible outcome of this node per Material mode. Common float _DielectricSpecular_Range = 0.5; float _DielectricSpecular_Out = lerp(0.034, 0.048, _DielectricSpecular_Range); RustedMetal float _DielectricSpecular_Out = 0.030; Water float _DielectricSpecular_Out = 0.020; Ice float _DielectricSpecular_Out = 0.018; Glass float _DielectricSpecular_Out = 0.040; Custom float _DielectricSpecular_IOR = 1; float _DielectricSpecular_Out = pow(_Node_IOR - 1, 2) / pow(_DielectricSpecular_IOR + 1, 2);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Diffusion-Profile-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Diffusion-Profile-Node.html",
    "title": "Diffusion Profile Node | FSM Unity Framework",
    "keywords": "Diffusion Profile Node The Diffusion Profile Node allows you to sample a Diffusion Profile Asset in your Shader Graph. For information on what a Diffusion Profile is and the properties that it contains, see the Diffusion Profile documentation. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Diffusion Profile Node No Yes Ports name Direction type description Out Output float Outputs a unique float that the Shader uses to identify the Diffusion Profile. Notes The output of this Node is a float value that represents a Diffusion Profile. The Shader can use this value to find settings for the Diffusion Profile Asset that this value represents. If you modify the output value, the Shader can no longer use it to find the settings for the Diffusion Profile Asset. You can use this behavior to enable and disable Diffusion Profiles in your Shader Graph. To disable a Diffusion Profile, multiply the output by 0. To enable a Diffusion Profile, multiply the output by 1. This allows you to use multiple Diffusion Profiles in different parts of your Shader Graph. Be aware that the High Definition Render Pipeline (HDRP) does not support blending between Diffusion Profiles. This is because HDRP can only evaluate a single Diffusion Profile per pixel."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Distance-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Distance-Node.html",
    "title": "Distance Node | FSM Unity Framework",
    "keywords": "Distance Node Description Returns the euclidean distance between the values of the inputs A and B. This is useful for, among other things, calculating the distance between two points in space and is commonly used in calculating a Signed Distance Function. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Distance_float4(float4 A, float4 B, out float Out) { Out = distance(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dither-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dither-Node.html",
    "title": "Dither Node | FSM Unity Framework",
    "keywords": "Dither Node Description Dither is an intentional form of noise used to randomize quantization error. It is used to prevent large-scale patterns such as color banding in images. The Dither node applies dithering in screen-space to ensure a uniform distribution of the pattern. This can be adjusted by connecting another node to input Screen Position. This Node is commonly used as an input to Alpha Clip Threshold on the Master Node to give the appearance of transparency to an opaque item. This is useful for creating geometry that appears to be transparent but has the advantages of rendering as opaque, such as writing depth or being rendered in deferred. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Screen Position Input Vector 4 Screen Position Coordinates used to apply dither pattern Out Output Dynamic Vector None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Dither_float4(float4 In, float4 ScreenPosition, out float4 Out) { float2 uv = ScreenPosition.xy * _ScreenParams.xy; float DITHER_THRESHOLDS[16] = { 1.0 / 17.0, 9.0 / 17.0, 3.0 / 17.0, 11.0 / 17.0, 13.0 / 17.0, 5.0 / 17.0, 15.0 / 17.0, 7.0 / 17.0, 4.0 / 17.0, 12.0 / 17.0, 2.0 / 17.0, 10.0 / 17.0, 16.0 / 17.0, 8.0 / 17.0, 14.0 / 17.0, 6.0 / 17.0 }; uint index = (uint(uv.x) % 4) * 4 + uint(uv.y) % 4; Out = In - DITHER_THRESHOLDS[index]; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Divide-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Divide-Node.html",
    "title": "Divide Node | FSM Unity Framework",
    "keywords": "Divide Node Description Returns the result of input A (dividend) divided by input B (divisor). Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Divide_float4(float4 A, float4 B, out float4 Out) { Out = A / B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dot-Product-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Dot-Product-Node.html",
    "title": "Dot Product Node | FSM Unity Framework",
    "keywords": "Dot Product Node Description Returns the dot product, or scalar product, of the two input vectors A and B. The dot product is a value equal to the magnitudes of the two vectors multiplied together and then multiplied by the cosine of the angle between them. For normalized input vectors, the Dot Product node returns 1 if they point in exactly the same direction, -1 if they point in completely opposite directions and 0 if the vectors are perpendicular. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_DotProduct_float4(float4 A, float4 B, out float Out) { Out = dot(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Edge.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Edge.html",
    "title": "Edge | FSM Unity Framework",
    "keywords": "Edge Description An Edge defines a connection between two Ports. Edges define how data flows through the Shader Graph node network. They can only be connected from an input Port to an output Port. Each Edge has a Data Type which defines what Ports it can be connected to. Each Data Type has an associated color for identifying its type. You can create a new Edge by clicking and dragging from a Port with the left mouse button. Edges can be deleted with Delete (Windows), Command + Backspace (OSX) or from the context menu by right clicking on the Node. You can open a contextual Create Node Menu by dragging an Edge from a Port with the left mouse button and releasing it in an empty area of the workspace."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ellipse-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Ellipse-Node.html",
    "title": "Ellipse Node | FSM Unity Framework",
    "keywords": "Ellipse Node Description Generates an ellipse shape based on input UV at the size specified by inputs Width and Height. The generated shape can be offset or tiled by connecting a Tiling And Offset Node. Note that in order to preserve the ability to offset the shape within the UV space the shape will not automatically repeat if tiled. To achieve a repeating dot effect first connect your input through a Fraction Node. NOTE: This Node can only be used in the Fragment Shader Stage. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Width Input Float None Ellipse width Height Input Float None Ellipse height Out Output Float None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Ellipse_float(float2 UV, float Width, float Height, out float4 Out) { float d = length((UV * 2 - 1) / float2(Width, Height)); Out = saturate((1 - d) / fwidth(d)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Emission-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Emission-Node.html",
    "title": "Emission Node | FSM Unity Framework",
    "keywords": "Emission Node The Emission Node allows you to apply emission in your Shader Graph. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Emission No Yes Ports Name Direction Type Description color Input LDR Color(RGB) Sets the low dynamic range (LDR) color of the emission. intensity Input Float Sets the intensity of the emission color. output Output HDR Color(RGB) Outputs the high dynamic range (HDR) color that this Node produces. Notes Emission Unit You can use two physical light units to control the strength of the emission: Nits. EV100. Exposure Weight You can use Exposure Weight to determine how exposure affects emission. It is a value between 0 and 1 where. A value of 0 means that exposure does not effect this part of the emission. A value of 1 means that exposure fully affects this part of the emission."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Foam-Data-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Foam-Data-Water-Node.html",
    "title": "Evaluate Foam Data | FSM Unity Framework",
    "keywords": "Evaluate Foam Data This node calculates water foam intensity. This node outputs foam as monochrome in the red channel. If you connect the output of this node to a Base Color block, all the channels are red. To prevent this, split the output and use only the red channel. You can't apply a tint to foam. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Foam Data No Yes Ports Name Direction Type Description SurfaceGradient Input Vector3 The perturbation of the normal, as a surface gradient. LowFrequencySurfaceGradient Input Vector3 The perturbation of the low frequency normal, as a surface gradient. The low frequency normal is the normal of the water surface without high frequency details such as ripples. SimulationFoam Input Float The amount of foam. HDRP uses this property in the default water shader graph to fetch foam data from the simulation. CustomFoam Input Float The amount of foam, if you create your own foam. SurfaceGradient Output Vector3 The calculated water surface normal, as a surface gradient. Foam Output Float The combination of the amount of foam and a foam texture. Smoothness Output Float The smoothness of the water surface. For more information about this property, see Settings and Properties Related to the Water System."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Refraction-Data-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Refraction-Data-Water-Node.html",
    "title": "Evaluate Refraction Data | FSM Unity Framework",
    "keywords": "Evaluate Refraction Data This node calculates water refraction. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Refraction Data No Yes Ports Name Direction Type Description NormalWS Input Vector3 The water surface normal in world space. LowFrequencyNormalWS Input Vector3 The low frequency normal of the water surface in world space. This is the normal of the water surface without high frequency details such as ripples. RefractedPositionWS Output Vector3 The refracted position of the water bed you observe through the water, in world space. DistortedWaterNDC Output Vector2 The screen space position of the refracted point. AbsorptionTint Output Vector3 An absorption factor that HDRP uses to blend between the water surface and the refracted underwater color."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Scattering-Color-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Scattering-Color-Water-Node.html",
    "title": "Evaluate Scattering Color | FSM Unity Framework",
    "keywords": "Evaluate Scattering Color This node calculates the scattered diffuse color of water. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Scattering Color No Yes Ports Name Direction Type Description AbsorptionTint Input Vector3 An absorption factor that HDRP uses to blend between the water surface and the refracted underwater color. LowFrequencyHeight Input Float The vertical displacement of the water surface. This doesn't include ripples. HorizontalDisplacement Input Float The horizontal displacement of the water surface. SSSMask Input Float Mask that defines where the water surface has subsurface scattering. DeepFoam Input Float The amount of foam under the water's surface. ScatteringColor Output Vector3 The diffuse color of the water."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Additional-Data-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Additional-Data-Water-Node.html",
    "title": "Evaluate Simulation Additional Data | FSM Unity Framework",
    "keywords": "Evaluate Simulation Additional Data This node provides access to Water's surface foam, surface gradient, and deep foam. You can also use this node to dampen the normals for each water band. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph to fetch data from the water simulation. Don't modify the settings of this node. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Simulation Additional Data No Yes Ports Name Direction Type Description BandsMultiplier Input Vector4 The amount to dampen displacement for each water band. Bands are different wave frequencies that create swells, agitations or ripples on the water. SurfaceGradient Output Vector3 The perturbation of the normal, as a surface gradient. LowFrequencySurfaceGradient Output Vector3 The perturbation of the low frequency normal, as a surface gradient. The low frequency normal is the normal of the water surface without high frequency details such as ripples. SurfaceFoam Output Float The amount of foam. DeepFoam Output Float The amount of foam under the water's surface."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Caustics-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Caustics-Water-Node.html",
    "title": "Evaluate Simulation Caustics | FSM Unity Framework",
    "keywords": "Evaluate Simulation Caustics This node calculates water caustics. This node outputs caustics as monochrome in the red channel. If you connect the output of this node to a Base Color block, all the channels are red. To prevent this, split the output and use only the red channel. You can't apply a tint to caustics. Caustics don't have an effect above the water unless you script this behavior. For example: If your scene contains a boat that sits in water, HDRP doesn't project caustics on the part of the boat's hull that's above water. A swimming pool inside a room doesn't bounce caustics off the walls or ceiling. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Simulation Caustics No Yes Ports Name Direction Type Description RefractedPositionWS Input Vector3 The refracted position of the water bed you observe through the water, in world space. DistortedWaterNDC Input Vector2 The screen space position of the refracted point. Caustics Output Float The intensity of the caustics."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Displacement-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Simulation-Displacement-Water-Node.html",
    "title": "Evaluate Water Simulation Displacement | FSM Unity Framework",
    "keywords": "Evaluate Water Simulation Displacement This node calculates water surface displacement. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Water Simulation Displacement No Yes Ports Name Direction Type Description PositionWS Input Vector3 The position of the water surface vertex in world space. BandsMultiplier Input Vector4 The amount to dampen displacement for each water band. Bands are different wave frequencies that create swells, agitations or ripples on the water. Displacement Output Vector3 The vertical and horizontal displacement of the water. LowFrequencyHeight Output Float The vertical displacement of the water surface. This doesn't include ripples. SSSMask Output Float Mask that defines where the water surface has subsurface scattering."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Tip-Thickness-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Evaluate-Tip-Thickness-Water-Node.html",
    "title": "Evaluate Tip Thickness | FSM Unity Framework",
    "keywords": "Evaluate Tip Thickness This node calculates the thickness of the water at the tips of waves. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Evaluate Tip Thickness No Yes Ports Name Direction Type Description LowFrequencyNormal Input Vector3 The low frequency normal of the water surface in world space. This is the normal of the water surface without high frequency details such as ripples. LowFrequencyHeight Input Float The vertical displacement of the water surface. This doesn't include ripples. TipThickness Output Float The thickness of the water in a wave tip."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Exponential-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Exponential-Node.html",
    "title": "Exponential Node | FSM Unity Framework",
    "keywords": "Exponential Node Description Returns the exponential value of input In. The exponential base can be switched between base-e and base 2 from the Base dropdown on the node. Base E : Returns e to the power of input In Base 2 : Returns 2 to the power of input In Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Controls Name Type Options Description Base Dropdown BaseE, Base2 Selects the exponential base Generated Code Example The following example code represents one possible outcome of this node per Base mode. Base E void Unity_Exponential_float4(float4 In, out float4 Out) { Out = exp(In); } Base 2 void Unity_Exponential2_float4(float4 In, out float4 Out) { Out = exp2(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Exposure-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Exposure-Node.html",
    "title": "Exposure Node | FSM Unity Framework",
    "keywords": "Exposure Node The Exposure Node allows you to get the Camera's exposure value from the current or previous frame. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Exposure No Yes Ports name Direction type description Output Output float The exposure value. Exposure Type You can use Exposure Type to select which exposure value to get. | name | description | |--- | ---| | CurrentMultiplier | Gets the Camera's exposure value from the current frame. | | InverseCurrentMultiplier | Gets the inverse of the Camera's exposure value from the current frame. | | PreviousMultiplier | Gets the Camera's exposure value from the previous frame. | | InversePreviousMultiplier | Gets the inverse of the Camera's exposure value from the previous frame. |"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Eye-Index-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Eye-Index-Node.html",
    "title": "Eye Index Node | FSM Unity Framework",
    "keywords": "Eye Index Node Description Provides access to the Eye Index when stereo rendering is enabled. Ports Name Direction Type Binding Description Out Output Float None Eye Index for the camera of a stereo draw."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Eye-Surface-Type-Debug-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Eye-Surface-Type-Debug-Node.html",
    "title": "Eye Surface Type Debug Node | FSM Unity Framework",
    "keywords": "Eye Surface Type Debug Node Debug node that allows you to visually validate the current pupil radius. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Eye Surface Type Debug Node No Yes Ports name Direction type description PositionOS Input Vector3 Position in object space of the current fragment to shade. EyeColor Input Color Final Diffuse color of the Eye. IrisRadius Input float The radius of the Iris in the used model. For the default model, this value should be 0.225. Pupil Radius Input float Radius of the pupil in the iris texture as a percentage. IsActive Input bool Flag that defines if the node should be active. SurfaceColor Output Color Final Diffuse color of the Eye."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fade-Transition-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fade-Transition-Node.html",
    "title": "Fade Transition Node | FSM Unity Framework",
    "keywords": "Fade Transition Node Description Fade Transition is a method of adding noise to add variation while a function transitions from on to off. This node takes in a fade value and remaps it using the noise value (usually from a texture). When FadeValue is 0, the output is always 0, and when FadeValue is 1, the output is always exactly 1. In between 0 and 1 the transition will follow the pattern in the noise. This Node is commonly used as an input to Alpha on a Master Node to provide an LOD transition. Ports Name Direction Type Binding Description Texture Input Texture 2D None Input value Noise Input Float None The noise variation to apply to the fade function FadeValue Input Float None The amount of transition to apply FadeContrast Input Float None The contrast at which a single pixel goes from fully transparent to fully opaque. Higher values cause sharper edges in the transition Fade Output Float None The resulting fade value Generated Code Example The following example code represents one possible outcome of this node. float Unity_FadeTransitionNode_ApplyFade_float(float noise, float fadeValue, float fadeContrast) { float ret = saturate(fadeValue*(fadeContrast+1)+(noise-1)*fadeContrast); return ret; } float Result = Unity_FadeTransitionNode_ApplyFade_float( _NoiseValue, _FadeValue, _FadeContrast);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/First-Shader-Graph.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/First-Shader-Graph.html",
    "title": "My first Shader Graph | FSM Unity Framework",
    "keywords": "My first Shader Graph Before you begin, make sure that your project is set up properly, and the graphs are loading correctly. See Getting started with Shader Graph for more information. Create a New Graph Use the Project Browser to create a new Shader Graph Asset in your project. The Create > Shader Graph will display the various creation options. A Blank Shader Graph will create a Shader Graph with no selected active targets or block nodes. You will need to select a target via the Graph Settings Menu to continue. Certain integrations, like Render Pipelines, can also provide pre-configured options for Shader Graphs. For this example, a Universal > Lit Shader Graph has been created and opened. Create a new node Use the Create Node menu to create new nodes. There are two ways to open the menu: Right click, and select Create Node from the context menu. Press the spacebar. In the menu, you can type in the search bar to look for specific nodes, or browse all nodes in the library. In this example, we'll create a Color node. First, type \"color\" in the Create Node menu's search bar. Then, click Color, or highlight Color and press Enter to create a Color node. Connect nodes To build a graph, you need to connect nodes together. To do so, click the Output Slot of a node, and drag that connection into the Input Slot of another node. Start by connecting the Color node to the Base Color block of our Fragment Stack. Change node output Notice that the connection updated the main preview, and the 3D Object in the Main Preview is now black, which is the color specified in the Color node. You can click on the color bar in that node, and use the color picker to change the color. Any changes you make on the node updates the object in the Main Preview in real time. For example, if you pick red, the 3D Object immediately reflects this change. Save the graph Currently, Shader Graphs do not automatically save. There are two ways to save your changes: Click the Save Asset button in the top left corner of the window. Close the graph. If Unity detects any unsaved changes, a pop-up window appears, and asks if you want to save those changes. Create a Material After saving your graph, use the shader to create a new Material. The process of creating a new Material and assigning it a Shader Graph shader is the same as that for regular shaders. In either the main menu or the Project View context menu, select Assets > Create > Material. Select the Material you just created. In its Inspector window, select the Shader drop-down menu, click Shader Graphs, and choose the Shader Graph shader you wish to apply to the Material. You can also right-click the Shader Graph shader, and select Create > Material. This method automatically assigns that Shader Graph shader to the newly created Material. A Material is also automatically generated as a subasset of the Shader Graph. You can assign it directly to an object in your scene. Modifying a property from the Blackboard on the Shader Graph will update this material in real time, which allows for quick visualization in the scene. Put the Material in the Scene Now that you have assigned your shader to a Material, you can apply it to objects in the Scene. Drag and drop the Material onto an object in the Scene. Alternatively, in the object's Inspector window, locate Mesh Renderer > Materials, and apply the Material to the Element. Use properties to edit the graph You can also use properties to alter your shader's appearance. Properties are options that are visible from the Material's Inspector, which lets others change settings in your shader without the need to open the Shader Graph. To create a new property, use the Add (+) button on the top right corner of the Blackboard, and select the type of property to create. In this example, we'll select Color. This adds a new property in the Blackboard with the following options in the Node Settings tab of the Graph Inspector when the property is selected. Option Description Property button To change the name of the property, right-click the button in the Blackboard, select Rename, then enter a new property name. To delete the property, right-click the button, and select Delete. Exposed Enable this checkbox to make the property visible from the Material's Inspector. Reference The property's name that appears in C# scripts. To change the Reference name, enter a new string. Default The default value of the property. Mode The mode of the property. Each property has different modes. For Color, you can select either Default or HDR. Precision The default precision of the property. Hybrid Instanced An experimental feature that enables this property to be instanced when using the Hybrid DOTS renderer. There are two ways to reference a property in your graph: Drag the property from the Blackboard onto the graph. Right-click and select Create Node. The property is listed in the Properties category. Try connecting the property to the Base Color block. The object immediately changes to black. Save your graph, and return to the Material's Inspector. The property now appears in the Inspector. Any changes you make to the property in the Inspector affects all objects that use this Material. More Tutorials Older tutorials use an outdated format of Shader Graph with master nodes. When looking at older tutorials, reference the Upgrade Guide for tips on how to convert the master node to a Master Stack. To keep exploring how to use Shader Graph to author shaders, check out these blog posts: Art That Moves: Creating Animated Materials with Shader Graph Shader Graph Updates and Sample Project Custom Lighting in Shader Graph: Expanding Your Graphs in 2019 Unity 2018.3 Shader Graph Update: Lit Master Node Creating an Interactive Vertex Effect using Shader Graph Introduction to Shader Graph: Build your shaders with a visual editor You can also visit the Unity YouTube Channel and look for video tutorials on Shader Graph, or head to our user forum to find the latest information and conversations about Shader Graph."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Flip-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Flip-Node.html",
    "title": "Flip Node | FSM Unity Framework",
    "keywords": "Flip Node Description Flips the individual channels of input In selected by the Node's parameters. Positive values become negative values and vice versa. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Dynamic Vector None Output value Controls Name Type Options Description Red Toggle True, False If true red channel will be flipped. Green Toggle True, False If true green channel will be flipped. Disabled if In is Float. Blue Toggle True, False If true blue channel will be flipped. Disabled if In is Vector 2 or smaller. Alpha Toggle True, False If true alpha channel will be flipped. Disabled if In is Vector 3 or smaller. Generated Code Example The following example code represents one possible outcome of this node. float2 _Flip_Flip = float4(Red, Green, Blue, Alpha); void Unity_Flip_float4(float4 In, float4 Flip, out float4 Out) { Out = (Flip * -2 + 1) * In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Flipbook-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Flipbook-Node.html",
    "title": "Flipbook Node | FSM Unity Framework",
    "keywords": "Flipbook Node Description Creates a flipbook, or texture sheet animation, of the UVs supplied to input UV. The amount of tiles on the sheet are defined by the values of the inputs Width and Height. The index of the current tile is defined by the value of the input Tile. This node can be used to create a texture animation functionality, commonly used for particle effects and sprites, by supplying Time to the input Tile and outputting to the UV input slot of a Texture Sampler. UV data is typically in the range of 0 to 1 starting from the bottom left of UV space. This can be seen by the black value at the bottom left corner of a UV preview. As flipbooks typically start from top left the parameter Invert Y is enabled by default, however you can change the direction of the Flipbook by switching the Invert X and Invert Y parameters. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Width Input Float None Amount of horizontal tiles Height Input Float None Amount of vertical tiles Tile Input Float None Current tile index Out Output Vector 2 None Output UV value Controls Name Type Options Description Invert X Toggle True, False If enabled tiles are iterated from right to left Invert Y Toggle True, False If enabled tiles are iterated from top to bottom Generated Code Example The following example code represents one possible outcome of this node. float2 _Flipbook_Invert = float2(FlipX, FlipY); void Unity_Flipbook_float(float2 UV, float Width, float Height, float Tile, float2 Invert, out float2 Out) { Tile = fmod(Tile, Width * Height); float2 tileCount = float2(1.0, 1.0) / float2(Width, Height); float tileY = abs(Invert.y * Height - (floor(Tile * tileCount.x) + Invert.y * 1)); float tileX = abs(Invert.x * Width - ((Tile - Width * floor(Tile * tileCount.x)) + Invert.x * 1)); Out = (UV + float2(tileX, tileY)) * tileCount; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Float.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Float.html",
    "title": "Float Node | FSM Unity Framework",
    "keywords": "Float Node Description Defines a Float value in the shader. If Port X is not connected with an Edge this Node defines a constant Float. Ports Name Direction Type Binding Description X Input Float None Input x component value Out Output Float None Output value Generated Code Example The following example code represents one possible outcome of this node. float _Vector1_Out = X;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Floor-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Floor-Node.html",
    "title": "Floor Node | FSM Unity Framework",
    "keywords": "Floor Node Description Returns the largest integer value, or whole number, that is less than or equal to the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Floor_float4(float4 In, out float4 Out) { Out = floor(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fog-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fog-Node.html",
    "title": "Fog Node | FSM Unity Framework",
    "keywords": "Fog Node Description Provides access to the Scene's Fog parameters. Note: The behavior of this Node is undefined globally. Shader Graph does not define the function of the node. Instead, each Render Pipeline defines what HLSL code to execute for this Node. Different Render Pipelines may produce different results. If you're building a shader in one Render Pipeline that you want to use in both, try checking it in both pipelines before production. A Node might be defined in one Render Pipeline and undefined in the other. If this Node is undefined, it returns 0 (black). Unity Render Pipelines Support Universal Render Pipeline The High Definition Render Pipeline does not support this Node. Ports Name Direction Type Binding Description Position Input Vector 3 Position (object space) Mesh vertex/fragment's position Color Output Vector 4 None Fog color Density Output Float None Fog density based on depth. Returns a value between 0 and 1, where 0 is no fog and 1 is full fog. Generated Code Example The following example code represents one possible outcome of this node. void Unity_Fog_float(float3 Position, out float4 Color, out float Density) { SHADERGRAPH_FOG(Position, Color, Density); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fraction-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fraction-Node.html",
    "title": "Fraction Node | FSM Unity Framework",
    "keywords": "Fraction Node Description Returns the fractional (or decimal) part of input In; which is greater than or equal to 0 and less than 1. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Fraction_float4(float4 In, out float4 Out) { Out = frac(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fresnel-Effect-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fresnel-Effect-Node.html",
    "title": "Fresnel Effect Node | FSM Unity Framework",
    "keywords": "Fresnel Effect Node Description Fresnel Effect is the effect of differing reflectance on a surface depending on viewing angle, where as you approach the grazing angle more light is reflected. The Fresnel Effect node approximates this by calculating the angle between the surface normal and the view direction. The wider this angle is, the greater the return value will be. This effect is often used to achieve rim lighting, common in many art styles. Ports Name Direction Type Description Normal Input Vector 3 Normal direction. By default bound to World Space Normal View Dir Input Vector 3 View direction. By default bound to World Space View Direction Power Input Float Exponent of the power calculation Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_FresnelEffect_float(float3 Normal, float3 ViewDir, float Power, out float Out) { Out = pow((1.0 - saturate(dot(normalize(Normal), normalize(ViewDir)))), Power); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fresnel-Equation-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Fresnel-Equation-Node.html",
    "title": "Fresnel Equation Node | FSM Unity Framework",
    "keywords": "Fresnel Equation Node Description The Fresnel Equation Node adds equations that affect Material interactions to the Fresnel Component. You can select an equation in the Mode dropdown. You can find Numerical values of refractive indices at refractiveindex.info. Ports (Schlick) Name Direction Type Binding Description f0 Input Vector{1, 2, 3} None Represente the reflection of the surface when we face typically 0.02-0.08 for a dielectric material. DotVector Input Float None The dot product between the normal and the surface. Fresnel Output same as f0 None Fresnel coefficient, which describe the amount of light reflected or transmitted. Ports (Dielectric) Name Direction Type Binding Description IOR Source Input Vector None The refractive index of the medium the light source originates in. IOR Medium Input Vector None The refractive index of the medium that the light refracts into. DotVector Input Float None The dot product between the normal and the surface. Fresnel Output same as f0 None The fresnel coefficient, which describe the amount of light reflected or transmitted. Ports (DielectricGeneric) Name Direction Type Binding Description IOR Source Input Vector None The refractive index of the medium the light source originates in. IOR Medium Input Vector None The refractive index of the medium that the light refracts into. IOR MediumK Input Vector None The refractive index Medium (imaginary part), or the medium causing the refraction. DotVector Input Float None The dot product between the normal and the surface. Fresnel Output same as f0 None Fresnel coefficient, which describe the amount of light reflected or transmitted. Controls Name Type Options Description Mode Dropdown • Schlick: This mode produces an approximation based on Schlick's Approximation. Use the Schlick mode for interactions between air and dielectric materials. • Dielectric: Use this mode for interactions between two dielectric Materials. For example, air to glass, glass to water, or water to air. • DielectricGeneric: This mode computes a Fresnel equation for interactions between a dielectric and a metal. For example, clear-coat- to metal, glass to metal, or water to metal. Note: if the IORMediumK value is 0, DielectricGeneric behaves in the same way as the Dielectric mode. Generated Code Example The following example code represents one possible outcome of this node. void Unity_FresnelEquation_Schlick(out float Fresnel, float cos0, float f0) { Fresnel = F_Schlick(f0, cos0); } void Unity_FresnelEquation_Dielectric(out float3 Fresnel, float cos0, float3 iorSource, float3 iorMedium) { FresnelValue = F_FresnelDielectric(iorMedium/iorSource, cos0); } void Unity_FresnelEquation_DielectricGeneric(out float3 Fresnel, float cos0, float3 iorSource, float3 iorMedium, float3 iorMediumK) { FresnelValue = F_FresnelConductor(iorMedium/iorSource, iorMediumK/iorSource, cos0); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gather-Texture-2D-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gather-Texture-2D-Node.html",
    "title": "Gather Texture 2D node | FSM Unity Framework",
    "keywords": "Gather Texture 2D node The Gather Texture 2D node samples the red channel of four neighboring pixels from a sample point. It returns a value of RRRR, and takes each R value from a different neighbor. Normal Texture sampling reads all four channels (RGBA) of a Texture. This node is useful when you want to modify the bilinear interpolation between pixels, such as when you want to create custom blends. This node uses the Gather HLSL intrinsic function. For platforms where this intrinsic function doesn't exist, Shader Graph uses an appropriate approximation, instead. Note When you use the Metal graphics API, the sample, sample_compare, gather, and gather_compare intrinsics use an integer (int2) offset argument when sampling or gathering from a 2D Texture. The intrinsics apply this value to Texture coordinates before looking up each pixel. The offset value must be in the range of -8 to +7, or the Metal API clamps the offset value. The pixels that the Gather Texture 2D samples are always from the top mip level of the Texture, from a 2×2 block of pixels around the sample point. Rather than blending the 2×2 sample, it returns the sampled pixels in counter-clockwise order. It starts with the sample to the lower left of the query location: Create Node menu category The Gather Texture 2D node is under the Input > Texture category in the Create Node menu. Compatibility The Gather Texture 2D node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes The Gather Texture 2D node can only be connected to a Block node in the Fragment Context. For more information on Block nodes and Contexts, refer to Master Stack. Inputs The Gather Texture 2D node has the following input ports: Name Type Binding Description Texture Texture 2D None The Texture to sample. UV Vector 2 UV The UV coordinates to use to take the sample. Sampler SamplerState None The Sampler State and its corresponding settings to use for the sample. Offset Vector 2 None The pixel offset to apply to the sample's UV coordinates. The Offset value is in pixels, not UV space. Outputs The Gather Texture 2D node has the following output ports: Name Type Description RGBA Vector 4 The sample value. This is the red channels of the 4 neighboring pixels from the specified sample position on the given Texture. R Float The first neighboring pixel's red channel. G Float The second neighboring pixel's red channel. B Float The third neighboring pixel's red channel. A Float The fourth neighboring pixel's red channel. Example graph usage In the following example, a Gather Texture 2D node creates a blurred version of a Texture by averaging its 4 samples: Then, the rest of the Shader Graph uses a Sample Texture 2D node to sample the Texture again, and uses a Lerp node to determine when to use the blurred Texture and when to use the regular Texture: By changing the value provided to the T port on the Lerp node, you can change whether you want to blur or sharpen the Texture in your Shader Graph: Related nodes The following nodes are related or similar to the Gather Texture 2D node: Sample Texture 2D node Sample Texture 2D LOD node Sampler State node Texture 2D Asset node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Getting-Started.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Getting-Started.html",
    "title": "Getting started with Shader Graph | FSM Unity Framework",
    "keywords": "Getting started with Shader Graph Use Shader Graph with either of the Scriptable Render Pipelines (SRPs) available in Unity version 2018.1 and later: The High Definition Render Pipeline (HDRP) The Universal Render Pipeline (URP) As of Unity version 2021.2, you can also use Shader Graph with the Built-In Render Pipeline. Note Shader Graph support for the Built-In Render Pipeline is for compatibility purposes only. Shader Graph doesn't receive updates for Built-In Render Pipeline support, aside from bug fixes for existing features. It's recommended to use Shader Graph with the Scriptable Render Pipelines. When you install HDRP or URP into your project, Unity also installs the Shader Graph package automatically. You can manually install Shader Graph for use with the Built-In Render Pipeline on Unity version 2021.2 and later with the Package Manager. For more information on how to install a package, see Adding and removing packages in the Unity User Manual. For more information about how to set up a Scriptable Render Pipeline, see Getting started with HDRP or Getting started with URP."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gradient-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gradient-Node.html",
    "title": "Gradient Node | FSM Unity Framework",
    "keywords": "Gradient Node Description Defines a constant Gradient for use in Shader Graph, although internally to the shader this is defined as a struct. To sample the Gradient it should be used in conjunction with a Sample Gradient Node. When using a separate Gradient Node, you can sample a Gradient multiple times with different Time parameters. Ports Name Direction Type Description Out Output Gradient Output value Controls Name Type Options Description Gradient Field Defines the gradient. Generated Code Example The following example code represents one possible outcome of this node. Gradient Unity_Gradient_float() { Gradient g; g.type = 1; g.colorsLength = 4; g.alphasLength = 4; g.colors[0] = 0.1; g.colors[1] = 0.2; g.colors[2] = 0.3; g.colors[3] = 0.4; g.colors[4] = 0; g.colors[5] = 0; g.colors[6] = 0; g.colors[7] = 0; g.alphas[0] = 0.1; g.alphas[1] = 0.2; g.alphas[2] = 0.3; g.alphas[3] = 0.4; g.alphas[4] = 0; g.alphas[5] = 0; g.alphas[6] = 0; g.alphas[7] = 0; return g; } Gradient _Gradient = Unity_Gradient_float();"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gradient-Noise-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Gradient-Noise-Node.html",
    "title": "Gradient Noise Node | FSM Unity Framework",
    "keywords": "Gradient Noise Node Description Generates a gradient, or Perlin, noise based on input UV. The scale of the generated noise is controlled by input Scale. In terms of performance cost, Gradient Noise node can be slightly more computationally intensive than sampling a texture map. You can also choose to use two different hashing methods for calculating the noise. As of Unity version 2021.2, the Gradient Noise node defaults to the Deterministic hash, to ensure consistent results for noise generation across platforms. Because the UV value is used as the seed for the noise generation, you can offset, scale, or distort the UV value to generate different noise patterns. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Scale Input Float None Noise scale Out Output Float None Output value in the range 0.0 to 1.0 Controls Name Type Options Description Hash Type Dropdown Deterministic, LegacyMod Selects the hash function used to generate random numbers for noise generation. Generated Code Example The following example code represents one possible outcome of this node. float2 unity_gradientNoise_dir(float2 p) { p = p % 289; float x = (34 * p.x + 1) * p.x % 289 + p.y; x = (34 * x + 1) * x % 289; x = frac(x / 41) * 2 - 1; return normalize(float2(x - floor(x + 0.5), abs(x) - 0.5)); } float unity_gradientNoise(float2 p) { float2 ip = floor(p); float2 fp = frac(p); float d00 = dot(unity_gradientNoise_dir(ip), fp); float d01 = dot(unity_gradientNoise_dir(ip + float2(0, 1)), fp - float2(0, 1)); float d10 = dot(unity_gradientNoise_dir(ip + float2(1, 0)), fp - float2(1, 0)); float d11 = dot(unity_gradientNoise_dir(ip + float2(1, 1)), fp - float2(1, 1)); fp = fp * fp * fp * (fp * (fp * 6 - 15) + 10); return lerp(lerp(d00, d01, fp.y), lerp(d10, d11, fp.y), fp.x); } void Unity_GradientNoise_float(float2 UV, float Scale, out float Out) { Out = unity_gradientNoise(UV * Scale) + 0.5; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Graph-Settings-Tab.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Graph-Settings-Tab.html",
    "title": "Graph Settings Tab | FSM Unity Framework",
    "keywords": "Graph Settings Tab Description The Graph Settings tab on the Graph Inspector make it possible to change settings that affect the Shader Graph as a whole. Graph Settings options Menu Item Description Precision A Precision Mode drop-down menu that lets you set the default precision for the entire graph. You can override the Precision setting here at the node level in your graph. Preview Mode (Subgraphs only) Your options are Inherit, Preview 2D, and Preview 3D. Active Targets A list that contains the Targets you've selected. You can add or remove entries using the Add (+) and Remove (-) buttons. Shader Graph supports three targets: the Universal Render Pipeline, the High Definition Render Pipeline, and Built-In Render Pipeline. Target-specific settings appear below the standard setting options. The displayed Target-specific settings change according to which Targets you select."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Graph-Target.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Graph-Target.html",
    "title": "Graph Target | FSM Unity Framework",
    "keywords": "Graph Target A Target determines the end point compatibility of a shader you generate using Shader Graph. You can select Targets for each Shader Graph asset, and use the Graph Settings Menu to change the Targets. Targets hold information such as the required generation format, and variables that allow compatibility with different render pipelines or integration features like Visual Effect Graph. You can select any number of Targets for each Shader Graph asset. If a Target you select isn't compatible with other Targets you've already selected, an error message that explains the problem appears. Target Settings are specific to each Target, and can vary between assets depending on which Targets you've selected. Be aware that Universal Render Pipeline (URP) Target Settings and High Definition Render Pipeline (HDRP) Target Settings might change in future versions. Typically, each Target you select generates a valid subshader from the graph. For example, a Shader Graph asset with both URP and HDRP Targets will generate two subshaders. When you use a graph that targets multiple render pipelines, you must reimport the Shader Graph asset if you change the active render pipeline. This updates the Material Inspector for any Materials that use your graph. Shader Graph supports three targets: the Universal Render Pipeline, the High Definition Render Pipeline, and the Built-In Render Pipeline. Not all blocks are compatible with all targets. If a block in your graph becomes inactive when you choose a target, that block is not compatible with that target. The visual results of a graph are not the same in all render pipelines. This is because of the technical differences between URP, Built-In, and HDRP. Shader Graphs that target the Built-In Render Pipeline replicate the results of shaders handwritten in ShaderLab, with the exception of normal maps. For mathematical correctness, normal maps created with Shader Graph behave as they do in URP even when your build targets the Built-In Render Pipeline."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Custom-Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Custom-Color-Node.html",
    "title": "Custom Color Node (HDRP) | FSM Unity Framework",
    "keywords": "Custom Color Node (HDRP) The Custom Color Node accesses the custom pass color buffer allocated by HDRP. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Custom Color Node No Yes Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Sets the normalized screen coordinates to sample. Output Output Vector 4 None The value the custom pass color buffer contains at the sampled coordinates. Generated Code Example The following example code represents one possible outcome of this node. void Unity_CustomDepth_LinearEye_float(float4 UV, out float Out) { Out = SampleCustomColor(UV.xy); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Custom-Depth-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Custom-Depth-Node.html",
    "title": "Custom Depth Node (HDRP) | FSM Unity Framework",
    "keywords": "Custom Depth Node (HDRP) The Custom Depth Node accesses the custom pass depth buffer allocated by HDRP. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Custom Depth Node No Yes Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Sets the normalized screen coordinates that this node samples. Output Output Vector 4 None The output value of this node. Depth Sampling modes Name Description Linear01 The linear depth value between 0 and 1. Raw The raw depth value. Eye The depth value converted to eye space units. Generated Code Example The following example code represents one possible outcome of this node. void Unity_CustomDepth_LinearEye_float(float4 UV, out float Out) { Out = LinearEyeDepth(SampleCustomDepth(UV.xy), _ZBufferParams); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Sample-Buffer-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Sample-Buffer-Node.html",
    "title": "HD Sample Buffer Node | FSM Unity Framework",
    "keywords": "HD Sample Buffer Node Description The HD Sample Buffer Node samples a buffer directly from the Camera. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) HD Sample Buffer No Yes Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value. Sampler Input SamplerState None Determines the sampler that Unity uses to sample the buffer. Output Output Float None Output value. Controls Name Type Options Description Source Buffer Dropdown World Normal, Roughness, Motion Vectors, PostProcess Input, Blit Source. Determines which buffer to sample. Generated Code Example The following example code represents one possible outcome of this node: float4 Unity_HDRP_SampleBuffer_float(float2 uv, SamplerState samplerState) { return SAMPLE_TEXTURE2D_X_LOD(_CustomPostProcessInput, samplerState, uv * _RTHandlePostProcessScale.xy, 0); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Scene-Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Scene-Color-Node.html",
    "title": "HD Scene Color Node | FSM Unity Framework",
    "keywords": "HD Scene Color Node The HD Scene Color Node does the same thing as the Scene Color Node, but allows you to access the mips of the color buffer. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) HD Scene Color No Yes Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Sets the normalized screen coordinates to sample. Lod Input float None Sets the mip level that the sampler uses to sample the color buffer. Output Output Vector 3 None Output value Notes Exposure You can use the Exposure property to specify if you want to output the Camera color with exposure applied or not. By default, this property is disabled to avoid double exposure. The sampler that this Node uses to sample the color buffer is in trilinear clamp mode. This allows the sampler to smoothly interpolate between the mip maps."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Scene-Depth-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/HD-Scene-Depth-Node.html",
    "title": "HD Scene Depth Node | FSM Unity Framework",
    "keywords": "HD Scene Depth Node Description The HD Scene Depth node uses a UV input to access the current Camera's depth buffer. Unity expects normalized screen coordinates for this value. You can also use this node to access the mipmaps in the depth buffer. You can only use the HD Scene Depth node in the Fragment Shader Stage and with non-opaque materials. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) HD Scene Color No Yes Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Sets the normalized screen coordinates to sample. Lod Input float None Sets the mip level that the sampler uses to sample the depth buffer. Output Output Vector 3 None Output value. Depth Sampling modes Name Description Linear01 Linear depth value between 0 and 1 Raw Raw depth value Eye Depth converted to eye space units Notes To use the HD Scene Depth node in a Custom Render Pipeline, you need to explicitly define its behavior, otherwise it returns white."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hue-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hue-Node.html",
    "title": "Hue Node | FSM Unity Framework",
    "keywords": "Hue Node Description Offsets the hue of input In by the amount of input Offset. The unit of the offset can be set with the parameter Range. Offset in Degrees is in the range -180 to 180. In Radians it is -Pi to Pi. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Offset Input Float None Amount to offset hue Out Output Vector 3 None Output value Controls Name Type Options Description Range Dropdown Degrees, Radians The unit used for the input Offset Generated Code Example The following example code represents one possible outcome of this node per Base mode. Degrees void Unity_Hue_Degrees_float(float3 In, float Offset, out float3 Out) { float4 K = float4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); float4 P = lerp(float4(In.bg, K.wz), float4(In.gb, K.xy), step(In.b, In.g)); float4 Q = lerp(float4(P.xyw, In.r), float4(In.r, P.yzx), step(P.x, In.r)); float D = Q.x - min(Q.w, Q.y); float E = 1e-10; float3 hsv = float3(abs(Q.z + (Q.w - Q.y)/(6.0 * D + E)), D / (Q.x + E), Q.x); float hue = hsv.x + Offset / 360; hsv.x = (hue < 0) ? hue + 1 : (hue > 1) ? hue - 1 : hue; float4 K2 = float4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0); float3 P2 = abs(frac(hsv.xxx + K2.xyz) * 6.0 - K2.www); Out = hsv.z * lerp(K2.xxx, saturate(P2 - K2.xxx), hsv.y); } Radians void Unity_Hue_Radians_float(float3 In, float Offset, out float3 Out) { float4 K = float4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0); float4 P = lerp(float4(In.bg, K.wz), float4(In.gb, K.xy), step(In.b, In.g)); float4 Q = lerp(float4(P.xyw, In.r), float4(In.r, P.yzx), step(P.x, In.r)); float D = Q.x - min(Q.w, Q.y); float E = 1e-10; float3 hsv = float3(abs(Q.z + (Q.w - Q.y)/(6.0 * D + E)), D / (Q.x + E), Q.x); float hue = hsv.x + Offset; hsv.x = (hue < 0) ? hue + 1 : (hue > 1) ? hue - 1 : hue; // HSV to RGB float4 K2 = float4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0); float3 P2 = abs(frac(hsv.xxx + K2.xyz) * 6.0 - K2.www); Out = hsv.z * lerp(K2.xxx, saturate(P2 - K2.xxx), hsv.y); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Cosine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Cosine-Node.html",
    "title": "Hyperbolic Cosine Node | FSM Unity Framework",
    "keywords": "Hyperbolic Cosine Node Description Returns the hyperbolic cosine of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_HyperbolicCosine_float4(float4 In, out float4 Out) { Out = cosh(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Sine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Sine-Node.html",
    "title": "Hyperbolic Sine Node | FSM Unity Framework",
    "keywords": "Hyperbolic Sine Node Description Returns the hyperbolic sine of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_HyperbolicSine_float4(float4 In, out float4 Out) { Out = sinh(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Tangent-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Hyperbolic-Tangent-Node.html",
    "title": "Hyperbolic Tangent Node | FSM Unity Framework",
    "keywords": "Hyperbolic Tangent Node Description Returns the hyperbolic tangent of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_HyperbolicTangent_float4(float4 In, out float4 Out) { Out = tanh(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Input-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Input-Nodes.html",
    "title": "Input Nodes | FSM Unity Framework",
    "keywords": "Input Nodes Basic Boolean Color Defines a constant Boolean value in the shader. Defines a constant Vector 4 value in the shader using a Color field. Constant Integer Defines a Float of a mathematical constant value in the shader. Defines a constant Float value in the shader using an Integer field. Slider Time Defines a constant Float value in the shader using a Slider field. Provides access to various Time parameters in the shader. Float Vector 2 Defines a Float value in the shader. Defines a Vector 2 value in the shader. Vector 3 Vector 4 Defines a Vector 3 value in the shader. Defines a Vector 4 value in the shader. Geometry Bitangent Vector Normal Vector Provides access to the mesh vertex or fragment's Bitangent Vector. Provides access to the mesh vertex or fragment's Normal Vector. Position Screen Position Provides access to the mesh vertex or fragment's Position. Provides access to the mesh vertex or fragment's Screen Position. Tangent Vector UV Provides access to the mesh vertex or fragment's Tangent Vector. Provides access to the mesh vertex or fragment's UV coordinates. Vertex Color View Direction Provides access to the mesh vertex or fragment's Vertex Color value. Provides access to the mesh vertex or fragment's View Direction vector. Vertex ID Provides access to the mesh vertex or fragment's Vertex ID value. Gradient Blackbody Gradient Samples a radiation based gradient from temperature input (in Kelvin). Defines a constant Gradient in the shader. Sample Gradient Samples a Gradient given the input of Time. Matrix Matrix 2x2 Matrix 3x3 Defines a constant Matrix 2x2 value in the shader. Defines a constant Matrix 3x3 value in the shader. Matrix 4x4 Transformation Matrix Defines a constant Matrix 4x4 value in the shader. Defines a constant Matrix 4x4 value for a default Unity Transformation Matrix in the shader. Mesh Deformation Compute Deformation Node Linear Blend Skinning Node Passes compute deformed vertex data to a vertex shader. Only works with the DOTS Hybrid Renderer. Applies Linear Blend Vertex Skinning. Only works with the DOTS Hybrid Renderer. PBR Dielectric Specular Metal Reflectance Returns a Dielectric Specular F0 value for a physically based material. Returns a Metal Reflectance value for a physically based material. Scene Ambient Camera Provides access to the Scene's Ambient color values. Provides access to various parameters of the current Camera. Fog Baked GI Provides access to the Scene's Fog parameters. Provides access to the Baked GI values at the vertex or fragment's position. Object Reflection Probe Provides access to various parameters of the Object. Provides access to the nearest Reflection Probe to the object. Scene Color Scene Depth Provides access to the current Camera's color buffer. Provides access to the current Camera's depth buffer. Screen Eye Index Provides access to parameters of the screen. Provides access to the Eye Index when stereo rendering. Texture Cubemap Asset Sample Cubemap Defines a constant Cubemap Asset for use in the shader. Samples a Cubemap and returns a Vector 4 color value for use in the shader. Sample Reflected Cubemap Node Sample Texture 2D Samples a Cubemap with reflected vector and returns a Vector 4 color value for use in the shader. Samples a Texture 2D and returns a color value for use in the shader. Sample Texture 2D Array Sample Texture 2D LOD Samples a Texture 2D Array at an Index and returns a color value for use in the shader. Samples a Texture 2D at a specific LOD and returns a color value for use in the shader. Sample Texture 3D Sample Virtual Texture Samples a Texture 3D and returns a color value for use in the shader. Samples a Virtual Texture and returns color values for use in the shader. Sampler State Texture Size Defines a Sampler State for sampling textures. Returns the Width and Height of the texel size of Texture 2D input. Texture 2D Array Asset Texture 2D Asset Defines a constant Texture 2D Array Asset for use in the shader. Defines a constant Texture 2D Asset for use in the shader. Texture 3D Asset Defines a constant Texture 3D Asset for use in the shader."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Instance-ID-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Instance-ID-Node.html",
    "title": "Instance ID Node | FSM Unity Framework",
    "keywords": "Instance ID Node Description When Unity renders with GPU instancing, it assigns an Instance ID to each geometry. Use this node to capture Instance ID values in Graphics.DrawMeshInstanced API calls. When Unity does not render with GPU instancing, this ID is 0. When Unity uses dynamic instancing, instance IDs might not be consistent across multiple frames. Ports Name Direction Type Binding Description Out Output Float None Instance ID for mesh of a given instanced draw call."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Integer-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Integer-Node.html",
    "title": "Integer Node | FSM Unity Framework",
    "keywords": "Integer Node Description Defines a constant Float value in the shader using an Integer field. Can be converted to a Float type Property with a Mode setting of Integer via the Node's context menu. Ports Name Direction Type Binding Description Out Output Float None Output value Controls Name Type Options Description Integer Defines the output value. Generated Code Example The following example code represents one possible outcome of this node. float _Integer = 1;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Internal-Inspector.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Internal-Inspector.html",
    "title": "Graph Inspector | FSM Unity Framework",
    "keywords": "Graph Inspector Description The Graph Inspector makes it possible for you to interact with any selectable graph elements and graph-wide settings for a Shader Graph Asset. You can use the Graph Inspector to edit attributes and default values. When you open a Shader Graph, the Graph Inspector displays the Graph Settings tab by default. Graph-wide settings for that specific Shader Graph appear in this tab. How to use Select a node in the graph to display settings available for that node in the Graph Inspector. Settings available for that node appear in the Node Settings tab of the Graph Inspector. For example, if you select a Property node either in the graph or the Blackboard, the Node Settings tab displays attributes of the Property that you can edit. Graph elements that currently work with the Graph Inspector: Properties Keywords Custom Function nodes Subgraph Output nodes Per-node precision Graph elements that currently do not work with the Graph Inspector: Edges Sticky Notes Groups Material Override Enabling the Allow Material Override option in the Graph Settings makes it possible for you to override certain graph properties via the Material Inspector."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Inverse-Lerp-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Inverse-Lerp-Node.html",
    "title": "Inverse Lerp Node | FSM Unity Framework",
    "keywords": "Inverse Lerp Node Description Returns the linear parameter that produces the interpolant specified by input T within the range of input A to input B. Inverse Lerp is the inverse operation of the Lerp Node. It can be used to determine what the input to a Lerp was based on its output. For example, the value of a Lerp between 0 and 2 with a T value of 0.5 is 1. Therefore the value of an Inverse Lerp between 0 and 2 with a T value of 1 is 0.5. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value T Input Dynamic Vector Time value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_InverseLerp_float4(float4 A, float4 B, float4 T, out float4 Out) { Out = (T - A)/(B - A); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Invert-Colors-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Invert-Colors-Node.html",
    "title": "Invert Colors Node | FSM Unity Framework",
    "keywords": "Invert Colors Node Description Inverts the colors of input In on a per channel basis. This Node assumes all input values are in the range 0 - 1. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Dynamic Vector None Output value Controls Name Type Options Description Red Toggle True, False If true red channel is inverted Green Toggle True, False If true green channel is inverted. Disabled if input vector dimension is less than 2 Blue Toggle True, False If true blue channel is inverted. Disabled if input vector dimension is less than 3 Alpha Toggle True, False If true alpha channel is inverted. Disabled if input vector dimension is less than 4 Generated Code Example The following example code represents one possible outcome of this node. float2 _InvertColors_InvertColors = float4(Red, Green, Blue, Alpha); void Unity_InvertColors_float4(float4 In, float4 InvertColors, out float4 Out) { Out = abs(InvertColors - In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Limbal-Ring-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Limbal-Ring-Node.html",
    "title": "Iris Limbal Ring Node | FSM Unity Framework",
    "keywords": "Iris Limbal Ring Node Calculates the intensity of the Limbal ring, a darkening feature of eyes. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Iris Limbal Ring Node No Yes Ports name Direction type description IrisUV Input Vector2 Normalized UV coordinates that can be used to sample either a texture or procedurally generate an Iris Texture. View Direction OS Input Vector3 Direction of the incident ray in object space. Either from the camera in rasterization or from the previous bounce in ray tracing. LimbalRingSize Input float Normalized [0, 1] value that defines the relative size of the limbal ring. LimbalRingFade Input float Normalized [0, 1] value that defines strength of the fade out of the limbal ring. LimbalRingIntensity Input float Positive value that defines how dark the limbal ring is. Iris Limbal Ring Color Output Color Intensity of the limbal ring."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Offset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Offset-Node.html",
    "title": "Iris Offset Node | FSM Unity Framework",
    "keywords": "Iris Offset Node Applies an offset to the center of the Iris as real world eyes are never symmetrical and centered. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Iris Offset Node No Yes Ports name Direction type description IrisUV Input Vector2 Normalized UV coordinates to sample either a texture or procedurally generate an Iris Texture. IrisOffset Input Vector2 Normalized [0, 1]x[0,1] value that defines on each axis the intensity of the offset of the Center of the pupil. IrisUV Output Vector2 Normalized UV coordinates to sample either a texture or procedurally generate an Iris Texture."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Out-Of-Bound-Color-Clamp-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-Out-Of-Bound-Color-Clamp-Node.html",
    "title": "Iris Out of Bound Color Clamp Node | FSM Unity Framework",
    "keywords": "Iris Out of Bound Color Clamp Node Clamps the color of the Iris to a given color. This is useful in case the refraction ray reaches the inside of the cornea. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Iris Out of Bound Color Clamp Node No Yes Ports name Direction type description IrisUV Input Vector2 Normalized UV coordinates to sample either a texture or procedurally generate an Iris Texture. Iris Color Input Color Previously sampled or generated color of the Iris. Clamp Color Input Color The color to clamp the Iris to. Iris Color Output Color Result Iris color for the rest of the pipeline."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-UV-Location-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Iris-UV-Location-Node.html",
    "title": "Iris UV Location Node | FSM Unity Framework",
    "keywords": "Iris UV Location Node This node converts the object position of the cornea/iris to a UV Sampling coordinate. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Iris UV Location Node No Yes Ports name Direction type description Position OS Input Vector3 Position on the iris Plane in object space. Iris Radius Input float The radius of the Iris in the used model. For the default model, this value should be 0.225. IrisUV Output Vector2 ormalized UV coordinates that can be used to sample either a texture or procedurally generate an Iris Texture."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-Front-Face-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-Front-Face-Node.html",
    "title": "Is Front Face Node | FSM Unity Framework",
    "keywords": "Is Front Face Node Description Returns true if currently rendering a front face and false if rendering a back face. This value is always true unless the Master Node's Two Sided value is set to true in the Material Options. This is useful for Branching. NOTE: This Node can only be used in the Fragment Shader Stage. Ports Name Direction Type Binding Description Out Output Boolean None Output value"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-Infinite-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-Infinite-Node.html",
    "title": "Is Infinite Node | FSM Unity Framework",
    "keywords": "Is Infinite Node Description Returns true if the input In is an infinite value. This is useful for Branching. Ports Name Direction Type Binding Description In Input Float None Input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_IsInfinite_float(float In, out float Out) { Out = isinf(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-NaN-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Is-NaN-Node.html",
    "title": "Is NaN Node | FSM Unity Framework",
    "keywords": "Is NaN Node Description Returns true if the input In is not a number (NaN). This is useful for Branching. Ports Name Direction Type Binding Description In Input Float None Input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_IsNan_float(float In, out float Out) { Out = (In < 0.0 || In > 0.0 || In == 0.0) ? 0 : 1; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Keyword-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Keyword-Node.html",
    "title": "Keyword node | FSM Unity Framework",
    "keywords": "Keyword node Description You can use a Keyword node to create a static branch in your Shader Graph that references a Keyword on the Blackboard. The appearance of a Keyword node, including its available ports, changes based on the Keyword it references. Creating new Keyword Nodes Because each Keyword node references a specific Keyword, you must first define at least one Keyword on the Blackboard. Drag a Keyword from the Blackboard to the workspace to make a Keyword node that corresponds to that Keyword. You can also right-click anywhere on the workspace, and use the Create Node menu to make a new Keyword node. Under Keywords, there is a list of Keywords that you defined on the Blackboard. Click on a Keyword in that list to create a corresponding Keyword node."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Keywords.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Keywords.html",
    "title": "Keywords | FSM Unity Framework",
    "keywords": "Keywords Description Use Keywords to create different variants for your Shader Graph. Keywords enable you to create shaders: With features that you can turn on or off for each Material instance. With features that behave differently on certain platforms. That scale in complexity based on conditions you set. There are three types of Keywords: Boolean, Enum, and Built-in. Unity defines a Keyword in the graph, shader, and optionally, the Material Inspector based on its type. See Boolean Keyword, Enum Keyword, and Built-in Keyword for more information about Keyword types. For more information about how these Keywords affect the final shader, see documentation on Making multiple shader program variants. In Shader Graph, you first define a Keyword on the Blackboard, then use a Keyword Node to create a branch in the graph. The Editor is able to compile variants on demand when it needs them to render content. If you declare many different variants, you can end up with millions or trillions of possibilities. However, the Player needs to determine at build time which variants are in use and include them when it pre-compiles your shaders. To manage memory effectively, the Player strips unused variants based on their keyword and Editor settings. See the next section, Common parameters, to learn more about how you can give the Player hints about what it needs to compile and what it can ignore. When the Player strips out a variant in the build process, it displays the pink error shader. Common parameters Although some fields are specific to certain types of Keywords, all Keywords have the following parameters. | Name | Type | Description | | ------------------ | -------- | ------------------------------------------------------------ | | Display Name | String | The display name of the Keyword. Unity shows this name in the title bar of nodes that reference the corresponding Keyword, and also in the Material Inspector if you expose that Keyword. | | Exposed | Boolean | When you set this parameter to true, Unity displays this Keyword in the Material Inspector. If you set it to false, the Keyword does not appear in the Material Inspector. If you intend to access a GLOBAL shader variable, be sure to add it as you would normally add an input variable, but deselect Exposed.| | Reference Name | String | The internal name for the Keyword in the shader. If you overwrite the Reference Name parameter, take note of the following: • Keyword Reference Names are always in full capitals, so Unity converts all lowercase letters to uppercase. • If the Reference Name contains any characters that HLSL does not support, Unity replaces those characters with underscores. • Right-click on a Reference Name, and select Reset Reference to revert to the default Reference Name. | | Definition | Enum | Sets how the Keyword is defined in the shader. Determines when to compile keyword variants. There are three available options. • Shader Feature: Unity only compiles keyword variants when a Material selects the relevant option. For this option to be available in the Player, a Material selecting it must exist at build-time. • Multi Compile:Pre-compiles all the variant possibilities. This is slower and uses more memory, but allows the option to be dynamically switched in the Player. • Predefined: The render pipeline defines this keyword and controls the settings for it. | | Scope | Enum | Sets the scope at which to define the Keyword. • Global Keywords: Defines Keyword for the entire project, and it counts towards the global keyword limit. • Local Keywords: Defines Keyword for only one shader, which has its own local keyword limit. When you use Predefined Keywords, Unity disables this field. | | Stages | | • All - Applies this keyword to all shader stages. •Vertex - Applies this keyword to the vertex stage. •Fragment - Applies this keyword to the fragment stage. | Boolean Keywords Boolean Keywords are either on or off. This results in two shader variants. Unity exposes Boolean Keywords in the Material Inspector if the Exposed parameter is set to is true. To enable the keyword from a script, use EnableKeyword on the keyword's Reference name. DisableKeyword disables the keyword. To learn more about Boolean Keywords, see Shader variants and keywords. Type-specific parameters Boolean Keywords have one Boolean-specific parameter in addition to the common parameters listed above. | Name | Type | Description | | ----------- | -------- | ------------------------------------------------------------ | | Default | Boolean |Enable this parameter to set the Keyword's default state to on, and disable it to set the Keyword's default state to off. This parameter determines the value to use for the Keyword when Shader Graph generates previews. It also defines the Keyword's default value when you use this shader to create a new Material. | Enum Keywords Enum Keywords can have two or more states, which you define in the Entries list. If you expose an Enum Keyword, the Display Names in its Entries list appear in a dropdown menu in the Material Inspector. Special characters such as ( ) or ! @ are not valid in the Entry Name of an Enum Keyword. Shader Graph converts invalid characters to underscores ( _ ). When you define an Enum Keyword, Shader Graph displays labels for each state consisting of a sanitized version of the Enum's Entry Name appended to the main Reference name. When controlling a keyword via script with a, Material.EnableKeyword or Shader.EnableKeyword function, enter the state label in the format {REFERENCE}_{REFERENCESUFFIX}. For example, if your reference name is MYENUM and the desired entry is OPTION1, then you would call Material.EnableKeyword(\"MYENUM_OPTION1\"). When you select an option, this disables the other options. Type-specific parameters In addition to the common parameters listed above, Enum Keywords have the following additional parameters. Name Type Description Default Enum Select an entry from the drop-down menu to determine which value to use for the Keyword when Shader Graph generates previews. This also defines the Keyword's default value when you use this shader to create a new Material. When you edit the Entries list, Shader Graph automatically updates the options in this control. Entries Reorderable List This list defines all the states for the Keyword. Each state has a separate Display Name and Reference Suffix. • Display Name: Appears in drop-down menus for the Keyword on the Internal Inspector and the Material Inspector. Shader Graph also uses this name for port labels on nodes that reference the Keyword. • Reference Suffix: This is the final keyword, presented in the format Reference_ReferenceSuffix. Built-in Keywords Built-in Keywords are always of either the Boolean or Enum type, but they behave slightly differently from Boolean or Enum Keywords that you create. The Unity Editor or active Render Pipeline sets their values, and you cannot edit these. All Built-in Keyword fields in the Node Settings tab of the Graph Inspector are grayed out except for the Default field, which you can enable or disable to show the differences in Shader Graph previews. You also cannot expose Built-in Keywords in the Material Inspector."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Length-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Length-Node.html",
    "title": "Length Node | FSM Unity Framework",
    "keywords": "Length Node Description Returns the length of input In. This is also known as magnitude. A vector's length is calculated with Pythagorean Theorum. The length of a Vector 2 can be calculated as: Where x and y are the components of the input vector. Length can be calculated for other dimension vectors by adding or removing components. And so on. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Length_float4(float4 In, out float Out) { Out = length(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Lerp-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Lerp-Node.html",
    "title": "Lerp Node | FSM Unity Framework",
    "keywords": "Lerp Node Description Returns the result of linearly interpolating between input A and input B by input T. For example, when the value of input T is 0 the return value is equal to the value of input A, when it is 1 the return value is equal to the value of input B and when it is 0.5 the return value is the midpoint of the two inputs A and B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value T Input Dynamic Vector Time value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Lerp_float4(float4 A, float4 B, float4 T, out float4 Out) { Out = lerp(A, B, T); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Linear-Blend-Skinning-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Linear-Blend-Skinning-Node.html",
    "title": "Linear Blend Skinning Node | FSM Unity Framework",
    "keywords": "Linear Blend Skinning Node Description This node lets you apply Linear Blend Vertex Skinning, and only works with the DOTS Hybrid Renderer. You must provide skinned matrices in the _SkinMatrices buffer. The node uses the _SkinMatrixIndex property to calculate where the matrices associated with the current mesh are located in the _SkinMatrices buffer. Ports Name Direction Type Stage Description Position Input Vector3 Vertex Position of the vertex in object space. Normal Input Vector3 Vertex Normal of the vertex in object space. Tangent Input Vector3 Vertex Tangent of the vertex in object space. Position Output Vector3 Vertex Outputs the skinned vertex position. Normal Output Vector3 Vertex Outputs the skinned vertex normal. Tangent Output Vector3 Vertex Outputs the skinned vertex tangent."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Log-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Log-Node.html",
    "title": "Log Node | FSM Unity Framework",
    "keywords": "Log Node Description Returns the logarithm of input In. Log is the inverse operation to the Exponential Node. For example, the result of a base-2 Exponential using an input value of 3 is 8. Therefore the result of a base-2 Log using an input value of 8 is 3. The logarithmic base can be switched between base-e, base-2 and base-10 from the Base dropdown on the node. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Controls Name Type Options Description Base Dropdown BaseE, Base2, Base10 Selects the logarithmic base Generated Code Example The following example code represents one possible outcome of this node per Base mode. Base E void Unity_Log_float4(float4 In, out float4 Out) { Out = log(In); } Base 2 void Unity_Log2_float4(float4 In, out float4 Out) { Out = log2(In); } Base 10 void Unity_Log10_float4(float4 In, out float4 Out) { Out = log10(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Main-Light-Direction-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Main-Light-Direction-Node.html",
    "title": "Get Main Light Direction Node | FSM Unity Framework",
    "keywords": "Get Main Light Direction Node Description Provides access to the direction of the main directional light in the scene. The main directional light is the one casting shadows if there is any. Otherwise, it fallbacks to the first non shadow casting directional light. Ports Name Direction Type Description Direction Output Vector3 The normalized direction of the sun light in world space."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Main-Preview.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Main-Preview.html",
    "title": "Main Preview | FSM Unity Framework",
    "keywords": "Main Preview Description The Main Preview displays a representation of the shader on the active Render Pipeline. It updates in real-time and automatically updates to display any changes you make in the Shader Graph. The title bar of the Main Preview displays the name of the current shader. The Main Preview can be moved to anywhere in the Shader Graph Window and will automatically move with the nearest corner of that window. Preview Mesh You can rotate the preview mesh by holding left mouse button and dragging on the Main Preview and you can scale it by using the scroll wheel. The preview mesh can be changed by right clicking on the Main Preview. Here you can select from any primitive mesh types or select a custom mesh."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Master-Stack.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Master-Stack.html",
    "title": "Master Stack | FSM Unity Framework",
    "keywords": "Master Stack Description The Master Stack is the end point of a Shader Graph that defines the final surface appearance of a shader. Your Shader Graph should always contain only one Master Stack. The content of the Master Stack might change depending on the Graph Settings you select. The Master Stack is made up of Contexts, which contain Block nodes. Contexts The Master Stack contains two Contexts: Vertex and Fragment. These represent the two stages of a shader. Nodes that you connect to Blocks in the Vertex Context become part of the final shader's vertex function. Nodes that you connect to Blocks in the Fragment Context become part of the final shader's fragment (or pixel) function. If you connect any nodes to both Contexts, they are executed twice, once in the vertex function and then again in the fragment function. You can't cut, copy, or paste Contexts."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Math-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Math-Nodes.html",
    "title": "Math Nodes | FSM Unity Framework",
    "keywords": "Math Nodes Advanced Absolute Exponential Returns the absolute value of input In. Returns the exponential value of input In. Length Log Returns the length of input In. Returns the logarithm of input In. Modulo Negate Returns the remainder of input A divided by input B. Returns the inverse value of input In. Normalize Posterize Returns the normalized vector of input In. Returns the input In converted into a number of values defined by input Steps. Reciprocal Reciprocal Square Root Returns the result of 1 divided by input In. Returns the result of 1 divided by the square root of input In. Basic Add Divide Returns the sum of the two input values. Returns the result of input A divided by input B. Multiply Power Returns the result of input A multiplied by input B. Returns the result of input A to the power of input B. Square Root Subtract Returns the square root of input In. Returns the result of input A minus input B. Derivative DDX DDXY Returns the partial derivative with respect to the screen-space x-coordinate. Returns the sum of both partial derivatives. DDY Returns the partial derivative with respect to the screen-space y-coordinate. Interpolation Inverse Lerp Lerp Returns the parameter that produces the interpolant specified by input T within the range of input A to input B. Returns the result of linearly interpolating between input A and input B by input T. Smoothstep Returns the result of a smooth Hermite interpolation between 0 and 1, if input In is between inputs Edge1 and Edge2. Matrix Matrix Construction Matrix Determinant Constructs square matrices from the four input vectors M0, M1, M2 and M3. Returns the determinant of the matrix defined by input In. Matrix Split Matrix Transpose Splits a square matrix defined by input In into vectors. Returns the transposed value of the matrix defined by input In. Range Clamp Fraction Returns the input In clamped between the minimum and maximum values defined by inputs Min and Max respectively. Returns the fractional (or decimal) part of input In; which is greater than or equal to 0 and less than 1. Maximum Minimum Returns the largest of the two inputs values A and B. Returns the smallest of the two inputs values A and B. One Minus Random Range Returns the result of input In subtracted from 1. Returns a pseudo-random number that is between the minimum and maximum values defined by inputs Min and Max. Remap Saturate Remaps the value of input In from between the values of input Out Min Max to between the values of input In Min Max. Returns the value of input In clamped between 0 and 1. Round Ceiling Floor Returns the smallest integer value, or whole number, that is greater than or equal to the value of input In. Returns the largest integer value, or whole number, that is less than or equal to the value of input In. Round Sign Returns the value of input In rounded to the nearest integer, or whole number. Returns -1 if the value of input In is less than zero, 0 if equal to zero and 1 if greater than zero. Step Truncate Returns 1 if the value of input In is greater than or equal to the value of input Edge, otherwise returns 0. Returns the integer, or whole number, component of the value of input In. Trigonometry Arccosine Arcsine Returns the arccosine of each component the input In as a vector of equal length. Returns the arcsine of each component the input In as a vector of equal length. Arctangent Arctangent2 Returns the arctangent of the value of input In. Each component should be within the range of -Pi/2 to Pi/2. Returns the arctangent of the values of both input A and input B. Cosine Degrees to Radians Returns the cosine of the value of input In. Returns the value of input In converted from degrees to radians. Hyperbolic Cosine Hyperbolic Sine Returns the hyperbolic cosine of input In. Returns the hyperbolic sine of input In. Hyperbolic Tangent Radians to Degrees Returns the hyperbolic tangent of input In. Returns the value of input In converted from radians to degrees. Sine Tangent Returns the sine of the value of input In. Returns the tangent of the value of input In. Vector Cross Product Distance Returns the cross product of the values of the inputs A and B. Returns the Euclidean distance between the values of the inputs A and B. Dot Product Fresnel Effect Returns the dot product, or scalar product, of the values of the inputs A and B. Fresnel Effect is the effect of differing reflectance on a surface depending on viewing angle, where as you approach the grazing angle more light is reflected. Projection Reflection Returns the result of projecting the value of input A onto a straight line parallel to the value of input B. Returns a reflection vector using input In and a surface normal Normal. Rejection Rotate About Axis Returns the result of the projection of the value of input A onto the plane orthogonal, or perpendicular, to the value of input B. Rotates the input vector In around the axis Axis by the value of Rotation. Projection Rejection Returns the result of projecting the value of input A onto a straight line parallel to the value of input B. Returns the result of the projection of the value of input A onto the plane orthogonal, or perpendicular, to the value of input B. Sphere Mask Transform Creates a sphere mask originating from input Center. Returns the result of transforming the value of input In from one coordinate space to another. Wave Noise Sine Wave Sawtooth Wave Returns the sine of the value of input In. For variance, random noise is added to the amplitude of the sine wave. Returns a sawtooth wave from the value of input In. Matrix Split Matrix Transpose Splits a square matrix defined by input In into vectors. Returns the transposed value of the matrix defined by input In. Noise Sine Wave Sawtooth Wave Square Wavve Triangle Wave"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-2x2-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-2x2-Node.html",
    "title": "Matrix 2x2 Node | FSM Unity Framework",
    "keywords": "Matrix 2x2 Node Description Defines a constant Matrix 2x2 value in the shader. Ports Name Direction Type Binding Description Out Output Matrix 2 None Output value Controls Name Type Options Description Matrix 2x2 Sets output value Generated Code Example The following example code represents one possible outcome of this node. float2x2 _Matrix2x2 = float2x2(1, 0, 0, 1);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-3x3-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-3x3-Node.html",
    "title": "Matrix 3x3 Node | FSM Unity Framework",
    "keywords": "Matrix 3x3 Node Description Defines a constant Matrix 3x3 value in the shader. Ports Name Direction Type Binding Description Out Output Matrix 3 None Output value Controls Name Type Options Description Matrix 3x3 Sets output value Generated Code Example The following example code represents one possible outcome of this node. float3x3 _Matrix3x3 = float3x3(1, 0, 0, 0, 1, 0, 0, 0, 1);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-4x4-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-4x4-Node.html",
    "title": "Matrix 4x4 Node | FSM Unity Framework",
    "keywords": "Matrix 4x4 Node Description Defines a constant Matrix 4x4 value in the shader. Ports Name Direction Type Binding Description Out Output Matrix 4 None Output value Controls Name Type Options Description Matrix 4x4 Sets output value Generated Code Example The following example code represents one possible outcome of this node. float4x4 _Matrix4x4 = float4x4(1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Construction-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Construction-Node.html",
    "title": "Matrix Construction Node | FSM Unity Framework",
    "keywords": "Matrix Construction Node Description Constructs square matrices from the four input vectors M0, M1, M2 and M3. This node can be used to generate matrices of types Matrix 2x2, Matrix 3x3 and Matrix 4x4. The dropdown on the node can be used to select whether the inputs values specify the matrix rows or columns. Row : Input vectors specify matrix rows from top to bottom. Column : Input vectors specify matrix columns from left to right. Matrix outputs are taken from the top left corner of the construction of the inputs. This can be used to generate different dimension square matrices from different dimension vectors. For example, connecting Vector 2 type values to inputs M0 and M1 will generate the desired matrix from the output 2x2. Ports Name Direction Type Description M0 Input Vector 4 First row or column M1 Input Vector 4 Second row or column M2 Input Vector 4 Third row or column M3 Input Vector 4 Fourth row or column 4x4 Output Matrix 4x4 Output as Matrix 4x4 3x3 Output Matrix 3x3 Output as Matrix 3x3 2x2 Output Matrix 2x2 Output as Matrix 2x2 Controls Name Type Options Description Dropdown Row, Column Selects how the output matrix should be filled Generated Code Example The following example code represents one possible outcome of this node per mode. Row void Unity_MatrixConstruction_Row_float(float4 M0, float4 M1, float4 M2, float3 M3, out float4x4 Out4x4, out float3x3 Out3x3, out float2x2 Out2x2) { Out4x4 = float4x4(M0.x, M0.y, M0.z, M0.w, M1.x, M1.y, M1.z, M1.w, M2.x, M2.y, M2.z, M2.w, M3.x, M3.y, M3.z, M3.w); Out3x3 = float3x3(M0.x, M0.y, M0.z, M1.x, M1.y, M1.z, M2.x, M2.y, M2.z); Out2x2 = float2x2(M0.x, M0.y, M1.x, M1.y); } Column void Unity_MatrixConstruction_Column_float(float4 M0, float4 M1, float4 M2, float3 M3, out float4x4 Out4x4, out float3x3 Out3x3, out float2x2 Out2x2) { Out4x4 = float4x4(M0.x, M1.x, M2.x, M3.x, M0.y, M1.y, M2.y, M3.y, M0.z, M1.z, M2.z, M3.z, M0.w, M1.w, M2.w, M3.w); Out3x3 = float3x3(M0.x, M1.x, M2.x, M0.y, M1.y, M2.y, M0.z, M1.z, M2.z); Out2x2 = float2x2(M0.x, M1.x, M0.y, M1.y); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Determinant-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Determinant-Node.html",
    "title": "Matrix Determinant | FSM Unity Framework",
    "keywords": "Matrix Determinant Description Returns the determinant of the matrix defined by input In. It can be viewed as the scaling factor of the transformation described by the matrix. Ports Name Direction Type Description In Input Dynamic Matrix Input value Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_MatrixDeterminant_float4x4(float4x4 In, out float Out) { Out = determinant(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Split-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Split-Node.html",
    "title": "Matrix Split Node | FSM Unity Framework",
    "keywords": "Matrix Split Node Description Splits a square matrix defined by input In into vectors. Output vector dimension is defined by the dimension of the input matrix. The dropdown on the node can be used to select whether the output values are taken from the rows or columns of the input matrix. Row : Output vectors are composed of matrix rows from top to bottom. Column : Output vectors are composed of matrix columns from left to right. An input matrix of type Matrix 2x2 or Matrix 3x3 will return 0 values in the rows (or columns, depending on dropdown selection) that are beyond their dimension. For example, connecting Matrix 2x2 type to input In will return the correct Vector 2 type outputs to output slots M0 and M1, leaving outputs M2 and M3 to return 0 values. Ports Name Direction Type Description In Input Dynamic Matrix Input value M0 Output Dynamic Vector First row or column M1 Output Dynamic Vector Second row or column M2 Output Dynamic Vector Third row or column M3 Output Dynamic Vector Fourth row or column Controls Name Type Options Description Dropdown Row, Column Selects how the output vectors should be filled Generated Code Example The following example code represents one possible outcome of this node. float2 _MatrixSplit_M0 = float2(In[0].r, In[0].g); float2 _MatrixSplit_M1 = float2(In[1].r, In[1].g); float2 _MatrixSplit_M2 = float2(0, 0); float2 _MatrixSplit_M3 = float2(0, 0);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Transpose-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Matrix-Transpose-Node.html",
    "title": "Matrix Transpose | FSM Unity Framework",
    "keywords": "Matrix Transpose Description Returns the transposed value of the matrix defined by input In. This can be seen as the operation of flipping the matrix over its diagonal. The result is that it switches the row and column indices of the matrix. Ports Name Direction Type Description In Input Dynamic Matrix Input value Out Output Dynamic Matrix Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_MatrixTranspose_float4x4(float4x4 In, out float4x4 Out) { Out = transpose(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Maximum-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Maximum-Node.html",
    "title": "Maximum Node | FSM Unity Framework",
    "keywords": "Maximum Node Description Returns the largest of the two inputs values A and B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Maximum_float4(float4 A, float4 B, out float4 Out) { Out = max(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Metal-Reflectance-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Metal-Reflectance-Node.html",
    "title": "Metal Reflectance Node | FSM Unity Framework",
    "keywords": "Metal Reflectance Node Description Returns a Metal Reflectance value for a physically based material. The material to use can be selected with the Material dropdown parameter on the Node. When using Specular Workflow on a PBR Master Node this value should be supplied to the Specular Port. When using Metallic Workflow this value should be supplied to the Albedo Port. Ports Name Direction Type Binding Description Out Output Vector 3 None Output value Controls Name Type Options Description Material Dropdown Iron, Silver, Aluminium, Gold, Copper, Chromium, Nickel, Titanium, Cobalt, Platform Selects the material value to output. Generated Code Example The following example code represents one possible outcome of this node. Iron float3 _MetalReflectance_Out = float3(0.560, 0.570, 0.580); Silver float3 _MetalReflectance_Out = float3(0.972, 0.960, 0.915); Aluminium float3 _MetalReflectance_Out = float3(0.913, 0.921, 0.925); Gold float3 _MetalReflectance_Out = float3(1.000, 0.766, 0.336); Copper float3 _MetalReflectance_Out = float3(0.955, 0.637, 0.538); Chromium float3 _MetalReflectance_Out = float3(0.550, 0.556, 0.554); Nickel float3 _MetalReflectance_Out = float3(0.660, 0.609, 0.526); Titanium float3 _MetalReflectance_Out = float3(0.542, 0.497, 0.449); Cobalt float3 _MetalReflectance_Out = float3(0.662, 0.655, 0.634); Platinum float3 _MetalReflectance_Out = float3(0.672, 0.637, 0.585);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Minimum-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Minimum-Node.html",
    "title": "Minimum Node | FSM Unity Framework",
    "keywords": "Minimum Node Description Returns the smallest of the two inputs values A and B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Minimum_float4(float4 A, float4 B, out float4 Out) { Out = min(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Modulo-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Modulo-Node.html",
    "title": "Modulo Node | FSM Unity Framework",
    "keywords": "Modulo Node Description Returns the remainder of dividing input A by input B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Modulo_float4(float4 A, float4 B, out float4 Out) { Out = fmod(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Multiply-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Multiply-Node.html",
    "title": "Multiply Node | FSM Unity Framework",
    "keywords": "Multiply Node Description Returns the result of input A multiplied by input B. If both inputs are a vector type, the output type will be a vector type with the same dimension as the evaluated type of those inputs. If both inputs are a matrix type, the output type will be a matrix type with the same dimension as the evaluated type of those inputs. If one input is a vector type and the other is a matrix type, then output type will be a vector with the same dimension as the vector type input. Ports Name Direction Type Description A Input Dynamic First input value B Input Dynamic Second input value Out Output Dynamic Output value Generated Code Example The following example code represents different possible outcomes of this node. Vector * Vector void Unity_Multiply_float4_float4(float4 A, float4 B, out float4 Out) { Out = A * B; } Vector * Matrix void Unity_Multiply_float4_float4x4(float4 A, float4x4 B, out float4 Out) { Out = mul(A, B); } Matrix * Matrix void Unity_Multiply_float4x4_float4x4(float4x4 A, float4x4 B, out float4x4 Out) { Out = mul(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Nand-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Nand-Node.html",
    "title": "Nand Node | FSM Unity Framework",
    "keywords": "Nand Node Description Returns true if both the inputs A and B are false. This is useful for Branching. Ports Name Direction Type Binding Description A Input Boolean None First input value B Input Boolean None Second input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Nand_float(float A, float B, out float Out) { Out = !A && !B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Negate-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Negate-Node.html",
    "title": "Negate Node | FSM Unity Framework",
    "keywords": "Negate Node Description Returns the flipped sign value of input In. Positive values become negative and negative values become positive. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Negate_float4(float4 In, out float4 Out) { Out = -1 * In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Node-Library.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Node-Library.html",
    "title": "Node Library | FSM Unity Framework",
    "keywords": "Node Library Description The Node Library contains documentation for all the individual Nodes in Shader Graph; including descriptions, ports, parameters, shader code and example images. The Nodes are organised in the same categories as found in the Create Node Menu for convenience. Graph Nodes Artistic Channel Input Math Procedural Utility UV Block Nodes Built In Universal High Definition"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Node.html",
    "title": "Node | FSM Unity Framework",
    "keywords": "Node Description A Node defines an input, output or operation on the Shader Graph, depending on its available Ports. A Node may have any number of input and/or output ports. You create a Shader Graph by connecting these ports with Edges. A Node might also have any number of Controls, these are controls on the Node that do not have ports. You can collapse a Node by clicking the Collapse button in the top-right corner of the Node. This will hide all unconnected ports. For components of a Node see: Port Edge There are many available Nodes in Shader Graph. For a full list of all available Nodes see the Node Library. Preview Many nodes include a preview. This preview displays the main output value at that stage in the graph. Hide this preview with the Collapse control that displays when you hover over the node. You can also collapse and expand node previews via the Context Menu in the Shader Graph Window. To configure the appearance of node previews, see Preview Mode Control. Context Menu Right clicking on a Node will open a context menu. This menu contains many operations that can be performed on the Node. Note that when multiple nodes are selected, these operations will be applied to the entire selection. Item Description Copy Shader Copies the generated HLSL code at this stage in the graph to the clipboard Disconnect All Removes all edges from all ports on the Node(s) Cut Cuts selected Node(s) to the clipboard Copy Copies selected Nodes(s) to the clipboard Paste Pastes Node(s) in the clipboard Delete Deletes selected Node(s) Duplicate Duplicates selected Node(s) Convert To Sub-graph Creates a new Sub-graph Asset with the selected Node(s) included Convert To Inline Node Converts a Property Node into a regular node of the appropriate Data Type Convert To Property Converts a Node into a new Property on the Blackboard of the appropriate Property Type Open Documentation Opens a new web browser to the selected Nodes documentation page in the Node Library Color Mode Nodes interact with the Shader Graph Window's Color Modes. Colors are displayed on nodes underneath the text on the node title bar. See Color Modes for more information on available colors for nodes."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Noise-Sine-Wave-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Noise-Sine-Wave-Node.html",
    "title": "Noise Sine Wave Node | FSM Unity Framework",
    "keywords": "Noise Sine Wave Node Description Returns the sine of the value of input In. For variance, psuedo-random noise is added to the amplitude of the sine wave, within a range determined by input Min Max. Ports Name Direction Type Description In Input Dynamic Vector Input value Min Max Input Vector 2 Minimum and Maximum values for noise intensity Out Output Dynamic Vector Output value Generated Code Example void Unity_NoiseSineWave_float4(float4 In, float2 MinMax, out float4 Out) { float sinIn = sin(In); float sinInOffset = sin(In + 1.0); float randomno = frac(sin((sinIn - sinInOffset) * (12.9898 + 78.233))*43758.5453); float noise = lerp(MinMax.x, MinMax.y, randomno); Out = sinIn + noise; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Blend-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Blend-Node.html",
    "title": "Normal Blend Node | FSM Unity Framework",
    "keywords": "Normal Blend Node Description Blends two normal maps defined by inputs A and B together, normalizing the result to create a valid normal map. Ports Name Direction Type Binding Description A Input Vector 3 None First input value B Input Vector 3 None Second input value Out Output Vector 3 None Output value Controls Name Type Options Description Mode Dropdown Default, Reoriented Selects the the method used for blending. Generated Code Example The following example code represents one possible outcome of this node per Mode. Default void Unity_NormalBlend_float(float3 A, float3 B, out float3 Out) { Out = normalize(float3(A.rg + B.rg, A.b * B.b)); } Reoriented void Unity_NormalBlend_Reoriented_float(float3 A, float3 B, out float3 Out) { float3 t = A.xyz + float3(0.0, 0.0, 1.0); float3 u = B.xyz * float3(-1.0, -1.0, 1.0); Out = (t / t.z) * dot(t, u) - u; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-From-Height-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-From-Height-Node.html",
    "title": "Normal From Height Node | FSM Unity Framework",
    "keywords": "Normal From Height Node Description Creates a normal map from a height value defined by input Input with a strength defined by input Strength. Ports Name Direction Type Description In Input Float Input height value Strength Input Float The strength of the output normal. Considered in real-world units, recommended range is 0 - 0.1 . Out Output Vector 3 Output value Controls Name Type Options Description Output Space Dropdown Tangent, World Sets the coordinate space of the output normal. Generated Code Example The following example code represents one possible outcome of this node per Output Space mode. Tangent void Unity_NormalFromHeight_Tangent_float(float In, float Strength, float3 Position, float3x3 TangentMatrix, out float3 Out) { float3 worldDerivativeX = ddx(Position); float3 worldDerivativeY = ddy(Position); float3 crossX = cross(TangentMatrix[2].xyz, worldDerivativeX); float3 crossY = cross(worldDerivativeY, TangentMatrix[2].xyz); float d = dot(worldDerivativeX, crossY); float sgn = d < 0.0 ? (-1.0f) : 1.0f; float surface = sgn / max(0.000000000000001192093f, abs(d)); float dHdx = ddx(In); float dHdy = ddy(In); float3 surfGrad = surface * (dHdx*crossY + dHdy*crossX); Out = normalize(TangentMatrix[2].xyz - (Strength * surfGrad)); Out = TransformWorldToTangent(Out, TangentMatrix); } World void Unity_NormalFromHeight_World_float(float In, float Strength, float3 Position, float3x3 TangentMatrix, out float3 Out) { float3 worldDerivativeX = ddx(Position); float3 worldDerivativeY = ddy(Position); float3 crossX = cross(TangentMatrix[2].xyz, worldDerivativeX); float3 crossY = cross(worldDerivativeY, TangentMatrix[2].xyz); float d = dot(worldDerivativeX, crossY); float sgn = d < 0.0 ? (-1.0f) : 1.0f; float surface = sgn / max(0.000000000000001192093f, abs(d)); float dHdx = ddx(In); float dHdy = ddy(In); float3 surfGrad = surface * (dHdx*crossY + dHdy*crossX); Out = normalize(TangentMatrix[2].xyz - (Strength * surfGrad)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-From-Texture-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-From-Texture-Node.html",
    "title": "Normal From Texture Node | FSM Unity Framework",
    "keywords": "Normal From Texture Node Description Converts a height map defined by input Texture into a normal map. UV values and sampler state can be defined by inputs UV and Sampler respectively. If nothing is connected to these ports they will use default values from the inputs. See Port Bindings for more information. The strength of the created normal map can be defined by inputs Offset and Strength, where Offset defines the maximum distance of a normal detail and Strength acts as a multiplier to the result. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading to version 10.3 or later. Ports Name Direction Type Binding Description Texture Input Texture None Height map UV Input Vector 2 UV Texture coordinates Sampler Input Sampler State None Sampler for Texture Offset Input Float None Amount to offset samples Strength Input Float None Strength multiplier Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_NormalFromTexture_float(Texture texture, SamplerState Sampler, float2 UV, float Offset, float Strength, out float3 Out) { Offset = pow(Offset, 3) * 0.1; float2 offsetU = float2(UV.x + Offset, UV.y); float2 offsetV = float2(UV.x, UV.y + Offset); float normalSample = Texture.Sample(Sampler, UV); float uSample = Texture.Sample(Sampler, offsetU); float vSample = Texture.Sample(Sampler, offsetV); float3 va = float3(1, 0, (uSample - normalSample) * Strength); float3 vb = float3(0, 1, (vSample - normalSample) * Strength); Out = normalize(cross(va, vb)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Reconstruct-Z-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Reconstruct-Z-Node.html",
    "title": "Normal Reconstruct Z Node | FSM Unity Framework",
    "keywords": "Normal Reconstruct Z Node Description Derives the correct Z value for generated normal maps using a given X and Y value from input In. Ports Name Direction Type Description In Input Vector 2 Normal X and Y value Out Output Vector 3 Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_NormalReconstructZ_float(float2 In, out float3 Out) { float reconstructZ = sqrt(1.0 - saturate(dot(In.xy, In.xy))); float3 normalVector = float3(In.x, In.y, reconstructZ); Out = normalize(normalVector); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Strength-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Strength-Node.html",
    "title": "Normal Strength Node | FSM Unity Framework",
    "keywords": "Normal Strength Node Description Adjusts the strength of the normal map defined by input In by the amount of input Strength. A Strength value of 1 will return the input unaltered. A Strength value of 0 will return a blank normal map. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Strength Input Float None Strength value Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_NormalStrength_float(float3 In, float Strength, out float3 Out) { Out = {precision}3(In.rg * Strength, lerp(1, In.b, saturate(Strength))); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Unpack-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Unpack-Node.html",
    "title": "Normal Unpack Node | FSM Unity Framework",
    "keywords": "Normal Unpack Node Description Unpacks a normal map defined by input In. This node is used to unpack a texture that is defined as a Normal Map in its Texture Import Settings when it is sampled as if it were a default texture. Note that in most cases this node is unnecessary as the normal map should be sampled as such by setting its Type parameter to Normal when it is sampled using a Sample Texture 2D or Triplanar node. Ports Name Direction Type Binding Description In Input Vector 4 None Input value Out Output Vector 3 None Output value Controls Name Type Options Description Space Dropdown Tangent, Object Sets the coordinate space of the input normal. Generated Code Example The following example code represents one possible outcome of this node per Space mode. Tangent void Unity_NormalUnpack_float(float4 In, out float3 Out) { Out = UnpackNormalmapRGorAG(In); } Object void Unity_NormalUnpackRGB_float(float4 In, out float3 Out) { Out = UnpackNormalmapRGB(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Vector-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normal-Vector-Node.html",
    "title": "| FSM Unity Framework",
    "keywords": "Description Provides access to the mesh vertex or fragment's Normal Vector. The coordinate space of the output value can be selected with the Space dropdown parameter. Ports Name Direction Type Binding Description Out Output Vector 3 None Mesh's Normal Vector. Parameters Name Type Options Description Space Dropdown Object, View, World, Tangent Selects coordinate space of Normal Vector to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normalize-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Normalize-Node.html",
    "title": "Normalize Node | FSM Unity Framework",
    "keywords": "Normalize Node Description Returns the normalized value of input In. The output vector will have the same direction as input In but a length of 1. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Normalize_float4(float4 In, out float4 Out) { Out = normalize(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Not-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Not-Node.html",
    "title": "Not Node | FSM Unity Framework",
    "keywords": "Not Node Description Returns the opposite of input In. If In is true the output will be false, otherwise it will be true. This is useful for Branching. Ports Name Direction Type Binding Description In Input Boolean None Input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_NormalUnpack_float(float In, out float Out) { Out = !In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Object-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Object-Node.html",
    "title": "Object Node | FSM Unity Framework",
    "keywords": "Object Node Description Provides access to various parameters of the currently rendering Object. Note: The behaviour of the Position Port can be defined per Render Pipeline. Different Render Pipelines may produce different results. If you're building a shader in one Render Pipeline that you want to use in both, try checking it in both pipelines before production. Unity Render Pipelines Support Universal Render Pipeline High Definition Render Pipeline Ports Name Direction Type Binding Description Position Output Vector 3 None Object position in world space Scale Output Vector 3 None Object scale in world space World Bounds Min Output Vector 3 None Minimum value of the renderer bounds in world space World Bounds Max Output Vector 3 None Maximum value of the renderer bounds in world space Bounds Size Output Vector 3 None Size of the renderer bounds Note: the bounds values are the equivalent of the bounds in the renderer component. This means that vertex deformation done in ShaderGraph doesn't affect these values. Generated Code Example The following example code represents one possible outcome of this node. float3 _Object_Position = SHADERGRAPH_OBJECT_POSITION; float3 _Object_Scale = float3(length(float3(UNITY_MATRIX_M[0].x, UNITY_MATRIX_M[1].x, UNITY_MATRIX_M[2].x)), length(float3(UNITY_MATRIX_M[0].y, UNITY_MATRIX_M[1].y, UNITY_MATRIX_M[2].y)), length(float3(UNITY_MATRIX_M[0].z, UNITY_MATRIX_M[1].z, UNITY_MATRIX_M[2].z))); float3 _Object_WorldBoundsMin = SHADERGRAPH_RENDERER_BOUNDS_MIN; float3 _Object_WorldBoundsMax = SHADERGRAPH_RENDERER_BOUNDS_MAX; float3 _Object_BoundsSize = (SHADERGRAPH_RENDERER_BOUNDS_MAX - SHADERGRAPH_RENDERER_BOUNDS_MIN);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/One-Minus-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/One-Minus-Node.html",
    "title": "One Minus Node | FSM Unity Framework",
    "keywords": "One Minus Node Description Returns the result of input In subtracted from 1. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_OneMinus_float4(float4 In, out float4 Out) { Out = 1 - In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Or-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Or-Node.html",
    "title": "Or Node | FSM Unity Framework",
    "keywords": "Or Node Description Returns true if either of the inputs A and B are true. This is useful for Branching. Ports Name Direction Type Binding Description A Input Boolean None First input value B Input Boolean None Second input value Out Output Boolean None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Or_float(float In, out float Out) { Out = A || B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Pack-Vertex-Data-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Pack-Vertex-Data-Water-Node.html",
    "title": "Pack Water Vertex Data | FSM Unity Framework",
    "keywords": "Pack Water Vertex Data This node packs multiple water properties into two UV properties for the vertex context. The High Definition Render Pipeline (HDRP) uses this node in the default water shader graph. Don't modify the settings of this node. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Pack Water Vertex Data No Yes Ports Name Direction Type Description PositionWS Input Vector3 The position of the water surface vertex in world space. Displacement Input Vector3 The vertical and horizontal displacement of the water. LowFrequencyHeight Input Float The vertical displacement of the water surface. This doesn't include ripples. SSSMask Input Float Mask that defines where the water surface has subsurface scattering. PositionOS Output Vector3 The position of the water surface vertex in object space. NormalOS Output Vector3 The water surface normal in object space. uv0 Output Vector4 The inputs packed into a UV coordinate set for the vertex context. uv1 Output Vector4 The inputs packed into a UV coordinate set for the vertex context."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Parallax-Mapping-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Parallax-Mapping-Node.html",
    "title": "Parallax Mapping Node | FSM Unity Framework",
    "keywords": "Parallax Mapping Node Description The Parallax Mapping node lets you create a parallax effect that displaces a Material's UVs to create the illusion of depth inside a Material. This implementation uses the single step process that does not account for occlusion. For information on how the effect looks, see the Height Map page. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading to version 10.3 or later. Ports Name Direction Type Description Heightmap Input Texture2D The Texture that specifies the depth of the displacement. Heightmap Sampler Input Sampler State The Sampler to sample Heightmap with. Amplitude Input Float A multiplier to apply to the height of the Heightmap (in centimeters). UVs Input Vector2 The UVs that the sampler uses to sample the Texture. Parallax UVs Output Vector2 The UVs after adding the parallax offset. Generated Code Example The following example code represents one possible outcome of this node. float2 _ParallaxMapping_ParallaxUVs = UVs.xy + ParallaxMapping(Heightmap, Heightmap_Sampler, IN.TangentSpaceViewDirection, Amplitude * 0.01, UVs.xy);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Parallax-Occlusion-Mapping-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Parallax-Occlusion-Mapping-Node.html",
    "title": "Parallax Occlusion Mapping Node | FSM Unity Framework",
    "keywords": "Parallax Occlusion Mapping Node Description You can use the Parallax Occlusion Mapping (POM) node to create a parallax effect that displaces a material's UVs and depth to create the illusion of depth inside that material. If you receive a texture sampling error while using this node in a graph that includes Custom Function nodes or Subgraphs, try upgrading to Shader Graph version 10.3 or later. This may resolve the errors. When you assign the same Texture2D to a POM node and a Sample Texture 2D node, you need to avoid transforming the UV coordinates twice. To prevent this, connect the Split Texture Transform node’s Texture Only port to the Sample Texture 2D Node’s UV port. Ports Name Direction Type Description Heightmap Input Texture2D The Texture that specifies the depth of the displacement. Heightmap Sampler Input Sampler State The Sampler to sample Heightmap with. Amplitude Input Float A multiplier to apply to the height of the Heightmap (in centimeters). Steps Input Float The number of steps that the linear search of the algorithm performs. UVs Input Vector2 The UVs that the sampler uses to sample the Texture. Tiling Input Vector2 The tiling to apply to the input UVs. Offset Input Vector2 The offset to apply to the input UVs. Primitive Size Input Vector2 Size of the UV space in object space. For example, a Unity built-in Plane mesh has a primitive size of (10,10). LOD Input Float The level of detail to use to sample the Heightmap. This value should always be positive. LOD Threshold Input Float The Heightmap mip level where the Parallax Occlusion Mapping effect begins to fade out. This is equivalent to the Fading Mip Level Start property in the High Definition Render Pipeline's (HDRP) Lit Material. Pixel Depth Offset Output Float The offset to apply to the depth buffer to produce the illusion of depth. Connect this output to the Depth Offset on the Master Node to enable effects that rely on the depth buffer, such as shadows and screen space ambient occlusion. Parallax UVs Output Vector2 UVs that you have added the parallax offset to. Generated Code Example The following example code represents one possible outcome of this node. float3 ParallaxOcclusionMapping_ViewDir = IN.TangentSpaceViewDirection * GetDisplacementObjectScale().xzy; float ParallaxOcclusionMapping_NdotV = ParallaxOcclusionMapping_ViewDir.z; float ParallaxOcclusionMapping_MaxHeight = Amplitude * 0.01; ParallaxOcclusionMapping_MaxHeight *= 2.0 / ( abs(Tiling.x) + abs(Tiling.y) ); float2 ParallaxOcclusionMapping_UVSpaceScale = ParallaxOcclusionMapping_MaxHeight * Tiling / PrimitiveSize; // Transform the view vector into the UV space. float3 ParallaxOcclusionMapping_ViewDirUV = normalize(float3(ParallaxOcclusionMapping_ViewDir.xy * ParallaxOcclusionMapping_UVSpaceScale, ParallaxOcclusionMapping_ViewDir.z)); // TODO: skip normalize PerPixelHeightDisplacementParam ParallaxOcclusionMapping_POM; ParallaxOcclusionMapping_POM.uv = UVs.xy; float ParallaxOcclusionMapping_OutHeight; float2 _ParallaxOcclusionMapping_ParallaxUVs = UVs.xy + ParallaxOcclusionMapping(Lod, Lod_Threshold, Steps, ParallaxOcclusionMapping_ViewDirUV, ParallaxOcclusionMapping_POM, ParallaxOcclusionMapping_OutHeight); float _ParallaxOcclusionMapping_PixelDepthOffset = (ParallaxOcclusionMapping_MaxHeight - ParallaxOcclusionMapping_OutHeight * ParallaxOcclusionMapping_MaxHeight) / max(ParallaxOcclusionMapping_NdotV, 0.0001);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Polar-Coordinates-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Polar-Coordinates-Node.html",
    "title": "Polar Coordinates Node | FSM Unity Framework",
    "keywords": "Polar Coordinates Node Description Converts the value of input UV to polar coordinates. In mathematics, the polar coordinate system is a two-dimensional coordinate system in which each point on a plane is determined by a distance from a reference point and an angle from a reference direction. The resulting effect is that the x channel of the input to UV is converted to a distance value from the point specified by the value of input Center and the y channel of same input is converted to the value of an angle of rotation around that point. These values can be scaled by the values of inputs Radial Scale and Length Scale respectively. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Center Input Vector 2 None Center reference point Radial Scale Input Float None Scale of distance value Length Scale Input Float None Scale of angle value Out Output Vector 2 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_PolarCoordinates_float(float2 UV, float2 Center, float RadialScale, float LengthScale, out float2 Out) { float2 delta = UV - Center; float radius = length(delta) * 2 * RadialScale; float angle = atan2(delta.x, delta.y) * 1.0/6.28 * LengthScale; Out = float2(radius, angle); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Polygon-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Polygon-Node.html",
    "title": "Polygon Node | FSM Unity Framework",
    "keywords": "Polygon Node Description Generates a regular polygon shape based on input UV at the size specified by inputs Width and Height. The polygon's amount of sides is determined by input Sides. The generated shape can be offset or tiled by connecting a Tiling And Offset Node. Note that in order to preserve the ability to offset the shape within the UV space the shape will not automatically repeat if tiled. To achieve a repeating polygon effect first connect your input through a Fraction Node. NOTE: This Node can only be used in the Fragment shader stage. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Sides Input Float None Amount of sides Width Input Float None Polygon width Height Input Float None Polygon height Out Output Float None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Polygon_float(float2 UV, float Sides, float Width, float Height, out float Out) { float pi = 3.14159265359; float aWidth = Width * cos(pi / Sides); float aHeight = Height * cos(pi / Sides); float2 uv = (UV * 2 - 1) / float2(aWidth, aHeight); uv.y *= -1; float pCoord = atan2(uv.x, uv.y); float r = 2 * pi / Sides; float distance = cos(floor(0.5 + pCoord / r) * r - pCoord) * length(uv); Out = saturate((1 - distance) / fwidth(distance)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Port-Bindings.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Port-Bindings.html",
    "title": "Port Bindings | FSM Unity Framework",
    "keywords": "Port Bindings Description Some input Ports might have Port Bindings. This means there is an expectation of the data that should be supplied to the Port, such as a Normal Vector or UV. However, a Port Binding only affects a Port that does not have a connected Edge. These Ports still have a regular Data Type that define what Edges can be connected to them. In practice this means that if no Edge is connected to the Port the default data used in that port will be taken from its Port Binding. A full list of Port Bindings and their associated default options is found below. Port Bindings List Name Data Type Options Description Bitangent Vector 3 Vertex or fragment bitangent, label describes expected transform space Color Vector 4 RGBA Color picker ColorRGB Vector 3 RGB Color picker Normal Vector 3 Vertex or fragment normal vector, label describes expected transform space Position Vector 3 Vertex or fragment position, label describes expected transform space Screen Position Vector 4 Default, Raw, Center, Tiled Vertex or fragment position in screen space. Dropdown selects mode. See Screen Position Node for details Tangent Vector 3 Vertex or fragment tangent vector, label describes expected transform space UV Vector 2 UV0, UV1, UV2, UV3 Mesh UV coordinates. Dropdown selects UV channel. Vertex Color Vector 4 RGBA vertex color value. View Direction Vector 3 Vertex or fragment view direction vector, label describes expected transform space"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Port.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Port.html",
    "title": "Port | FSM Unity Framework",
    "keywords": "Port Description A Port defines an input or output on a Node. Connecting Edges to a Port allows data to flow through the Shader Graph node network. Each Port has a Data Type which defines what edges can be connected to it. Each data type has an associated color for identifying its type. Only one edge can be connected to any input Port but multiple edges can be connected to an output Port. You can open a contextual Create Node Menu by dragging an edge from a Port with left mouse button and releasing it in an empty area of the workspace. Default Inputs Each Input Port, a Port on the left side of a node implying that it is for inputting data into the node, has a Default Input. This appears as a small field connected to the Port when there is no edge connected. This field will display an input for the ports data type unless the Port has a Port Binding. If a Port does have a port binding the default input field might display a special field, such as a dropdown for selecting UV channels, or just a label to help you understand the intended input, such as coordinate space labels for geometry data."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Position-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Position-Node.html",
    "title": "Position Node | FSM Unity Framework",
    "keywords": "Position Node Description Provides access to the mesh vertex's or fragment's Position, depending on the effective Shader Stage of the graph section that the Node is part of. Use the Space drop-down parameter to select the coordinate space of the output value. Ports Name Direction Type Binding Description Out Output Vector 3 None Position for the Mesh Vertex/Fragment. Controls Name Type Options Description Space Dropdown Object, View, World, Tangent, Absolute World Selects the coordinate space of Position to output. World and Absolute World The Position Node provides drop-down options for both World and Absolute World space positions. The Absolute World option always returns the absolute world position of the object in the Scene for all Scriptable Render Pipelines. The World option returns the default world space of the selected Scriptable Render Pipeline. The High Definition Render Pipeline uses Camera Relative as its default world space. The Universal Render Pipeline uses Absolute World as its default world space. Upgrading from previous versions If you use a Position Node in World space on a graph authored in Shader Graph version 6.7.0 or earlier, it automatically upgrades the selection to Absolute World. This ensures that the calculations on your graph remain accurate to your expectations, since the World output might change. If you use a Position Node in World space in the High Definition Render Pipeline to manually calculate Camera Relative world space, you can now change your node from Absolute World to World, which lets you use Camera Relative world space out of the box."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Posterize-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Posterize-Node.html",
    "title": "Posterize Node | FSM Unity Framework",
    "keywords": "Posterize Node Description Posterization or posterisation of an image entails conversion of a continuous gradation of tone to several regions of fewer tones, with abrupt changes from one tone to another. https://en.wikipedia.org/wiki/Posterization This node returns the posterized (also known as quantized) value of the input In into an amount of values specified by input Steps. Ports Name Direction Type Description In Input Dynamic Vector Input value Steps Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Posterize_float4(float4 In, float4 Steps, out float4 Out) { Out = floor(In / (1 / Steps)) * (1 / Steps); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Power-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Power-Node.html",
    "title": "Power Node | FSM Unity Framework",
    "keywords": "Power Node Description Returns the result of input A to the power of input B. Note: If the input A is negative, the output might be inconsistent or NaN. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Power_float4(float4 A, float4 B, out float4 Out) { Out = pow(A, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Precision-Modes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Precision-Modes.html",
    "title": "Precision Modes | FSM Unity Framework",
    "keywords": "Precision Modes Description Shader Graph provides specific data precision modes for nodes, graphs, and Sub Graphs to help you optimize your content for different platforms. To set the precision of an entire graph, select the Graph Settings tab in the Graph Inspector and adjust the Precision control. Select a node in your graph and select the Node Settings tab in the Graph Inspector to adjust the precision of individual nodes. Precision mode settings Name Description Single This is a high-precision floating point value. The number of bits is platform-specific. For modern desktop computers, it is 32 bits. This mode is useful for world space positions, texture coordinates, and scalar computations that involve complex functions such as trigonometry, power, and exponentiation. Half This is a low-precision floating point value. The number of bits is platform-specific. For modern desktop computers, it is 16 bits. This mode is useful for short vectors, directions, object space positions, and many high dynamic range colors, but not very strong light sources, such as the sun. Switchable This mode is only for Sub Graphs. When you enable this mode for a Sub Graph, the default precision of the Sub Graph is decided by its Sub Graph node. See Use Graph Precision below. Inherit This mode determines a node's precision based on a set of inheritance rules. See Precision inheritance. Use Graph Precision This mode forces this node to use the same precision setting as the graph. If this is a node in a Sub Graph, and that Sub Graph’s Precision is set to Switchable, then the precision of this node is the precision of the Sub Graph node representing this Sub Graph. Using Precision Modes Visualizing Precision in a graph To visualize data precision in a graph, set the Color Mode control to Precision. This applies color coding to your nodes: Single nodes are blue Half nodes are red Switchable nodes are Green. Setting graph Precision To set the default precision for the entire graph to Single or Half, open the Graph Settings and set the Precision property. Newly-created nodes in a graph default to the Inherit precision mode, and inherit the graph's precision. Setting node Precision Select a node to access its precision setting. The precision you set for a node determines the precision of the data types which that node uses for its calculations. Precision Inheritance All nodes use the Inherit precision mode by default. In this mode, a node that has one or more edge connections takes on the precision mode of an incoming edge. Nodes that do not have any edge connections take on Graph Precision. If you change the Graph Precision mode, the precision of those nodes also changes. Inputs on the node Final precision determined by inheritance No inputs Graph Precision Only Half inputs Half Only Single inputs Single Half and Single inputs Single Only Switchable inputs Switchable Switchable and Half inputs Switchable Switchable and Single inputs Single Switchable, Half and Single inputs Single Simple inheritance Simple inheritance refers to the inheritance behaviour of a node with only one precision type on its inputs. In the figure below, Node A has the Inherit mode. Because it has no incoming edge, it takes the Graph Precision, which is Half. Node B also has the Inherit mode, so it inherits the Half precision mode from Node A. Complex inheritance Complex inheritance refers to the inheritance behaviour of a node with multiple precision types on its inputs. A node reads precision settings from each input port. If you connect a node to several others with a variety of precision modes, the node with the highest resolution determines the precision mode for the group. In the figure below, node D has the Inherit mode. It receives input from the adjacent edges via inputs 1 and 2. Node B passes the Half mode through input 1. Node C passes the Single mode through input 2. Because Single is 32-bit and Half only 16-bit, Single takes precedence, so Node D uses Single precision. Mixed inheritance Mixed inheritance refers to the inheritance behaviour on a node with both simple and complex inheritance types. Nodes with no input ports, such as Input nodes, inherit the Graph Precision. However, complex inheritance rules still affect other nodes in the same group, as illustrated in the figure below. Switchable precision The Switchable mode overrides Half mode but not Single. Sub Graph precision Precision behavior and user interface elements for Sub Graphs and their nodes do not differ from other graphs and nodes. Sub Graphs represent a function, and you can affect that function's inputs, outputs, and operators by modifying the relevant set of precision settings. The Sub Graph properties correspond to the function's inputs. The internal node properties correspond to the function's operators. The output node corresponds to the function's outputs. Outputs To manually determine the precision of a Sub Graph's output, modify the Output node’s Precision Mode setting. Inputs To manually determine the precision of Sub Graph Inputs, open the Graph Inspector and set precision modes for each individual Property. Properties that use the Inherit option take on the Graph Precision you set for the Sub Graph. Sub Graph Precision within other graphs By default, a Sub Graph has a Precision Mode of Switchable. You can modify Precision Mode of any Sub Graph node for that Sub Graph, as long as you set the Precision Mode on the Sub Graph as Switchable. Shader Graph won't allow you to change the Precision Mode for any Sub Graph node that doesn't have its Sub Graph set to Switchable. This is because the input and output precision you set in a Sub Graph define the precision of its associated Sub Graph Node. For example, let's say that Sub Graph A is Switchable. You open Graph 1, which includes a Sub Graph Node referencing Sub Graph A. Like all other nodes, Sub Graph Node A defaults to Inherit. You change the precision of Sub Graph Node A to Half. The precision of Sub Graph A also becomes Half."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Preview-Mode-Control.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Preview-Mode-Control.html",
    "title": "Preview mode control | FSM Unity Framework",
    "keywords": "Preview mode control Description This control enables you to manually select your preferred preview mode for a node that has a preview. When you select Inherit in the Preview Mode Control, the Editor automatically selects the preview mode to use. That decision is determined by either the type of the node you are previewing, the Sub Graph setting (if this node is in a Sub Graph) or other upstream nodes. To override the inheritance mode, select Preview 2D or Preview 3D. This mode control functionality also applies to Sub Graph previews. See Graph Settings menu. How to use For nodes: Add a node which includes a preview. Select the node. In the Graph Inspector or Node Settings, find the Preview control. Select an option. For SubGraphs: Select a mode in the Sub Graph Graph Settings menu. Related Preview node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Preview-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Preview-Node.html",
    "title": "Preview Node | FSM Unity Framework",
    "keywords": "Preview Node Description This node enables you to inspect a preview at a specific point in a Shader Graph. It does not modify any input values. By default, the Editor automatically selects a preview mode. That decision is determined by both the type of the node you are previewing and other upstream nodes. With Preview Mode Control, you can manually select your preferred preview mode. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Dynamic Vector None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Preview_float4(float4 In, out float4 Out) { Out = In; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Procedural-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Procedural-Nodes.html",
    "title": "Procedural Nodes | FSM Unity Framework",
    "keywords": "Procedural Nodes Checkerboard Generates a checkerboard of alternating colors between inputs Color A and Color B based on input UV. Noise Gradient Noise Simple Noise Generates a gradient, or Perlin, noise based on input UV. Generates a simple, or Value, noise based on input UV. Voronoi Generates a Voronoi, or Worley, noise based on input UV. Shape Ellipse Polygon Generates an ellipse shape based on input UV at the size specified by inputs Width and Height. Generates a regular polygon shape based on input UV at the size specified by inputs Width and Height. The polygon's amount of sides is determined by input Sides. Rectangle Rounded Rectangle Generates a rectangle shape based on input UV at the size specified by inputs Width and Height. Generates a rounded rectangle shape based on input UV at the size specified by inputs Width and Height. The input Radius defines the radius of each corner. Rounded Polygon Generates a rounded polygon shape based on input UV at the size specified by inputs Width and Height. The input Sides specifies the number of sides, and the input Roundness defines the roundness of each corner."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Projection-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Projection-Node.html",
    "title": "Projection Node | FSM Unity Framework",
    "keywords": "Projection Node Description Returns the result of projecting the value of input A onto a straight line parallel to the value of input B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Projection_float4(float4 A, float4 B, out float4 Out) { Out = B * dot(A, B) / dot(B, B); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Property-Types.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Property-Types.html",
    "title": "Property Types | FSM Unity Framework",
    "keywords": "Property Types Description Property Types are the types of Property than can be defined on the Blackboard for use in the Graph. These Properties are exposed to the Inspector for Materials that use the shader. Each property has an associated Data Type. See Data Types for more information. Common Parameters In addition to values specific to their Data Types, most properties have the following common parameters. Name Type Description Display Name String The display name of the property Exposed Boolean If true this property will be exposed on the material inspector Reference Name String The internal name used for the property inside the shader Override Property Declaration Boolean An advanced option to enable explicit control of the shader declaration for this property Shader Declaration Enumeration Controls the shader declaration of this property NOTE: If you overwrite the Reference Name parameter be aware of the following conditions: If your Reference Name does not begin with an underscore, one will be automatically appended. If your Reference Name contains any characters which are unsupported in HLSL they will be removed. You can revert to the default Reference Name by right clicking on it and selecting Reset Reference. Float Defines a Float value. Data Type Modes Float Default, Slider, Integer Default Displays a scalar input field in the material inspector. Field Type Description Default Float The default value of the Property. Slider Displays a slider field in the material inspector. Field Type Description Default Float The default value of the Property. Min Float The minimum value of the slider. Max Float The maximum value of the slider. Integer Displays an integer input field in the material inspector. Field Type Description Default Integer The default value of the Property. Vector 2 Defines a Vector 2 value. Displays a Vector 4 input field in the material inspector, where the z and w components are not used. Data Type Modes Vector 2 Field Type Description Default Vector 2 The default value of the Property. Vector 3 Defines a Vector 3 value. Displays a Vector 4 input field in the material inspector, where the w component is not used. Data Type Modes Vector 3 Field Type Description Default Vector 3 The default value of the Property. Vector 4 Defines a Vector 4 value. Displays a Vector 4 input field in the material inspector. Data Type Modes Vector 4 Field Type Description Default Vector 4 The default value of the Property. Color Defines a Color value. If the Property Inspector displays Main Color, this is the Main Color for the shader. To select or deselect this node as the Main Color, right-click it in the graph or Blackboard and select Set as Main Color or Clear Main Color. Corresponds to the MainColor ShaderLab Properties attribute. Data Type Modes Color Default, HDR Default Displays an sRGB color field in the material inspector. Field Type Description Default Vector 4 The default value of the Property. HDR Displays an HDR color field in the material inspector. Field Type Description Default Vector 4 The default value of the Property. NOTE: In versions prior to 10.0, Shader Graph didn't correct HDR colors for the project colorspace. Version 10.0 corrected this behavior. HDR color properties that you created with older versions maintain the old behavior, but you can use the Graph Inspector to upgrade them. To mimic the old behavior in a gamma space project, you can use the Colorspace Conversion Node to convert a new HDR Color property from RGB to Linear space. Texture 2D Defines a Texture 2D value. Displays an object field of type Texture in the material inspector. If the Property Inspector displays Main Texture, this is the Main Texture for the shader. To select or deselect this node as the Main Texture, right-click on it in the graph or Blackboard and select Set as Main Texture or Clear Main Texture. Corresponds to the MainTexture ShaderLab Properties attribute. Data Type Modes Texture White, Black, Grey, Bump Field Type Description Default Texture The default value of the Property. Use Tiling and Offset Boolean When set to false, activates the property NoScaleOffset, to enable manipulation of scale and offset separately from other texture properties. See SplitTextureTransformNode. Texture 3D Defines a Texture 3D value. Displays an object field of type Texture 3D in the material inspector. Data Type Modes Texture Field Type Description Default Texture The default value of the Property. Texture 2D Array Defines a Texture 2D Array value. Displays an object field of type Texture 2D Array in the material inspector. Data Type Modes Texture Field Type Description Default Texture The default value of the Property. Cubemap Defines a Cubemap value. Displays an object field of type Texture in the material inspector. Data Type Modes Cubemap | Field | Type | Description | |:-------------|:------|:------------| | Default | Cubemap | The default value of the Property. | Virtual Texture Defines a Texture Stack, which appears as object fields of type Texture in the Material Inspector. The number of fields correspond to the number of layers in the property. Data Type Modes Virtual Texture Field Type Description Default Texture The default value of the Property. Boolean Defines a Boolean value. Displays a ToggleUI field in the material inspector. Note that internally to the shader this value is a Float. The Boolean type in Shader Graph is merely for usability. Data Type Modes Boolean Field Type Description Default Boolean The default value of the Property."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Radial-Shear-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Radial-Shear-Node.html",
    "title": "Radial Shear Node | FSM Unity Framework",
    "keywords": "Radial Shear Node Description Applies a radial shear warping effect similar to a wave to the value of input UV. The center reference point of the warping effect is defined by input Center and the overall strength of the effect is defined by the value of input Strength. Input Offset can be used to offset the individual channels of the result. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Center Input Vector 2 None Center reference point Strength Input Float None Strength of the effect Offset Input Vector 2 None Individual channel offsets Out Output Vector 2 None Output UV value Generated Code Example The following example code represents one possible outcome of this node. void Unity_RadialShear_float(float2 UV, float2 Center, float Strength, float2 Offset, out float2 Out) { float2 delta = UV - Center; float delta2 = dot(delta.xy, delta.xy); float2 delta_offset = delta2 * Strength; Out = UV + float2(delta.y, -delta.x) * delta_offset + Offset; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Radians-To-Degrees-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Radians-To-Degrees-Node.html",
    "title": "Radians To Degrees Node | FSM Unity Framework",
    "keywords": "Radians To Degrees Node Description Returns the value of input In converted from radians to degrees. One radian is equivalent to approximately 57.2958 degrees and a full rotation of 2 Pi radians is equal to 360 degrees. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_RadiansToDegrees_float4(float4 In, out float4 Out) { Out = degrees(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Random-Range-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Random-Range-Node.html",
    "title": "Random Range Node | FSM Unity Framework",
    "keywords": "Random Range Node Description Returns a pseudo-random number value based on input Seed that is between the minimum and maximum values defined by inputs Min and Max respectively. Whilst the same value in input Seed will always result in the same output value, the output value itself will appear random. Input Seed is a Vector 2 value for the convenience of generating a random number based on a UV input, however for most cases a Float input will suffice. Ports Name Direction Type Description Seed Input Vector 2 Seed value used for generation Min Input Float Minimum value Max Input Float Maximum value Out Output Float Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_RandomRange_float(float2 Seed, float Min, float Max, out float Out) { float randomno = frac(sin(dot(Seed, float2(12.9898, 78.233)))*43758.5453); Out = lerp(Min, Max, randomno); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reciprocal-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reciprocal-Node.html",
    "title": "Reciprocal Node | FSM Unity Framework",
    "keywords": "Reciprocal Node Description Returns the result of dividing 1 by the input In. This can be calculated by a fast approximation on Shader Model 5 by setting Method to Fast. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Controls Name Type Options Description Method Dropdown Default, Fast Selects the method used Generated Code Example The following example code represents one possible outcome of this node per Method mode. Default void Unity_Reciprocal_float4(float4 In, out float4 Out) { Out = 1.0/In; } Fast (Requires Shader Model 5) void Unity_Reciprocal_Fast_float4(float4 In, out float4 Out) { Out = rcp(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reciprocal-Square-Root-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reciprocal-Square-Root-Node.html",
    "title": "Reciprocal Square Root Node | FSM Unity Framework",
    "keywords": "Reciprocal Square Root Node Description Returns the result of 1 divided by the square root of the input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_ReciprocalSquareRoot_float4(float4 In, out float4 Out) { Out = rsqrt(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rectangle-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rectangle-Node.html",
    "title": "Rectangle Node | FSM Unity Framework",
    "keywords": "Rectangle Node Description Generates a rectangle shape based on input UV at the size specified by inputs Width and Height. The generated shape can be offset or tiled by connecting a Tiling And Offset Node. Note that in order to preserve the ability to offset the shape within the UV space the shape will not automatically repeat if tiled. To achieve a repeating rectangle effect first connect your input through a Fraction Node. NOTE: This Node can only be used in the Fragment Shader Stage. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Width Input Float None Rectangle width Height Input Float None Rectangle height Out Output Float None Output value Controls Name Type Options Description Dropdown Fastest, Nicest Robustness of computation Generated Code Example The following example code represents one possible outcome of this node. void Unity_Rectangle_float(float2 UV, float Width, float Height, out float Out) { float2 d = abs(UV * 2 - 1) - float2(Width, Height); d = 1 - d / fwidth(d); Out = saturate(min(d.x, d.y)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reflection-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reflection-Node.html",
    "title": "Reflection Node | FSM Unity Framework",
    "keywords": "Reflection Node Description Returns a reflection vector using input In and a surface normal Normal. Ports Name Direction Type Description In Input Dynamic Vector Incident vector value Normal Input Dynamic Vector Normal vector value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Reflection_float4(float4 In, float4 Normal, out float4 Out) { Out = reflect(In, Normal); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reflection-Probe-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Reflection-Probe-Node.html",
    "title": "Reflection Probe Node | FSM Unity Framework",
    "keywords": "Reflection Probe Node Description Provides access to the nearest Reflection Probe to the object. Requires Normal and View Direction to sample the probe. You can achieve a blurring effect by sampling at a different Level of Detail using the LOD input. Note: The behavior of this Node is undefined globally. Shader Graph does not define the function of the node. Instead, each Render Pipeline defines what HLSL code to execute for this Node. Different Render Pipelines may produce different results. If you're building a shader in one Render Pipeline that you want to use in both, try checking it in both pipelines before production. A Node might be defined in one Render Pipeline and undefined in the other. If this Node is undefined, it returns 0 (black). Unity Render Pipelines Support Universal Render Pipeline The High Definition Render Pipeline does not support this Node. Ports Name Direction Type Binding Description View Dir Input Vector 3 View Direction (object space) Mesh's view direction Normal Input Vector 3 Normal (object space) Mesh's normal vector LOD Input Float None Level of detail for sampling Out Output Vector 3 None Output color value Generated Code Example The following example code represents one possible outcome of this node. void Unity_ReflectionProbe_float(float3 ViewDir, float3 Normal, float LOD, out float3 Out) { Out = SHADERGRAPH_REFLECTION_PROBE(ViewDir, Normal, LOD); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Refract-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Refract-Node.html",
    "title": "Refract Node | FSM Unity Framework",
    "keywords": "Refract Node Description You can use the Refract node to give a shader a refraction effect. The Refract node generates a refraction using the following to produce a new refracted vector: A normalized incident vector. A normalized normal vector of the surface. The refractive index of the source and the medium. The Refract node uses the principles described in Snell's Law. A medium's refractive index has an angle where the surface behaves like a perfect mirror. This angle is called total internal reflection. To avoid a NaN result, set the Refract node's Mode to Safe. This makes the Refract node generate a null vector when it reaches the critical angle before total internal reflection. Ports Name Direction Type Binding Description Incident Input Vector None The normalized vector from the light source to the surface. For example, this could be from a light source to a pixel, or from the Camera to a surface. Normal Input Vector None The normalized normal of the surface that causes the refraction. IOR Source Input Float None The refractive index of the medium the light source originates in. IOR Medium Input Float None The refractive index of the medium that the light refracts into. Refracted Output Vector None The refracted vector. Intensity Output Float None Intensity of the refraction. Controls Name Type Options Description Mode Dropdown • Safe: Returns a null vector result instead of a NaN result at the point of critical angle refraction. • CriticalAngle: Avoids the Safe check for a potential NaN result. Generated Code Example The following example code represents one possible outcome of this node. void Unity_RefractCriticalAngle(float3 Incident, float3 Normal, float IORInput, float IORMedium, out float Out) { $precision internalIORInput = max(IORInput, 1.0); $precision internalIORMedium = max(IORMedium, 1.0); $precision eta = internalIORInput/internalIORMedium; $precision cos0 = dot(Incident, Normal); $precision k = 1.0 - eta*eta*(1.0 - cos0*cos0); Refracted = k >= 0.0 ? eta*Incident - (eta*cos0 + sqrt(k))*Normal : reflect(Incident, Normal); Intensity = internalIORSource <= internalIORMedium ?; saturate(F_Transm_Schlick(IorToFresnel0(internalIORMedium, internalIORSource), -cos0)) : (k >= 0.0 ? saturate(F_FresnelDielectric(internalIORMedium/internalIORSource, -cos0)) : 0.0); } void Unity_RefractSafe(float3 Incident, float3 Normal, float IORInput, float IORMedium, out float Out) { $precision internalIORInput = max(IORInput, 1.0); $precision internalIORMedium = max(IORMedium, 1.0); $precision eta = internalIORInput/internalIORMedium; $precision cos0 = dot(Incident, Normal); $precision k = 1.0 - eta*eta*(1.0 - cos0*cos0); Refracted = eta*Incident - (eta*cos0 + sqrt(max(k, 0.0)))*Normal; Intensity = internalIORSource <= internalIORMedium ?; saturate(F_Transm_Schlick(IorToFresnel0(internalIORMedium, internalIORSource), -cos0)) : (k >= 0.0 ? saturate(F_FresnelDielectric(internalIORMedium/internalIORSource, -cos0)) : 1.0); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rejection-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rejection-Node.html",
    "title": "Rejection Node | FSM Unity Framework",
    "keywords": "Rejection Node Description Returns the result of the projection of the value of input A onto the plane orthogonal, or perpendicular, to the value of input B. The value of the rejection vector is equal to the original vector, the value of input A, minus the value of the Projection of the same inputs. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Rejection_float4(float4 A, float4 B, out float4 Out) { Out = A - (B * dot(A, B) / dot(B, B)) }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Remap-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Remap-Node.html",
    "title": "Remap Node | FSM Unity Framework",
    "keywords": "Remap Node Description Returns a value between the x and y components of input Out Min Max based on the linear interpolation of the value of input In between the x and y components of input In Min Max. Ports Name Direction Type Description In Input Dynamic Vector Input value In Min Max Input Vector 2 Minimum and Maximum values for input interpolation Out Min Max Input Vector 2 Minimum and Maximum values for output interpolation Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Remap_float4(float4 In, float2 InMinMax, float2 OutMinMax, out float4 Out) { Out = OutMinMax.x + (In - InMinMax.x) * (OutMinMax.y - OutMinMax.x) / (InMinMax.y - InMinMax.x); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Replace-Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Replace-Color-Node.html",
    "title": "Replace Color Node | FSM Unity Framework",
    "keywords": "Replace Color Node Description Replaces values in input In equal to input From to the value of input To. Input Range can be used to define a wider range of values around input From to replace. Input Fuzziness can be used to soften the edges around the selection similar to anti-aliasing. Ports Name Direction Type Binding Description In Input Vector 3 None Input value From Input Vector 3 Color Color to replace To Input Vector 3 Color Color to replace with Range Input Float None Replace colors within this range from input From Fuzziness Input Float None Soften edges around selection Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_ReplaceColor_float(float3 In, float3 From, float3 To, float Range, float Fuzziness, out float3 Out) { float Distance = distance(From, In); Out = lerp(To, In, saturate((Distance - Range) / max(Fuzziness, 1e-5f))); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rotate-About-Axis-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rotate-About-Axis-Node.html",
    "title": "Rotate About Axis Node | FSM Unity Framework",
    "keywords": "Rotate About Axis Node Description Rotates the input vector In around the axis Axis by the value of Rotation. The unit for rotation angle can be selected by the parameter Unit. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Axis Input Vector 3 None Axis to rotate around Rotation Input Float None Amount of rotation to apply Out Output Vector 3 None Output value Controls Name Type Options Description Unit Dropdown Radians, Degrees Switches the unit for input Rotation Generated Code Example The following example code represents one possible outcome of this node per Unit mode. Radians void Unity_RotateAboutAxis_Radians_float(float3 In, float3 Axis, float Rotation, out float3 Out) { float s = sin(Rotation); float c = cos(Rotation); float one_minus_c = 1.0 - c; Axis = normalize(Axis); float3x3 rot_mat = { one_minus_c * Axis.x * Axis.x + c, one_minus_c * Axis.x * Axis.y - Axis.z * s, one_minus_c * Axis.z * Axis.x + Axis.y * s, one_minus_c * Axis.x * Axis.y + Axis.z * s, one_minus_c * Axis.y * Axis.y + c, one_minus_c * Axis.y * Axis.z - Axis.x * s, one_minus_c * Axis.z * Axis.x - Axis.y * s, one_minus_c * Axis.y * Axis.z + Axis.x * s, one_minus_c * Axis.z * Axis.z + c }; Out = mul(rot_mat, In); } Degrees void Unity_RotateAboutAxis_Degrees_float(float3 In, float3 Axis, float Rotation, out float3 Out) { Rotation = radians(Rotation); float s = sin(Rotation); float c = cos(Rotation); float one_minus_c = 1.0 - c; Axis = normalize(Axis); float3x3 rot_mat = { one_minus_c * Axis.x * Axis.x + c, one_minus_c * Axis.x * Axis.y - Axis.z * s, one_minus_c * Axis.z * Axis.x + Axis.y * s, one_minus_c * Axis.x * Axis.y + Axis.z * s, one_minus_c * Axis.y * Axis.y + c, one_minus_c * Axis.y * Axis.z - Axis.x * s, one_minus_c * Axis.z * Axis.x - Axis.y * s, one_minus_c * Axis.y * Axis.z + Axis.x * s, one_minus_c * Axis.z * Axis.z + c }; Out = mul(rot_mat, In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rotate-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rotate-Node.html",
    "title": "Rotate Node | FSM Unity Framework",
    "keywords": "Rotate Node Description Rotates value of input UV around a reference point defined by input Center by the amount of input Rotation. The unit for rotation angle can be selected by the parameter Unit. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Center Input Vector 2 None Center point to rotate around Rotation Input Float None Amount of rotation to apply Out Output Vector 2 None Output UV value Controls Name Type Options Description Unit Dropdown Radians, Degrees Switches the unit for input Rotation Generated Code Example The following example code represents one possible outcome of this node per Unit mode. Radians void Unity_Rotate_Radians_float(float2 UV, float2 Center, float Rotation, out float2 Out) { UV -= Center; float s = sin(Rotation); float c = cos(Rotation); float2x2 rMatrix = float2x2(c, -s, s, c); rMatrix *= 0.5; rMatrix += 0.5; rMatrix = rMatrix * 2 - 1; UV.xy = mul(UV.xy, rMatrix); UV += Center; Out = UV; } Degrees void Unity_Rotate_Degrees_float(float2 UV, float2 Center, float Rotation, out float2 Out) { Rotation = Rotation * (3.1415926f/180.0f); UV -= Center; float s = sin(Rotation); float c = cos(Rotation); float2x2 rMatrix = float2x2(c, -s, s, c); rMatrix *= 0.5; rMatrix += 0.5; rMatrix = rMatrix * 2 - 1; UV.xy = mul(UV.xy, rMatrix); UV += Center; Out = UV; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Round-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Round-Node.html",
    "title": "Round Node | FSM Unity Framework",
    "keywords": "Round Node Description Returns the value of input In rounded to the nearest integer, or whole number. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Round_float4(float4 In, out float4 Out) { Out = round(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rounded-Polygon-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rounded-Polygon-Node.html",
    "title": "Rounded Polygon Node | FSM Unity Framework",
    "keywords": "Rounded Polygon Node Description Generates a rounded polygon shape based on input UV at the size specified by inputs Width and Height. The input Sides specifies the number of sides, and the input Roundness defines the roundness of each corner. You can connect a Tiling And Offset Node to offset or tile the shape. To preserve the ability to offset the shape within the UV space, the shape does not automatically repeat if you tile it. To achieve a repeating rounded polygon effect, first connect your UV input through a Fraction Node. You can only use the Rounded Polygon Node in the Fragment Shader Stage. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Width Input Float None Rounded Polygon width Height Input Float None Rounded Polygon height Sides Input Float None Number of sides of the polygon Roundness Input Float None Roundness of corners Out Output Float None Output value Generated Code Example The following example code represents one possible outcome of this node. void RoundedPolygon_Func_float(float2 UV, float Width, float Height, float Sides, float Roundness, out float Out) { UV = UV * 2. + float2(-1.,-1.); float epsilon = 1e-6; UV.x = UV.x / ( Width + (Width==0)*epsilon); UV.y = UV.y / ( Height + (Height==0)*epsilon); Roundness = clamp(Roundness, 1e-6, 1.); float i_sides = floor( abs( Sides ) ); float fullAngle = 2. * PI / i_sides; float halfAngle = fullAngle / 2.; float opositeAngle = HALF_PI - halfAngle; float diagonal = 1. / cos( halfAngle ); // Chamfer values float chamferAngle = Roundness * halfAngle; // Angle taken by the chamfer float remainingAngle = halfAngle - chamferAngle; // Angle that remains float ratio = tan(remainingAngle) / tan(halfAngle); // This is the ratio between the length of the polygon's triangle and the distance of the chamfer center to the polygon center // Center of the chamfer arc float2 chamferCenter = float2( cos(halfAngle) , sin(halfAngle) )* ratio * diagonal; // starting of the chamfer arc float2 chamferOrigin = float2( 1., tan(remainingAngle) ); // Using Al Kashi algebra, we determine: // The distance distance of the center of the chamfer to the center of the polygon (side A) float distA = length(chamferCenter); // The radius of the chamfer (side B) float distB = 1. - chamferCenter.x; // The refence length of side C, which is the distance to the chamfer start float distCref = length(chamferOrigin); // This will rescale the chamfered polygon to fit the uv space // diagonal = length(chamferCenter) + distB; float uvScale = diagonal; UV *= uvScale; float2 polaruv = float2 ( atan2( UV.y, UV.x ), length(UV) ); polaruv.x += HALF_PI + 2*PI; polaruv.x = fmod( polaruv.x + halfAngle, fullAngle ); polaruv.x = abs(polaruv.x - halfAngle); UV = float2( cos(polaruv.x), sin(polaruv.x) ) * polaruv.y; // Calculate the angle needed for the Al Kashi algebra float angleRatio = 1. - (polaruv.x-remainingAngle) / chamferAngle; // Calculate the distance of the polygon center to the chamfer extremity float distC = sqrt( distA*distA + distB*distB - 2.*distA*distB*cos( PI - halfAngle * angleRatio ) ); Out = UV.x; float chamferZone = ( halfAngle - polaruv.x ) < chamferAngle; Out = lerp( UV.x, polaruv.y / distC, chamferZone ); // Output this to have the shape mask instead of the distance field Out = saturate((1 - Out) / fwidth(Out)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rounded-Rectangle-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Rounded-Rectangle-Node.html",
    "title": "Rounded Rectangle Node | FSM Unity Framework",
    "keywords": "Rounded Rectangle Node Description Generates a rounded rectangle shape based on input UV at the size specified by inputs Width and Height. The radius of each corner is defined by input Radius. The generated shape can be offset or tiled by connecting a Tiling And Offset Node. Note that in order to preserve the ability to offset the shape within the UV space the shape will not automatically repeat if tiled. To achieve a repeating rounded rectangle effect first connect your input through a Fraction Node. NOTE: This Node can only be used in the Fragment Shader Stage. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Width Input Float None Rounded Rectangle width Height Input Float None Rounded Rectangle height Radius Input Float None Corner radius Out Output Float None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_RoundedRectangle_float(float2 UV, float Width, float Height, float Radius, out float Out) { Radius = max(min(min(abs(Radius * 2), abs(Width)), abs(Height)), 1e-5); float2 uv = abs(UV * 2 - 1) - float2(Width, Height) + Radius; float d = length(max(0, uv)) / Radius; Out = saturate((1 - d) / fwidth(d)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Cubemap-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Cubemap-Node.html",
    "title": "Sample Cubemap Node | FSM Unity Framework",
    "keywords": "Sample Cubemap Node Description Samples a Cubemap and returns a Vector 4 color value for use in the shader. Requires a Direction (Dir) input in world space to sample the Cubemap. You can achieve a blurring effect by using the LOD input to sample at a different Level of Detail. You can also use the Sampler input to define a custom Sampler State. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading to version 10.3 or later. Ports Name Direction Type Binding Description Cube Input Cubemap None Cubemap to sample Dir Input Vector 3 Normal (world space) Direction or Mesh's normal vector Sampler Input Sampler State Default sampler state Sampler for the Cubemap LOD Input Float None Level of detail for sampling Out Output Vector 4 None Output value Generated Code Example The following example code represents one possible outcome of this node. float4 _SampleCubemap_Out = SAMPLE_TEXTURECUBE_LOD(Cubemap, Sampler, Dir, LOD);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Gradient-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Gradient-Node.html",
    "title": "Sample Gradient Node | FSM Unity Framework",
    "keywords": "Sample Gradient Node Description Samples a Gradient given the input of Time. Returns a Vector 4 color value for use in the shader. Ports Name Direction Type Binding Description Gradient Input Gradient None Gradient to sample Time Input Float None Point at which to sample gradient (0.0–1.0) Out Output Vector 4 None Output value as Vector4 Generated Code Example The following example code represents one possible outcome of this node. void Unity_SampleGradient_float(float4 Gradient, float Time, out float4 Out) { float3 color = Gradient.colors[0].rgb; [unroll] for (int c = 1; c < 8; c++) { float colorPos = saturate((Time - Gradient.colors[c-1].w) / (Gradient.colors[c].w - Gradient.colors[c-1].w)) * step(c, Gradient.colorsLength-1); color = lerp(color, Gradient.colors[c].rgb, lerp(colorPos, step(0.01, colorPos), Gradient.type)); } #ifndef UNITY_COLORSPACE_GAMMA color = SRGBToLinear(color); #endif float alpha = Gradient.alphas[0].x; [unroll] for (int a = 1; a < 8; a++) { float alphaPos = saturate((Time - Gradient.alphas[a-1].y) / (Gradient.alphas[a].y - Gradient.alphas[a-1].y)) * step(a, Gradient.alphasLength-1); alpha = lerp(alpha, Gradient.alphas[a].x, lerp(alphaPos, step(0.01, alphaPos), Gradient.type)); } Out = float4(color, alpha); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Reflected-Cubemap-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Reflected-Cubemap-Node.html",
    "title": "Sample Reflected Cubemap Node | FSM Unity Framework",
    "keywords": "Sample Reflected Cubemap Node Description Samples a Cubemap with reflected vector and returns a Vector 4 color value for use in the shader. Requires View Direction (View Dir) and Normal inputs to sample the Cubemap. You can achieve a blurring effect by using the LOD input to sample at a different Level of Detail. You can also use the Sampler input to define a custom Sampler State. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading to version 10.3 or later. Ports Name Direction Type Binding Description Cube Input Cubemap None Cubemap to sample View Dir Input Vector 3 View Direction (object space) Mesh's view direction Normal Input Vector 3 Normal (object space) Mesh's normal vector Sampler Input Sampler State Default sampler state Sampler for the Cubemap LOD Input Float None Level of detail for sampling Out Output Vector 4 None Output value Generated Code Example The following example code represents one possible outcome of this node. float4 _SampleCubemap_Out = SAMPLE_TEXTURECUBE_LOD(Cubemap, Sampler, reflect(-ViewDir, Normal), LOD);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-Array-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-Array-Node.html",
    "title": "Sample Texture 2D Array node | FSM Unity Framework",
    "keywords": "Sample Texture 2D Array node The Sample Texture 2D Array node samples a Texture 2D Array asset and returns a Vector 4 color value. You can specify the UV coordinates for a texture sample and use a Sampler State node to define a specific Sampler State. The node's Index input port specifies which index of a Texture 2D Array to sample. For more information about Texture 2D Arrays, see Texture Arrays in the Unity User manual. Note If you experience texture sampling errors for this node in a graph with Custom Function Nodes or Sub Graphs, upgrade to Shader Graph version 10.3 or later. Create Node menu category The Sample Texture 2D Array node is under the Input > Texture category in the Create Node menu. Compatibility The Sample Texture 3D node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes With its default settings, this node can only connect to a Block node in the Fragment Context of a Shader Graph. To sample a Texture for the Vertex Context of a Shader Graph, set the Mip Sampling Mode to LOD. Inputs The Sample Texture 3D node has the following input ports: Name Type Binding Description Texture Array Texture 2D Array None The Texture 2D Array asset to sample. Index Float None The index of the specific Texture in the Texture array to sample. The index value is the Texture's location in the Texture array. The index values in an array always start at 0. An array with four textures would have locations 0, 1, 2, and 3. UV Vector 2 None UV coordinates to use to sample the Texture. Sampler Sampler State Default Sampler State The Sampler State and settings to use to sample the texture. LOD Float LOD NOTE: The LOD Input port only displays if Mip Sampling Mode is LOD. For more information, refer to Additional node settings. The specific mip to use when sampling the Texture. UV Vector 2 UV The UV coordinates to use to sample the texture. Sampler Sampler State Default Sampler State The Sampler State and settings to use to sample the texture. LOD Float LOD The specific mip to use when sampling the Texture. NOTE The LOD Input port only displays if Mip Sampling Mode is LOD. For more information, refer to Additional node settings. Bias Float Bias NOTE: The Bias Input port only displays if Mip Sampling Mode is Bias. For more information, refer to Additional node settings. If Use Global Mip Bias is enabled, Unity adds this Bias amount to the Global Mip Bias for a texture's mip calculation. If Global Mip Bias is disabled, Unity uses this Bias amount instead of the Global Mip Bias. DDX Float DDY NOTE: The DDX Input port only displays if Mip Sampling Mode is Gradient. For more information, refer to Additional node settings. The specific DDX value to use to calculate the texture's mip when sampling. For more information on DDX values for mipmaps, refer to Mipmaps introduction in the Unity User Manual. DDY Float DDY NOTE The DDY Input port only displays if Mip Sampling Mode is Gradient. For more information, refer to Additional node settings. The specific DDY value to use to calculate the texture's mip when sampling. For more information on DDY values for mipmaps, refer to Mipmaps introduction> in the Unity User Manual. Additional node settings The Sample Texture 3D node has some additional settings that you can access from the Graph Inspector: Name Type Description Use Global Mip Bias Toggle Enable Use Global Mip Bias to use the render pipeline's Global Mip Bias. This bias adjusts the percentage of texture information taken from a specific mip when sampling. For more information on mip bias, see Mipmaps introduction in the Unity User Manual. Enabled Shader Graph uses the render pipeline's Global Mip Bias to adjust the texture information taken when sampling. Disabled Shader Graph doesn't use the render pipeline's Global Mip Bias to adjust texture information when sampling. Mip Sampling Mode Dropdown Choose the sampling mode to use to calculate the mip level of the texture. Standard The render pipeline calculates and automatically selects the mip for the texture. LOD The render pipeline lets you set an explicit mip for the texture on the node. The texture will always use this mip, regardless of the DDX or DDY calculations between pixels. Set the Mip Sampling Mode to LOD to connect the node to a Block node in the Vertex Context. For more information on Block nodes and Contexts, see Master Stack. Gradient The render pipeline lets you set the DDX and DDY values to use for its mip calculation, instead of using the values calculated from the texture's UV coordinates. For more information on DDX and DDY values, see Mipmaps introduction in the User Manual. Bias The render pipeline lets you set a bias to adjust the calculated mip for a texture up or down. Negative values bias the mip to a higher resolution. Positive values bias the mip to a lower resolution. The render pipeline can add this value to the value of the Global Mip Bias, or use this value instead of its Global Mip Bias. For more information on mip bias, see Mipmaps introduction in the User Manual. Outputs The Sample Texture 3D node has the following output ports: Name Type Description RGBA Vector 4 The full RGBA Vector 4 color value of the texture sample. R Float The Red channel or X component of the texture sample. G Float The Green channel or Y component of the texture sample. B Float The Blue channel or Z component of the texture sample. A Float The Alpha channel or W component of the texture sample. Example graph usage In the following example, the Sample Texture 2D Array node samples a Texture array that has 4 different cloth normal maps. Change the number given to the Index port as an input, and the Sample Texture 2D Array node can sample a specific normal map from the array. The Index value changes the output the node sends to the Normal Unpack node, and the Normal (Tangent Space) Block node in the Master Stack. Generated code example The following code represents this node in Unity's shader code: float4 _SampleTexture2DArray_RGBA = SAMPLE_TEXTURE2D_ARRAY(Texture, Sampler, UV, Index); float _SampleTexture2DArray_R = _SampleTexture2DArray_RGBA.r; float _SampleTexture2DArray_G = _SampleTexture2DArray_RGBA.g; float _SampleTexture2DArray_B = _SampleTexture2DArray_RGBA.b; float _SampleTexture2DArray_A = _SampleTexture2DArray_RGBA.a; Related nodes The following nodes are related or similar to the Sample Texture 3D node: Sample Texture 2D node Sample Texture 3D node Sampler State node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-LOD-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-LOD-Node.html",
    "title": "Sample Texture 2D LOD Node | FSM Unity Framework",
    "keywords": "Sample Texture 2D LOD Node Description Samples a Texture 2D and returns a Vector 4 color value for use in the shader. You can override the UV coordinates using the UV input and define a custom Sampler State using the Sampler input. Use the LOD input to adjust the level of detail of the sample. To use the Sample Texture 2D LOD Node to sample a normal map, set the Type dropdown parameter to Normal. This Node is useful for sampling a Texture in the vertex Shader Stage as the Sample Texture 2D Node is unavailable in this Shader Stage. On platforms that do not support this operation, opaque black is returned instead. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading to version 10.3 or later. Ports Name Direction Type Binding Description Texture Input Texture 2D None Texture 2D to sample UV Input Vector 2 UV UV coordinates Sampler Input Sampler State Default sampler state Sampler for the texture LOD Input Float None Level of detail to sample RGBA Output Vector 4 None Output value as RGBA R Output Float None red (x) component of RGBA output G Output Float None green (y) component of RGBA output B Output Float None blue (z) component of RGBA output A Output Float None alpha (w) component of RGBA output Controls Name Type Options Description Type Dropdown Default, Normal Selects the texture type Generated Code Example The following example code represents one possible outcome of this node per Type mode. Default float4 _SampleTexture2DLOD_RGBA = SAMPLE_TEXTURE2D_LOD(Texture, Sampler, UV, LOD); float _SampleTexture2DLOD_R = _SampleTexture2DLOD_RGBA.r; float _SampleTexture2DLOD_G = _SampleTexture2DLOD_RGBA.g; float _SampleTexture2DLOD_B = _SampleTexture2DLOD_RGBA.b; float _SampleTexture2DLOD_A = _SampleTexture2DLOD_RGBA.a; Normal float4 _SampleTexture2DLOD_RGBA = SAMPLE_TEXTURE2D_LOD(Texture, Sampler, UV, LOD); _SampleTexture2DLOD_RGBA.rgb = UnpackNormalRGorAG(_SampleTexture2DLOD_RGBA); float _SampleTexture2DLOD_R = _SampleTexture2DLOD_RGBA.r; float _SampleTexture2DLOD_G = _SampleTexture2DLOD_RGBA.g; float _SampleTexture2DLOD_B = _SampleTexture2DLOD_RGBA.b; float _SampleTexture2DLOD_A = _SampleTexture2DLOD_RGBA.a;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-2D-Node.html",
    "title": "Sample Texture 2D node | FSM Unity Framework",
    "keywords": "Sample Texture 2D node The Sample Texture 2D node samples a Texture 2D asset and returns a Vector 4 color value. You can specify the UV coordinates for a texture sample and use a Sampler State node to define a specific Sampler State. A Sample Texture 2D node can also sample a normal map. For more information, see the Controls section, or Normal map (Bump mapping) in the Unity User manual. Note If you experience texture sampling errors for this node in a graph with Custom Function Nodes or Sub Graphs, upgrade to Shader Graph version 10.3 or later. Create Node menu category The Sample Texture 2D node is under the Input > Texture category in the Create Node menu. Compatibility The Sample Texture 2D node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes With its default settings, this node can only connect to a Block node in the Fragment Context of a Shader Graph. To sample a Texture for the Vertex Context of a Shader Graph, set the Mip Sampling Mode to LOD. Inputs The Sample Texture 2D node has the following input ports: Name Type Binding Description Texture Texture 2D None The Texture 2D asset to sample. UV Vector 2 UV The UV coordinates to use to sample the texture. Sampler Sampler State Default Sampler State The Sampler State and settings to use to sample the texture. LOD Float LOD The specific mip to use when sampling the Texture. NOTE The LOD Input port only displays if Mip Sampling Mode is LOD. For more information, refer to Additional node settings. Bias Float Bias NOTE: The Bias Input port only displays if Mip Sampling Mode is Bias. For more information, refer to Additional node settings. If Use Global Mip Bias is enabled, Unity adds this Bias amount to the Global Mip Bias for a texture's mip calculation. If Global Mip Bias is disabled, Unity uses this Bias amount instead of the Global Mip Bias. DDX Float DDY NOTE: The DDX Input port only displays if Mip Sampling Mode is Gradient. For more information, refer to Additional node settings. The specific DDX value to use to calculate the texture's mip when sampling. For more information on DDX values for mipmaps, refer to Mipmaps introduction in the Unity User Manual. DDY Float DDY NOTE The DDY Input port only displays if Mip Sampling Mode is Gradient. For more information, refer to Additional node settings. The specific DDY value to use to calculate the texture's mip when sampling. For more information on DDY values for mipmaps, refer to Mipmaps introduction> in the Unity User Manual. Controls The Sample Texture 2D node has the following controls: Name Type Description Type Dropdown Select whether the texture is a Texture asset or a normal map. Default The texture is a Texture asset. Normal The texture is a normal map. Space Dropdown When the node's Type is Normal to use a texture as a normal map, choose the Space for the normal map. Tangent Use a Tangent normal map whenever the mesh for a geometry needs to deform or change, such as when animating a character. With Tangent Space, the normal map's normals are relative to the existing vertex normals of any geometry rendered with your Shader Graph. Your Shader Graph only adjusts the vertex normals and not override them. Object Use an Object normal map whenever the mesh for a geometry is static and doesn't deform. With Object Space, the normal map's normals are explicit and override the normals of any geometry rendered with your Shader Graph. Because a static mesh's normals never change, an Object normal map also maintains consistent lighting across different levels of detail (LODs). For more information about normal maps, see Normal map (Bump mapping) in the User manual. Additional node settings The Sample Texture 2D node has some additional settings that you can access from the Graph Inspector: Name Type Description Use Global Mip Bias Toggle Enable Use Global Mip Bias to use the render pipeline's Global Mip Bias. This bias adjusts the percentage of texture information taken from a specific mip when sampling. For more information on mip bias, see Mipmaps introduction in the Unity User Manual. Enabled Shader Graph uses the render pipeline's Global Mip Bias to adjust the texture information taken when sampling. Disabled Shader Graph doesn't use the render pipeline's Global Mip Bias to adjust texture information when sampling. Mip Sampling Mode Dropdown Choose the sampling mode to use to calculate the mip level of the texture. Standard The render pipeline calculates and automatically selects the mip for the texture. LOD The render pipeline lets you set an explicit mip for the texture on the node. The texture will always use this mip, regardless of the DDX or DDY calculations between pixels. Set the Mip Sampling Mode to LOD to connect the node to a Block node in the Vertex Context. For more information on Block nodes and Contexts, see Master Stack. Gradient The render pipeline lets you set the DDX and DDY values to use for its mip calculation, instead of using the values calculated from the texture's UV coordinates. For more information on DDX and DDY values, see Mipmaps introduction in the User Manual. Bias The render pipeline lets you set a bias to adjust the calculated mip for a texture up or down. Negative values bias the mip to a higher resolution. Positive values bias the mip to a lower resolution. The render pipeline can add this value to the value of the Global Mip Bias, or use this value instead of its Global Mip Bias. For more information on mip bias, see Mipmaps introduction in the User Manual. Outputs The Sample Texture 2D node has the following output ports: Name Type Description RGBA Vector 4 The full RGBA Vector 4 color value of the texture sample. R Float The Red channel or X component of the texture sample. G Float The Green channel or Y component of the texture sample. B Float The Blue channel or Z component of the texture sample. A Float The Alpha channel or W component of the texture sample. Example graph usage In the following example, the Sample Texture 2D node uses a Subgraph node that generates UV coordinates in latitude and longitude format. These latitude and longitude UV coordinates help render the latlong_test 2D Texture asset, which was created and formatted with a latitude and longitude projection. The generated latitude and longitude UVs accurately map the 2D Texture asset onto a spherical geometry. If the Sample Texture 2D node uses the Standard Mip Sampling Mode, the Texture displays with a seam along the side of the sphere where the left and right sides of the texture meet. The latitude and longitude UV coordinates for sampling the texture jump from 0 to 1 at the seam on the model, which causes a problem with the mip level calculation in the sample. The error in the mip level calculation causes the seam. The texture requires a different mip sampling mode to remove the seam. When the Mip Sampling Mode is set to Gradient, the Sample Texture 2D node can use the standard set of UVs for the model in the mip level calculation, instead of the latitude and longitude UVs needed for sampling the texture. The new UV coordinates passed into the DDX and DDY input ports result in a continuous mip level, and remove the seam. Generated code example The following code represents this node in Unity's shader code, based on the selected Type on the Sample Texture 2D node: Default float4 _SampleTexture2D_RGBA = SAMPLE_TEXTURE2D(Texture, Sampler, UV); float _SampleTexture2D_R = _SampleTexture2D_RGBA.r; float _SampleTexture2D_G = _SampleTexture2D_RGBA.g; float _SampleTexture2D_B = _SampleTexture2D_RGBA.b; float _SampleTexture2D_A = _SampleTexture2D_RGBA.a; Normal float4 _SampleTexture2D_RGBA = SAMPLE_TEXTURE2D(Texture, Sampler, UV); _SampleTexture2D_RGBA.rgb = UnpackNormalmapRGorAG(_SampleTexture2D_RGBA); float _SampleTexture2D_R = _SampleTexture2D_RGBA.r; float _SampleTexture2D_G = _SampleTexture2D_RGBA.g; float _SampleTexture2D_B = _SampleTexture2D_RGBA.b; float _SampleTexture2D_A = _SampleTexture2D_RGBA.a; Related nodes The following nodes are related or similar to the Sample Texture 2D node: Sample Texture 2D Array node Sample Texture 3D node Sampler State node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-3D-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Texture-3D-Node.html",
    "title": "Sample Texture 3D node | FSM Unity Framework",
    "keywords": "Sample Texture 3D node The Sample Texture 3D node samples a Texture 3D asset and returns a Vector 4 color value. You can specify the UV coordinates for a texture sample and use a Sampler State node to define a specific Sampler State. For more information about Texture 3D assets, see 3D textures in the Unity User manual. Note If you experience texture sampling errors for this node in a graph with Custom Function Nodes or Sub Graphs, upgrade to Shader Graph version 10.3 or later. Create Node menu category The Sample Texture 2D node is under the Input > Texture category in the Create Node menu. Compatibility The Sample Texture 3D node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes With its default settings, this node can only connect to a Block node in the Fragment Context of a Shader Graph. To sample a Texture for the Vertex Context of a Shader Graph, set the Mip Sampling Mode to LOD. Inputs The Sample Texture 3D node has the following input ports: Name Type Binding Description Texture Texture 3D None The 3D Texture asset to sample. UV Vector 3 None The 3D UV coordinates to use to sample the Texture. Sampler Sampler State Default Sampler State The Sampler State and settings to use to sample the texture. LOD Float LOD The specific mip to use when sampling the Texture. NOTE The LOD Input port only displays if Mip Sampling Mode is LOD. For more information, refer to Additional node settings. Additional node settings The Sample Texture 3D node has some additional settings that you can access from the Graph Inspector: Name Type Description Mip Sampling Mode Dropdown Choose the sampling mode that the Sample Texture 3D node uses to calculate the mip level of the texture. Standard The mip is calculated and selected automatically for the texture. LOD Set an explicit mip for the texture. The texture will always use this mip, regardless of the DDX or DDY calculations between pixels. If the Mip Sampling Mode is set to LOD, you can connect the node to a Block node in the Vertex Context. For more information on Block nodes and Contexts, see Master Stack. Outputs The Sample Texture 3D node has the following output ports: Name Type Description RGBA Vector 4 The full RGBA Vector 4 color value of the texture sample. R Float The Red channel or X component of the texture sample. G Float The Green channel or Y component of the texture sample. B Float The Blue channel or Z component of the texture sample. A Float The Alpha channel or W component of the texture sample. Example graph usage In the following example, the Sample Texture 3D node samples a 3D fractal noise Texture asset. It takes its input UV coordinates from a Position node, set to Object Space. The Sample Texture 3D node needs a Vector 3 for its UV coordinate input, rather than a Vector 2, because the Texture asset exists as a volume in imaginary 3D space. The node uses the default Sampler State because there is no Sampler State node connected. This specific Texture 3D asset stores its Texture data in the Alpha channel, so the Sample Texture 3D node uses its A output port as an input for the Base Color Block node in the Fragment Context of the Master Stack: Generated code example The following code represents this node in Unity's shader code: float4 _SampleTexture3D_Out = SAMPLE_TEXTURE3D(Texture, Sampler, UV); Related nodes The following nodes are related or similar to the Sample Texture 3D node: Sample Texture 2D Array node Sample Texture 2D node Sampler State node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Virtual-Texture-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sample-Virtual-Texture-Node.html",
    "title": "Sample Virtual Texture Node | FSM Unity Framework",
    "keywords": "Sample Virtual Texture Node Description Samples a Virtual Texture and returns up to four Vector 4 color values for use in the shader. You can use the UV input to override the UV coordinate. The Sample Virtual Texture node takes one UV coordinate as the input, and uses that UV coordinate to sample all of the textures in the Virtual Texture. If you want to use the Sample Virtual Texture node to sample normal maps, navigate to each layer that you want to sample as a normal map, open the Layer Type drop-down menu, and select Normal. By default, you can only use this node in the fragment shader stage. For more information about how to use this node, or how to configure it for use in the vertex shader stage, see Using Streaming Virtual Texturing in Shader Graph. If you disable Virtual Texturing in your project, this node works the same way as the Sample 2D Texture Node, and performs standard 2D sampling on each texture. You must connect a Sample Virtual Texture node to a Virtual Texture property for the Shader Graph Asset to compile. If you don't connect the node to a property, an error appears, indicating that the node requires a connection. For information about Streaming Virtual Texturing, see Streaming Virtual Texturing. Ports Name Direction Type Binding Description UV Input Vector 2 UV The UV coordinate. VT Input Virtual Texture None The Virtual Texture to sample. Must be connected to a Virtual Texture property. Out Output Vector 4 None The output value of layer 1 as RGBA. Out2 Output Vector 4 None The output of layer 2 as RGBA. Out3 Output Vector 4 None The output of layer 3 as RGBA. Out4 Output Vector 4 None The output of layer 4 as RGBA. Settings The Sample Virtual Texture node has several settings available for you to specify its behavior. These settings work in combination with any scripts you might have set up in your project. To view the settings, select the node with the Graph Inspector open. For more information, see Streaming Virtual Texturing. Name Type Options Description Lod Mode Dropdown Automatic, Lod Level, Lod Bias, Derivatives Sets the specific Lod mode to use when sampling the textures. Quality Dropdown Low, High Sets the quality mode to use when sampling the textures. Automatic Streaming Toggle Enabled/Disabled Determines whether the node uses automatic streaming or manual streaming. Enable Global Mip Bias Toggle Enabled/Disabled Enables the global mipmap bias that Unity automatically imposes at runtime. Unity sets this bias during certain dynamic resolution scaling algorithms to improve detail reconstruction. Layer 1 Type Dropdown Default, Normal The texture type of layer 1. Layer 2 Type Dropdown Default, Normal The texture type of layer 2. Layer 3 Type Dropdown Default, Normal The texture type of layer 3. This option only appears if the Virtual Texture has at least 3 layers. Layer 4 Type Dropdown Default, Normal The texture type of layer 4. This option only appears if the Virtual Texture has at least 4 layers. Generated Code Example The following example code represents one possible outcome of this node. float4 SampleVirtualTexture(float2 uv, VTPropertyWithTextureType vtProperty, out float4 Layer0) { VtInputParameters vtParams; vtParams.uv = uv; vtParams.lodOrOffset = 0.0f; vtParams.dx = 0.0f; vtParams.dy = 0.0f; vtParams.addressMode = VtAddressMode_Wrap; vtParams.filterMode = VtFilter_Anisotropic; vtParams.levelMode = VtLevel_Automatic; vtParams.uvMode = VtUvSpace_Regular; vtParams.sampleQuality = VtSampleQuality_High; #if defined(SHADER_STAGE_RAY_TRACING) if (vtParams.levelMode == VtLevel_Automatic || vtParams.levelMode == VtLevel_Bias) { vtParams.levelMode = VtLevel_Lod; vtParams.lodOrOffset = 0.0f; } #endif StackInfo info = PrepareVT(vtProperty.vtProperty, vtParams); Layer0 = SampleVTLayerWithTextureType(vtProperty, vtParams, info, 0); return GetResolveOutput(info); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sampler-State-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sampler-State-Node.html",
    "title": "Sampler State Node | FSM Unity Framework",
    "keywords": "Sampler State Node Description Defines a Sampler State for sampling textures. It should be used in conjunction with sampling Nodes such as the Sample Texture 2D Node. You can set a filter mode with the dropdown parameter Filter and a wrap mode with the dropdown parameter Wrap. When using a separate Sample State Node you can sample a Texture 2D twice, with different sampler parameters, without defining the Texture 2D itself twice. Not all filtering, wrap, and Anisotropic filtering modes are available on all platforms. Ports Name Direction Type Binding Description Out Output Sampler State None Output value Controls Name Type Options Description Filter Dropdown Linear, Point, Trilinear Specifies which filtering mode to use for sampling. Wrap Dropdown Repeat, Clamp, Mirror, MirrorOnce Specifies which wrap mode to use for sampling. Node Settings Controls The following control appears on the Node Settings tab of the Graph Inspector when you select the Sampler State Node. Name Type Options Description Anisotropic Filtering Dropdown None, x2, x4, x8, x16 Specifies the level of Anisotropic filtering to use to sample textures. Generated Code Example The following example code represents one possible outcome of this node. SamplerState _SamplerState_Out = _SamplerState_Linear_Repeat_sampler;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Saturate-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Saturate-Node.html",
    "title": "Saturate Node | FSM Unity Framework",
    "keywords": "Saturate Node Description Returns the value of input In clamped between 0 and 1. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Saturate_float4(float4 In, out float4 Out) { Out = saturate(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Saturation-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Saturation-Node.html",
    "title": "Saturation Node | FSM Unity Framework",
    "keywords": "Saturation Node Description Adjusts the saturation of input In by the amount of input Saturation. A Saturation value of 1 will return the input unaltered. A Saturation value of 0 will return the input completely desaturated. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Saturation Input Float None Saturation value Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Saturation_float(float3 In, float Saturation, out float3 Out) { float luma = dot(In, float3(0.2126729, 0.7151522, 0.0721750)); Out = luma.xxx + Saturation.xxx * (In - luma.xxx); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sawtooth-Wave-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sawtooth-Wave-Node.html",
    "title": "Sawtooth Wave Node | FSM Unity Framework",
    "keywords": "Sawtooth Wave Node Description Returns a sawtooth wave from the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_SawtoothWave_float4(float4 In, out float4 Out) { Out = 2 * (In - floor(0.5 + In)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Color-Node.html",
    "title": "Scene Color Node | FSM Unity Framework",
    "keywords": "Scene Color Node Description Provides access to the current Camera's color buffer using input UV, which is expected to be normalized screen coordinates. The behavior of the Scene Color node isn't defined globally. The executed HLSL code for the Scene Color node is defined per Render Pipeline, and different Render Pipelines can produce different results. Custom Render Pipelines that wish to support the Scene Color node need to explicitly define the behavior for it. If the behavior is undefined, the Scene Color node returns 0 (black). In the Universal Render Pipeline the Scene Color node returns the value of the Camera Opaque Texture. Refer to the Universal Render Pipeline for more documentation on this feature. The contents of this texture are only available for Transparent objects. Set the Surface Type dropdown on the Material Options panel of the Master Node to Transparent to receive the correct values from this node. Note You can only use the Scene Color node in the Fragment Shader Stage. Supported Unity render pipelines The following table indicates which render pipelines support the Scene Color node. When used with unsupported render pipelines, the Scene Color node returns 0 (black). Pipeline Supported Built-in Render Pipeline No Universal Render Pipeline Yes High Definition Render Pipeline Yes Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Normalized screen coordinates Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_SceneColor_float(float4 UV, out float3 Out) { Out = SHADERGRAPH_SAMPLE_SCENE_COLOR(UV); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Depth-Difference-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Depth-Difference-Node.html",
    "title": "Scene Depth Difference | FSM Unity Framework",
    "keywords": "Scene Depth Difference Description Provide a difference between a World Space Position and a Depth value for a given UV. Ports Name Direction Type Binding Description Scene UV Input Vector4 None UV where to sample the depth. Position WS Input Vector3 None The world space position to compare with scene depth. Out Output Float None The difference between PositionWS and the depth. The difference is given relative to camera with Eye mode, in depth-buffer-value with Raw mode and in Linear value remap between 0 and 1 with the Linear01 Mode. Controls Name Type Options Description Mode Dropdown Select Linear01 to have a value between 0 and 1, Eye to have a World-Space value comparable to unit used on the scene and Raw if it's used with SceneDepthBuffer."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Depth-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Scene-Depth-Node.html",
    "title": "Scene Depth Node | FSM Unity Framework",
    "keywords": "Scene Depth Node Description Provides access to the current Camera's depth buffer using input UV, which is expected to be normalized screen coordinates. Note: Depth buffer access requires depth buffer to be enabled on the active Render Pipeline. This process is different per Render Pipeline. It is recommended you read the documentation of your active Render Pipeline for information on enabling the depth buffer. If the depth buffer is unavailable this Node will return mid grey. Note: The executed HLSL code for this Node is defined per Render Pipeline, and different Render Pipelines may produce different results. Custom Render Pipelines that wish to support this Node will also need to explicitly define the behaviour for it. If undefined this Node will return 1 (white). NOTE: This Node can only be used in the Fragment Shader Stage and it is not guaranteed to work with an opaque material. Unity Render Pipelines Support High Definition Render Pipeline Universal Render Pipeline Ports Name Direction Type Binding Description UV Input Vector 4 Screen Position Normalized screen coordinates Out Output Float None Output value Depth Sampling modes Name Description Linear01 Linear depth value between 0 and 1 Raw Raw depth value Eye Depth converted to eye space units Generated Code Example The following example code represents one possible outcome of this node. void Unity_SceneDepth_Raw_float(float4 UV, out float Out) { Out = SHADERGRAPH_SAMPLE_SCENE_DEPTH(UV); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-Iris-Blend-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-Iris-Blend-Node.html",
    "title": "Sclera Iris Blend Node | FSM Unity Framework",
    "keywords": "Sclera Iris Blend Node This node blends all the properties of the Iris and the Sclera so that they can be fed to the master node. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Sclera Iris Blend Node No Yes Ports name Direction type description Sclera Color Input Color Color of the sclera at the target fragment. Sclera Normal Input Vector3 Normal of the sclera at the target fragment. Sclera Smoothness Input float Smoothness of the sclera at the target fragment. Iris Color Input Color Color of the iris at the target fragment. Iris Normal Input Vector3 Normal of the iris at the target fragment. Cornea Smoothness Input float Smoothness of the cornea at the target fragment. IrisRadius Input float The radius of the Iris in the model. For the default model, this value should be 0.225. PositionOS Input Vector3 Position in object space of the current fragment to shade. Diffusion Profile Sclera Input Diffusion Profile Diffusion profile used to compute the subsurface scattering of the sclera. Diffusion Profile Iris Input Diffusion Profile Diffusion profile used to compute the subsurface scattering of the iris. EyeColor Output Color Final Diffuse color of the Eye. Surface Mask Output float Linear, normalized value that defines where the fragment is. On the Cornea, this is 1 and on the Sclera, this is 0. Diffuse Normal Output Vector3 Normal of the diffuse lobes. Specular Normal Output Vector3 Normal of the specular lobes. EyeSmoothness Output float Final smoothness of the Eye. SurfaceDiffusionProfile Output Diffusion Profile Diffusion profile of the target fragment."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-Limbal-Ring-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-Limbal-Ring-Node.html",
    "title": "Sclera Limbal Ring Node | FSM Unity Framework",
    "keywords": "Sclera Limbal Ring Node Calculates the intensity of the Sclera ring, a darkening feature of eyes. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Sclera Limbal Ring Node No Yes Ports name Direction type description PositionOS Input Vector3 Position in object space of the current fragment to shade. View Direction OS Input Vector3 Direction of the incident ray in object space. Either from the camera in rasterization or from the previous bounce in ray tracing. IrisRadius Input float The radius of the Iris in the used model. For the default model, this value should be 0.225. LimbalRingSize Input float Normalized [0, 1] value that defines the relative size of the limbal ring. LimbalRingFade Input float Normalized [0, 1] value that defines strength of the fade out of the limbal ring.** LimbalRing Intensity Input float Positive value that defines how dark the limbal ring is. Iris Limbal Ring Color Output Color Intensity of the limbal ring (blackscale)."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-UV-Location-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sclera-UV-Location-Node.html",
    "title": "Sclera UV Location Node | FSM Unity Framework",
    "keywords": "Sclera UV Location Node This node converts the object position of the sclera to a UV Sampling coordinate. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Sclera UV Location Node No Yes Ports name Direction type description PositionOS Input Vector3 Position of the fragment to shade in object space. ScleraUV Output Vector2 Normalized UV coordinates that can be used to sample either a texture or procedurally generate a Sclera Texture."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Screen-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Screen-Node.html",
    "title": "Screen Node | FSM Unity Framework",
    "keywords": "Screen Node Description Provides access to parameters of the screen. Unity Render Pipelines Support Universal Render Pipeline The High Definition Render Pipeline does not support this Node. Ports Name Direction Type Binding Description Width Output Float None Screen's width in pixels Height Output Float None Screen's height in pixels Generated Code Example The following example code represents one possible outcome of this node. float _Screen_Width = _ScreenParams.x; float _Screen_Height = _ScreenParams.y;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Screen-Position-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Screen-Position-Node.html",
    "title": "Screen Position Node | FSM Unity Framework",
    "keywords": "Screen Position Node Description Provides access to the screen position of the mesh vertex or fragment. The X and Y values represent the horizontal and vertical positions respectively. Use the Mode dropdown control to select the mode of the output value. The available modes are as follows: Default - Returns X and Y values that represent the normalized Screen Position. The normalized Screen Position is the Screen Position divided by the clip space position W component. The X and Y value ranges are between 0 and 1 with position float2(0,0) at the lower left corner of the screen. The Z and W values aren't used in this mode, so they're always 0. Raw - Returns the raw Screen Position values, which are the Screen Position values before the clip space position W component is divided out. Position float2(0,0) is at the lower left corner of the screen. This mode is useful for projection. Center - Returns X and Y values that represent the normalized Screen Position offset so position float2(0,0) is at the center of the screen. The range of the X and Y values is –1 to 1. The Z and W values aren't used in this mode, so they're always 0. Tiled - Returns Screen Position offset so position float2(0,0) is at the center of the screen and tiled using frac. Pixel - Returns Screen Position in terms of the actual pixel width and height values of the screen. In this mode, position float2(0,0) is at the lower left corner of the screen. Whereas the range of Default mode is always 0 to 1, the range of Pixel mode depends on the screen resolution. The Z and W values aren't used in this mode, so they're always 0. Ports Name Direction Type Binding Description Out Output Vector 4 None Get the Screen Position of the mesh. Controls Name Type Options Description Mode Dropdown Default, Raw, Center, Tiled, Pixel Select which coordinate space to use for the Screen Position output. Generated Code Example The following code examples represent one possible outcome for each mode. Default float4 Out = float4(IN.NDCPosition.xy, 0, 0); Raw float4 Out = IN.ScreenPosition; Center float4 Out = float4(IN.NDCPosition.xy * 2 - 1, 0, 0); Tiled float4 Out = frac(float4((IN.NDCPosition.x * 2 - 1) * _ScreenParams.x / _ScreenParams.y, IN.{0}.y * 2 - 1, 0, 0)); Pixel float4 Out = float4(IN.PixelPosition.xy, 0, 0);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Asset.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Asset.html",
    "title": "Shader Graph Asset | FSM Unity Framework",
    "keywords": "Shader Graph Asset Description The Shader Graph Asset is the new Asset type introduced with the shader graph. You can create a Shader Graph Asset from the Project Window from the Create menu. For convenience there is a Create menu entry for Blank Shader Graph and Sub-graph. They can be found in the Shader sub-menu. Additional options may be provided by render pipelines. These options will create a new Shader Graph with required settings and Block nodes in the Master Stack for the selected shading model. You can open the Shader Graph Window by double clicking a Shader Graph Asset or by clicking Open Shader Editor in the Inspector when the Shader Graph Asset is selected."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Preferences.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Preferences.html",
    "title": "Shader Graph Preferences | FSM Unity Framework",
    "keywords": "Shader Graph Preferences To access the Shader Graph Project-wide settings, click Edit > Preferences, and then select Shader Graph. Settings Name Description Shader Variant Limit Enter a value to set the maximum number of shader variants. If your graph exceeds this maximum value, Unity throws the following error: Validation: Graph is generating too many variants. Either delete Keywords, reduce Keyword variants or increase the Shader Variant Limit in Preferences > Shader Graph. For more information about shader variants, see Making multiple shader program variants. Automatically Add or Remove Block Nodes Toggle either on or off. If this option is on, when changing Graph Settings any needed Block nodes will be added to the Master Stack. Any incompatible Block nodes that have no incoming connections will be removed from the Master Stack. If this option is off, no Block nodes will be added to or removed from the Master Stack. Enable Deprecated Nodes Enable this setting to turn off warnings for deprecated nodes and properties, which also allows you to create older versions of nodes and properties. If you don't enable this setting, Shader Graph displays warnings for deprecated nodes and properties, and any new nodes and properties you create use the latest version."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Window.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Graph-Window.html",
    "title": "Shader Graph Window | FSM Unity Framework",
    "keywords": "Shader Graph Window Description The Shader Graph Window contains the workspace for creating shaders using the Shader Graph system. To open the Shader Graph Window you must first create a Shader Graph Asset. For more information see the Getting Started section. The Shader Graph window contains various individual elements such as the Blackboard, Graph Inspector, and Main Preview. These elements can be moved inside the workspace. They will automatically anchor to the nearest corner when scaling the Shader Graph Window. Title Bar The title bar at the top of the Shader Graph Window contains actions that can be performed on the Graph. Item Description Save Asset Saves the graph to update the Shader Graph Asset Save As Opens a file dialog that allows the user to save out the Shader Graph Asset under a new name. Show In Project Highlights the Shader Graph Asset in the Project Window Check Out If version control is enabled, this will check out the Shader Graph Asset from the source control provider. Color Mode Provides the drop down menu to select a Color Mode for the graph. Blackboard Toggles visibility of the Blackboard. Graph Inspector Toggles visibility of the Graph Inspector. Main Preview Toggles visbility of the Main Preview. Workspace The workspace is where you create Node networks. You can navigate the workspace by holding Alt and left mouse button to pan and zoom with the scroll wheel. You can hold left mouse button and drag to select multiple Nodes with a marquee. There are also various shortcut keys to use for better workflow. Hotkey Windows OSX Description Cut Ctrl + X Command + X Cuts selected Nodes to the clipboard Copy Ctrl + C Command + C Copies selected Nodes to the clipboard Paste Ctrl + V Command + V Pastes Nodes in the clipboard Focus F F Focus the workspace on all or selected Nodes Create Node Spacebar Spacebar Opens the Create Node Menu Context Menu Right clicking within the workspace will open a context menu. Note that right clicking on an item within the workspace, such as a Node, will open the context menu for that item and not the workspace. Item Description Create Node Opens the Create Node Menu Create Sticky Note Creates a new Sticky Note on the Graph. Collapse All Previews Collapses previews on all Nodes Cut Cuts selected Nodes to the clipboard Copy Copies selected Nodes to the clipboard Paste Pastes Nodes in the clipboard Delete Deletes selected Nodes Duplicate Duplicates selected Nodes Select / Unused Nodes Selects all nodes on the graph that are not contributing to the final shader output from the Master Stack. View / Collapse Ports Collapses unused ports on all selected Nodes View / Expand Ports Expands unused ports on all selected Nodes View / Collapse Previews Collapses previews on all selected Nodes View / Expand Previews Expands previews on all selected Nodes Precision / Inherit Sets precision of all selected Nodes to Inherit. Precision / Float Sets precision on all selected nodes to Float. Precision / Half Sets precision on all selected nodes to Half."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Stage.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Shader-Stage.html",
    "title": "Shader Stage | FSM Unity Framework",
    "keywords": "Shader Stage Description Shader Stage refers to the part of the shader pipeline a Node or Port is part of. For example, Vertex or Fragment. In Shader Graph, Shader Stage is defined per port but often all ports on a node are locked to the same Shader Stage. Ports on some nodes are unavailable in certain Shader Stages due to limitations in the underlying shader language. See the Node Library documentation for nodes that have Shader Stage restrictions. Shader Stage List Name Description Vertex Operations calculated per vertex Fragment Operations calculated per fragment"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/ShaderGraph-Samples.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/ShaderGraph-Samples.html",
    "title": "Shader Graph samples | FSM Unity Framework",
    "keywords": "Shader Graph samples Description The Shader Graph package offers sample Assets, which you can download through Package Manager. When you import these samples, Unity places the files in your Project's Asset folder. The files contain examples that demonstrate how to use Shader Graph features. Add samples To add samples to your Project, go to Window > Package Manager. Locate Shader Graph in the list of available packages, and select it. Under the package description, there is list of available samples. Click the Import into Project button next to the sample you wish to add. Unity places imported samples in your Project's Asset folder under Assets > Samples > Shader Graph > [version number] > [sample name]. The example below shows the samples for Procedural Patterns. Available samples The following samples are currently available for Shader Graph. Procedural Patterns This collection of Assets showcases various procedural techniques possible with Shader Graph. Use them directly in your Project, or edit them to create other procedural patterns. The patterns in this collection are: Bacteria, Brick, Dots, Grid, Herringbone, Hex Lattice, Houndstooth, Smooth Wave, Spiral, Stripes, Truchet, Whirl, Zig Zag. Node Reference This set of Shader Graph assets provides reference material for the nodes available in the Shader Graph node library. Each graph contains a description for a specific node, examples of how it can be used, and useful tips. Some example assets also show a break-down of the math that the node is doing. You can use these samples along with the documentation to learn more about the behavior of individual nodes."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sign-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sign-Node.html",
    "title": "Sign Node | FSM Unity Framework",
    "keywords": "Sign Node Description Per component, returns -1 if the value of input In is less than zero, 0 if equal to zero and 1 if greater than zero. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Sign_float4(float4 In, out float4 Out) { Out = sign(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Simple-Noise-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Simple-Noise-Node.html",
    "title": "Simple Noise Node | FSM Unity Framework",
    "keywords": "Simple Noise Node Description Generates a simple, or Value, noise based on input UV. The scale of the generated noise is controlled by input Scale. You can also choose to use two different hashing methods for calculating the noise. As of Unity version 2021.2, the Simple Noise node defaults to the Deterministic hash, to ensure consistent results for noise generation across platforms. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Scale Input Float None Noise scale Out Output Float None Output value Controls Name Type Options Description Hash Type Dropdown Deterministic, LegacySine Selects the hash function used to generate random numbers for noise generation. Generated Code Example The following example code represents one possible outcome of this node. inline float unity_noise_randomValue (float2 uv) { return frac(sin(dot(uv, float2(12.9898, 78.233)))*43758.5453); } inline float unity_noise_interpolate (float a, float b, float t) { return (1.0-t)*a + (t*b); } inline float unity_valueNoise (float2 uv) { float2 i = floor(uv); float2 f = frac(uv); f = f * f * (3.0 - 2.0 * f); uv = abs(frac(uv) - 0.5); float2 c0 = i + float2(0.0, 0.0); float2 c1 = i + float2(1.0, 0.0); float2 c2 = i + float2(0.0, 1.0); float2 c3 = i + float2(1.0, 1.0); float r0 = unity_noise_randomValue(c0); float r1 = unity_noise_randomValue(c1); float r2 = unity_noise_randomValue(c2); float r3 = unity_noise_randomValue(c3); float bottomOfGrid = unity_noise_interpolate(r0, r1, f.x); float topOfGrid = unity_noise_interpolate(r2, r3, f.x); float t = unity_noise_interpolate(bottomOfGrid, topOfGrid, f.y); return t; } void Unity_SimpleNoise_float(float2 UV, float Scale, out float Out) { float t = 0.0; float freq = pow(2.0, float(0)); float amp = pow(0.5, float(3-0)); t += unity_valueNoise(float2(UV.x*Scale/freq, UV.y*Scale/freq))*amp; freq = pow(2.0, float(1)); amp = pow(0.5, float(3-1)); t += unity_valueNoise(float2(UV.x*Scale/freq, UV.y*Scale/freq))*amp; freq = pow(2.0, float(2)); amp = pow(0.5, float(3-2)); t += unity_valueNoise(float2(UV.x*Scale/freq, UV.y*Scale/freq))*amp; Out = t; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sine-Node.html",
    "title": "Sine Node | FSM Unity Framework",
    "keywords": "Sine Node Description Returns the sine of the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value in radians. Out Output Dynamic Vector Output value. Range (-1 to +1). Generated Code Example The following example code represents one possible outcome of this node. void Unity_Sine_float4(float4 In, out float4 Out) { Out = sin(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Slider-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Slider-Node.html",
    "title": "Slider Node | FSM Unity Framework",
    "keywords": "Slider Node Description Defines a constant Float value in the shader using a Slider field. Can be converted to a Float type Property with a Mode setting of Slider via the Node's context menu. Ports Name Direction Type Binding Description Out Output Float None Output value Controls Name Type Options Description Slider Defines the output value. Min Float Defines the slider parameter's minimum value. Max Float Defines the slider parameter's maximum value. Generated Code Example The following example code represents one possible outcome of this node. float _Slider_Out = 1.0;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Smoothstep-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Smoothstep-Node.html",
    "title": "Smoothstep Node | FSM Unity Framework",
    "keywords": "Smoothstep Node Description Returns the result of a smooth Hermite interpolation between 0 and 1, if the value of input In is between the values of inputs Edge1 and Edge2 respectively. Returns 0 if the value of input In is less than the value of input Edge1 and 1 if greater than the value of input Edge2. The Smoothstep node is similar to the Lerp Node but there are two notable differences. Firstly, with the Smoothstep node, the user specifies the range and the return value is between 0 and 1. You can consider this the opposite of the Lerp Node. Secondly, the Smoothstep node uses smooth Hermite interpolation instead of linear interpolation, which means the interpolation gradually speeds up from the start and slows down toward the end. This interpolation is useful for creating natural-looking animation, fading, and other transitions. Ports Name Direction Type Description Edge1 Input Dynamic Vector Minimum step value Edge2 Input Dynamic Vector Maximum step value In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Smoothstep_float4(float4 Edge1, float4 Edge2, float4 In, out float4 Out) { Out = smoothstep(Edge1, Edge2, In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/SpeedTree8-SubGraphAssets.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/SpeedTree8-SubGraphAssets.html",
    "title": "SpeedTree 8 Sub Graph Assets | FSM Unity Framework",
    "keywords": "SpeedTree 8 Sub Graph Assets Prerequisite information This documentation assumes that you are already familiar with the concepts described in the following pages: SpeedTree Sub Graph Nodes Sub Graph Assets Keywords The documentation on ShaderLab material properties might also be contextually helpful. Description SpeedTree is a third-party solution that includes both ready-to-use tree assets, and modeling software for creating your own tree assets. Shader Graph has three built-in SpeedTree Sub Graph Assets: SpeedTree8ColorAlpha SpeedTree8Wind SpeedTree8Billboard These Sub Graph Assets provide SpeedTree 8 functionality for both the Universal Render Pipeline (URP) and High Definition Render Pipeline (HDRP), so that you can work with SpeedTree 8 assets and create your own custom SpeedTree 8 Shader Graphs. Note: The URP-specific versions of these SpeedTree 8 Sub Graph Assets use transparent billboard back faces instead of culling billboard back faces. These Sub Graph Assets can only replace their URP equivalents as a default once URP supports per-material culling overrides in Shader Graphs. SpeedTree8ColorAlpha Each SpeedTree asset has four maps: a basemap (color/albedo), bump map (which provides surface normals), extra map (which provides metallic and ambient occlusion data), and subsurface map (which provides the subsurface scattering color). The basemap provides input color and alpha data. This Sub Graph Asset applies all SpeedTree 8 features that modify the basemap's color and alpha data. These features are particularly useful for the following actions: Tinting the basemap color Varying tree hues Crossfading between levels of detail (LODs) Hiding geometry seams Tinting the basemap color You can use the SpeedTree8ColorAlpha Sub Graph Asset to apply a tint to the basemap color. This is useful if, for example, you want to adjust tree colors for different seasons of the year. Property Support Purpose Behavior _ColorTint URP, HDRP Tint the basemap. Multiplies the _ColorTint property value by the basemap color. Varying tree hues To improve the visual diversity of SpeedTrees, you can use this Sub Graph Asset to modify the color of each tree instance. Both _OldHueVarBehavior and _HueVariationColor use the tree’s absolute world-space position to determine a pseudorandomized hue variation intensity value. Property Support Purpose Behavior _OldHueVarBehavior URP To match the behavior of URP-specific and Built-In SpeedTree 8 shaders. Uses the pseudorandom hue variation intensity value to parameterize the linear interpolation between the basemap color (t=0) and HueVariation color (t=1) . _HueVariationColor URP, HDRP To provide SpeedTrees with more hue diversity. Uses the pseudorandom hue variation intensity value as its opacity and applies that to the basemap color as an overlay blend. A more subtle effect than that provided by _OldHueBehavior. EFFECT_HUE_VARIATION N/A N/A This keyword was used in handwritten SpeedTree 8 shaders, but it's not used in these SpeedTree 8 Shader Graphs. This is to ensure compliance with the default shader variant limit. _HueVariationKwToggle URP, HDRP, Built-In Only to support upgrade functionality. See SpeedTreeImporter.hueVariation. Crossfading between levels of detail (LODs) Crossfading dithers between different levels of detail (LODs) to minimize popping during abrupt transitions. The SpeedTree8ColorAlpha Sub Graph Asset uses a Custom Function Node for that purpose. This Custom Function Node is not SpeedTree-specific. Enable Animate Cross-fading and select an LOD Fade setting to use it with any asset that has the LOD Group component. See Transitioning between LOD levels for more information. Hiding geometry seams The SpeedTree8ColorAlpha Sub Graph asset applies an alpha gradient to soften the transitions between geometry segments that sample different parts of the basemap. SpeedTree8Wind The SpeedTree8Wind Sub Graph Asset uses a Custom Function Node to deform the vertices of SpeedTree 8 models in response to your application’s wind data. You can use this to make trees appear to bend in the wind. Unity applies wind data to the SpeedTree8Wind Sub Graph Asset is as follows: When a WindZone affects a SpeedTree 8 GameObject that has Wind enabled, Unity generates SpeedTree 8 wind simulation data. Unity populates the SpeedTreeWind Cbuffer with that wind simulation data. The SpeedTree8Wind Sub Graph Asset bases its deformation behavior on the data in the SpeedTreeWind Cbuffer. This asset includes automated LOD vertex interpolation when LOD Fade is set to SpeedTree. However, it does not support instancing. SpeedTree8Billboard The SpeedTree8Billboard Sub Graph Asset calculates billboard normals from a SpeedTree 8 model's bump map, geometric tangent, and bitangent data. It includes dithering functionality to improve the appearance of billboards at view angles diagonal to the model. The keyword toggle associated with this feature is named EFFECT_BILLBOARD.This supports backwards compatibility with previous versions of ShaderGraph, which require keywords and their toggling properties to have identical names. SpeedTree 8 InterpolatedNormals All SpeedTree 8 shaders that Unity provides interpolate geometric normals, tangents, and bitangents in the vertex stage, because this results in a better visual appearance than the per-pixel data that Shader Graph nodes provide. You do not need to use this feature if your SpeedTree 8 Shader Graph does not include custom interpolators. HDRP and URP do not have identical backface normal transformation behavior. This can become a problem when you use Custom Interpolators for geometric normal, tangent, and bitangent data. The purpose of the SpeedTree 8 InterpolatedNormals Sub Graph Asset is to allow for that difference. It combines geometric normal data with bump maps in a way that is compatible with the target pipeline's backface normal transformation behavior."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sphere-Mask-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sphere-Mask-Node.html",
    "title": "Sphere Mask Node | FSM Unity Framework",
    "keywords": "Sphere Mask Node Description Creates a sphere mask originating from input Center. The sphere is calculated using Distance and modified using the Radius and Hardness inputs. Sphere mask functionality works in both 2D and 3D spaces, and is based on the vector coordinates in the Coords input. These vector coordinates can either be 3D like world space position, or 2D like UV coordinates. Ports Name Direction Type Binding Description Coords Input Dynamic Vector None Coordinate space input Center Input Dynamic Vector None Coordinates of the sphere origin Radius Input Float None Radius of the sphere Hardness Input Float None Soften falloff of the sphere Out Output Dynamic Vector None Output mask value Generated Code Example The following example code represents one possible outcome of this node. void Unity_SphereMask_float4(float4 Coords, float4 Center, float Radius, float Hardness, out float4 Out) { Out = 1 - saturate((distance(Coords, Center) - Radius) / (1 - Hardness)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Spherize-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Spherize-Node.html",
    "title": "Spherize Node | FSM Unity Framework",
    "keywords": "Spherize Node Description Applies a spherical warping effect similar to a fisheye camera lens to the value of input UV. The center reference point of the warping effect is defined by input Center and the overall strength of the effect is defined by the value of input Strength. Input Offset can be used to offset the individual channels of the result. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Center Input Vector 2 None Center reference point Strength Input Float None Strength of the effect Offset Input Vector 2 None Individual channel offsets Out Output Vector 2 None Output UV value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Spherize_float(float2 UV, float2 Center, float Strength, float2 Offset, out float2 Out) { float2 delta = UV - Center; float delta2 = dot(delta.xy, delta.xy); float delta4 = delta2 * delta2; float2 delta_offset = delta4 * Strength; Out = UV + delta * delta_offset + Offset; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Split-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Split-Node.html",
    "title": "Split Node | FSM Unity Framework",
    "keywords": "Split Node Description Splits the input vector In into four Float outputs R, G, B and A. These output vectors are defined by the individual channels of the input In; red, green, blue and alpha respectively. If the input vector In's dimension is less than 4 (Vector 4) the output values not present in the input will be 0. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value R Output Float None Red channel from input G Output Float None Green channel from input B Output Float None Blue channel from input A Output Float None Alpha channel from input Generated Code Example The following example code represents one possible outcome of this node. float _Split_R = In[0]; float _Split_G = In[1]; float _Split_B = 0; float _Split_A = 0;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Split-Texture-Transform-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Split-Texture-Transform-Node.html",
    "title": "Split Texture Transform Node | FSM Unity Framework",
    "keywords": "Split Texture Transform Node Description This node makes it possible to separately output tiling, offset, and texture data for a Texture 2D asset. That enables you to present an asset differently in a specific context—to warp it in a mirror, for example—and put it into the UV without modifying the original asset. This node outputs the texture with its tiling set to (0,0) and scale set to (1,1). That activates the shader property NoScaleOffset, which enables you to modify Tiling Offset values via the Material Inspector. Another term you may hear for tiling in this context is scale. Both terms refer to the size of the texture tiles. Ports Name Direction Type Description In Input Texture2D The Texture 2D Node input. Tiling Output Vector 2 Amount of tiling to apply per channel, set via the Material Inspector. Offset Output Vector 2 Amount of offset to apply per channel, set via the Material Inspector. Texture Only Output Vector 2 The input Texture2D, without tiling and offset data."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Square-Root-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Square-Root-Node.html",
    "title": "Square Root Node | FSM Unity Framework",
    "keywords": "Square Root Node Description Returns the square root of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_SquareRoot_float4(float4 In, out float4 Out) { Out = sqrt(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Square-Wave-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Square-Wave-Node.html",
    "title": "Square Wave Node | FSM Unity Framework",
    "keywords": "Square Wave Node Description Returns a square wave from the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_SquareWave_float4(float4 In, out float4 Out) { Out = 1.0 - 2.0 * round(frac(In)); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Step-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Step-Node.html",
    "title": "Step Node | FSM Unity Framework",
    "keywords": "Step Node Description Per component, returns 1 if the value of input In is greater than or equal to the value of input Edge, otherwise returns 0. Ports Name Direction Type Description Edge Input Dynamic Vector Step value In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Step_float4(float4 Edge, float4 In, out float4 Out) { Out = step(Edge, In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sticky-Notes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sticky-Notes.html",
    "title": "Sticky Notes | FSM Unity Framework",
    "keywords": "Sticky Notes Sticky Notes are objects in a graph view that you can write in. They are the graph view equivalent of a comment in code, and consist of a title and body. You can create as many as you want in the graph, and use them for a variety of purposes, for example: To describe how a section of your graph works. To leave notes for yourself or others collaborating in your Unity Project. As a to-do list that includes tasks to complete at a later date. Using Sticky Notes To create a Sticky Note, right-click an empty space in the graph view and, in the context menu, click Create Sticky Note. You can then customize and add content to the new Sticky Note. There are two text areas that you can write in: Title: The text area at the top of the Sticky Note is the title. You can use it to concisely describe what information the Sticky Note contains. Body: The larger text area below the title area is the body. You can write the full contents of the note here. Editing text To edit text on a Sticky Note, double-click on a text area. This also selects the entire text area, so be sure to move the cursor before you edit the text. Moving and resizing You can move Sticky Notes anywhere on the graph. You can also click and drag to manually resize Sticky Notes, or have a Sticky Note automatically resize itself to fit the content. For information on how to make the Sticky Note resize itself, see Fit To Text in the Context menu section below. Duplicating Use the following keyboard shortcuts to cut, copy, paste, and duplicate Sticky Notes. Copy: Ctrl+C Cut: Ctrl+X Paste: Ctrl+V Duplicate: Ctrl+D Context menu To open the context menu for a Sticky Note, right-click anywhere on it. The options in the context menu are as follows. Option Description Dark Theme/Light Theme Toggles the color theme of the Sticky Note between light theme and dark theme. Text Size Resizes the font in the text areas to the following point values. Small Title: 20, Body: 11 Medium Title: 40, Body: 24 Large Title: 60, Body: 36 Huge Title: 80, Body: 56 Fit To Text Resizes the Sticky Note so that it precisely fits the text areas. If your title exceeds a single line, Unity resizes the Sticky Note such that title text fits on a single line. Delete Deletes the Sticky Note you selected. Group Selection Places any Sticky Notes you select in a group. Ungroup Selection Removes any Sticky Notes you select from the group."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-Graph-Dropdown-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-Graph-Dropdown-Node.html",
    "title": "Subgraph Dropdown node | FSM Unity Framework",
    "keywords": "Subgraph Dropdown node The Subgraph Dropdown node is a node representation of a Dropdown property. It allows you to create a custom dropdown menu on a Subgraph node in its parent Shader Graph. You can specify the number of options that appear in the dropdown menu, and their names. After you create a Dropdown property and add a Dropdown node to a Subgraph, the Subgraph node in any parent Shader Graph displays with a dropdown control: Create Node menu category The Subgraph Dropdown node isn't accessible from the Create Node menu. To add a Subgraph Dropdown node to a Subgraph: In the Shader Graph window, open a Subgraph. In the Blackboard, select Add (+) and select Dropdown. Enter a name for your new Dropdown property, and press Enter. Select your Dropdown property and drag it onto your graph to create a new Subgraph Dropdown node. Select your new Dropdown node in your graph or the Dropdown property in the Blackboard and open the Graph Inspector. Select the Node Settings tab. In the Entries table, select Add to the list (+) to add a new option to your dropdown. Each Entry adds a corresponding input port to your node. To remove an Entry, select its handle in the list and select Remove selection from the list (-). (Optional) In the Default list, select the default Entry that you want Shader Graph to select on your property. Compatibility The Subgraph Dropdown node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes Ports Note The Subgraph Dropdown node's number of input ports and their names directly correspond to the settings you specify in the Graph Inspector's Node Settings tab. The node always has one output port. A Subgraph Dropdown node's input ports always have the DynamicVector type. This means that you can make a connection to an input port from any node that outputs a float, Vector 2, Vector 3, Vector 4, or Boolean value. For more information, see Dynamic Data Types. It has one output port: Name Type Description Out DynamicVector The selected option from the dropdown menu on the parent Shader Graph's Subgraph node. This value can also be the specified Default for the property in the Graph Inspector's Node Settings tab. Example graph usage In the following example, a Subgraph Dropdown node changes the UV channel it sends to the Subgraph's Output node. The selection on the Subgraph node in the parent graph changes whether the Subgraph outputs UV1 or UV0. If the Subgraph is used in multiple Shader Graphs, the Subgraph Dropdown node can change the UV channel output without changing the Subgraph: Related nodes The following nodes are related or similar to the Subgraph Dropdown node: Subgraph node"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph-Asset.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph-Asset.html",
    "title": "Subgraph Asset | FSM Unity Framework",
    "keywords": "Subgraph Asset Description The Subgraph Asset is a new Asset type introduced with the Shader Graph. A Subgraph Asset defines a Subgraph. This is different to a Shader Graph. You can create a Subgraph Asset from the Project window from the Create menu via Subgraph in the Shader sub-menu. You can open the Shader Graph Window by double clicking a Subgraph Asset or by clicking Open Shader Editor in the Inspector when the Subgraph Asset is selected."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph-Node.html",
    "title": "Subgraph node | FSM Unity Framework",
    "keywords": "Subgraph node Description Provides a reference to a Subgraph Asset. All ports on the reference node are defined by the properties and outputs defined in the Subgraph Asset. This is useful for sharing functionality between graphs or duplicating the same functionality within a graph. The preview used for a Subgraph Node is determined by the first port of that Subgraph Output node. Valid Data Types for the first port are Float, Vector 2, Vector 3, Vector 4, Matrix2, Matrix3, Matrix4, and Boolean. Any other data type will produce an error in the preview shader and the Subgraph will become invalid. Subgraph Nodes and Shader Stages If a Node within a Subgraph specifies a Shader Stage, such as how Sample Texture 2D Node specifies the fragment Shader Stage, then that entire Subgraph) is now locked to that stage. As such a Subgraph node that references the graph will also be locked to that Shader Stage. Furthermore, when an Edge connected to an output Port on a Subgraph Node flows into a port on the Master Stack that Subgraph Node is now locked to the Shader Stage of that Block Node in the Master Stack."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Sub-graph.html",
    "title": "Sub Graph | FSM Unity Framework",
    "keywords": "Sub Graph Description A Sub Graph is a special type of Shader Graph, which you can reference from inside other graphs. This is useful when you wish to perform the same operations multiple times in one graph or across multiple graphs. A Sub Graph differs from a Shader Graph in three main ways: Properties in the Blackboard of a Sub Graph define the input Ports of a Sub Graph Node when you reference the Sub Graph from inside another graph. A Sub Graph has its own Asset type. For more information, including instructions on how to make a new Sub Graph, see Sub Graph Asset. A Sub Graph does not have a Master Stack. Instead, it has a Node called Output. For information about the components of a Sub Graph, see Sub Graph Asset. Output Node The Output Node defines the output ports of a Sub Graph Node when you reference the Sub Graph from inside another graph. To add and remove ports, use the Custom Port Menu in the Node Settings tab of the Graph Inspector by clicking on the Sub Graph Output node. The preview used for Sub Graphs is determined by the first port of the Output Node. Valid Data Types for the first port are Float, Vector 2, Vector 3, Vector 4, Matrix2, Matrix3, Matrix4, and Boolean. Any other data type will produce an error in the preview shader and the Sub Graph will become invalid. Sub Graphs and shader stages If a Node within a Sub Graph specifies a shader stage (for example, how the Sample Texture 2D Node specifies the fragment shader stage), the Editor locks the entire Sub Graph to that stage. You cannot connect any Nodes that specify a different shader stage to the Sub Graph Output Node, and the Editor locks any Sub Graph Nodes that references the graph to that shader stage. From 10.3 onward, Texture and SamplerState type inputs and outputs to Sub Graphs benefit from an improved data structure. For a detailed explanation, see Custom Function Node. Sub Graphs and Keywords Keywords that you define on the Blackboard in a Sub Graph behave similarly to those in regular Shader Graphs. When you add a Sub Graph Node to a Shader Graph, Unity defines all Keywords in that Sub Graph in the Shader Graph as well, so that the Sub Graph works as intended. To use a Sub Graph Keyword inside a Shader Graph, or to expose that Keyword in the Material Inspector, copy it from the Sub Graph to the Shader Graph's Blackboard."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Subtract-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Subtract-Node.html",
    "title": "Subtract Node | FSM Unity Framework",
    "keywords": "Subtract Node Description Returns the result of input A minus input B. Ports Name Direction Type Description A Input Dynamic Vector First input value B Input Dynamic Vector Second input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Subtract_float4(float4 A, float4 B, out float4 Out) { Out = A - B; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Swizzle-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Swizzle-Node.html",
    "title": "Swizzle Node | FSM Unity Framework",
    "keywords": "Swizzle Node Description Creates a new vector from the reordered elements of the input vector. This is called swizzling. To specify how input elements should be swizzled, enter a formatting string in the input mask. To invert the order of the input elements, for example, use the string \"wzyx\" or \"abgr\". The length of the input mask determines the dimensions of the output vector. The error \"Invalid Mask\" indicates an input mask value which includes one or more channels that do not exist in the input vector. To output a vector3 with the x, y and z elements of the input vector, for example, use the input mask “xyz” or “rgb”. Ports Name Direction Type Binding Description In Input Dynamic Vector None Input value Out Output Dynamic Vector None Output value Controls Name Type Options Description Mask Inputfield x, y, z, w (depending on input vector dimension) The swizzle mask is a combination of one to four characters that can be x, y, z, w (or r, g, b, a). The size of output value depends on the length of the mask input. Generated Code Example The following example code represents one possible outcome of this node. float4 _Swizzle_Out = In.wzyx;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Shader Graph Getting started with Shader Graph Creating a new Shader Graph Asset My first Shader Graph Shader Graph Window Blackboard Main Preview Graph Inspector Create Node Menu Graph Settings Tab Master Stack Sticky Notes Sub Graph Color Modes Precision Modes Preview Mode Control Custom Function Node Shader Graph Preferences Samples Material Variants Upgrade Guides Upgrade to Shader Graph 10.0.x Inside Shader Graph Shader Graph Asset Graph Target Sub Graph Asset SpeedTree 8 Sub Graph Assets Node Port Custom Port Menu Edge Property Types Keywords Data Types Port Bindings Shader Stage Surface options Custom Interpolators Node Library Artistic Adjustment Channel Mixer Contrast Hue Invert Colors Replace Color Saturation White Balance Blend Blend Filter Dither Fade Transition Mask Channel Mask Color Mask Normal Normal Blend Normal From Height Normal From Texture Normal Reconstruct Z Normal Strength Normal Unpack Utility Colorspace Conversion Channel Combine Flip Split Swizzle Input Basic Boolean Color Constant Integer Slider Time Float Vector 2 Vector 3 Vector 4 Geometry Bitangent Vector Instance ID Normal Vector Position Screen Position Tangent Vector UV Vertex Color Vertex ID View Direction View Vector Gradient Blackbody Gradient Sample Gradient High Definition Render Pipeline Custom Color Buffer Custom Depth Buffer Diffusion Profile Exposure HD Scene Color HD Scene Depth HD Sample Buffer Lighting Ambient Baked GI Main Light Direction Reflection Probe Matrix Matrix 2x2 Matrix 3x3 Matrix 4x4 Transformation Matrix Mesh Deformation Compute Deformation Linear Blend Skinning PBR Dielectric Specular Metal Reflectance Fresnel Equation Scene Camera Eye Index Fog Object Scene Color Scene Depth Scene Depth Difference Screen Texture Calculate Level Of Detail Texture 2D Node Cubemap Asset Gather Texture 2D Node Sample Cubemap Sample Reflected Cubemap Sample Texture 2D Sample Texture 2D Array Sample Texture 2D LOD Sample Texture 3D Sample Virtual Texture Sampler State Split Texture Transform Texture 2D Array Asset Texture 2D Asset Texture 3D Asset Texture Size Math Advanced Absolute Exponential Length Log Modulo Negate Normalize Posterize Reciprocal Reciprocal Square Root Basic Add Divide Multiply Power Square Root Subtract Derivative DDX DDXY DDY Interpolation Inverse Lerp Lerp Smoothstep Matrix Matrix Construction Matrix Determinant Matrix Split Matrix Transpose Range Clamp Fraction Maximum Minimum One Minus Random Range Remap Saturate Round Ceiling Floor Round Sign Step Truncate Trigonometry Arccosine Arcsine Arctangent Arctangent2 Cosine Degrees To Radians Hyperbolic Cosine Hyperbolic Sine Hyperbolic Tangent Radians To Degrees Sine Tangent Vector Cross Product Distance Dot Product Fresnel Effect Projection Reflection Refract Rejection Rotate About Axis Sphere Mask Transform Wave Noise Sine Wave Sawtooth Wave Square Wave Triangle Wave Procedural Noise Gradient Noise Simple Noise Voronoi Shapes Ellipse Polygon Rectangle Rounded Polygon Rounded Rectangle Checkerboard Utility Logic All And Any Branch Branch On Input Connection Comparison Is Front Face Is Infinite Is NaN Nand Not Or High Definition Render Pipeline Emission Eye CirclePupilAnimation CorneaRefraction EyeSurfaceTypeDebug IrisLimbalRing IrisOffset IrisOutOfBoundColorClamp IrisUVLocation ScleraIrisBlend ScleraLimbalRing ScleraUVLocation Water Compute Vertex Position Evaluate Foam Data Evaluate Refraction Data Evaluate Scattering Color Evaluate Simulation Additional Data Evaluate Simulation Caustics Evaluate Simulation Displacement Evaluate Tip Thickness Pack Water Vertex Data Unpack Water Data Fabric *ThreadMapDetail UVCombine Custom Function Keyword Preview Subgraph Subgraph Dropdown node UV Flipbook Polar Coordinates Radial Shear Rotate Spherize Tiling And Offset Triplanar Twirl Parallax Mapping Parallax Occlusion Mapping Block Nodes Built In Blocks"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tangent-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tangent-Node.html",
    "title": "Tangent Node | FSM Unity Framework",
    "keywords": "Tangent Node Description Returns the tangent of the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Tangent_float4(float4 In, out float4 Out) { Out = tan(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tangent-Vector-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tangent-Vector-Node.html",
    "title": "| FSM Unity Framework",
    "keywords": "Description Provides access to the mesh vertex or fragment's Tangent Vector. The coordinate space of the output value can be selected with the Space dropdown parameter. Ports Name Direction Type Binding Description Out Output Vector 3 None Mesh's Tangent Vector. Parameters Name Type Options Description Space Dropdown Object, View, World, Tangent Selects coordinate space of Tangent Vector to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-2D-Array-Asset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-2D-Array-Asset-Node.html",
    "title": "Texture 2D Array Asset Node | FSM Unity Framework",
    "keywords": "Texture 2D Array Asset Node Description Defines a constant Texture 2D Array Asset for use in the shader. To sample the Texture 2D Array Asset it should be used in conjunction with a Sample Texture 2D Array Node. When using a separate Texture 2D Array Asset Node, you can sample a Texture 2D Array twice, with different parameters, without defining the Texture 2D Array itself twice. Ports Name Direction Type Description Out Output Texture 2D Array Output value Controls Name Type Options Description Object Field (Texture 2D Array) Defines the texture 2D array asset from the project. Generated Code Example The following example code represents one possible outcome of this node. TEXTURE2D_ARRAY(_Texture2DArrayAsset); SAMPLER(sampler_Texture2DArrayAsset);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-2D-Asset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-2D-Asset-Node.html",
    "title": "Texture 2D Asset Node | FSM Unity Framework",
    "keywords": "Texture 2D Asset Node Description Defines a constant Texture 2D Asset for use in the shader. To sample the Texture 2D Asset it should be used in conjunction with a Sample Texture 2D Node. When using a separate Texture 2D Asset Node, you can sample a Texture 2D twice, with different parameters, without defining the Texture 2D itself twice. Ports Name Direction Type Description Out Output Texture 2D Output value Controls Name Type Options Description Object Field (Texture) Select which texture 2D asset to use from the project. Generated Code Example The following example code represents one possible outcome of this node. TEXTURE2D(_Texture2DAsset); SAMPLER(sampler_Texture2DAsset);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-3D-Asset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-3D-Asset-Node.html",
    "title": "Texture 3D Asset Node | FSM Unity Framework",
    "keywords": "Texture 3D Asset Node Description Defines a constant Texture 3D Asset for use in the shader. To sample the Texture 3D Asset it should be used in conjunction with a Sample Texture 3D Node. When using a separate Texture 3D Asset Node, you can sample a Texture 3D twice, with different parameters, without defining the Texture 3D itself twice. Ports Name Direction Type Description Out Output Texture 3D Output value Controls Name Type Options Description Object Field (Texture 3D) Defines the texture 3D asset from the project. Generated Code Example The following example code represents one possible outcome of this node. TEXTURE3D(_Texture3DAsset); SAMPLER(sampler_Texture3DAsset);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-Size-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Texture-Size-Node.html",
    "title": "Texture Size Node | FSM Unity Framework",
    "keywords": "Texture Size Node The Texture Size node takes a Texture 2D input and returns the width and height texel resolution of the texture. It also returns the width and height size of each texel of the texture. The node uses the built in variable {texturename}_TexelSize to access the special properties of the given Texture 2D input. The term \"texel\" is short for \"texture element\" or \"texture pixel.\" It represents a single pixel in the texture. So, for example, if Texture resolution is 512x512 texels, the texture is sampled over the range [0-1] in UV space, so each texel is 1/512 x 1/512 in size in UV coordinates. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, you can resolve them by upgrading your version of Shader Graph to version 10.3 or later. Note Don't use the default input to reference your Texture 2D, as this affects the performance of your graph. Connect a Texture 2D Asset Node to the Texture Size node's Texture input port and re-use this definition for sampling. Create Node menu category The Texture Size node is under the Input > Texture category in the Create Node menu. Compatibility The Texture Size node is compatible with all render pipelines. Ports Name Direction Type Binding Description Texture Input Texture None The Texture 2D asset to measure. Width Output Float None The width of the Texture 2D asset in texels. Height Output Float None The height of the Texture 2D asset in texels. Texel Width Output Float None The texel width of the Texture 2D asset in UV coordinates. Texel Height Output Float None The texel height of the Texture 2D asset in UV coordinates. Generated Code Example The following example code represents one possible outcome of this node. float _TexelSize_Width = Texture_TexelSize.z; float _TexelSize_Height = Texture_TexelSize.w;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/ThreadMapDetail-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/ThreadMapDetail-Node.html",
    "title": "ThreadMapDetail node | FSM Unity Framework",
    "keywords": "ThreadMapDetail node The ThreadMapDetail node adds tileable thread map detail information to a fabric material. The node outputs a thread map that you can apply to a fabric material. Note This node is a Subgraph node: it represents a Subgraph instead of directly representing Unity's shader code. Double-click the node in any Shader Graph to view its Subgraph. A thread map is a Texture with 4 channels. Like a detail map, a thread map contains information about ambient occlusion, the normal x-axis and normal y-axis, and smoothness. For more information on Detail maps, see Secondary Maps (Detail Maps) & Detail Mask in the Unity User Manual. Create Node menu category The ThreadMapDetail node is under the Utility > High Definition Render Pipeline > Fabric category in the Create Node menu. Compatibility node is supported on the following render pipelines: Built-in Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) No No Yes For more information on the HDRP, see Unity's HDRP package documentation. node can also be connected to any Block node in either Context. For more information on Block nodes and Contexts, see Master Stack. Inputs node has the following input ports: Name Type Binding Description Use Thread Map Boolean None Use the port's default input to enable or disable the ThreadMapDetail node. You can also connect a node that outputs a Boolean to choose when to enable or disable the thread map. ThreadMap Texture 2D None The texture that contains the detailed information of a fabric's thread pattern. The texture should contain 4 channels: R - The ambient occlusion G - The normal Y-axis B - The smoothness A - The normal X-axis UV Vector 2 UV The UV coordinates the ThreadMapDetail node should use to map the ThreadMap texture on the geometry. Normals Vector 3 None The base normal map that you want your Shader Graph to apply to the geometry before it applies the thread map. Smoothness Float None The base smoothness value that you want your Shader Graph to apply to the geometry before it applies the thread map. Alpha Float None The base alpha value that you want your Shader Graph to apply to the geometry before it applies the thread map. Ambient Occlusion Float None The base ambient occlusion value that you want your Shader Graph to apply to the geometry before it applies the thread map. Thread AO Strength Float None Specify a value of 0 or 1 to determine how the ThreadMap's ambient occlusion should impact the final shader result: If you provide a value of 0, the ThreadMap's ambient occlusion has no effect on the final output of the shader. If you provide a value of 1, Shader Graph multiplies your base Ambient Occlusion value by the ambient occlusion value specified in your ThreadMap to determine the final output of the shader. Thread Normal Strength Float None Specify a value of 0 or 1 to determine how the ThreadMap's normal should impact the final shader result: If you provide a value of 0, the ThreadMap's normal has no effect on the final output of the shader. If you provide a value of 1, Shader Graph blends the values from your base Normals with the normal specified in your ThreadMap to determine the final output of the shader. Thread Smoothness Strength Float None Specify a value of 0 or 1 to determine how the ThreadMap's smoothness should impact the final shader result: If you provide a value of 0, the ThreadMap's smoothness value has no effect on the final output of the shader. If you provide a value of 1, Shader Graph adds the smoothness value specified in your ThreadMap to your base Smoothness value to determine the final output of the shader. For this calculation, Shader Graph remaps the value of your ThreadMap's smoothness from (0,1) to (-1, 1). Outputs node has the following output ports: Name Type Description Normal Vector 3 The final normal output of the thread map. Smoothness Float The final smoothness output of the thread map. Ambient Occlusion Float The final ambient occlusion output of the thread map. Alpha Float The final alpha output of the thread map. Shader Graph calculates this alpha value by multiplying the input Alpha value by the Thread AO Strength value. Example graph usage For an example use of the ThreadMapDetail node, see either of the HDRP's Fabric shaders. To view these Shader Graphs: Create a new material and assign it the HDRP > Fabric > Silk or HDRP > Fabric > CottonWool shader, as described in the Unity User Manual section Creating a material asset, and assigning a shader to it. Next to the Shader dropdown, select Edit. Your chosen Fabric's Shader Graph opens. You can view the ThreadMapDetail node, its Subgraph, and the other nodes that create HDRP's Fabric shaders."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tiling-And-Offset-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Tiling-And-Offset-Node.html",
    "title": "Tiling And Offset Node | FSM Unity Framework",
    "keywords": "Tiling And Offset Node Description Tiles and offsets the value of input UV by the inputs Tiling and Offset respectively. This is commonly used for detail maps and scrolling textures over Time. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Tiling Input Vector 2 None Amount of tiling to apply per channel Offset Input Vector 2 None Amount of offset to apply per channel Out Output Vector 2 None Output UV value Generated Code Example The following example code represents one possible outcome of this node. void Unity_TilingAndOffset_float(float2 UV, float2 Tiling, float2 Offset, out float2 Out) { Out = UV * Tiling + Offset; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Time-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Time-Node.html",
    "title": "Time Node | FSM Unity Framework",
    "keywords": "Time Node Description Provides access to various Time parameters in the shader. Ports Name Direction Type Binding Description Time Output Float None Time value Sine Time Output Float None Sine of Time value Cosine Time Output Float None Cosine of Time value Delta Time Output Float None Current frame time Smooth Delta Output Float None Current frame time smoothed Generated Code Example The following example code represents one possible outcome of this node. float Time_Time = _Time.y; float Time_SineTime = _SinTime.w; float Time_CosineTime = _CosTime.w; float Time_DeltaTime = unity_DeltaTime.x; float Time_SmoothDelta = unity_DeltaTime.z;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Transform-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Transform-Node.html",
    "title": "Transform Node | FSM Unity Framework",
    "keywords": "Transform Node Description Returns the result of transforming the input value (In) from one coordinate space to another. Select dropdown options on the node to define which spaces to transform from and to. Ports Name Direction Type Description In Input Vector 3 Input value Out Output Vector 3 Output value Controls Name Type Options Description From Dropdown Object, View, World, Tangent, Absolute World, Screen Select the space to convert from. To Dropdown Object, View, World, Tangent, Absolute World, Screen Select the space to convert to. Type Dropdown Position, Direction, Normal Select how you want to handle the conversion. Node Settings Controls The following control appears on the Node Settings tab of the Graph Inspector when you select the Direction or Normal conversion types for the Transform Node. The Normalize Output setting helps to improve performance as you can disable it if the output is already normalized, or if you don't need the output to remain normalized. Name Type Description Normalize Output Checkbox Reduces the length of the output vector to 1. World and Absolute World Use the World and Absolute World space options to transform the coordinate space of position values. The World space option uses the Scriptable Render Pipeline default world space to convert position values. The Absolute World space option uses absolute world space to convert position values in all Scriptable Render Pipelines. If you use the Transform Node to convert coordinate spaces that aren't for position values, Unity recommends that you use the World space option. Using Absolute World on values that don't represent position might result in unexpected behavior. Conversion type Select the Position type to apply translation to the transformation. Select Direction if the input doesn't describe a surface normal (the direction a surface faces). Select Normal if the input describes a surface normal (the direction the surface faces). Generated Code Example The following example code represents one possible outcome of this node per Base mode. World > World float3 _Transform_Out = In; World > Object float3 _Transform_Out = TransformWorldToObject(In); World > Tangent float3x3 tangentTransform_World = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); float3 _Transform_Out = TransformWorldToTangent(In, tangentTransform_World); World > View float3 _Transform_Out = TransformWorldToView(In); World > Absolute World float3 _Transform_Out = GetAbsolutePositionWS(In); World > Screen float4 hclipPosition = TransformWorldToHClipDir(In); float3 screenPos = hclipPosition.xyz / hclipPosition.w; float3 _Transform_Out = float3(screenPos.xy * 0.5 + 0.5, screenPos.z); Object > World float3 _Transform_Out = TransformObjectToWorld(In); Object > Object float3 _Transform_Out = In; Object > Tangent float3x3 tangentTransform_World = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); float3 _Transform_Out = TransformWorldToTangent(TransformObjectToWorld(In), tangentTransform_World); Object > View float3 _Transform_Out = TransformWorldToView(TransformObjectToWorld(In)); Object > Absolute World float3 _Transform_Out = GetAbsolutePositionWS(TransformObjectToWorld(In)); Object > Screen float4 hclipPosition = TransformObjectToHClip(In); float3 screenPos = hclipPosition.xyz / hclipPosition.w; float3 _Transform_Out = float3(screenPos.xy * 0.5 + 0.5, screenPos.z); Tangent > World float3x3 transposeTangent = transpose(float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal)); float3 _Transform_Out = mul(In, transposeTangent).xyz; Tangent > Object float3x3 transposeTangent = transpose(float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal)); float3 _Transform_Out = TransformWorldToObject(mul(In, transposeTangent).xyz); Tangent > Tangent float3 _Transform_Out = In; Tangent > View float3x3 transposeTangent = transpose(float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal)); float3 _Transform_Out = TransformWorldToView(mul(In, transposeTangent).xyz); Tangent > Absolute World float3x3 transposeTangent = transpose(float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal)); float3 _Transform_Out = GetAbsolutePositionWS(mul(In, transposeTangent)).xyz; Tangent > Screen float3x3 transposeTangent = transpose(float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal)); float4 hclipPosition = TransformWorldToHClipDir(mul(In, transposeTangent).xyz); float3 screenPos = hclipPosition.xyz / hclipPosition.w; float3 _Transform_Out = float3(screenPos.xy * 0.5 + 0.5, screenPos.z); View > World float3 _Transform_Out = mul(UNITY_MATRIX_I_V, float4(In, 1)).xyz; View > Object float3 _Transform_Out = TransformWorldToObject(mul(UNITY_MATRIX_I_V, float4(In, 1) ).xyz); View > Tangent float3x3 tangentTransform_World = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); float3 _Transform_Out = TransformWorldToTangent(mul(UNITY_MATRIX_I_V, float4(In, 1) ).xyz, tangentTransform_World); View > View float3 _Transform_Out = In; View > Absolute World float3 _Transform_Out = GetAbsolutePositionWS(mul(UNITY_MATRIX_I_V, float4(In, 1))).xyz; View > Screen float4 hclipPosition = TransformWViewToHClip(In); float3 screenPos = hclipPosition.xyz / hclipPosition.w; float3 _Transform_Out = float3(screenPos.xy * 0.5 + 0.5, screenPos.z); Absolute World > World float3 _Transform_Out = GetCameraRelativePositionWS(In); Absolute World > Object float3 _Transform_Out = TransformWorldToObject(In); Absolute World > Object (in the High Definition Render Pipeline) float3 _Transform_Out = TransformWorldToObject(GetCameraRelativePositionWS(In)); Absolute World > Tangent float3x3 tangentTransform_World = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); float3 _Transform_Out = TransformWorldToTangent(In, tangentTransform_World); Absolute World > View float3 _Transform_Out = GetCameraRelativePositionWS(In) Absolute World > Absolute World float3 _Transform_Out = In; Absolute World > Screen float4 hclipPosition = TransformWorldToHClip(GetCameraRelativePositionWS(In)); float3 screenPos = hclipPosition.xyz / hclipPosition.w; float3 _Transform_Out = float3(screenPos.xy * 0.5 + 0.5, screenPos.z); Screen > World float3 _Transform_Out = ComputeWorldSpacePosition(In.xy, In.z, UNITY_MATRIX_I_VP); Screen > Object float3 worldPos = ComputeWorldSpacePosition(In.xy, In.z, UNITY_MATRIX_I_VP); float3 _Transform_Out = TransformWorldToObject(worldPos); Screen > Tangent float3 worldPos = ComputeWorldSpacePosition(In.xy, In.z, UNITY_MATRIX_I_VP); float3x3 tangentTransform_World = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); float3 _Transform_Out = TransformWorldToTangent(worldPos, tangentTransform_World); Screen > View float4 positionCS = ComputeClipSpacePosition(In.xy, In.z); float4 result = mul(UNITY_MATRIX_I_V, positionCS); float3 _Transform_Out = result.xyz / result.w; Screen > Absolute World float3 _Transform_Out = GetAbsolutePositionWS(ComputeWorldSpacePosition(In.xy, In.z, UNITY_MATRIX_I_VP));"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Transformation-Matrix-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Transformation-Matrix-Node.html",
    "title": "Transformation Matrix Node | FSM Unity Framework",
    "keywords": "Transformation Matrix Node Description Defines a constant Matrix 4x4 value for a common Transformation Matrix in the shader. The Transformation Matrix can be selected from the dropdown parameter. Two output value options for this node, Inverse Projection and Inverse View Projection, are not compatible with the Built-In Render Pipeline target. When you choose either of these options and target the Built-In Render Pipeline, this node produces an entirely black result. Ports Name Direction Type Binding Description Out Output Matrix 4 None Output value Controls Name Type Options Description Dropdown Model, InverseModel, View, InverseView, Projection, InverseProjection, ViewProjection, InverseViewProjection Sets output value Generated Code Example The following example code represents one possible outcome of this node per mode. Model float4x4 _TransformationMatrix_Out = UNITY_MATRIX_M; InverseModel float4x4 _TransformationMatrix_Out = UNITY_MATRIX_I_M; View float4x4 _TransformationMatrix_Out = UNITY_MATRIX_V; InverseView float4x4 _TransformationMatrix_Out = UNITY_MATRIX_I_V; Projection float4x4 _TransformationMatrix_Out = UNITY_MATRIX_P; InverseProjection float4x4 _TransformationMatrix_Out = UNITY_MATRIX_I_P; ViewProjection float4x4 _TransformationMatrix_Out = UNITY_MATRIX_VP; InverseViewProjection float4x4 _TransformationMatrix_Out = UNITY_MATRIX_I_VP;"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Triangle-Wave-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Triangle-Wave-Node.html",
    "title": "Triangle Wave Node | FSM Unity Framework",
    "keywords": "Triangle Wave Node Description Returns a triangle wave from the value of input In. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_TriangleWave_float4(float4 In, out float4 Out) { Out = 2.0 * abs( 2 * (In - floor(0.5 + In)) ) - 1.0; }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Triplanar-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Triplanar-Node.html",
    "title": "Triplanar Node | FSM Unity Framework",
    "keywords": "Triplanar Node Description Generates UVs and samples a texture by projecting in world space. This method is commonly used to texture large models such as terrain, where hand authoring UV coordinates would either be problematic or not performant. Samples the input Texture 3 times, once in each of the world x, y, and z axes. The resulting information is planar projected onto the model, blended by the normal, or surface angle. You can scale the generated UVs with the input Tile and you can control the final blending strength with the input Blend. Blend controls the way the normal affects the blending of each plane sample and should be greater than or equal to 0. The larger Blend is, the more contribution will be given to the sample from the plane towards which the normal is most oriented. (The maximum blend exponent is between 17 and 158 depending on the platform and the precision of the node.) A Blend of 0 makes each plane get equal weight regardless of normal orientation. To choose the projection, change the Input Space. You can also modify the projection via the inputs Position and Normal. Use the Type dropdown to change the expected type of the input Texture. If set to Normal, the Out port returns the blended normals in Normal Output Space. If you experience texture sampling errors while using this node in a graph which includes Custom Function Nodes or Sub Graphs, upgrade to version 10.3 or later. NOTE: You can only use the Triplanar Node in the Fragment shader stage. Ports Name Direction Type Binding Description Texture Input Texture None Input texture value Sampler Input Sampler State None Sampler for input Texture Position Input Vector 3 Input Space Position Fragment position Normal Input Vector 3 Input Space Normal Fragment normal Tile Input Float None Tiling amount for generated UVs Blend Input Float None Blend factor between different samples Out Output Vector 4 None Output value Controls Name Type Options Description Type Dropdown Default, Normal Type of input Texture Node Settings Controls The following controls appear on the Node Settings tab of the Graph Inspector, when you select the Triplanar Node. Name Type Options Description Input Space Dropdown Object, View, World, Tangent, AbsoluteWorld Controls the coordinate space used by the input ports Position and Normal. When you change the Input Space value, it changes the bindings on the Position and Normal ports to use the specified space. The default value is AbsoluteWorld. Normal Output Space Dropdown Object, View, World, Tangent, AbsoluteWorld Controls the coordinate space used for the Out port. The Normal Output Space control is only available when Type is set to Normal. The default value is Tangent. Generated Code Example The following example code represents one possible outcome of this node. Default float3 Node_UV = Position * Tile; float3 Node_Blend = pow(abs(Normal), Blend); Node_Blend /= dot(Node_Blend, 1.0); float4 Node_X = SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.zy); float4 Node_Y = SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.xz); float4 Node_Z = SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.xy); float4 Out = Node_X * Node_Blend.x + Node_Y * Node_Blend.y + Node_Z * Node_Blend.z; Normal float3 Node_UV = Position * Tile; float3 Node_Blend = max(pow(abs(Normal), Blend), 0); Node_Blend /= (Node_Blend.x + Node_Blend.y + Node_Blend.z ).xxx; float3 Node_X = UnpackNormal(SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.zy)); float3 Node_Y = UnpackNormal(SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.xz)); float3 Node_Z = UnpackNormal(SAMPLE_TEXTURE2D(Texture, Sampler, Node_UV.xy)); Node_X = float3(Node_X.xy + Normal.zy, abs(Node_X.z) * Normal.x); Node_Y = float3(Node_Y.xy + Normal.xz, abs(Node_Y.z) * Normal.y); Node_Z = float3(Node_Z.xy + Normal.xy, abs(Node_Z.z) * Normal.z); float4 Out = float4(normalize(Node_X.zyx * Node_Blend.x + Node_Y.xzy * Node_Blend.y + Node_Z.xyz * Node_Blend.z), 1); float3x3 Node_Transform = float3x3(IN.WorldSpaceTangent, IN.WorldSpaceBiTangent, IN.WorldSpaceNormal); Out.rgb = TransformWorldToTangent(Out.rgb, Node_Transform);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Truncate-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Truncate-Node.html",
    "title": "Truncate Node | FSM Unity Framework",
    "keywords": "Truncate Node Description Returns the integer, or whole number, component of the value of input In. For example, given an input value of 1.7, this node will return the value 1.0. Ports Name Direction Type Description In Input Dynamic Vector Input value Out Output Dynamic Vector Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Truncate_float4(float4 In, out float4 Out) { Out = trunc(In); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Twirl-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Twirl-Node.html",
    "title": "Twirl Node | FSM Unity Framework",
    "keywords": "Twirl Node Description Applies a twirl warping effect similar to a black hole to the value of input UV. The center reference point of the warping effect is defined by input Center and the overall strength of the effect is defined by the value of input Strength. Input Offset can be used to offset the individual channels of the result. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Center Input Vector 2 None Center reference point Strength Input Float None Strength of the effect Offset Input Vector 2 None Individual channel offsets Out Output Vector 2 None Output UV value Generated Code Example The following example code represents one possible outcome of this node. void Unity_Twirl_float(float2 UV, float2 Center, float Strength, float2 Offset, out float2 Out) { float2 delta = UV - Center; float angle = Strength * length(delta); float x = cos(angle) * delta.x - sin(angle) * delta.y; float y = sin(angle) * delta.x + cos(angle) * delta.y; Out = float2(x + Center.x + Offset.x, y + Center.y + Offset.y); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UV-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UV-Node.html",
    "title": "UV Node | FSM Unity Framework",
    "keywords": "UV Node Description Provides access to the mesh vertex or fragment's UV coordinates. The coordinate channel of the output value can be selected with the Channel dropdown parameter. Ports Name Direction Type Binding Description Out Output Vector 4 None Mesh's UV coordinates. Controls Name Type Options Description Channel Dropdown UV0, UV1, UV2, UV3 Selects coordinate channel of UV to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UV-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UV-Nodes.html",
    "title": "UV Nodes | FSM Unity Framework",
    "keywords": "UV Nodes Flipbook Polar Coordinates Creates a flipbook, or texture sheet animation, of the UVs supplied to input In. Converts the value of input UV to polar coordinates. Radial Shear Rotate Applies a radial shear warping effect similar to a wave to the value of input UV. Rotates the value of input UV around a reference point defined by input Center by the amount of input Rotation. Spherize Tiling and Offset Applies a spherical warping effect similar to a fisheye camera lens to the value of input UV. Tiles and offsets the value of input UV by the inputs Tiling and Offset respectively. Triplanar Twirl A method of generating UVs and sampling a texture by projecting in world space. Applies a twirl warping effect similar to a black hole to the value of input UV."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UVCombine-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/UVCombine-Node.html",
    "title": "UVCombine node | FSM Unity Framework",
    "keywords": "UVCombine node The UVCombine node lets you select which UV channel you want to use for mapping your shader to geometry in your application. You can also choose to apply tiling and offset to your UV coordinates. Note This node is a Subgraph node: it represents a Subgraph instead of directly representing Unity's shader code. Double-click the node in any Shader Graph to view its Subgraph. Create Node menu category The UVCombine node is under the Utility > High Definition Render Pipeline category in the Create Node menu. Compatibility node is supported on the following render pipelines: Built-in Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) No No Yes For more information on the HDRP, see Unity's HDRP package documentation. node can also be connected to any Block node in either Context. For more information on Block nodes and Contexts, see Master Stack. Inputs node has the following input ports: Name Type Description UV Channel Mask Vector 4 Select which UV channel you want to use for your UV coordinates by entering a 1 in the corresponding default input on the port: X: UV channel 0 Y: UV channel 1 Z: UV channel 2 W: UV channel 3 Set all other default inputs to 0. You can also connect a node that outputs a Vector 4. UV Tile and Offset Vector 4 Use the port's default input to specify the amount of offset or tiling that you want to apply to your shader's UV coordinates: Use X and Y to specify the tiling. Use W and Z to specify the offset. You can also connect a node that outputs a Vector 4. Outputs node has one output port: Name Type Binding Description UV Vector 2 UV The final UV output, after selecting a UV channel and, if specified, any tiling or offset. Example graph usage For an example use of the UVCombine node, see either of the HDRP's Fabric shaders. To view these Shader Graphs: Create a new material and assign it the HDRP > Fabric > Silk or HDRP > Fabric > CottonWool shader, as described in the Unity User Manual section Creating a material asset, and assigning a shader to it. Next to the Shader dropdown, select Edit. Your chosen Fabric's Shader Graph opens. You can view the UVCombine node, its Subgraph, and the other nodes that create HDRP's Fabric shaders."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Unpack-Data-Water-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Unpack-Data-Water-Node.html",
    "title": "Unpack Water Data | FSM Unity Framework",
    "keywords": "Unpack Water Data This node unpacks and outputs water properties for the fragment context. See the HDRP documentation for more information about the Water System. Render pipeline compatibility Node Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Unpack Water Data No Yes Ports Name Direction Type Description LowFrequencyHeight Output Float The vertical displacement of the water surface. This doesn't include ripples. HorizontalDisplacement Output Float The horizontal displacement of the water surface. SSSMask Output Float A mask that defines where the water surface has subsurface scattering."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Upgrade-Guide-10-0-x.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Upgrade-Guide-10-0-x.html",
    "title": "Upgrade to version 10.0.x of Shader Graph | FSM Unity Framework",
    "keywords": "Upgrade to version 10.0.x of Shader Graph Renamed Vector 1 property and Float precision Shader Graph has renamed the Vector 1 property as Float in both the Vector 1 node and the exposed parameter list. The Float precision was also renamed as Single. Behavior is exactly the same, and only the names have changed. Renamed Sample Cubemap Node Shader Graph has renamed the previous Sample Cubemap Node to Sample Reflected Cubemap Node, and has added a new Sample Cubemap Node, which uses world space direction. Master Stack graph output Shader Graph has removed the Master Nodes and introduced a more flexible Master Stack solution for defining graph output in 10.0. You can still open all graphs created in previous versions, because Shader Graph automatically upgrades them. This page describes the expected behavior and explains when you might need to perform manual upgrade steps. Automatic upgrade from Master Nodes Upgrade one Master Node to the Master Stack If your graph only has one Master Node, Shader Graph automatically upgrades all of the data from that Master Node to a Master Stack output, as described in this section. Shader Graph automatically adds the correct Targets to the Graph Settings tab of the Graph Inspector. It also copies all settings from the Master Node settings menu (gear icon) that describe surface options from the Master Node to the Target Settings. Shader Graph then adds a Block node for each port on the Master Node to the Master Stack. It connects any nodes that you connected to the Master Node ports to the corresponding Block node. Also, Shader Graph copies any values that you entered into the default value inputs of the Master Node ports to the corresponding Block node. After this upgrade process, the final shader is identical in appearance. Upgrade multiple Master Nodes to the Master Stack If your graph has more than one Master Node, Shader Graph applies the above process for automatically upgrading one Master Node to the currently selected Active Master Node. When you upgrade to the Master Stack format, Shader Graph removes any inactive Master Nodes from your graph, and you might lose this data. If you plan to upgrade a graph with multiple Master Nodes, it's best practice to keep a record of the ports, connected nodes, and any non-default settings in the settings menu (gear icon) of inactive Master Nodes. After the upgrade, you can add any required Block nodes that went missing, and reconnect the nodes to the Master Stack. You also need to go to the Graph Inspector > Graph Settings tab > settings menu (gear icon), and manually enter the settings for inactive Master Nodes in the corresponding Target Setting. Upgrade cross-pipeline Master Nodes to the Master Stack If your graph contains PBR or Unlit Master Nodes that are compatible with both the Universal Render Pipeline (URP) and the High Definition Render Pipeline (HDRP), Shader Graph automatically upgrades them to the Master Stack based on the render pipeline currently available in your project. With Master Stacks, when you switch from one render pipeline to another, you must reimport your Shader Graph assets to update the Material Inspector for any Materials in your project. In URP, you can now find all PBR Master Node settings in the URP Lit Target. The Unlit Master Node settings are in the URP Unlit Target. These settings are the same, and the final shader should appear the same as before the upgrade. In HDRP, settings from the PBR and Unlit Master Nodes are not the same as the HDRP Lit and Unlit Targets. Thus, there might be unexpected behavior when you upgrade PBR or Unlit Master Nodes to HDRP Lit and Unlit Master Stacks. The final shader might not appear the same as before the upgrade. When this happens, you can use the Bug Reporter to submit your upgrade issue, but keep in mind that some upgrade paths don't have immediate automated solutions and will require manual adjustments. \"View Generated Shader\" has moved Previously, you could right-click the Master Node to bring up a context menu, and select View Generated Shader to preview the generated shader. In 10.0, you must now use the Unity Inspector, and click the View Generated Shader button on the Shader Graph asset. Settings in Graph Inspector Shader Graph introduced an internal Graph Inspector in version 10.0. The Graph Inspector is a floating window that displays settings related to objects you select in the graph. Graph settings Graph-wide settings are now available only in the Graph Inspector's Graph Settings tab. Most notably, you can now go to the Graph Settings tab to access the Precision toggle, which was previously located on the Shader Graph Toolbar. There were no changes to data, and things like the Precision setting of the graph remain the same. In the Graph Settings tab, you can also find settings that describe surface options for each Target, which were previously located in the Master Node cog menu. For more information about how Shader Graph automatically upgrades this data, see Automatic upgrade from Master Nodes above. Property settings Property settings that were previously in Blackboard foldouts are now available in the Graph Inspector. You can now select multiple properties from the Blackboard and edit them all at the same time. There were no changes to data, and all settings you made on properties of the graph remain the same. Per-Node settings All per-node settings that you previously managed by opening a settings (gear icon) sub-menu are now accessible through the Graph Inspector. There were no changes to data, and all settings you previously set on nodes, such as precision settings and Custom Function Node settings, remain the same. Any settings on the Master Node that define surface options are now located in the Graph Inspector’s Graph Settings tab. For more information, see Automatic upgrade from Master Nodes above. Custom Function Nodes and Shader Graph Preview To avoid errors in the preview shader compilation for Custom Function Nodes, you might need to use keywords for the in-graph preview rendering. If you have any Custom Function Nodes with custom Shader Graph Preview code that uses #if SHADERGAPH_PREVIEW, you need to upgrade it to an #ifdef declaration, as follows. #ifdef SHADERGAPH_PREVIEW Out = 1; #else Out = MainLight; #endif Deprecated node and property behaviors Previously, some nodes and properties such as the Color Node didn't behave as intended, but they now work correctly in Shader Graph version 10.0. Older graphs that rely on the incorrect behavior still function the same as before, and you can choose to individually upgrade any deprecated nodes and properties. If you don't enable Allow Deprecated Behaviors in Shader Graph Preferences, newly-created nodes and properties use the latest version node and property behaviors. For deprecated nodes, (Deprecated) appears after the node title in the main graph view. For deprecated properties, (Deprecated) appears after the property name in the Blackboard. When you select a deprecated node or property, a warning appears in the Internal Inspector along with an Update button that allows you to upgrade the selection. You can use undo/redo to reverse this upgrade process. If you enable Allow Deprecated Behaviors in Shader Graph Preferences, Shader Graph displays the version of the deprecated node or property, and doesn't display any warnings even though the Update button appears. You can also use the Blackboard or Searcher to create deprecated nodes and properties."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Utility-Nodes.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Utility-Nodes.html",
    "title": "Utility Nodes | FSM Unity Framework",
    "keywords": "Utility Nodes Preview Sub-Graph Provides a preview window and passes the input value through without modification. Provides a reference to a Sub-graph asset. Logic All And Returns true if all components of the input In are non-zero. Returns true if both the inputs A and B are true. Any Branch Returns true if any of the components of the input In are non-zero. Provides a dynamic branch to the shader. Comparison Is Infinite Compares the two input values A and B based on the condition selected on the dropdown. Returns true if any of the components of the input In is an infinite value. Is NaN Nand Returns true if any of the components of the input In is not a number (NaN). Returns true if both the inputs A and B are false. Not Or Returns the opposite of input In. If In is true, the output is false. Otherwise, it returns true. Returns true if either input A or input B is true."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-2-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-2-Node.html",
    "title": "Vector 2 Node | FSM Unity Framework",
    "keywords": "Vector 2 Node Description Defines a Vector 2 value in the shader. If Ports X and Y are not connected with Edges this Node defines a constant Vector 2, otherwise this Node can be used to combine various Float values. Ports Name Direction Type Binding Description X Input Float None Input x component value Y Input Float None Input y component value Out Output Vector 2 None Output value Generated Code Example The following example code represents one possible outcome of this node. float2 _Vector2_Out = float2(X, Y);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-3-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-3-Node.html",
    "title": "Vector 3 Node | FSM Unity Framework",
    "keywords": "Vector 3 Node Description Defines a Vector 3 value in the shader. If Ports X, Y and Z are not connected with Edges this Node defines a constant Vector 3, otherwise this Node can be used to combine various Float values. Ports Name Direction Type Binding Description X Input Float None Input x component value Y Input Float None Input y component value Z Input Float None Input z component value Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. float3 _Vector3_Out = float3(X, Y, Z);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-4-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vector-4-Node.html",
    "title": "Vector 4 Node | FSM Unity Framework",
    "keywords": "Vector 4 Node Description Defines a Vector 4 value in the shader. If Ports X, Y, Z and W are not connected with Edges this Node defines a constant Vector 4, otherwise this Node can be used to combine various Float values. Ports Name Direction Type Binding Description X Input Float None Input x component value Y Input Float None Input y component value Z Input Float None Input z component value W Input Float None Input w component value Out Output Vector 4 None Output value Generated Code Example The following example code represents one possible outcome of this node. float4 _Vector4_Out = float4(X, Y, Z, W);"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vertex-Color-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vertex-Color-Node.html",
    "title": "Vertex Color Node | FSM Unity Framework",
    "keywords": "Vertex Color Node Description Provides access to the mesh vertex or fragment's Vertex Color value. Ports Name Direction Type Binding Description Out Output Vector 4 None Vertex Color for the Mesh Vertex/Fragment."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vertex-ID-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Vertex-ID-Node.html",
    "title": "Vertex ID Node | FSM Unity Framework",
    "keywords": "Vertex ID Node Description Provides access to the mesh vertex or fragment's Vertex ID value. Ports Name Direction Type Binding Description Out Output Float None Vertex ID for the Mesh Vertex/Fragment."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/View-Direction-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/View-Direction-Node.html",
    "title": "View Direction Node | FSM Unity Framework",
    "keywords": "View Direction Node Description Provides access to the mesh vertex or fragment's View Direction vector. This is the vector from the vertex or fragment to the camera. Select a Space to modify the coordinate space of the output value. Prior to version 11.0, the View Direction Node works differently in HDRP than in URP. In URP, it only stored Object space vectors normalized. HDRP stores all vectors normalized. From 11.0 onwards, this node stores all vectors normalized in both the High-Definition Render Pipeline and the Universal Render Pipeline. If you want to keep using the old behavior in URP outside of object space, replace this node with a View Vector Node. Ports Name Direction Type Binding Description Out Output Vector 3 None View Vector for the Mesh Vertex/Fragment. Controls Name Type Options Description Space Dropdown Object, View, World, Tangent Selects coordinate space of View Direction to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/View-Vector-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/View-Vector-Node.html",
    "title": "View Vector Node | FSM Unity Framework",
    "keywords": "View Vector Node Description This node provides access to an unnormalized version of the mesh vertex or fragment's View Direction vector. It does not normalize any of the values it stores. For a normalized option, see View Direction Node. Select a Space to modify the output value's coordinate space. Ports Name Direction Type Binding Description Out Output Vector 3 None View Vector for the Mesh Vertex/Fragment. Controls Name Type Options Description Space Dropdown Object, View, World, Tangent Selects coordinate space of View Direction to output."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Voronoi-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/Voronoi-Node.html",
    "title": "Voronoi Node | FSM Unity Framework",
    "keywords": "Voronoi Node Description Generates a Voronoi, or Worley, noise based on input UV. Voronoi noise is generated by calculating distances between a pixel and a lattice of points. By offsetting these points by a pseudo-random number, controlled by input Angle Offset, a cluster of cells can be generated. The scale of these cells, and the resulting noise, is controlled by input Cell Density. The output Cells contains the raw cell data. You can also choose to use two different hashing methods for calculating the noise. As of Unity version 2021.2, the Voronoi node defaults to the Deterministic hash, to ensure consistent results for noise generation across platforms. Ports Name Direction Type Binding Description UV Input Vector 2 UV Input UV value Angle Offset Input Float None Offset value for points Cell Density Input Float None Density of cells generated Out Output Float None Output noise value Cells Output Float None Raw cell data Controls Name Type Options Description Hash Type Dropdown Deterministic, LegacySine Selects the hash function used to generate random numbers for noise generation. Generated Code Example The following example code represents one possible outcome of this node. inline float2 unity_voronoi_noise_randomVector (float2 UV, float offset) { float2x2 m = float2x2(15.27, 47.63, 99.41, 89.98); UV = frac(sin(mul(UV, m)) * 46839.32); return float2(sin(UV.y*+offset)*0.5+0.5, cos(UV.x*offset)*0.5+0.5); } void Unity_Voronoi_float(float2 UV, float AngleOffset, float CellDensity, out float Out, out float Cells) { float2 g = floor(UV * CellDensity); float2 f = frac(UV * CellDensity); float t = 8.0; float3 res = float3(8.0, 0.0, 0.0); for(int y=-1; y<=1; y++) { for(int x=-1; x<=1; x++) { float2 lattice = float2(x,y); float2 offset = unity_voronoi_noise_randomVector(lattice + g, AngleOffset); float d = distance(lattice + offset, f); if(d < res.x) { res = float3(d, offset.x, offset.y); Out = res.x; Cells = res.y; } } } }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/White-Balance-Node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/White-Balance-Node.html",
    "title": "White Balance Node | FSM Unity Framework",
    "keywords": "White Balance Node Description Adjusts the temperature and tint of input In by the amount of inputs Temperature and Tint respectively. Temperature has the effect of shifting the values towards yellow or blue. Tint has the effect of shifting towards pink or green. Ports Name Direction Type Binding Description In Input Vector 3 None Input value Temperature Input Float None Temperature offset value Tint Input Float None Tint offset value Out Output Vector 3 None Output value Generated Code Example The following example code represents one possible outcome of this node. void Unity_WhiteBalance_float(float3 In, float Temperature, float Tint, out float3 Out) { // Range ~[-1.67;1.67] works best float t1 = Temperature * 10 / 6; float t2 = Tint * 10 / 6; // Get the CIE xy chromaticity of the reference white point. // Note: 0.31271 = x value on the D65 white point float x = 0.31271 - t1 * (t1 < 0 ? 0.1 : 0.05); float standardIlluminantY = 2.87 * x - 3 * x * x - 0.27509507; float y = standardIlluminantY + t2 * 0.05; // Calculate the coefficients in the LMS space. float3 w1 = float3(0.949237, 1.03542, 1.08728); // D65 white point // CIExyToLMS float Y = 1; float X = Y * x / y; float Z = Y * (1 - x - y) / y; float L = 0.7328 * X + 0.4296 * Y - 0.1624 * Z; float M = -0.7036 * X + 1.6975 * Y + 0.0061 * Z; float S = 0.0030 * X + 0.0136 * Y + 0.9834 * Z; float3 w2 = float3(L, M, S); float3 balance = float3(w1.x / w2.x, w1.y / w2.y, w1.z / w2.z); float3x3 LIN_2_LMS_MAT = { 3.90405e-1, 5.49941e-1, 8.92632e-3, 7.08416e-2, 9.63172e-1, 1.35775e-3, 2.31082e-2, 1.28021e-1, 9.36245e-1 }; float3x3 LMS_2_LIN_MAT = { 2.85847e+0, -1.62879e+0, -2.48910e-2, -2.10182e-1, 1.15820e+0, 3.24281e-4, -4.18120e-2, -1.18169e-1, 1.06867e+0 }; float3 lms = mul(LIN_2_LMS_MAT, In); lms *= balance; Out = mul(LMS_2_LIN_MAT, lms); }"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/index.html",
    "title": "About Shader Graph | FSM Unity Framework",
    "keywords": "About Shader Graph Description Shader Graph enables you to build shaders visually. Instead of writing code, you create and connect nodes in a graph framework. Shader Graph gives instant feedback that reflects your changes, and it’s simple enough for users who are new to shader creation. For an introduction to Shader Graph, see Getting Started. Shader Graph is available through the Package Manager window in supported versions of the Unity Editor. If you install a Scriptable Render Pipeline (SRP) such as the Universal Render Pipeline (URP) or the High Definition Render Pipeline (HDRP), Unity automatically installs Shader Graph in your project. Shader Graph package versions on Unity Engine 2018.x are Preview versions, which do not receive bug fixes and feature maintenance. To work with an actively supported version of Shader Graph, use Unity Engine 2019.1 or higher. SRP packages are part of the core With the release of Unity 2021.1, graphics packages are relocating to the core of Unity. This move simplifies the experience of working with new Unity graphics features, as well as ensuring that your projects are always running on the latest verified graphics code. For each release of Unity (alpha / beta / patch release) graphics packages are embedded within the main Unity installer. When you install the latest release of Unity, you also get the latest Universal Render Pipeline (URP), High Definition Render Pipeline (HDRP), Shader Graph, Visual Effect (VFX) Graph packages, among others. Tying graphics packages to the main Unity release allows better testing to ensure that the graphics packages you use have been tested extensively with the version of Unity you have downloaded. You can also use a local copy or a custom version of the graphics packages with an override in the manifest file. For more information, see the following post on the forum: SRP v11 beta is available now."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/materialvariant-SG.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/materialvariant-SG.html",
    "title": "Material Variants | FSM Unity Framework",
    "keywords": "Material Variants When you create materials in a project, you might want to create variations based on a single material: outfits with different color schemes, damaged and undamaged versions of scenery, or shiny and weathered instances of props. You can use Material Variants to manage these variations. For more information on Material Variants in Unity, see Material Variants in the Unity User Manual. You can create a Material Variant from any Shader Graph material. For more information on how to create a Material Variant, see Create, modify, and apply Material Variants in the User Manual."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/hdrp-latest-link.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/hdrp-latest-link.html",
    "title": "hdrp-latest-link | FSM Unity Framework",
    "keywords": "For more information on the HDRP, see Unity's HDRP package documentation."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-additional-settings.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-additional-settings.html",
    "title": "nodes-additional-settings | FSM Unity Framework",
    "keywords": "node has some additional settings that you can access from the Graph Inspector:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-all-contexts.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-all-contexts.html",
    "title": "nodes-all-contexts.md | FSM Unity Framework",
    "keywords": "node can also be connected to any Block node in either Context. For more information on Block nodes and Contexts, see Master Stack."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-compatibility-all.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-compatibility-all.html",
    "title": "nodes-compatibility-all | FSM Unity Framework",
    "keywords": "node is supported on the following render pipelines: Built-In Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) Yes Yes Yes"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-compatibility-hdrp.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-compatibility-hdrp.html",
    "title": "nodes-compatibility-hdrp | FSM Unity Framework",
    "keywords": "node is supported on the following render pipelines: Built-in Render Pipeline Universal Render Pipeline (URP) High Definition Render Pipeline (HDRP) No No Yes"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-controls.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-controls.html",
    "title": "nodes-controls | FSM Unity Framework",
    "keywords": "node has the following controls:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-fragment-only.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-fragment-only.html",
    "title": "nodes-fragment-only | FSM Unity Framework",
    "keywords": "node can only be connected to a Block node in the Fragment Context. For more information on Block nodes and Contexts, refer to Master Stack."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-generated-code.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-generated-code.html",
    "title": "nodes-generated-code | FSM Unity Framework",
    "keywords": "The following code represents this node in Unity's shader code"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-inputs.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-inputs.html",
    "title": "nodes-inputs | FSM Unity Framework",
    "keywords": "node has the following input ports:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-outputs.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-outputs.html",
    "title": "nodes-outputs | FSM Unity Framework",
    "keywords": "node has the following output ports:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-related.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-related.html",
    "title": "nodes-related | FSM Unity Framework",
    "keywords": "The following nodes are related or similar to the"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-control.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-control.html",
    "title": "nodes-single-control | FSM Unity Framework",
    "keywords": "node has one control:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-input.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-input.html",
    "title": "nodes-single-input | FSM Unity Framework",
    "keywords": "node has one input port:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-output.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-single-output.html",
    "title": "nodes-single-output | FSM Unity Framework",
    "keywords": "node has one output port:"
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-subgraph-node.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/nodes-subgraph-node.html",
    "title": "nodes-subgraph-node | FSM Unity Framework",
    "keywords": "Note This node is a Subgraph node: it represents a Subgraph instead of directly representing Unity's shader code. Double-click the node in any Shader Graph to view its Subgraph."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-errors.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-errors.html",
    "title": "nodes-sample-errors | FSM Unity Framework",
    "keywords": "Note If you experience texture sampling errors for this node in a graph with Custom Function Nodes or Sub Graphs, upgrade to Shader Graph version 10.3 or later."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-fragment-lod.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-fragment-lod.html",
    "title": "nodes-sample-fragment-lod | FSM Unity Framework",
    "keywords": "With its default settings, this node can only connect to a Block node in the Fragment Context of a Shader Graph. To sample a Texture for the Vertex Context of a Shader Graph, set the Mip Sampling Mode to LOD."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-mip-bias-sample-mode-table.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-mip-bias-sample-mode-table.html",
    "title": "nodes-sample-mip-bias-sample-mode-table.md | FSM Unity Framework",
    "keywords": "Name Type Description Use Global Mip Bias Toggle Enable Use Global Mip Bias to use the render pipeline's Global Mip Bias. This bias adjusts the percentage of texture information taken from a specific mip when sampling. For more information on mip bias, see Mipmaps introduction in the Unity User Manual. Enabled Shader Graph uses the render pipeline's Global Mip Bias to adjust the texture information taken when sampling. Disabled Shader Graph doesn't use the render pipeline's Global Mip Bias to adjust texture information when sampling. Mip Sampling Mode Dropdown Choose the sampling mode to use to calculate the mip level of the texture. Standard The render pipeline calculates and automatically selects the mip for the texture. LOD The render pipeline lets you set an explicit mip for the texture on the node. The texture will always use this mip, regardless of the DDX or DDY calculations between pixels. Set the Mip Sampling Mode to LOD to connect the node to a Block node in the Vertex Context. For more information on Block nodes and Contexts, see Master Stack. Gradient The render pipeline lets you set the DDX and DDY values to use for its mip calculation, instead of using the values calculated from the texture's UV coordinates. For more information on DDX and DDY values, see Mipmaps introduction in the User Manual. Bias The render pipeline lets you set a bias to adjust the calculated mip for a texture up or down. Negative values bias the mip to a higher resolution. Positive values bias the mip to a lower resolution. The render pipeline can add this value to the value of the Global Mip Bias, or use this value instead of its Global Mip Bias. For more information on mip bias, see Mipmaps introduction in the User Manual."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-mip-mode-table.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-mip-mode-table.html",
    "title": "nodes-sample-mip-mode-table | FSM Unity Framework",
    "keywords": "LOD Float LOD NOTE The LOD Input port only displays if Mip Sampling Mode is LOD. For more information, see Additional Node Settings. The specific mip that the node uses when sampling the Texture. Bias Float Bias NOTE The Bias Input port only displays if Mip Sampling Mode is Bias. For more information, see Additional Node Settings. If Use Global Mip Bias is enabled, Unity adds this Bias amount to the Global Mip Bias for a texture's mip calculation. If Global Mip Bias is disabled, Unity uses this Bias amount instead of the Global Mip Bias. DDX Float DDX NOTE The DDX Input port only displays if Mip Sampling Mode is Gradient. For more information, see Additional Node Settings. The specific DDX value to use to calculate the Texture's mip when sampling. For more information on DDX values for mipmaps, see Mipmaps introduction in the Unity User Manual.. DDY Float DDY NOTE The DDY Input port only displays if Mip Sampling Mode is Gradient. For more information, see Additional Node Settings. The specific DDY value to use to calculate the Texture's mip when sampling. For more information on DDY values for mipmaps, see Mipmaps introduction in the Unity User Manual."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-rgba-output-table.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sample-nodes/nodes-sample-rgba-output-table.html",
    "title": "nodes-sample-rgba-output-table | FSM Unity Framework",
    "keywords": "Name Type Description RGBA Vector 4 The full RGBA Vector 4 color value of the texture sample. R Float The Red channel or X component of the texture sample. G Float The Green channel or Y component of the texture sample. B Float The Blue channel or Z component of the texture sample. A Float The Alpha channel or W component of the texture sample."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sg-node-fragment-only.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/snippets/sg-node-fragment-only.html",
    "title": "node-fragment-only | FSM Unity Framework",
    "keywords": "node can only connect to a Block node in the Fragment Context of your Shader Graph. For more information on Block nodes and Contexts, see Master Stack."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/surface-options.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/Documentation~/surface-options.html",
    "title": "Modify surface options without changing your graph | FSM Unity Framework",
    "keywords": "Modify surface options without changing your graph Description Enable Allow Material Override to modify a specific set of properties for Universal Render Pipeline Lit and Unlit Shader Graphs and for Built-In Render Pipeline Shader Graphs in the Material Inspector. Property URP Lit URP Unlit Built-In Render Pipeline Workflow Mode See the URP documentation for the Lit URP Shader. Not applicable. Not applicable. Receive Shadows Cast Shadows This property is only exposed if Allow Material Override is enabled for this Shader Graph. Enable this property to make it possible for a GameObject using this shader to cast shadows onto itself and other GameObjects. This corresponds to the SubShader Tag ForceNoShadowCasting. Not applicable. Surface Type See the URP documentation for the Lit and Unlit Shaders. In the Built-In Render Pipeline, this feature has the same behavior as in URP. Consult the URP documentation. Render Face In the Built-In Render Pipeline, this feature has the same behavior as in URP. Consult the URP documentation. Alpha Clipping In the Built-In Render Pipeline, this feature has the same behavior as in URP. Consult the URP documentation. Depth Write This property is only exposed if Allow Material Override is enabled for this Shader Graph. Use this property to determine whether the GPU writes pixels to the depth buffer when it uses this shader to render geometry. Options: Auto (default): Unity writes pixels to the depth buffer for opaque materials, but not for transparent materials. Force Enabled Unity always writes pixels to the depth buffer. Force Disabled Unity never writes pixels to the depth buffer. This option's functionality corresponds to the command ZWrite in ShaderLab. To override this setting in a RenderStateBlock, set the depthState. Depth Test This property is only exposed if Allow Material Override is enabled for this Shader Graph. Use this property to set the conditions under which pixels pass or fail depth testing. The GPU does not draw pixels that fail a depth test. If you choose anything other than LEqual (the default setting for this property), consider also changing the rendering order of this material. Options: LEqual (default): Unity draws the pixel, if its depth value is less than or equal to the value on the depth texture. Less: Unity draws pixels of the affected surface if their coordinates are less than the current depth buffer value. Never: Unity never draws the pixels of the affected surface. Less: Unity draws pixels of the affected surface if their coordinates are less than the current depth buffer value. Greater: Unity draws pixels of the affected surface if their coordinates are greater than the current depth buffer value. GEqual: Unity draws pixels of the affected surface if their coordinates are greater than or equal to the current depth buffer value. Equal: Unity draws pixels of the affected surface if their coordinates are equal to the current depth buffer value. NotEqual: Unity draws pixels of the affected surface if their coordinates are not the same as the current depth buffer value. Always: Unity draws this surface to your screen regardless of its z-coordinate. This option's functionality corresponds to the command ZTest in ShaderLab. To override this setting in a RenderStateBlock, set the depthState property. Support VFX Graph This property is only available if the Visual Effect Graph package is installed. Indicates whether this Shader Graph supports the Visual Effect Graph. If you enable this property, output contexts can use this Shader Graph to render particles. The internal setup that Shader Graph does to support visual effects happens when Unity imports the Shader Graph. This means that if you enable this property, but don't use the Shader Graph in a visual effect, there is no impact on performance. It only affects the Shader Graph import time. Not applicable. How to use To use the Material Override feature: Create a new graph in Shader Graph. Save this graph. Open the Graph Inspector. Set Active Targets to Universal or Built In. In the Graph Inspector’s Universal or Built In section, enable Allow Material Override. Create or select a Material or GameObject which uses your Shader Graph. In the Material Inspector, modify Surface Options for the target Material or GameObject."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "com.unity.shadergraph copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.shadergraph@14.0.9/README.html": {
    "href": "Library/PackageCache/com.unity.shadergraph@14.0.9/README.html",
    "title": "Shader Graph | FSM Unity Framework",
    "keywords": "Shader Graph A Shader Graph enables you to build shaders visually. Instead of hand writing code you create and connect nodes in a graph network. The graph framework gives instant feedback on the changes, and it’s simple enough that new users can become involved in shader creation. Unless you intend to modify Shader Graph or want to try out the latest and unsupported features, Unity recommends that you install Shader Graph through the Unity Package Manager: Open a Unity project. Open the Package Manager window (Window > Package Manager). In the Package Manager window, in the Packages menu, select Unity Registry. Do one of the following, based on your project needs: To use Shader Graph and the Universal Render Pipeline (URP) in your project, select Universal RP. To use Shader Graph and the High Definition Render Pipeline (HDRP), select High Definition RP. To use Shader Graph with Unity's Built-In Render Pipeline, select Shader Graph. Unity recommends using Shader Graph with URP or HDRP. Instructions If you want to try out the latest features, we recommend obtaining the most recent version of Shader Graph through the Unity Scriptable Render Pipeline (SRP) repository, which includes the Shader Graph project as a Git submodule. For more information on Git submodules, see Git's documentation on Submodules. If you don't install Shader Graph through the SRP repository, you don't have any Master Node backends available and your shaders are invalid. Invalid shaders appear pink in the Unity Editor. Installing through the repository also ensures you have a compatible set of render pipeline and Shader Graph versions. For more detailed instructions for installing from the repository, see the SRP repository's README."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.1.33] - 2022-07-12 Fixed an issue where using Assert.Expect with the same string multiple times can lead to incorrect errors in some cases (DSTR-442). Improved the logging when using multiple Assert.Expect that the logs appear in another order than expected (DSTR-442). Moved the targetPlatform specified when running tests in the TestRunnerApi from the Filter to the ExecutionSettings (DSTR-186). Fixed an issue where an inheritance of UnityPlatformAttribute which was not working (ESTT-70). Fixed the log of excluded platforms which was not displaying the right information. Added filename and linenumber to test finished message (DSTR-505). Add the possibility of running tests in a specified order from a test list (DSTR-494). [1.1.32] - 2022-04-06 Ensured that BuildTargetGroup is set correctly before TestPlayerBuildModifier is invoked (DSTR-394). Added a TestSetting that allows to build an Android App Bundle instead of APK. [1.1.31] - 2022-02-03 Fixed \"Open source code\" on tests when located inside a package. Added editor analytics events. Added buildPlayerPath argument. Path to where built player with tests is saved. [1.1.30] - 2021-10-15 Added validation of IEnumerator return type for parameterized tests with UnityTest attribute (DSTP-743). Fixed runInBackground reset to original value after finishing to run playmode tests (DSTR-248). Fixed issue with circular assembly references when constructing the test tree (DSTR-300). [1.1.29] - 2021-08-12 Nested enumerator execution order fix (DSTR-227). Fix UI not running any tests if run select on a nested namespaces (DSTR-256). [1.1.28] - 2021-06-25 Fix CountDownEvent reference due to com.unity.ext.nunit update. Various performance optimization to fix \"Test execution timed out. No activity received from the player in 600 seconds.\"(DSTR-100). [1.1.27] - 2021-06-15 Fix empty reason on passed tests results xml (DSTR-63) Fix Repeat and Retry attribute for UnityTest in PlayMode (DSTR-237). Remove XDK Xbox One platform after Unity 2020.3 Fixed issue when . suffix was applied to BuildTargets without extension. Added support for GameCoreXboxOne and GameCoreXboxSeries reduced location path length. [1.1.26] - 2021-05-25 Fix html bug in TestRunnerApi API code snippet (DS-1973). Fix typo bug in PreBuildSetup code example (DS-1974). Fix incorrect syntax in command line reference (DS-1971). Fixed a bug where test filter would match project or player path (DSTP-412). Added playerGraphicsAPI TestSettings parameter [1.1.25] - 2021-05-05 Fixed a bug where test filter would match project or player path (DSTP-412). Added playerGraphicsAPI TestSettings parameter [1.1.24] - 2021-03-04 Improving UTF documentation(DSTR-120) Updated \"Actions outside of tests\" section of user manual. Added flow charts to clarify execution order for SetUp/TearDown, TestActions, and complete flow (DSTR-121). Fixed accepted values for scriptingBackend argument to be string literals instead of int values (DSTR-122). Fixed possible values of ResultState to be Passed, Failed, Skipped, Inconclusive, plus labels instead of Success and Failure (DSTR-125). Added NUNit version information (DSTR-130). Added namespace information for LogAsset in user manual (DSTR-124). Added instructions for creating additional sets of tests (DSTR-129). Added information on testResults XML output format and exit codes (DSTR-131). Updated description of testPlatform command line argument to clarify accepted values and their meaning (DSTR-123). Reduce time taken by filtering operations when only a subset of tests is run. Reduced the time taken to rebuild the test tree and to scan for assets a test created but did not delete. Reduce the per-test overhead of running tests in the editor. Added profiler markers around test setup, teardown, and execution. Fixed unstable timeout bug (DSTR-21). [1.1.23] - 2021-01-21 Improving UTF documentation(DSTR-120) Updated \"Actions outside of tests\" section of user manual. Added flow charts to clarify execution order for SetUp/TearDown, TestActions, and complete flow (DSTR-121). Fixed accepted values for scriptingBackend argument to be string literals instead of int values (DSTR-122). Fixed possible values of ResultState to be Passed, Failed, Skipped, Inconclusive, plus labels instead of Success and Failure (DSTR-125). Added NUNit version information (DSTR-130). Added namespace information for LogAsset in user manual (DSTR-124). Added instructions for creating additional sets of tests (DSTR-129). Added information on testResults XML output format and exit codes (DSTR-131). Updated description of testPlatform command line argument to clarify accepted values and their meaning (DSTR-123). [1.1.22] - 2021-01-21 Fixed issue where test result of an explicit test was set to skipped in case it was passing and running from command line with testfilter set to the explicit test (DS-1236). Fixed an issue where tests located in assemblies that did not directly reference any test assemblies were not included (DSTR-30). Fixed an issue where UnitySetup methods were incorrectly being rerun when entering playmode, rather than being skipped (DSTR-68). Internal: Remove ##utp message AssemblyCompilationErrors (DS-1277) Fixed issue where if the timeout was exceeded in SetUp the timeout exception was not thrown(DSTR-21). Removed ability to Enable playmode tests for all assemblies from the TestRunner UI, since it is a deprecated behavior. It enforces to use of assembly definition files (DSTR-45). Fixed typo in LogAssert.cs documentation. [1.1.21] - 2020-12-04 Fixed issue where test result of an explicit test was set to skipped in case it was passing and running from command line with testfilter set to the explicit test (DS-1236). Fixed an issue where tests located in assemblies that did not directly reference any test assemblies were not included (DSTR-30). Fixed an issue where UnitySetup methods were incorrectly being rerun when entering playmode, rather than being skipped (DSTR-68). Internal: Remove ##utp message AssemblyCompilationErrors (ds-1277) Fixed issue where if the timeout was exceeded in SetUp the timeout exception was not thrown(DSTR-21). Removed ability to Enable playmode tests for all assemblies from the TestRunner UI, since it is a deprecated behavior. It enforces to use of assembly definition files (DSTR-45). [1.1.20] - 2020-12-04 The logscope is now available in OneTimeTearDown. Fixed an issue where failing tests would not result in the correct exit code if a domain reload happens after the test has run (DS-1304). If a player build fails, the test specific build settings should be cleaned up and the original values restored as intended (DS-1001). Added better error message when using TestRunCallbackAttribute and the implementation is stripped away (DS-454). Fixed an issue where the test results xml would have a zero end-time for tests executed before a domain reload (DSTR-63). Fixed OpenSource in case of a Test in a nested class (DSTR-6) UnityTests with a domain reload now works correctly in combination with Retry and Repeat attributes (DS-428). Fixed OpenSource in case of Tests located inside a package (DS-432) [1.1.19] - 2020-11-17 Command line runs with an inconclusive test result now exit with exit code 2 (case DS-951). Fixed timeout during UnitySetUp which caoused test to pass instead of failing due to wrong time format. Timeout exeption thrown when timeout time is exeded in the UnitySetup when using WaitForSeconds(n). Updating com.unity.ext.nunit version Method marked with UnityTest that are not returning IEnumerator is now giving a proper error (DS-1059). [1.1.18] - 2020-10-07 Fixed issue of timeout during UnitySetUp which wasn't detected and allowed the test to pass instead of failing (case DSTR-21) [1.1.17] - 2020-10-05 Fixed an issue where the WaitForDomainReload yield instruction would sometimes let the test continue for one frame before the domain reload. Added support for negation in filters using !. E.g. !CategoryToExclude. Fixed an issue where if the first test enters PlayMode from UnitySetup then the test body will not run on consecutive runs (case 1260901). Clear Results button clears the test results in the GUI (DSTR-16) Improved UI in Test Runner window, added new options: Run Selected Tests in player Build/Export project with all tests in player Build/Export project with selected tests in player Fixed issue on loading EditMode or Playmode test tree in the wrong tab when switching between tabs when TestRunner is loading (DS-865) [1.1.16] - 2020-07-09 Follow up on fix when UTF picks up on outdated compilation errors [1.1.15] - 2020-07-02 Fixed an issue where an exception is thrown on getting the enumerator of a UnityTest would result in stopping the test run instead of failing it (case 1212000). Including a trailing semi-colon in a testName filter no longer results in all tests being run (case 1171200). Fixed and issue when Unity Test Framework exits editor on an outdated script compilation error (during api updates) [1.1.14] - 2020-04-03 Added the 'assemblyNames' command line argument for filtering on the assembly level. The dll and project level of the tree view should now correctly show the results when running tests in a player (case 1197026). Optimize usage of player connection when transfering test results (case 1229200). Ignore internal test framework tests assertions (case 1206961). [1.1.13] - 2020-03-16 Fixed an issue where a combination of Entering / Exiting playmode and recompiling scripts would result in the test run repeating (case 1213958). Fixed a regression from 1.1.12 where prefabs left in the scene would be cleaned up to aggressively. Fixed Test execution timed out. No activity received from the player in 600 seconds error when player is not supposed to start (case 1225147) [1.1.12] - 2020-03-02 Now 'Open error line' for a failed UTF test does not throw exceptions for corrupted testable pdb in Editor release mode (case 1118259) Fixed an issue where running a test fixture would also run other fixtures with the same full name (namespace plus classname) in other assemblies (case 1197385). Running tests with the same full name, with a domain reload inbetween, will no longer fail to initialize the fixture of the second class (case 1205240). Running a playmode tests with \"Maximize on Play\" will now correctly show the result of the tests in the test runner window (case 1014908). Fixed an issue where leaving a game object in a scene with a DontSaveInEditor hideFlags would result in an error on cleanup (case 1136883). Now ITestPlayerBuildModifier.ModifyOptions is called as expected when running tests on a device (case 1213845) [1.1.11] - 2020-01-16 Fixed test runner dlls got included into player build (case 1211624) Passing a non-full-path of XML file for -testResults in Unity Batchmode issue resolved, now passing \"result.xml\" creates the result file in the project file directory (case 959078) Respect Script Debugging build setting when running tests [1.1.10] - 2019-12-19 Introduced PostSuccessfulLaunchAction callback Fixed an issue where canceling a UnityTest while it was running would incorrectly mark it as passed instead of canceled. Added command line argument for running tests synchronously. The test search bar now handles null values correctly. The test output pane now retains its size on domain reloads. [1.1.9] - 2019-12-12 Rolled back refactoring to the test run system, as it caused issues in some corner cases. [1.1.8] - 2019-11-15 Ensured that a resumed test run is continued instantly. [1.1.7] - 2019-11-14 Fixed an issue with test runs after domain reload. [1.1.6] - 2019-11-12 Building a player for test will no longer look in unrelated assemblies for relevant attributes. [1.1.5] - 2019-10-23 Fixed a regression to synchronous runs introduced in 1.1.4. [1.1.4] - 2019-10-15 Running tests in batch mode now correctly returns error code 3 (RunError) when a timeout or a build error occurs. Fixed an issue where a test run in a player would time out, if the player takes longer than 10 minutes to run. Added command line argument and api setting for specifying custom heartbeat timeout for running on players. [1.1.3] - 2019-09-23 Fixed a regression where tests in a player would report a timeout after a test run is finished. Made it possible for the ui to change its test items when the test tree changes without script compilation. Added synchronous runs as an option to the TestRunnerApi. [1.1.2] - 2019-09-11 Fixed an issue where Run Selected would run all tests in the category, if a category filter was selected, regardless of what tests were selected. Unsupported attributes used in UnityTests now give an explicit error. Added support for the Repeat and Retry attributes in UnityTests (case 1131940). Tests with a explicit timeout higher than 10 minutes, no longer times out after running longer than 10 minutes when running from command line (case 1125991). Fixed a performance regression in the test runner api result reporting, introduced in 2018.3 (case 1109865). Fixed an issue where parameterized test fixtures would not run if selected in the test tree (case 1092244). Pressing Clear Results now also correctly clears the counters on the test list (case 1181763). Prebuild setup now handles errors logged with Debug.LogError and stops the run if any is logged (case 1115240). It now also supports LogAssert.Expect. [1.1.1] - 2019-08-07 Tests retrieved as a test list with the test runner api incorrectly showed both mode as their TestMode. Fixed a compatibility issue with running tests from rider. [1.1.0] - 2019-07-30 Introduced the TestRunnerApi for running tests programmatically from elsewhere inside the Editor. Introduced yield instructions for recompiling scripts and awaiting a domain reload in Edit Mode tests. Added a button to the Test Runner UI for clearing the results. [1.0.18] - 2019-07-15 Included new full documentation of the test framework. [1.0.17] - 2019-07-11 Fixed an issue where the Test Runner window wouldn’t frame selected items after search filter is cleared. Fixed a regression where playmode test application on the IOS platform would not quit after the tests are done. [1.0.16] - 2019-06-20 Fixed an issue where the Test Runner window popped out if it was docked, or if something else was docked next to it, when re-opened (case 1158961) Fixed a regression where the running standalone playmode tests from the ui would result in an error. [1.0.15] - 2019-06-18 Added new [TestMustExpectAllLogs] attribute, which automatically does LogAssert.NoUnexpectedReceived() at the end of affected tests. See docs for this attribute for more info on usage. Fixed a regression where no tests would be run if multiple filters are specified. E.g. selecting both a whole assembly and an individual test in the ui. Fixed an issue where performing Run Selected on a selected assembly would run all assemblies. Introduced the capability to do a split build and run, when running playmode tests on standalone devices. Fixed an error in ConditionalIgnore, if the condition were not set. [1.0.14] - 2019-05-27 Fixed issue preventing scene creation in IPrebuildSetup.Setup callback when running standalone playmode tests. Fixed an issue where test assemblies would sometimes not be ordered alphabetically. Added module references to the package for the required modules: imgui and jsonserialize. Added a ConditionalIgnore attribute to help ignoring tests only under specific conditions. Fixed a typo in the player test window (case 1148671). [1.0.13] - 2019-05-07 Fixed a regression where results from the player would no longer update correctly in the UI (case 1151147). [1.0.12] - 2019-04-16 Added specific unity release to the package information. [1.0.11] - 2019-04-10 Fixed a regression from 1.0.10 where test-started events were triggered multiple times after a domain reload. [1.0.10] - 2019-04-08 Fixed an issue where test-started events would not be fired correctly after a test performing a domain reload (case 1141530). The UI should correctly run tests inside a nested class, when that class is selected. All actions should now correctly display a prefix when reporting test result. E.g. \"TearDown :\". Errors logged with Debug.LogError in TearDowns now append the error, rather than overwriting the existing result (case 1114306). Incorrect implementations of IWrapTestMethod and IWrapSetUpTearDown now gives a meaningful error. Fixed a regression where the Test Framework would run TearDown in a base class before the inheriting class (case 1142553). Fixed a regression introduced in 1.0.9 where tests with the Explicit attribute could no longer be executed. [1.0.9] - 2019-03-27 Fixed an issue where a corrupt instance of the test runner window would block for a new being opened. Added the required modules to the list of package requirements. Fixed an issue where errors would happen if the test filter ui was clicked before the ui is done loading. Fix selecting items with duplicate names in test hierarchy of Test Runner window (case 987587). Fixed RecompileScripts instruction which we use in tests (case 1128994). Fixed an issue where using multiple filters on tests would sometimes give an incorrect result. [1.0.7] - 2019-03-12 This is the first release of Unity Package com.unity.test-framework. Migrated the test-framework from the current extension in unity."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/CONTRIBUTING.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/CONTRIBUTING.html",
    "title": "Contributing | FSM Unity Framework",
    "keywords": "Contributing If you are interested in contributing, here are some ground rules: ... Define guidelines & rules for what contributors need to know to successfully make Pull requests against your repo ... All contributions are subject to the Unity Contribution Agreement(UCA) By making a pull request, you are confirming agreement to the terms and conditions of the UCA, including that your Contributions are your original creation and that you have complete right and authority to make your Contributions. Once you have a change ready following these ground rules. Simply make a pull request"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "Unity Test Framework overview Edit Mode vs. Play Mode tests Getting started with UTF How to create a new test assembly How to create a test How to run a test How to create a Play Mode test How to run a Play Mode test as standalone Resources Extending UTF How to split the build and run process for standalone Play Mode tests How to run tests programmatically How to get test results How to retrieve the list of tests Reference Running tests from the command-line UnityTest attribute Setup and cleanup at build time IPrebuildSetup IPostBuildCleanup Actions outside of tests Action execution order UnitySetUp and UnityTearDown OuterUnityTestAction Domain Reloads Custom attributes ConditionalIgnore attribute PostBuildCleanup attribute PrebuildSetup attribute TestMustExpectAllLogs attribute TestPlayerBuildModifier attribute TestRunCallback attribute UnityPlatform attribute UnitySetUp attribute UnityTearDown attribute UnityTest attribute Custom equality comparers ColorEqualityComparer FloatEqualityComparer QuaternionEqualityComparer Vector2EqualityComparer Vector3EqualityComparer Vector4EqualityComparer Custom equality comparers with equals operator Test Utils Custom yield instructions IEditModeTestYieldInstruction EnterPlayMode ExitPlayMode RecompileScripts WaitForDomainReload Custom assertion LogAssert Custom constraints Is Parameterized tests MonoBehaviour tests MonoBehaviourTest<T> IMonoBehaviourTest TestRunnerApi ExecutionSettings Filter ITestRunSettings ICallbacks IErrorCallbacks"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/edit-mode-vs-play-mode-tests.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/edit-mode-vs-play-mode-tests.html",
    "title": "Edit Mode vs. Play Mode tests | FSM Unity Framework",
    "keywords": "Edit Mode vs. Play Mode tests Let’s clarify a bit what Play Mode and Edit Mode test means from the Unity Test Framework perspective: Edit Mode tests Edit Mode tests (also known as Editor tests) are only run in the Unity Editor and have access to the Editor code in addition to the game code. With Edit Mode tests it is possible to test any of your Editor extensions using the UnityTest attribute. For Edit Mode tests, your test code runs in the EditorApplication.update callback loop. Note: You can also control entering and exiting Play Mode from your Edit Mode test. This allow your test to make changes before entering Play Mode. Edit Mode tests should meet one of the following conditions: They should have an assembly definition with reference to nunit.framework.dll and has only the Editor as a target platform: \"includePlatforms\": [ \"Editor\" ], Legacy condition: put tests in the project’s Editor folder. Play Mode tests You can run Play Mode tests as a standalone in a Player or inside the Editor. Play Mode tests allow you to exercise your game code, as the tests run as coroutines if marked with the UnityTest attribute. Play Mode tests should correspond to the following conditions: Have an assembly definition with reference to nunit.framework.dll. Have the test scripts located in a folder with the .asmdef file. The test assembly should reference an assembly within the code that you need to test. \"references\": [ \"NewAssembly\" ], \"optionalUnityReferences\": [ \"TestAssemblies\" ], \"includePlatforms\": [], Recommendations Attributes Use the NUnit Test attribute instead of the UnityTest attribute, unless you need to yield special instructions, in Edit Mode, or if you need to skip a frame or wait for a certain amount of time in Play Mode. References It is possible for your Test Assemblies to reference the test tools in UnityEngine.TestRunner and UnityEditor.TestRunner. The latter is only available in Edit Mode. You can specify these references in the Assembly Definition References on the Assembly Definition."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extending.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extending.html",
    "title": "Extending Unity Test Framework | FSM Unity Framework",
    "keywords": "Extending Unity Test Framework It is possible to extend the Unity Test Framework (UTF) in many ways, for custom workflows for your projects and for other packages to build on top of UTF. These extensions are a supplement to the ones already offered by NUnit. Some workflows for extending UTF include: How to split the build and run process for standalone Play Mode tests How to run tests programmatically How to get test results How to retrieve the list of tests"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-get-test-results.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-get-test-results.html",
    "title": "How to get test results | FSM Unity Framework",
    "keywords": "How to get test results You can receive callbacks when the active test run, or individual tests, starts and finishes. You can register callbacks by invoking RegisterCallbacks on the TestRunnerApi with an instance of a class that implements ICallbacks. There are four ICallbacks methods for the start and finish of both the whole run and each level of the test tree. Example An example of how listeners can be set up: Note: Listeners receive callbacks from all test runs, regardless of the registered TestRunnerApi for that instance. public void SetupListeners() { var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.RegisterCallbacks(new MyCallbacks()); } private class MyCallbacks : ICallbacks { public void RunStarted(ITestAdaptor testsToRun) { } public void RunFinished(ITestResultAdaptor result) { } public void TestStarted(ITestAdaptor test) { } public void TestFinished(ITestResultAdaptor result) { if (!result.HasChildren && result.ResultState != \"Passed\") { Debug.Log(string.Format(\"Test {0} {1}\", result.Test.Name, result.ResultState)); } } } Note: The registered callbacks are not persisted on domain reloads. So it is necessary to re-register the callback after a domain reloads, usually with InitializeOnLoad. It is possible to provide a priority as an integer as the second argument when registering a callback. This influences the invocation order of different callbacks. The default value is zero. It is also possible to provide RegisterCallbacks with a class instance that implements IErrorCallbacks that is an extended version of ICallbacks. IErrorCallbacks also has a callback method for OnError that invokes if the run fails to start, for example, due to compilation errors or if an IPrebuildSetup throws an exception."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-retrieve-test-list.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-retrieve-test-list.html",
    "title": "How to retrieve the list of tests | FSM Unity Framework",
    "keywords": "How to retrieve the list of tests It is possible to use the TestRunnerApi to retrieve the test tree for a given test mode (Edit Mode or Play Mode). You can retrieve the test tree by invoking RetrieveTestList with the desired TestMode and a callback action, with an ITestAdaptor representing the test tree. Example The following example retrieves the test tree for Edit Mode tests and prints the number of total test cases: var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.RetrieveTestList(TestMode.EditMode, (testRoot) => { Debug.Log(string.Format(\"Tree contains {0} tests.\", testRoot.TestCaseCount)); });"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-run-tests.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/extension-run-tests.html",
    "title": "How to run tests programmatically | FSM Unity Framework",
    "keywords": "How to run tests programmatically Filters Run tests by calling Execute on the TestRunnerApi, and provide some execution settings that consists of a Filter. The Filter specifies what tests to run. Example The following is an example of how to run all Play Mode tests in a project: var testRunnerApi = ScriptableObject.CreateInstance<TestRunnerApi>(); var filter = new Filter() { testMode = TestMode.PlayMode }; testRunnerApi.Execute(new ExecutionSettings(filter)); Multiple filter values It is possible to specify a more specific filter by filling out the fields on the Filter class in more detail. Many of the fields allow for multiple values. The runner tries to match tests against at least one of the values provided and then runs any tests that match. Example In this example, the API runs tests with full names that fit either of the two names provided: var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.Execute(new ExecutionSettings(new Filter() { testNames = new[] {\"MyTestClass.NameOfMyTest\", \"SpecificTestFixture.NameOfAnotherTest\"} })); Multiple filter fields If using multiple different fields on the filter, then it matches against tests that fulfill all the different fields. Example In this example, it runs any test that fits either of the two test names, and that also belongs to a test assembly that fits the given name. var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.Execute(new ExecutionSettings(new Filter() { assemblyNames = new [] {\"MyTestAssembly\"}, testNames = new [] {\"MyTestClass.NameOfMyTest\", \"MyTestClass.AnotherNameOfATest\"} })); Multiple constructor filters The execution settings take one or more filters in its constructor. If there is no filter provided, then it runs all Edit Mode tests by default. If there are multiple filters provided, then a test runs if it matches any of the filters. Example In this example, it runs any tests that are either in the assembly named MyTestAssembly or if the full name of the test matches either of the two provided test names: var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.Execute(new ExecutionSettings( new Filter() { assemblyNames = new[] {\"MyTestAssembly\"}, }, new Filter() { testNames = new[] {\"MyTestClass.NameOfMyTest\", \"MyTestClass.AnotherNameOfATest\"} } )); Note: Specifying different test modes or platforms in each Filter is not currently supported. The test mode and platform is from the first Filter only and defaults to Edit Mode, if not supplied."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/getting-started.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/getting-started.html",
    "title": "Getting started with Unity Test Framework | FSM Unity Framework",
    "keywords": "Getting started with Unity Test Framework To access the Unity Test Framework (UTF) in the Unity Editor, open the Test Runner window; go to Window > General > Test Runner. To get started with UTF, follow the workflows below: How to create a new test assembly How to create a test How to run a test How to create a Play Mode test How to run a Play Mode test as standalone For further information, see the resources and reference sections."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/index.html",
    "title": "About Unity Test Framework | FSM Unity Framework",
    "keywords": "About Unity Test Framework The Unity Test Framework (UTF) enables Unity users to test their code in both Edit Mode and Play Mode, and also on target platforms such as Standalone, Android, iOS, etc. This package provides a standard test framework for users of Unity and developers at Unity so that both benefit from the same features and can write tests the same way. UTF uses a Unity integration of NUnit library, which is an open-source unit testing library for .Net languages. UTF currently uses NUnit version 3.5. For more information about NUnit, see the official NUnit website and the NUnit documentation. Note: UTF is not a new concept or toolset; it is an adjusted and more descriptive naming for the toolset otherwise known as Unity Test Runner, which is now available as this package. Installing Unity Test Framework To install this package, follow the instructions in the Package Manager documentation. Note: Search for the Test Framework package. In Unity 2019.2 and higher, you may need to enable the package before use. Using Unity Test Framework To learn how to use the Unity Test Framework package in your project, read the manual. Technical details Requirements This version of the Unity Test Framework is compatible with the following versions of the Unity Editor: 2019.2 and later. Known limitations Unity Test Framework version 1.0.18 includes the following known limitations: The UnityTest attribute does not support WSA platform. The UnityTest attribute does not support Parameterized tests (except for ValueSource). The UnityTest attribute does not support the NUnit Repeat attribute. Nested test fixture cannot run from the Editor UI. When using the NUnit Retry attribute in PlayMode tests, it throws InvalidCastException. Async tests are not supported in the current version of UTF. Package contents The following table indicates the root folders in the package where you can find useful resources: Location Description /com.unity.test-framework/Documentation~ Contains the documentation for the package. Document revision history Date Reason February 4, 2021 Applied user feedback to the documentation. Matches package version 1.1.22 August 23, 2019 Applied feedback to the documentation July 25, 2019 Documentation updated to include features in version 1.1.0 July 11, 2019 Documentation updated. Matches package version 1.0.18 May 27, 2019 Documentation created. Matches package version 1.0.14"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/manual.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/manual.html",
    "title": "Unity Test Framework manual | FSM Unity Framework",
    "keywords": "Unity Test Framework manual This is the manual for the Unity Test Framework (UTF): Introduction Unity Test Framework overview Edit Mode vs. Play Mode tests Getting started Getting started with UTF Workflows: How to create a new test assembly How to create a test How to run a test How to create a Play Mode test How to run a Play Mode test in player Resources Extending UTF Extending UTF Workflows: How to split the build and run process for standalone Play Mode tests How to run tests programmatically How to get test results How to retrieve the list of tests Reference Running tests from the command-line UnityTest attribute Setup and cleanup at build time IPrebuildSetup IPostBuildCleanup Actions outside of tests Action execution order UnitySetUp and UnityTearDown OuterUnityTestAction Domain Reloads Custom attributes ConditionalIgnore attribute PostBuildCleanup attribute PrebuildSetup attribute TestMustExpectAllLogs attribute TestPlayerBuildModifier attribute TestRunCallback attribute UnityPlatform attribute UnitySetUp attribute UnityTearDown attribute UnityTest attribute Custom equality comparers ColorEqualityComparer FloatEqualityComparer QuaternionEqualityComparer Vector2EqualityComparer Vector3EqualityComparer Vector4EqualityComparer Custom equality comparers with equals operator Test Utils Custom yield instructions IEditModeTestYieldInstruction EnterPlayMode ExitPlayMode Custom assertion LogAssert Custom constraints Is Parameterized tests MonoBehaviour tests MonoBehaviourTest IMonoBehaviourTest TestRunnerApi ExecutionSettings Filter ITestRunSettings ICallbacks IErrorCallbacks"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-actions-outside-tests.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-actions-outside-tests.html",
    "title": "Actions outside of tests | FSM Unity Framework",
    "keywords": "Actions outside of tests When writing tests, it is possible to avoid duplication of code by using the SetUp and TearDown methods built into NUnit. The Unity Test Framework has extended these methods with extra functionality, which can yield commands and skip frames, in the same way as UnityTest. Action execution order The actions related to a test run in the following order: Attributes implementing IApplyToContext Any attribute implementing OuterUnityTestAction has its BeforeTest invoked Tests with UnitySetUp methods in their test class Attributes implementing IWrapSetUpTearDown Any method with the [SetUp]) attribute Action attributes have their BeforeTest method invoked Attributes implementing IWrapTestMethod The test itself runs Action attributes have their AfterTest method invoked Any method with the TearDown attribute Tests with UnityTearDown methods in their test class Any OuterUnityTestAction has its AfterTest invoked The list of actions is the same for both Test and UnityTest. Execution order flow Note: Some browsers do not support SVG image files. If the image above does not display properly (for example, if you cannot see any text), please try another browser, such as Google Chrome or Mozilla Firefox. Domain Reloads In Edit Mode tests it is possible to yield instructions that can result in a domain reload, such as entering or exiting Play Mode (see Custom yield instructions). When a domain reload happens, all non-Unity actions (such as OneTimeSetup and Setup) are rerun before the code, which initiated the domain reload, continues. Unity actions (such as UnitySetup) are not rerun. If the Unity action is the code that initiated the domain reload, then the rest of the code in the UnitySetup method runs after the domain reload."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-conditionalignore.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-conditionalignore.html",
    "title": "ConditionalIgnore attribute | FSM Unity Framework",
    "keywords": "ConditionalIgnore attribute This attribute is an alternative to the standard Ignore attribute in NUnit. It allows for ignoring tests only under a specified condition. The condition evaluates during OnLoad, referenced by ID. Example The following example shows a method to use the ConditionalIgnore attribute to ignore a test if the Unity Editor is running macOS: using UnityEditor; using NUnit.Framework; using UnityEngine.TestTools; [InitializeOnLoad] public class OnLoad { static OnLoad() { var editorIsOSX = false; #if UNITY_EDITOR_OSX editorIsOSX = true; #endif ConditionalIgnoreAttribute.AddConditionalIgnoreMapping(\"IgnoreInMacEditor\", editorIsOSX); } } public class MyTestClass { [Test, ConditionalIgnore(\"IgnoreInMacEditor\", \"Ignored on Mac editor.\")] public void TestNeverRunningInMacEditor() { Assert.Pass(); } } Note: You can only use InitializeOnLoad in Edit Mode tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testmustexpectalllogs.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testmustexpectalllogs.html",
    "title": "TestMustExpectAllLogs attribute | FSM Unity Framework",
    "keywords": "TestMustExpectAllLogs attribute The presence of this attribute causes the Test Runner to expect every single log. By default, the Test Runner only fails on error logs, but TestMustExpectAllLogs fails on warnings and info level messages as well. It is the same as calling the method LogAssert.NoUnexpectedReceived at the bottom of every affected test. Assembly-wide usage You can apply this attribute to test assemblies (that affects every test in the assembly), fixtures (affects every test in the fixture), or on individual test methods. It is also inherited from base fixtures. The MustExpect property (true by default) lets you enable or disable the higher level value. For example when migrating an assembly to this more strict checking method, you might attach [assembly:TestMustExpectAllLogs] to the assembly itself, but then whitelist failing fixtures and test methods with [TestMustExpectAllLogs(MustExpect=false)] until you have migrated them. This also means new tests in that assembly would have the more strict checking."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testplayerbuildmodifier.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testplayerbuildmodifier.html",
    "title": "TestPlayerBuildModifier attribute | FSM Unity Framework",
    "keywords": "TestPlayerBuildModifier attribute You can use the TestPlayerBuildModifier attribute to accomplish a couple of different scenarios: Modify the Player build options for Play Mode tests It is possible to change the BuildPlayerOptions for the test Player, to achieve custom behavior when running Play Mode tests. Modifying the build options allows for changing the target location of the build as well as changing BuildOptions. To modify the BuildPlayerOptions, do the following: Implement the ITestPlayerBuildModifier Reference the implementation type in a TestPlayerBuildModifier attribute on an assembly level. Example using UnityEditor; using UnityEditor.TestTools; [assembly:TestPlayerBuildModifier(typeof(BuildModifier))] public class BuildModifier : ITestPlayerBuildModifier { public BuildPlayerOptions ModifyOptions(BuildPlayerOptions playerOptions) { if (playerOptions.target == BuildTarget.iOS) { playerOptions.options |= BuildOptions.SymlinkLibraries; // Enable symlink libraries when running on iOS } playerOptions.options |= BuildOptions.AllowDebugging; // Enable allow Debugging flag on the test Player. return playerOptions; } } Note: When building the Player, it includes all TestPlayerBuildModifier attributes across all loaded assemblies, independent of the currently used test filter. As the implementation references the UnityEditor namespace, the code is typically implemented in an Editor only assembly, as the UnityEditor namespace is not available otherwise. Split build and run It is possible to use the Unity Editor for building the Player with tests, without running the tests. This allows for running the Player on e.g. another machine. In this case, it is necessary to modify the Player to build and implement a custom handling of the test result. By using TestPlayerBuildModifier, you can alter the BuildOptions to not start the Player after the build as well as build the Player at a specific location. Combined with PostBuildCleanup, you can automatically exit the Editor on completion of the build. Example using System; using System.IO; using System.Linq; using Tests; using UnityEditor; using UnityEditor.TestTools; using UnityEngine; using UnityEngine.TestTools; [assembly:TestPlayerBuildModifier(typeof(HeadlessPlayModeSetup))] [assembly:PostBuildCleanup(typeof(HeadlessPlayModeSetup))] namespace Tests { public class HeadlessPlayModeSetup : ITestPlayerBuildModifier, IPostBuildCleanup { private static bool s_RunningPlayerTests; public BuildPlayerOptions ModifyOptions(BuildPlayerOptions playerOptions) { // Do not launch the player after the build completes. playerOptions.options &= ~BuildOptions.AutoRunPlayer; // Set the headlessBuildLocation to the output directory you desire. It does not need to be inside the project. var headlessBuildLocation = Path.GetFullPath(Path.Combine(Application.dataPath, \".//..//PlayModeTestPlayer\")); var fileName = Path.GetFileName(playerOptions.locationPathName); if (!string.IsNullOrEmpty(fileName)) { headlessBuildLocation = Path.Combine(headlessBuildLocation, fileName); } playerOptions.locationPathName = headlessBuildLocation; // Instruct the cleanup to exit the Editor if the run came from the command line. // The variable is static because the cleanup is being invoked in a new instance of the class. s_RunningPlayerTests = true; return playerOptions; } public void Cleanup() { if (s_RunningPlayerTests && IsRunningTestsFromCommandLine()) { // Exit the Editor on the next update, allowing for other PostBuildCleanup steps to run. EditorApplication.update += () => { EditorApplication.Exit(0); }; } } private static bool IsRunningTestsFromCommandLine() { var commandLineArgs = Environment.GetCommandLineArgs(); return commandLineArgs.Any(value => value == \"-runTests\"); } } } If the Editor is still running after the Play Mode tests have run, the Player tries to report the results back, using PlayerConnection, which has a reference to the IP address of the Editor machine, when built. To implement a custom way of reporting the results of the test run, let one of the assemblies in the Player include a TestRunCallback. At RunFinished, it is possible to get the full test report as XML from the NUnit test result by calling result.ToXml(true). You can save the result and then save it on the device or send it to another machine as needed."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testruncallback.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-testruncallback.html",
    "title": "TestRunCallback attribute | FSM Unity Framework",
    "keywords": "TestRunCallback attribute It is possible for the test framework to invoke callbacks as the current test run progresses. To do this, there is a TestRunCallback attribute which takes the type of ITestRunCallback implementation. You can invoke the callbacks with NUnit ITest and ITestResult classes. At the RunStarted and RunFinished methods, the test and test results are for the whole test tree. These methods invoke at each node in the test tree; first with the whole test assembly, then with the test class, and last with the test method. From these callbacks, it is possible to read the partial or the full results, and it is furthermore possible to save the XML version of the result for further processing or continuous integration. Example using NUnit.Framework.Interfaces; using UnityEngine; using UnityEngine.TestRunner; [assembly:TestRunCallback(typeof(MyTestRunCallback))] public class MyTestRunCallback : ITestRunCallback { public void RunStarted(ITest testsToRun) { } public void RunFinished(ITestResult testResults) { } public void TestStarted(ITest test) { } public void TestFinished(ITestResult result) { if (!result.Test.IsSuite) { Debug.Log($\"Result of {result.Name}: {result.ResultState.Status}\"); } } } Note: The TestRunCallback does not need any references to the UnityEditor namespace and is thus able to run in standalone Players, on the Player side."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-unityplatform.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-unityplatform.html",
    "title": "UnityPlatform attribute | FSM Unity Framework",
    "keywords": "UnityPlatform attribute Use this attribute to define a specific set of platforms you want or do not want your test(s) to run on. You can use this attribute on the test method, test class, or test assembly level. Use the supported RuntimePlatform enumeration values to specify the platforms. You can also specify which platforms to test by passing one or more RuntimePlatform values along with or without the include or exclude properties as parameters to the Platform attribute constructor. The test(s) skips if the current target platform is: Not explicitly specified in the included platforms list In the excluded platforms list using UnityEngine; using UnityEngine.TestTools; using NUnit.Framework; [TestFixture] public class TestClass { [Test] [UnityPlatform(RuntimePlatform.WindowsPlayer)] public void TestMethod() { Assert.AreEqual(Application.platform, RuntimePlatform.WindowsPlayer); } } Properties Syntax Description RuntimePlatform[] exclude List the platforms you do not want to have your tests run on. RuntimePlatform[] include A subset of platforms you need to have your tests run on."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-unitytest.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-attribute-unitytest.html",
    "title": "UnityTest attribute | FSM Unity Framework",
    "keywords": "UnityTest attribute UnityTest attribute is the main addition to the standard NUnit library for the Unity Test Framework. This type of unit test allows you to skip a frame from within a test (so background tasks can finish) or give certain commands to the Unity Editor, such as performing a domain reload or entering Play Mode from an Edit Mode test. In Play Mode, the UnityTest attribute runs as a coroutine. Whereas Edit Mode tests run in the EditorApplication.update callback loop. The UnityTest attribute is, in fact, an alternative to the NUnit Test attribute, which allows yielding instructions back to the framework. Once the instruction is complete, the test run continues. If you yield return null, you skip a frame. That might be necessary to ensure that some changes do happen on the next iteration of either the EditorApplication.update loop or the game loop. Edit Mode example The most simple example of an Edit Mode test could be the one that yields null to skip the current frame and then continues to run: [UnityTest] public IEnumerator EditorUtility_WhenExecuted_ReturnsSuccess() { var utility = RunEditorUtilityInTheBackground(); while (utility.isRunning) { yield return null; } Assert.IsTrue(utility.isSuccess); } Play Mode example In Play Mode, a test runs as a coroutine attached to a MonoBehaviour. So all the yield instructions available in coroutines, are also available in your test. From a Play Mode test you can use one of Unity’s Yield Instructions: WaitForFixedUpdate: to ensure changes expected within the next cycle of physics calculations. WaitForSeconds: if you want to pause your test coroutine for a fixed amount of time. Be careful about creating long-running tests. The simplest example is to yield to WaitForFixedUpdate: [UnityTest] public IEnumerator GameObject_WithRigidBody_WillBeAffectedByPhysics() { var go = new GameObject(); go.AddComponent<Rigidbody>(); var originalPosition = go.transform.position.y; yield return new WaitForFixedUpdate(); Assert.AreNotEqual(originalPosition, go.transform.position.y); }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-command-line.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-command-line.html",
    "title": "Running tests from the command line | FSM Unity Framework",
    "keywords": "Running tests from the command line It’s pretty simple to run a test project from the command line. Here is an example in Windows: Unity.exe -runTests -batchmode -projectPath PATH_TO_YOUR_PROJECT -testResults C:\\temp\\results.xml -testPlatform PS4 Note: Use the -batchmode option when running tests on the command line to remove the need for manual user inputs. For more information, see Unity Command line arguments. Test Framework command line arguments forgetProjectPath Don't save your current Project into the Unity launcher/hub history. runTests Runs tests in the Project. testCategory A semicolon-separated list of test categories to include in the run. A semi-colon separated list should be formatted as a string enclosed in quotation marks, e.g. testCategory \"firstCategory;secondCategory\". If using both testFilter and testCategory, then only tests that match both are run. This argument supports negation using '!'. If using '!MyCategory' then no tests with the 'MyCategory' category will be included in the run. testFilter A semicolon-separated list of test names to run, or a regular expression pattern to match tests by their full name. A semi-colon separated list should be formatted as a string enclosed in quotation marks, e.g. testFilter \"Low;Medium\". This argument supports negation using '!'. If using the test filter '!MyNamespace.Something.MyTest', then all tests except that test will be run. testPlatform The platform to run tests on. Accepted values: EditMode Edit Mode tests. Equivalent to running tests from the EditMode tab of the Test Runner window. PlayMode Play Mode tests that run in the Editor. Equivalent to running tests from the PlayMode tab of the Test Runner window. Any value from the BuildTarget enum. Play Mode tests that run on a player built for the specified platform. Equivalent to using the Run all tests (<target_platform>) dropdown in the PlayMode tab of the Test Runner window. Note: If no value is specified for this argument, tests run in Edit Mode. assemblyNames A semicolon-separated list of test assemblies to include in the run. A semi-colon separated list should be formatted as a string enclosed in quotation marks, e.g. assemblyNames \"firstAssembly;secondAssembly\". testResults The path where Unity should save the result file. By default, Unity saves it in the Project’s root folder. Test results follow the XML format as defined by NUnit, see the NUnit documentation. There is currently no common definition for exit codes reported by individual Unity components under test. The best way to understand the source of a problem is the content of error messages and stack traces. playerHeartbeatTimeout The time, in seconds, the editor should wait for heartbeats after starting a test run on a player. This defaults to 10 minutes. runSynchronously If included, the test run will run tests synchronously, guaranteeing that all tests runs in one editor update call. Note that this is only supported for EditMode tests, and that tests which take multiple frames (i.e. [UnityTest] tests, or tests with [UnitySetUp] or [UnityTearDown] scaffolding) will be filtered out. testSettingsFile Path to a TestSettings.json file that allows you to set up extra options for your test run. An example of the TestSettings.json file could look like this: { \"scriptingBackend\":\"WinRTDotNET\", \"Architecture\":null, \"apiProfile\":0 } apiProfile The .Net compatibility level. Set to one of the following values: 1 - .Net 2.0 2 - .Net 2.0 Subset 3 - .Net 4.6 5 - .Net micro profile (used by Mono scripting backend if Stripping Level is set to Use micro mscorlib) 6 - .Net Standard 2.0 appleEnableAutomaticSigning Sets option for automatic signing of Apple devices. appleDeveloperTeamID Sets the team ID for the apple developer account. architecture Target architecture for Android. Set to one of the following values: None = 0 ARMv7 = 1 ARM64 = 2 X86 = 4 All = 4294967295 iOSManualProvisioningProfileType Set to one of the following values: 0 - Automatic 1 - Development 2 - Distribution iOSManualProvisioningProfileID scriptingBackend Set to one of the following values, which should be given as a string literal enclosed in quotes: Mono2x IL2CPP WinRTDotNET playerGraphicsAPI Set graphics API that will be used during test execution in the player. Value can be any GraphicsDeviceType as a string literal enclosed in quotes. Value will only be set if it is supported on the target platform. androidBuildAppBundle A boolean setting that allows to build an Android App Bundle (AAB) instead of APK for tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-color.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-color.html",
    "title": "ColorEqualityComparer | FSM Unity Framework",
    "keywords": "ColorEqualityComparer Use this class to compare two Color objects. ColorEqualityComparer.Instance has default calculation error value set to 0.01f. To set a test specific error value instantiate a comparer instance using the one argument constructor. Static properties Syntax Description Instance A singleton instance of the comparer with a default error value set to 0.01f. Constructors Syntax Description ColorEqualityComparer(float error) Creates an instance of the comparer with a custom error value. Public methods Syntax Description bool Equals(Color expected, Color actual); Compares the actual and expected Color objects for equality using Utils.AreFloatsEqualAbsoluteError to compare the RGB and Alpha attributes of Color. Returns true if expected and actual objects are equal otherwise, it returns false. Example [TestFixture] public class ColorEqualityTest { [Test] public void GivenColorsAreEqual_WithAllowedCalculationError() { // Using default error var firstColor = new Color(0f, 0f, 0f, 0f); var secondColor = new Color(0f, 0f, 0f, 0f); Assert.That(firstColor, Is.EqualTo(secondColor).Using(ColorEqualityComparer.Instance)); // Allowed error 10e-5f var comparer = new ColorEqualityComparer(10e-5f); firstColor = new Color(0f, 0f, 0f, 1f); secondColor = new Color(10e-6f, 0f, 0f, 1f); Assert.That(firstColor, Is.EqualTo(secondColor).Using(comparer)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-equals.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-equals.html",
    "title": "Custom equality comparers with equals operator | FSM Unity Framework",
    "keywords": "Custom equality comparers with equals operator If you need to compare Vectors using the overloaded operator == (see Vector2.operator ==, Vector3.operator ==, and Vector4.operator ==) you should use the respective comparer implementations: Vector2ComparerWithEqualsOperator Vector3ComparerWithEqualsOperator Vector4ComparerWithEqualsOperator The interface is the same as for other equality comparers except the public constructor error parameter is inapplicable in this case. Example [TestFixture] public class Vector3Test { [Test] public void VerifyThat_TwoVector3ObjectsAreEqual() { var actual = new Vector3(10e-7f, 10e-7f, 10e-7f); var expected = new Vector3(0f, 0f, 0f); Assert.That(actual, Is.EqualTo(expected).Using(Vector3ComparerWithEqualsOperator.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-float.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-float.html",
    "title": "FloatEqualityComparer | FSM Unity Framework",
    "keywords": "FloatEqualityComparer Use this class to compare two float values for equality with NUnit constraints. Use FloatEqualityComparer.Instance comparer to have the default error value set to 0.0001f. For any other error, use the one argument constructor to create a comparer. Static Properties Syntax Description Instance A singleton instance of the comparer with a default error value set to 0.0001f. Constructors Syntax Description FloatEqualityComparer(float allowedError) Creates an instance of the comparer with a custom error value. Public methods Syntax Description bool Equals(float expected, float actual); Compares the actual and expected float values for equality using Utils.AreFloatsEqual. Example [TestFixture] public class FloatsTest { [Test] public void VerifyThat_TwoFloatsAreEqual() { var comparer = new FloatEqualityComparer(10e-6f); var actual = -0.00009f; var expected = 0.00009f; Assert.That(actual, Is.EqualTo(expected).Using(comparer)); // Default relative error 0.0001f actual = 10e-8f; expected = 0f; Assert.That(actual, Is.EqualTo(expected).Using(FloatEqualityComparer.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-quaternion.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-quaternion.html",
    "title": "QuaternionEqualityComparer | FSM Unity Framework",
    "keywords": "QuaternionEqualityComparer Use this utility to compare two Quaternion objects for equality with NUnit assertion constraints. Use the static instance QuaternionEqualityComparer.Instance to have the default calculation error value set to 0.00001f. For any other custom error value, use the one argument constructor. Static properties Syntax Description Instance A comparer instance with the default error value 0.00001f. Constructors Syntax Description QuaternionEqualityComparer(float allowedError) Creates an instance of the comparer with a custom allowed error value. Public methods Syntax Description bool Equals(Quaternion expected, Quaternion actual) Compares the actual and expected Quaternion objects for equality using the Quaternion.Dot method. Example [TestFixture] public class QuaternionTest { [Test] public void VerifyThat_TwoQuaternionsAreEqual() { var actual = new Quaternion(10f, 0f, 0f, 0f); var expected = new Quaternion(1f, 10f, 0f, 0f); var comparer = new QuaternionEqualityComparer(10e-6f); Assert.That(actual, Is.EqualTo(expected).Using(comparer)); //Using default error 0.00001f actual = new Quaternion(10f, 0f, 0.1f, 0f); expected = new Quaternion(1f, 10f, 0.1f, 0f); Assert.That(actual, Is.EqualTo(expected).Using(QuaternionEqualityComparer.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector2.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector2.html",
    "title": "Vector2EqualityComparer | FSM Unity Framework",
    "keywords": "Vector2EqualityComparer Use this class to compare two Vector2 objects for equality with NUnit constraints. Use the static Vector2EqualityComparer.Instance to have the calculation error value set to default 0.0001f. For any other error value, instantiate a new comparer object with the one argument constructor. Static properties Syntax Description Instance A comparer instance with the default error value set to 0.0001f. Constructors Syntax Description Vector2EqualityComparer(float error) Creates an instance with a custom error value. Public methods Syntax Description Equals(Vector2 expected, Vector2 actual) Compares the actual and expected Vector2 objects for equality using the Utils.AreFloatsEqual method. Example [TestFixture] public class Vector2Test { [Test] public void VerifyThat_TwoVector2ObjectsAreEqual() { // Custom calculation error var actual = new Vector2(10e-7f, 10e-7f); var expected = new Vector2(0f, 0f); var comparer = new Vector2EqualityComparer(10e-6f); Assert.That(actual, Is.EqualTo(expected).Using(comparer)); //Default error 0.0001f actual = new Vector2(0.01f, 0.01f); expected = new Vector2(0.01f, 0.01f); Assert.That(actual, Is.EqualTo(expected).Using(Vector2EqualityComparer.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector3.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector3.html",
    "title": "Vector3EqualityComparer | FSM Unity Framework",
    "keywords": "Vector3EqualityComparer Use this class to compare two Vector3 objects for equality with NUnit constraints. Call Vector3EqualityComparer.Instance comparer to perform a comparison with the default calculation error value 0.0001f. To specify a different error value, use the one argument constructor to instantiate a new comparer. Static properties Syntax Description Instance A comparer instance with the default calculation error value equal to 0.0001f. Constructors Syntax Description Vector3EqualityComparer(float allowedError) Creates an instance with a custom error value. Public methods Syntax Description bool Equals(Vector3 expected, Vector3 actual) Compares the actual and expected Vector3 objects for equality using Utils.AreFloatsEqual to compare the x, y, and z attributes of Vector3. Example [TestFixture] public class Vector3Test { [Test] public void VerifyThat_TwoVector3ObjectsAreEqual() { // Custom error 10e-6f var actual = new Vector3(10e-8f, 10e-8f, 10e-8f); var expected = new Vector3(0f, 0f, 0f); var comparer = new Vector3EqualityComparer(10e-6f); Assert.That(actual, Is.EqualTo(expected).Using(comparer)); //Default error 0.0001f actual = new Vector3(0.01f, 0.01f, 0f); expected = new Vector3(0.01f, 0.01f, 0f); Assert.That(actual, Is.EqualTo(expected).Using(Vector3EqualityComparer.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector4.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-comparer-vector4.html",
    "title": "Vector4EqualityComparer | FSM Unity Framework",
    "keywords": "Vector4EqualityComparer Use this class to compare two Vector4 objects for equality with NUnit constraints. Call Vector4EqualityComparer.Instance to perform comparisons using default calculation error value 0.0001f. To set a custom test value, instantiate a new comparer using the one argument constructor. Static Properties Syntax Description Vector4EqualityComparer Instance A comparer instance with the default calculation error value set to 0.0001f. Constructors Syntax Description Vector4EqualityComparer(float allowedError) Creates an instance with a custom error value. Public methods Syntax Description bool Equals(Vector4 expected, Vector4 actual); Compares the actual and expected Vector4 objects for equality using Utils.AreFloatsEqual to compare the x, y, z, and w attributes of Vector4. Example [TestFixture] public class Vector4Test { [Test] public void VerifyThat_TwoVector4ObjectsAreEqual() { // Custom error 10e-6f var actual = new Vector4(0, 0, 1e-6f, 1e-6f); var expected = new Vector4(1e-6f, 0f, 0f, 0f); var comparer = new Vector4EqualityComparer(10e-6f); Assert.That(actual, Is.EqualTo(expected).Using(comparer)); // Default error 0.0001f actual = new Vector4(0.01f, 0.01f, 0f, 0f); expected = new Vector4(0.01f, 0.01f, 0f, 0f); Assert.That(actual, Is.EqualTo(expected).Using(Vector4EqualityComparer.Instance)); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-assertion.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-assertion.html",
    "title": "Custom assertion | FSM Unity Framework",
    "keywords": "Custom assertion A test fails if Unity logs a message other than a regular log or warning message. Use LogAssert to check for an expected message in the log so that the test does not fail when Unity logs the message. Use LogAssert.Expect before running the code under test, as the check for expected logs runs at the end of each frame. A test also reports a failure, if an expected message does not appear, or if Unity does not log any regular log or warning messages. Example [Test] public void LogAssertExample() { // Expect a regular log message LogAssert.Expect(LogType.Log, \"Log message\"); // The test fails without the following expected log message Debug.Log(\"Log message\"); // An error log Debug.LogError(\"Error message\"); // Without expecting an error log, the test would fail LogAssert.Expect(LogType.Error, \"Error message\"); } LogAssert LogAssert lets you expect Unity log messages that would otherwise cause the test to fail. It is available under the namespace UnityEngine.TestTools, see the Scripting API for more details. Static properties Syntax Description bool ignoreFailingMessages Set this property to true to prevent unexpected error log messages from triggering an assertion. By default, it is false. Static Methods Syntax Description void Expect(LogType type, string message); void Expect(LogType type, Regex message); Verifies that a log message of a specified type appears in the log. A test won’t fail from an expected error, assertion, or exception log message. It does fail if an expected message does not appear in the log. void NoUnexpectedReceived(); Triggers an assertion when receiving any log messages and fails the test if some are unexpected messages. If multiple tests need to check for no received unexpected logs, consider using the TestMustExpectAllLogs attribute instead. Expect string message void Expect(LogType type, string message); Parameters Syntax Description LogType type A type of log to expect. It can take one of the LogType enum values. string message A string value that should equate to the expected message. Expect Regex message void Expect(LogType type, Regex message); Parameters Syntax Description LogType type A type of log to expect. It can take one of the LogType enum values. Regex message A regular expression pattern to match the expected message."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-attributes.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-attributes.html",
    "title": "Custom attributes | FSM Unity Framework",
    "keywords": "Custom attributes As a part of UTF’s public API we provide the following attributes: ConditionalIgnore attribute PostBuildCleanup attribute PrebuildSetup attribute TestMustExpectAllLogs attribute TestPlayerBuildModifier attribute TestRunCallback attribute UnityPlatform attribute UnitySetUp attribute UnityTearDown attribute UnityTest attribute"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-constraints.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-constraints.html",
    "title": "Custom constraints | FSM Unity Framework",
    "keywords": "Custom constraints NUnit allows you to write test assertions in a more descriptive and human readable way using the Assert.That mechanism, where the first parameter is an object under test and the second parameter describes conditions that the object has to meet. Is We’ve extended NUnit API with a custom constraint type and declared an overlay Is class. To resolve ambiguity between the original implementation and the custom one you must explicitly declare it with a using statement or via addressing through the full type name UnityEngine.TestTools.Constraints.Is. Static Methods Syntax Description AllocatingGCMemory A constraint type that invokes the delegate you provide as the parameter of Assert.That and checks whether it causes any GC memory allocations. It passes if any GC memory is allocated and fails if not. Example using Is = UnityEngine.TestTools.Constraints.Is; class MyTestClass { [Test] public void MyTest() { Assert.That(() => { var i = new int[500]; }, Is.AllocatingGCMemory()); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-equality-comparers.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-equality-comparers.html",
    "title": "Custom equality comparers | FSM Unity Framework",
    "keywords": "Custom equality comparers To enable easier verification of custom Unity type values in your tests we provide you with some custom equality comparers: ColorEqualityComparer FloatEqualityComparer QuaternionEqualityComparer Vector2EqualityComparer Vector3EqualityComparer Vector4EqualityComparer Use these classes to compare two objects of the same type for equality within the range of a given tolerance using NUnit or custom constraints . Call Instance to apply the default calculation error value to the comparison. To set a specific error value, instantiate a new comparer object using a one argument constructor ctor(float error). Static properties Syntax Description Instance A singleton instance of the comparer with a predefined default error value. Constructors Syntax Description ctor(float error) Creates an instance of comparer with a custom error value.allowedError. The relative error to be considered while comparing two values. Public methods Syntax Description bool Equals(T expected, T actual); Compares the actual and expected objects for equality using a custom comparison mechanism. Returns true if expected and actual objects are equal, otherwise it returns false."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-yield-instructions.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-custom-yield-instructions.html",
    "title": "Custom yield instructions | FSM Unity Framework",
    "keywords": "Custom yield instructions By implementing this interface below, you can define custom yield instructions in Edit Mode tests. IEditModeTestYieldInstruction In an Edit Mode test, you can use IEditModeTestYieldInstruction interface to implement your own instruction. There are also a couple of commonly used implementations available: EnterPlayMode ExitPlayMode RecompileScripts WaitForDomainReload Example [UnityTest] public IEnumerator PlayOnAwakeDisabled_DoesntPlayWhenEnteringPlayMode() { var videoPlayer = PrefabUtility.InstantiatePrefab(m_VideoPlayerPrefab.GetComponent<VideoPlayer>()) as VideoPlayer; videoPlayer.playOnAwake = false; yield return new EnterPlayMode(); var videoPlayerGO = GameObject.Find(m_VideoPlayerPrefab.name); Assert.IsFalse(videoPlayerGO.GetComponent<VideoPlayer>().isPlaying); yield return new ExitPlayMode(); Object.DestroyImmediate(GameObject.Find(m_VideoPlayerPrefab.name)); } Properties Syntax Description bool ExpectDomainReload Returns true if the instruction expects a domain reload to occur. bool ExpectedPlaymodeState Returns true if the instruction expects the Unity Editor to be in Play Mode. Methods Syntax Description IEnumerator Perform() Used to define multi-frame operations performed when instantiating a yield instruction. EnterPlayMode Implements IEditModeTestYieldInstruction. Creates a yield instruction to enter Play Mode. When creating an Editor test that uses the UnityTest attribute, use this to trigger the Editor to enter Play Mode. Throws an exception if the Editor is already in Play Mode or if there is a script compilation error. ExitPlayMode Implements IEditModeTestYieldInstruction. A new instance of the class is a yield instruction to exit Play Mode. Throws an exception if the Editor is not in Play Mode."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-execution-settings.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-execution-settings.html",
    "title": "ExecutionSettings | FSM Unity Framework",
    "keywords": "ExecutionSettings The ExecutionSettings is a set of filters and other settings provided when running a set of tests from the TestRunnerApi. Constructors Syntax Description ExecutionSettings(params Filter[] filtersToExecute) Creates an instance with a given set of filters, if any. Fields Syntax Description Filter[] filters A collection of Filters to execute tests on. ITestRunSettings overloadTestRunSettings An instance of ITestRunSettings to set up before running tests on a Player. bool runSynchronously If true, the call to Execute() will run tests synchronously, guaranteeing that all tests have finished running by the time the call returns. Note that this is only supported for EditMode tests, and that tests which take multiple frames (i.e. [UnityTest] tests, or tests with [UnitySetUp] or [UnityTearDown] scaffolding) will be filtered out. 'int playerHeartbeatTimeout' The time, in seconds, the editor should wait for heartbeats after starting a test run on a player. This defaults to 10 minutes."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-filter.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-filter.html",
    "title": "Filter | FSM Unity Framework",
    "keywords": "Filter The filter class provides the TestRunnerApi with a specification of what tests to run when running tests programmatically. Fields Syntax Description TestMode testMode An enum flag that specifies if Edit Mode or Play Mode tests should run. Applying both Edit Mode and Play Mode is currently not supported when running tests from the API. string[] testNames The full name of the tests to match the filter. This is usually in the format FixtureName.TestName. If the test has test arguments, then include them in parenthesis. E.g. MyTestClass2.MyTestWithMultipleValues(1). string[] groupNames The same as testNames, except that it allows for Regex. This is useful for running specific fixtures or namespaces. E.g. \"^MyNamespace\\\\.\" Runs any tests where the top namespace is MyNamespace. string[] categoryNames The name of a Category to include in the run. Any test or fixtures runs that have a Category matching the string. string[] assemblyNames The name of assemblies included in the run. That is the assembly name, without the .dll file extension. E.g., MyTestAssembly. BuildTarget? targetPlatform The BuildTarget platform to run the test on. If set to null, then the Editor is the target for the tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-icallbacks.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-icallbacks.html",
    "title": "ICallbacks | FSM Unity Framework",
    "keywords": "ICallbacks An interface for receiving callbacks when running tests. All test runs invoke the callbacks until the next domain reload. The RunStarted method runs when the whole test run starts. Then the TestStarted method runs with information about the tests it is about to run on an assembly level. Afterward, it runs on a test fixture level and then on the individual test. If the test is a parameterized test, then it is also invoked for each parameter combination. After each part of the test tree have completed running, the corresponding TestFinished method runs with the test result. At the end of the run, the RunFinished event runs with the test result. An extended version of the callback, IErrorCallbacks, extends this ICallbacks to receive calls when a run fails due to a build error. Public methods Syntax Description void RunStarted(ITestAdaptor testsToRun) Invoked when the test run starts. The ITestAdaptor represents the tree of tests to run. void RunFinished(ITestResultAdaptor result) Invoked when the test run finishes. The ITestResultAdaptor represents the results of the set of tests that have run. void TestStarted(ITestAdaptor test) Invoked on each node of the test tree, as that part of the tree starts to run. void TestFinished(ITestResultAdaptor result) Invoked on each node of the test tree once that part of the test tree has finished running. The ITestResultAdaptor represents the results of the current node of the test tree. Example An example that sets up a listener on the API. The listener prints the number of failed tests after the run has finished: public void SetupListeners() { var api = ScriptableObject.CreateInstance<TestRunnerApi>(); api.RegisterCallbacks(new MyCallbacks()); } private class MyCallbacks : ICallbacks { public void RunStarted(ITestAdaptor testsToRun) { } public void RunFinished(ITestResultAdaptor result) { Debug.Log(string.Format(\"Run finished {0} test(s) failed.\", result.FailCount)); } public void TestStarted(ITestAdaptor test) { } public void TestFinished(ITestResultAdaptor result) { } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-ierror-callbacks.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-ierror-callbacks.html",
    "title": "IErrorCallbacks | FSM Unity Framework",
    "keywords": "IErrorCallbacks An extended version of the ICallbacks, which get invoked if the test run fails due to a build error or if any IPrebuildSetup has a failure. Public methods Syntax Description void OnError(string message) The error message detailing the reason for the run to fail."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-adaptor.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-adaptor.html",
    "title": "ITestAdaptor | FSM Unity Framework",
    "keywords": "ITestAdaptor ITestAdaptor is a representation of a node in the test tree implemented as a wrapper around the NUnit ITest interface. Properties Syntax Description string Id The ID of the test tree node. The ID can change if you add new tests to the suite. Use UniqueName, if you want to have a more permanent point of reference. string Name The name of the test. E.g., MyTest. string FullName The full name of the test. E.g., MyNamespace.MyTestClass.MyTest. int TestCaseCount The total number of test cases in the node and all sub-nodes. bool HasChildren Whether the node has any children. bool IsSuite Whether the node is a test suite/fixture. IEnumerable<ITestAdaptor> Children The child nodes. ITestAdaptor Parent The parent node, if any. int TestCaseTimeout The test case timeout in milliseconds. Note that this value is only available on TestFinished. ITypeInfo TypeInfo The type of test class as an NUnit ITypeInfo. If the node is not a test class, then the value is null. IMethodInfo Method The Nunit IMethodInfo of the test method. If the node is not a test method, then the value is null. string[] Categories An array of the categories applied to the test or fixture. bool IsTestAssembly Whether the node represents a test assembly. RunState RunState The run state of the test node. Either NotRunnable, Runnable, Explicit, Skipped, or Ignored. string Description The description of the test. string SkipReason The skip reason. E.g., if ignoring the test. string ParentId The ID of the parent node. string ParentFullName The full name of the parent node. string UniqueName A unique generated name for the test node. E.g., Tests.dll/MyNamespace/MyTestClass/[Tests][MyNamespace.MyTestClass.MyTest]. string ParentUniqueName A unique name of the parent node. E.g., Tests.dll/MyNamespace/[Tests][MyNamespace.MyTestClass][suite]. int ChildIndex The child index of the node in its parent. TestMode TestMode The mode of the test. Either Edit Mode or Play Mode. Note: Some properties are not available when receiving the test tree as a part of a test result coming from a standalone Player, such as TypeInfo and Method."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-result-adaptor.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-result-adaptor.html",
    "title": "ITestResultAdaptor | FSM Unity Framework",
    "keywords": "ITestResultAdaptor The ITestResultAdaptor is the representation of the test results for a node in the test tree implemented as a wrapper around the NUnit ITest interface. Properties Syntax Description ITestAdaptor Test The test details of the test result tree node as a TestAdaptor. string Name The name of the test node. string FullName Gets the full name of the test result string ResultState The state of the result as a string. E.g., Success, Skipped, Failure, Explicit, Cancelled. TestStatus TestStatus The status of the test as an enum. Either Inconclusive, Skipped, Passed, or Failed. double Duration Gets the elapsed time for running the test in seconds. DateTime StartTime Gets or sets the time the test started running. DateTime EndTime Gets or sets the time the test finished running. string Message Gets the message associated with a test failure or with not running the test string StackTrace Gets any stack trace associated with an error or failure. Not available in the Compact Framework 1.0. int AssertCount Gets the number of asserts that ran during the test and all its children. int FailCount Gets the number of test cases that failed when running the test and all its children. int PassCount Gets the number of test cases that passed when running the test and all its children. int SkipCount Gets the number of test cases skipped when running the test and all its children. int InconclusiveCount Gets the number of test cases that were inconclusive when running the test and all its children. bool HasChildren Indicates whether this result has any child results. Accessing HasChildren should not force the creation of the Children collection in classes implementing this interface. IEnumerable<ITestResultAdaptor> Children Gets the collection of child results. string Output Gets any text output written to this result. TNode ToXml Gets the test results as an NUnit XML node. Use this to save the results to an XML file."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-run-settings.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-itest-run-settings.html",
    "title": "ITestRunSettings | FSM Unity Framework",
    "keywords": "ITestRunSettings ITestRunSettings lets you set any of the global settings right before building a Player for a test run and then reverts the settings afterward. ITestRunSettings implements IDisposable, and runs after building the Player with tests. Public methods Syntax Description void Apply() A method called before building the Player. void Dispose() A method called after building the Player or if the build failed. Example The following example sets the iOS SDK version to be the simulator SDK and resets it to the original value after the run. public class MyTestSettings : ITestRunSettings { private iOSSdkVersion originalSdkVersion; public void Apply() { originalSdkVersion = PlayerSettings.iOS.sdkVersion; PlayerSettings.iOS.sdkVersion = iOSSdkVersion.SimulatorSDK; } public void Dispose() { PlayerSettings.iOS.sdkVersion = originalSdkVersion; } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-outerunitytestaction.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-outerunitytestaction.html",
    "title": "OuterUnityTestAction | FSM Unity Framework",
    "keywords": "OuterUnityTestAction OuterUnityTestAction is a wrapper outside of the tests, which allows for any tests with this attribute to run code before and after the tests. This method allows for yielding commands in the same way as UnityTest. The attribute must inherit the NUnit attribute and implement IOuterUnityTestAction. OuterUnityTestAction Example using System.Collections; using NUnit.Framework; using NUnit.Framework.Interfaces; using UnityEngine; using UnityEngine.TestTools; public class MyTestClass { [UnityTest, MyOuterActionAttribute] public IEnumerator MyTestInsidePlaymode() { Assert.IsTrue(Application.isPlaying); yield return null; } } public class MyOuterActionAttribute : NUnitAttribute, IOuterUnityTestAction { public IEnumerator BeforeTest(ITest test) { yield return new EnterPlayMode(); } public IEnumerator AfterTest(ITest test) { yield return new ExitPlayMode(); } } Execution order Unity outer test action is not rerun on domain reload but non-Unity action attributes are: Note: Some browsers do not support SVG image files. If the image above does not display properly (for example, if you cannot see any text), please try another browser, such as Google Chrome or Mozilla Firefox. Test actions with domain reload example using NUnit.Framework.Interfaces; public class TestActionOnSuiteAttribute : NUnitAttribute, ITestAction { public void BeforeTest(ITest test) { Debug.Log(\"TestAction OnSuite BeforeTest\"); } public void AfterTest(ITest test) { } public ActionTargets Targets { get { return ActionTargets.Suite; } } } public class TestActionOnTestAttribute : NUnitAttribute, ITestAction { public void BeforeTest(ITest test) { Debug.Log(\"TestAction OnTest BeforeTest\"); } public void AfterTest(ITest test) { Debug.Log(\"TestAction OnTest AfterTest\"); } public ActionTargets Targets { get { return ActionTargets.Test; } } } public class OuterTestAttribute : NUnitAttribute, IOuterUnityTestAction { public IEnumerator BeforeTest(ITest test) { Debug.Log(\"OuterTestAttribute BeforeTest\"); yield return null; } public IEnumerator AfterTest(ITest test) { Debug.Log(\"OuterTestAttribute AfterTest\"); yield return null; } } [TestActionOnSuite] public class ActionOrderTestBase { [Test, OuterTest, TestActionOnTest] public void UnitTest() { Debug.Log(\"Test\"); } [UnityTest, OuterTest, TestActionOnTest] public IEnumerator UnityTestWithDomainReload() { Log(\"Test part 1\"); yield return new EnterPlayMode(); //Domain reload yield return new ExitPlayMode(); Log(\"Test part 2\"); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-recompile-scripts.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-recompile-scripts.html",
    "title": "RecompileScripts | FSM Unity Framework",
    "keywords": "RecompileScripts RecompileScripts is an IEditModeTestYieldInstruction that you can yield in Edit Mode tests. It lets you trigger a recompilation of scripts in the Unity Editor. Constructors Syntax Description RecompileScripts(bool expectScriptCompilation = true, bool expectScriptCompilationSuccess = true) Creates a new instance of the RecompileScripts yield instruction. The parameter expectScriptCompilation indicates if you expect a script compilation to start (defaults to true). If a script compilation does not start and expectScriptCompilation is true, then it throws an exception. Example [UnitySetUp] public IEnumerator SetUp() { using (var file = File.CreateText(\"Assets/temp/myScript.cs\")) { file.Write(\"public class ATempClass { }\"); } AssetDatabase.Refresh(); yield return new RecompileScripts(); }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-setup-and-cleanup.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-setup-and-cleanup.html",
    "title": "Setup and cleanup at build time | FSM Unity Framework",
    "keywords": "Setup and cleanup at build time In some cases, it is relevant to perform changes to Unity or the file system before building the tests. In the same way, it may be necessary to clean up such changes after the test run. In response to such needs, you can incorporate the pre-build setup and post-build cleanup concepts into your tests in one of the following ways: Via implementation of IPrebuildSetup and IPostBuildCleanup interfaces by a test class. Via applying the PrebuildSetup attribute and PostBuildCleanup attribute on your test class, one of the tests or the test assembly, providing a class name that implements the corresponding interface as an argument (fx [PrebuildSetup(\"MyTestSceneSetup\")]). Execution order All setups run in a deterministic order one after another. The first to run are the setups defined with attributes. Then any test class implementing the interface runs, in alphabetical order inside their namespace, which is the same order as the tests run. Note: Cleanup runs right away for a standalone test run, but only after related tests run in the Unity Editor. PrebuildSetup and PostBuildCleanup Both PrebuildSetup and PostBuildCleanup attributes run if the respective test or test class is in the current test run. The test is included either by running all tests or setting a filter that includes the test. If multiple tests reference the same pre-built setup or post-build cleanup, then it only runs once. IPrebuildSetup Implement this interface if you want to define a set of actions to run as a pre-build step. Public methods Syntax Description void Setup() Implement this method to call actions automatically before the build process. IPostBuildCleanup Implement this interface if you want to define a set of actions to execute as a post-build step. Cleanup runs right away for a standalone test run, but only after all the tests run within the Editor. Public methods Syntax Description void Cleanup() Implement this method to specify actions that should run as a post-build cleanup step. Example [TestFixture] public class CreateSpriteTest : IPrebuildSetup { Texture2D m_Texture; Sprite m_Sprite; public void Setup() { #if UNITY_EDITOR var spritePath = \"Assets/Resources/Circle.png\"; var ti = UnityEditor.AssetImporter.GetAtPath(spritePath) as UnityEditor.TextureImporter; ti.textureCompression = UnityEditor.TextureImporterCompression.Uncompressed; ti.SaveAndReimport(); #endif } [SetUp] public void SetUpTest() { m_Texture = Resources.Load<Texture2D>(\"Circle\"); } [Test] public void WhenNullTextureIsPassed_CreateShouldReturnNullSprite() { // Check with Valid Texture. LogAssert.Expect(LogType.Log, \"Circle Sprite Created\"); Sprite.Create(m_Texture, new Rect(0, 0, m_Texture.width, m_Texture.height), new Vector2(0.5f, 0.5f)); Debug.Log(\"Circle Sprite Created\"); // Check with NULL Texture. Should return NULL Sprite. m_Sprite = Sprite.Create(null, new Rect(0, 0, m_Texture.width, m_Texture.height), new Vector2(0.5f, 0.5f)); Assert.That(m_Sprite, Is.Null, \"Sprite created with null texture should be null\"); } } Tip: Use #if UNITY_EDITOR if you want to access Editor only APIs, but the setup/cleanup is inside a Play Mode assembly."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-test-runner-api.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-test-runner-api.html",
    "title": "TestRunnerApi | FSM Unity Framework",
    "keywords": "TestRunnerApi The TestRunnerApi retrieves and runs tests programmatically from code inside the project, or inside other packages. TestRunnerApi is a ScriptableObject. You can initialize the API like this: var testRunnerApi = ScriptableObject.CreateInstance<TestRunnerApi>(); Note: You can subscribe and receive test results in one instance of the API, even if the run starts from another instance. The TestRunnerApi supports the following workflows: How to run tests programmatically How to get test results How to retrieve the list of tests Public methods Syntax Description void Execute(ExecutionSettings executionSettings) Starts a test run with a given set of ExecutionSettings. void RegisterCallbacks(ICallbacks testCallbacks, int priority = 0) Sets up a given instance of ICallbacks to be invoked on test runs. void UnregisterCallbacks(ICallbacks testCallbacks) Unregisters an instance of ICallbacks to no longer receive callbacks from test runs. void RetrieveTestList(TestMode testMode, Action<ITestAdaptor> callback) Retrieve the full test tree as ITestAdaptor for a given test mode."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-test-utils.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-test-utils.html",
    "title": "Test Utils | FSM Unity Framework",
    "keywords": "Test Utils This contains test utility functions for float value comparison and creating primitives. Static Methods Syntax Description bool AreFloatsEqual(float expected, float actual, float allowedRelativeError) Relative epsilon comparison of two float values for equality. allowedRelativeError is the relative error to be used in relative epsilon comparison. The relative error is the absolute error divided by the magnitude of the exact value. Returns true if the actual value is equivalent to the expected value. bool AreFloatsEqualAbsoluteError(float expected, float actual, float allowedAbsoluteError) Compares two floating point numbers for equality under the given absolute tolerance. allowedAbsoluteError is the permitted error tolerance. Returns true if the actual value is equivalent to the expected value under the given tolerance. GameObject CreatePrimitive( type) Creates a GameObject with a primitive MeshRenderer. This is an analogue to the GameObject.CreatePrimitive, but creates a primitive MeshRenderer with a fast Shader instead of the default built-in Shader, optimized for testing performance. type is the primitive type of the required GameObject. Returns a GameObject with primitive MeshRenderer and Collider. Example [TestFixture] class UtilsTests { [Test] public void CheckThat_FloatsAreEqual() { float expected = 10e-8f; float actual = 0f; float allowedRelativeError = 10e-6f; Assert.That(Utils.AreFloatsEqual(expected, actual, allowedRelativeError), Is.True); } [Test] public void CheckThat_FloatsAreAbsoluteEqual() { float expected = 0f; float actual = 10e-6f; float error = 10e-5f; Assert.That(Utils.AreFloatsEqualAbsoluteError(expected, actual, error), Is.True); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-tests-monobehaviour.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-tests-monobehaviour.html",
    "title": "MonoBehaviour tests | FSM Unity Framework",
    "keywords": "MonoBehaviour tests MonoBehaviourTest is a coroutine and a helper for writing MonoBehaviour tests. Yield a MonoBehaviourTest when using the UnityTest attribute to instantiate the MonoBehaviour you wish to test and wait for it to finish running. Implement the IMonoBehaviourTest interface on the MonoBehaviour to state when the test completes. Example [UnityTest] public IEnumerator MonoBehaviourTest_Works() { yield return new MonoBehaviourTest<MyMonoBehaviourTest>(); } public class MyMonoBehaviourTest : MonoBehaviour, IMonoBehaviourTest { private int frameCount; public bool IsTestFinished { get { return frameCount > 10; } } void Update() { frameCount++; } } MonoBehaviourTest<T> This is a wrapper that allows running tests on MonoBehaviour scripts. Inherits from CustomYieldInstruction. Properties Syntax Description T component A MonoBehaviour component created for the test and attached to the test’s GameObject. GameObject gameObject A GameObject created as a container for the test component. bool keepWaiting (Inherited) Returns true if the test is not finished yet, which keeps the coroutine suspended. IMonoBehaviourTest An interface implemented by a MonoBehaviour test. Properties Syntax Description bool IsTestFinished Indicates when the test is considered finished."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-tests-parameterized.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-tests-parameterized.html",
    "title": "Parameterized tests | FSM Unity Framework",
    "keywords": "Parameterized tests For data-driven testing, you may want to have your tests parameterized. You may use both the NUnit attributes TestCase and ValueSource with a unit test. Note: With UnityTest it is recommended to use ValueSource since TestCase is not supported. Example static int[] values = new int[] { 1, 5, 6 }; [UnityTest] public IEnumerator MyTestWithMultipleValues([ValueSource(\"values\")] int value) { yield return null; }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-unitysetup-and-unityteardown.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-unitysetup-and-unityteardown.html",
    "title": "UnitySetUp and UnityTearDown | FSM Unity Framework",
    "keywords": "UnitySetUp and UnityTearDown The UnitySetUp and UnityTearDown attributes are identical to the standard SetUp and TearDown attributes, with the exception that they allow for yielding instructions. The UnitySetUp and UnityTearDown attributes expect a return type of IEnumerator. UnitySetUp and UnityTeardown example public class SetUpTearDownExample { [UnitySetUp] public IEnumerator SetUp() { yield return new EnterPlayMode(); } [Test] public void MyTest() { Debug.Log(\"This runs inside playmode\"); } [UnityTearDown] public IEnumerator TearDown() { yield return new ExitPlayMode(); } } Execution order UnitySetUp and UnityTearDown can be used with either the Test or UnityTest test attributes. In both cases the relative execution order of Unity and non-Unity SetUp and TearDown attributes is the same. The only difference is that a UnityTest allows for yielding instructions during the test that can result in a domain reload, in which case the non-Unity SetUp and TearDown methods are re-run before proceeding to the second part of the test. Note: Some browsers do not support SVG image files. If the image above does not display properly (for example, if you cannot see any text), please try another browser, such as Google Chrome or Mozilla Firefox. Base and Derived classes The term base in the execution order denotes a base class from which a test class inherits. UnitySetUp and UnityTearDown follow the same pattern as NUnit SetUp and TearDown attributes in determining execution order between base classes and their derivatives. SetUp methods are called on base classes first, and then on derived classes. TearDown methods are called on derived classes first, and then on the base class. See the NUnit Documentation for more details. Base and Derived class example public class BaseClass { [OneTimeSetUp] public void OneTimeSetUp() { Debug.Log(\"OneTimeSetUp Base\"); } [SetUp] public void SetUp() { Debug.Log(\"SetUp Base\"); } [UnitySetUp] public IEnumerator UnitySetUp() { Debug.Log(\"UnitySetup Base\"); yield return null; } [TearDown] public void TearDown() { Debug.Log(\"TearDown Base\"); } [UnityTearDown] public IEnumerator UnityTearDown() { Debug.Log(\"UnityTearDown Base\"); yield return null; } } public class DerivedClass: BaseClass { [OneTimeSetUp] public new void OneTimeSetUp() { Debug.Log(\"OneTimeSetUp\"); } [SetUp] public new void SetUp() { Debug.Log(\"SetUp\"); } [UnitySetUp] public new IEnumerator UnitySetUp() { Debug.Log(\"UnitySetup\"); yield return null; } [Test] public void UnitTest() { Debug.Log(\"Test\"); } [UnityTest] public IEnumerator UnityTest() { Debug.Log(\"UnityTest before yield\"); yield return null; Debug.Log(\"UnityTest after yield\"); } [TearDown] public new void TearDown() { Debug.Log(\"TearDown\"); } [UnityTearDown] public new IEnumerator UnityTearDown() { Debug.Log(\"UnityTearDown\"); yield return null; } [OneTimeTearDown] public void OneTimeTearDown() { Debug.Log(\"OneTimeTearDown\"); } } Domain reload example public class BaseClass { [OneTimeSetUp] public void OneTimeSetUp() { Debug.Log(\"OneTimeSetUp Base\"); } [SetUp] public void SetUp() { Debug.Log(\"SetUp Base\"); } [UnitySetUp] public IEnumerator UnitySetUp() { Debug.Log(\"UnitySetup Base\"); yield return null; } [TearDown] public void TearDown() { Debug.Log(\"TearDown Base\"); } [UnityTearDown] public IEnumerator UnityTearDown() { Debug.Log(\"UnityTearDown Base\"); yield return null; } } public class DerivedClass: BaseClass { [OneTimeSetUp] public new void OneTimeSetUp() { Debug.Log(\"OneTimeSetUp\"); } [SetUp] public new void SetUp() { Debug.Log(\"SetUp\"); } [UnitySetUp] public new IEnumerator UnitySetUp() { Debug.Log(\"UnitySetup\"); yield return null; } [Test] public void UnitTest() { Debug.Log(\"Test\"); } [UnityTest] public IEnumerator UnityTest() { Debug.Log(\"UnityTest before yield\"); yield return new EnterPlayMode(); //Domain reload happening yield return new ExitPlayMode(); Debug.Log(\"UnityTest after yield\"); } [TearDown] public new void TearDown() { Debug.Log(\"TearDown\"); } [UnityTearDown] public new IEnumerator UnityTearDown() { Debug.Log(\"UnityTearDown\"); yield return null; } [OneTimeTearDown] public void OneTimeTearDown() { Debug.Log(\"OneTimeTearDown\"); } }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-wait-for-domain-reload.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/reference-wait-for-domain-reload.html",
    "title": "WaitForDomainReload | FSM Unity Framework",
    "keywords": "WaitForDomainReload WaitForDomainReload is an IEditModeTestYieldInstruction that you can yield in Edit Mode tests. It delays the execution of scripts until after an incoming domain reload. If the domain reload results in a script compilation failure, then it throws an exception. Constructors Syntax Description WaitForDomainReload() Create a new instance of the WaitForDomainReload yield instruction. Example [UnitySetUp] public IEnumerator SetUp() { File.Copy(\"Resources/MyDll.dll\", @\"Assets/MyDll.dll\", true); // Trigger a domain reload. AssetDatabase.Refresh(); yield return new WaitForDomainReload(); }"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/resources.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/resources.html",
    "title": "Resources | FSM Unity Framework",
    "keywords": "Resources Here you can find other related resources to the Unity Test Framework: Performance Benchmarking in Unity: How to Get Started [Blog] Testing Test-Driven Development with the Unity Test Runner [Blog]"
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-playmode-test.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-playmode-test.html",
    "title": "Workflow: How to create a Play Mode test | FSM Unity Framework",
    "keywords": "Workflow: How to create a Play Mode test To create a Play Mode test, you can follow a similar process as when you want to create an Edit Mode test. Start with switching to the PlayMode tab in the Test Runner window. Create a test assembly folder (see How to create a new test assembly). The folder name is Tests by default (or Tests 1, Tests 2, etc. if the preceding name is already in use). Note: If you don’t see the Create Play Mode Test Assembly Folder button enabled, make sure that in the Project window you navigate out of a folder with another .asmdef (such as one for Edit Mode tests). When you have your Play Mode test assembly folder ready, then create your Play Mode test. Note: Pre-defined Unity assemblies (such as Assembly-CSharp.dll) do not reference your new assembly. References and builds Unity Test Framework adds a reference to TestAssemblies in the Assembly Definition file but does not include any other references (e.g., to other scripting assemblies within the Unity project). To test other assemblies, you need to add them to the assembly definition yourself. For how to add assembly references, see Assembly Definition. We recommend putting tests into separate assemblies and using assembly definitions files. This way you will have more control over which assemblies need to reference test related dlls. Playmode build with TestsAssemblies Note: Enabling Play Mode tests for all assemblies includes additional assemblies in your project build, which can increase the project’s size as well as the build time. The supported workflow is to exclude TestAssemblies from Player builds. You can choose to enable playmode tests for all assemblies to run your tests inside the Editor, but this should be disabled again before building the Player to prevent build failures. To enable play mode tests for all assemblies you need to set the flag playModeTestRunnerEnabled to 1 inside the ProjectSettings/ProjectSetting.asset file in your project. Before building the Player you must disable it again, either by setting the flag back to 0 or by clicking on Disable playmode tests for all assemblies in the dropdown menu, accessed by right-clicking on the Test Runner window tab. For more information, see Edit Mode vs. Play Mode tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-test-assembly.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-test-assembly.html",
    "title": "Workflow: How to create a new test assembly | FSM Unity Framework",
    "keywords": "Workflow: How to create a new test assembly Unity Test Framework looks for a test inside any assembly that references NUnit. We refer to such assemblies as TestAssemblies. The Test Runner UI can help you set up TestAssemblies. Play Mode and Edit Mode tests need to be in separate assemblies. In the Test Runner window, you will see an EditMode tab enabled by default, as well as a Create EditMode Test Assembly Folder button. Click the button to create a Tests folder with a respective .asmdef file by default. Change the name of the new Assembly Definition, if necessary, and press Enter to accept it. In the Inspector window, it should have references to nunit.framework.dll*,* UnityEngine.TestRunner, and UnityEditor.TestRunner assemblies, as well as Editor preselected as a target platform. Note: The UnityEditor.TestRunner reference is only available for Edit Mode tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-test.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-create-test.html",
    "title": "Workflow: How to create a test | FSM Unity Framework",
    "keywords": "Workflow: How to create a test To create a test, do the following: Create your Test assembly folder and select it in the Project window. Click the button Create Test Script in current folder option in the Test Runner window. It creates a NewTestScript.cs file in the Tests folder. Change the name of the script, if necessary, and press Enter to accept it. Now you’ll see two sample tests in the Test Runner window: Now you can open the tests in your favorite script editor. You can also create test scripts by navigating to Assets > Create > Testing > C# Test Script, unless adding a test script would result in a compilation error. Note: Unity does not include TestAssemblies (NUnit, Unity Test Framework, and user script assemblies) when using the normal build pipeline, but does include them when using Run on <Platform> in the Test Runner window. Create additional tests To create another set of tests: In the Project window, select Assets. Create a new test assembly folder (menu: Assets > Create> Testing > Tests Assembly Folder). In the Project window, select the new folder. Create a new test script in the folder (menu: Assets > Create > Testing > C# Test Script). The assembly definition is assigned the same name as your new asset. To rename it, change the Name in the Insepctor window. Assembly definition names must be unique. Note: Changing the file name of the assembly definition file does not affect the value of the Name property in the file. Use the Inspector window to make sure the name is properly changed. By default Any Platform is preselected as the target platform for the new assembly, which means the test script appears as a PlayMode test in the TestRunner window. To change it to an EditMode test, in the Inspector window select Editor only under Platforms. New assemblies created through the Assets menu should automatically include references to UnityEngine.TestRunner and UnityEditor.TestRunner. If these references are missing, add them in the Inspector window under Assembly Definition References: Filters If you have a lot of tests, and you only want to view/run a sub-set of them, you can filter them in three ways (see image above): Type in the search box in the top left Click a test class or fixture (such as NewTestScript in the image above) Click one of the test result icon buttons in the top right For more information, see Edit Mode vs. Play Mode tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-run-playmode-test-standalone.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-run-playmode-test-standalone.html",
    "title": "Workflow: How to run a Play Mode test in player | FSM Unity Framework",
    "keywords": "Workflow: How to run a Play Mode test in player If you run a Play Mode test in the same way as an Editor test, it runs inside the Unity Editor. You can also run Play Mode tests on specific platforms. Click Run all in the player to build and run your tests on the currently active target platform. Note: Your current platform displays in brackets on the button. For example, in the image above, the button reads Run all in player (StandaloneWindows), because the current platform is Windows. The target platform is always the current Platform selected in Build Settings (menu: File > Build Settings). The test result displays in the build once the test completes: The application running on the platform reports back the test results to the Editor UI then displays the executed tests and shuts down. To make sure you receive the test results from the Player on your target platform back into the Editor that’s running the test, both should be on the same network. Note: Some platforms do not support shutting down the application with Application.Quit, so it will continue running after reporting the test results. If Unity cannot instantiate the connection, you can see the tests succeed in the running application. Running tests on platforms with arguments, in this state, does not provide XML test results. For more information, see Edit Mode vs Play Mode tests."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-run-test.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/Documentation~/workflow-run-test.html",
    "title": "Workflow: How to run a test | FSM Unity Framework",
    "keywords": "Workflow: How to run a test To run a test, you need to double-click on the test or test fixture name in the Test Runner window. You can also use one of the buttons on the top bar, Run All or Run Selected. As a result, you’ll see the test status icon changed and a counter in the top right corner updated: You may also use a context menu option Run, right-click on any item in the test tree to have it (with all its children if any) run. Run tests within Rider It is possible to run unit tests in the Unity Test Framework directly from JetBrains Rider. For more information, see the JetBrains official documentation and their blog post Run Unity tests in Rider 2018.1."
  },
  "Library/PackageCache/com.unity.test-framework@1.1.33/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.test-framework@1.1.33/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Test Framework copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to the Code Coverage package will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. [1.2.4] - 2023-06-02 Fixes Fixed failing to gather code coverage for normal methods in generic classes (case COV-27). Documentation: Corrected Settings.json path in useProjectSettings section in -coverageOptions (case COV-26). Make sure Auto Generate Report defaults to true when running from the command line (case COV-25). [1.2.3] - 2023-04-14 Fixes Fixed failing to gather code coverage for generic methods (case COV-17). Improvements Added filtersFromFile in -coverageOptions for batchmode. This allows specifying an external Json file which contains path and assembly filtering rules. When this file contains relative paths, the sourcePaths option can be used to specify the source directories. Note: The pathFiltersFromFile option will be deprecated in the next package major release. Please use the new filtersFromFile option instead. Make sure --burst-disable-compilation is expected to be passed with two dashes, unlike other editor command line options. [1.2.2] - 2022-11-18 Fixes Temporary fix for the Test Framework 1.3 issue where the RunFinished callback is not called when running from the command line and there is a domain reload (case DSTR-692). Improvements Reduced the number of logs for the default Verbosity:Info. Added Uncoverable lines definition in How to interpret the results page in the documentation. Updated documentation to match version 1.2.2. [1.2.1] - 2022-10-27 Fixes Fixed compatibility with Test Framework package version 1.3. Improvements A single file summary version of the report is now generated in Json format, in addition to the XML and Markdown formats. Added a warning when an invalid coverage option is passed in -coverageOptions in batchmode. [1.2.0] - 2022-08-01 Fixes Ensure assemblies are removed from the Included Assemblies field if they no longer exist (case 1318668). Ensure hidden sequence points are ignored (case 1372305). Changes Updated Report Generator to version 5.0.4. Updated the UI of the Code Coverage window moving the action buttons into a toolbar at the top. Renamed assemblyFilters aliases in batchmode; <user> was renamed to <assets> and <project> was renamed to <all>. Replaced pathStrippingPatterns with pathReplacePatterns in batchmode. The pathReplacePatterns option allows stripping and replacing specific sections from the paths that are stored in the coverage results xml files. See the Upgrade guide if upgrading to Code Coverage package version 1.2. Improvements The size of the coverage result files and the Code Coverage session duration have been optimized. At the start of the session a coverage xml result file is generated which includes all the lines but with zero coverage. The following coverage xml result files that are generated within a Code Coverage session include only the coverage data of the visited lines. Added Help IconButton in the toolbar in the Code Coverage window. Updated the mechanic for opening the containing folder, change the location or reset to the default location for Results Location and Report History Location. Refactored the Code Coverage window UI to include a new Report Options section and removing the word 'Generate' from the options. Introduced new selection buttons under the Included Assemblies dropdown in the Code Coverage window; use the All button to select all the assemblies in the project. Use the Assets button to select only the assemblies under the Assets folder. Use the Packages button to select only the Packages' assemblies. If searching, the buttons will apply only to the assemblies visible in the list. Updated What's new and Upgrade guide pages in the documentation. Added Using relative paths in path filters section in documentation. Updated the editor and console logs; added information about the assembly and path filters, improved coverage session logs. Improved the progress bars for Writing coverage results and Generating the report. Added an icon for the Code Coverage window. Updated documentation to match version 1.2.0. Features Added Pause Recording and Resume Recording buttons in the toolbar in the Code Coverage window. Added Log Verbosity Level setting in the Code Coverage window which allows setting the verbosity level for the editor and console logs. Added Additional Reports option in the Code Coverage window which if checked SonarQube, Cobertura and LCOV reports will be generated. Added generateAdditionalReports in -coverageOptions for batchmode. Added Test Runner References report option in the Code Coverage window which if checked includes test references to the generated coverage results and enables the Coverage by test methods section in the HTML report, allowing you to see how each test contributes to the overall coverage. Added generateTestReferences in -coverageOptions for batchmode. Added Auto Open Report option in the Code Coverage window which if checked the coverage report will open automatically after it has been generated. Added pathFiltersFromFile in -coverageOptions for batchmode which allows specifying an external file which contains a list of path filters. When this file contains relative paths, the sourcePaths option can be used to specify the source directories. Added dontClear in -coverageOptions for batchmode which allows coverage results to be accumulated after every code coverage session. If not passed the results are cleared before a new session. For more information see Generate combined report from EditMode and PlayMode tests. When the pathFilters option or the pathFiltersFromFile option in -coverageOptions contains relative paths, the sourcePaths option can be used to specify the source directories. [1.1.1] - 2021-12-17 Fixes Ensure assemblies are removed from the Included Assemblies field if they no longer exist (case 1318668) Changes Updated Report Generator to version 4.8.13 Improvements Added Help IconButton in the Code Coverage window for Unity versions 2021.2.2f1 and above Added What's new and Upgrade guide pages in the documentation Updated documentation to match version 1.1.1 [1.1.0] - 2021-06-09 Fixes Ensure Results and History folders are created if they do not exist (case 1334551) Added support for ExcludeFromCoverage/ExcludeFromCodeCoverage for lambda expressions and yield statements (case 1338636) Added support for ExcludeFromCodeCoverage for getter/setter properties (case 1338665) -coverageOptions are only parsed when running from the command line (feedback) Changes Updated Report Generator to version 4.8.9 Improvements Implemented changes to support Test Framework package version 1.2 Logs while the Report is generated are output per message rather than at the end of the generation Do not log burst warning when --burst-disable-compilation is passed in the command line Added Ignoring tests for Code Coverage section in documentation Updated the Generate combined report from separate projects section in documentation Updated documentation to match version 1.1.0 Features Added Code Coverage session Events API to subscribe to events invoked during a Code Coverage session Added useProjectSettings in -coverageOptions for batchmode which allows using the settings specified in ProjectSettings/Settings.json Added pathStrippingPatterns in -coverageOptions for batchmode which allows stripping specific sections from the paths that are stored in the coverage results xml files Added sourcePaths in -coverageOptions for batchmode which allows specifying the source directories which contain the corresponding source code [1.0.0] - 2021-03-09 Fixes Fixed issues with Path Filtering (cases 1318896, 1318897) Improvements Selection/focus is cleared when mouse is clicked outside of the individual settings' areas Added Quickstart guide in documentation Renamed the Code Coverage Workshop sample to Code Coverage Tutorial Updated documentation and workshop to match version 1.0.0 Note: In Unity 2019 and 2020 you can enable Code Coverage in General Preferences. This was removed in Unity 2021; the user interface for managing Code Coverage is now entirely inside the Code Coverage package. [1.0.0-pre.4] - 2021-02-26 Fixes Fixed assembly version validation error due to internal libraries included in the ReportGeneratorMerged.dll (case 1312121) Changes Added Enable Code Coverage checkbox under Settings in Code Coverage window. Note: In Unity 2019 and 2020 you can enable Code Coverage in General Preferences. This was removed in Unity 2021; the user interface for managing Code Coverage is now entirely inside the Code Coverage package. The settings and options passed in the command line override/disable the settings in the Code Coverage window and relevant warnings display to indicate this Updated Report Generator to version 4.8.5 Updated documentation and workshop to match version 1.0.0-pre.4 Improvements Added verbosity in -coverageOptions for batchmode Added Generate combined report from separate projects section in documentation, under Using Code Coverage in batchmode [1.0.0-pre.3] - 2021-01-21 Fixes Updated Include Platforms to Editor only in the ReportGeneratorMerged.dll settings. Fixes an Android build error introduced in 1.0.0-pre.2 (case 1306557) [1.0.0-pre.2] - 2021-01-13 Fixes Fixed multiple reports generated in batchmode when passing generateHtmlReport in -coverageOptions without passing -runTests Changes All project assemblies are included when there are included paths specified in pathFilters but no included assemblies in assemblyFilters, when running in batchmode Updated Report Generator to version 4.8.4 Updated documentation to match version 1.0.0-pre.2 Improvements Introduced new assemblyFilters aliases in batchmode, used for referencing a group of assemblies to include or exclude. These are <user>, <project> and <packages> [1.0.0-pre.1] - 2020-11-12 1.0.0-pre.1 matches 0.4.0-preview [0.4.0-preview] - 2020-11-11 Changes Moved Code Coverage window under Window > Analysis Included Assemblies now use a single dropdown instead of an editable text field which acted as a dropdown Added CommandLineParser and removed dependency to internals in Test Framework Removed the old EditorPref workflow from CoveragePreferences Moved Generate History outside of Generate HTML Report. It is now disabled only if both Generate HTML Report and Generate Badges are not selected Updated Report Generator to version 4.7.1 Updated documentation and workshop to match version 0.4.0-preview Improvements Implemented {ProjectPath} alias in Settings.json Added a console warning when Burst Compilation is enabled and an info HelpBox with a button to disable Added Analytics to help improve the user experience Disabled Generate from Last button when there are no assemblies selected Display an info HelpBox when there are no assemblies selected Paths are now stored with forward slashes on Windows Added warning about Code Coverage not being supported currently when running PlayMode tests in standalone player Refactored code; in Utils, Filtering, ResultWriter, Window and API classes Added CoverageWindow and Filtering folders Features Added Included Paths and Excluded Paths as ReorderableLists in the Code Coverage window Added support for ExcludeFromCoverage and ExcludeFromCodeCoverage attributes Added CodeCoverage.VerbosityLevel API to set the verbosity level used in editor and console logs [0.3.1-preview] - 2020-08-03 Fixes Fixed issue where CRAP calculation was incorrect when generic methods were parsed Corrected Six Labors License copyright in Third Party Notices Changes If assemblyFilters is not specified in -coverageOptions in batchmode, include only the assemblies found under the Assets folder Updated Report Generator to version 4.6.4 [0.3.0-preview] - 2020-05-20 Fixes Make sure operator and anonymous function names are generated correctly Changes Added Generate Additional Metrics setting in the Code Coverage window and removed Cyclomatic Complexity (it is now included in Additional Metrics) Updated Report Generator to version 4.5.8 Updated documentation to match version 0.3.0-preview Improvements Added Code Coverage Workshop sample project Using the Settings Manager package to handle the serialization of project settings Added an info HelpBox when Code Optimization is set to Release mode with a button to switch to Debug mode Execute Stop Recording on the update loop, instead of the OnGUI (removes an EndLayoutGroup error) Refactored code; in OpenCoverReporter class (to reduce Cyclomatic Complexity), in CodeCoverageWindow class and others Features Added History Location and Generate History settings in the Code Coverage window Added coverageHistoryPath and generateHtmlReportHistory in -coverageOptions for batchmode Added generateAdditionalMetrics in -coverageOptions for batchmode and removed enableCyclomaticComplexity (it is now included in Additional Metrics) Added Crap Score in Additional Metrics. See How to interpret the results. [0.2.3-preview] - 2020-02-18 Fixes Included Assemblies dropdown is now resizing to the longest assembly name (1215600) When closing (selecting outside of) the Included Assemblies dropdown, input is not accidentally propagated to the Code Coverage window Improvements If more than one instance of the -coverageOptions command-line argument is specified, they will now be merged into a single instance If more than one instance of the -coverageResultsPath command-line argument is specified, only the first instance will be accepted Added Generate combined report from EditMode and PlayMode tests section in documentation, under Using Code Coverage in batchmode [0.2.2-preview] - 2019-12-11 Fixes Fixed unassigned CodeCoverageWindow.m_IncludeWarnings warning in 2019.3 Changes The default Included Assemblies are now only the assemblies found under the project's Assets folder, instead of all project assemblies Improvements After the report is generated, the file viewer window highlights the index.htm file, if Generate HTML Report is selected [0.2.1-preview] - 2019-12-04 Improvements Improved globbing for pathFilters and assemblyFilters Added new sections and examples in documentation Added confirmation dialogs when selecting Clear Data and Clear History buttons Added warning and button to switch to debug mode, when using Code Optimization in release mode in 2020.1 and above Features Added pathFilters in -coverageOptions for batchmode [0.2.0-preview] - 2019-11-13 Fixes Make sure recording coverage results are saved in the Recording folder, and starting a new recording session does not affect existing non-recording data Changes Updated Report Generator to version 4.3.6 Split documentation into separate pages Improvements Updated UX design of the Code Coverage window Make sure settings and Record button are disabled when coverage is running Make sure coverage window is disabled before unity is restarted when Enabling Code Coverage in Preferences Only parse xml files with the correct filename format when generating the report Implemented try/catch when deleting files/folders when selecting Clear Data or Clear History Handle nested classes, nested generic classes and anonymous functions Features Exposed CodeCoverage.StartRecording(), CodeCoverage.StopRecording(), CodeCoverage.PauseRecording() and CodeCoverage.UnpauseRecording() API [0.1.0-preview.3] - 2019-09-27 Improvements Passing -coverageOptions generateHtmlReport on the command line now creates a report if -runTests is not passed [0.1.0-preview.2] - 2019-09-23 Changes Updated Report Generator to version 4.2.20 Improvements Added support for correct naming of c# operators Added support for correct naming of constructors Added declaring type name as a prefix Added support for return types in method names Features Added Coverage Recording feature [0.1.0-preview.0] - 2019-03-18 This is the first release of Code Coverage Package"
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CodeCoverageWindow.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CodeCoverageWindow.html",
    "title": "Code Coverage window | FSM Unity Framework",
    "keywords": "Code Coverage window Toolbar Description Select Start Recording to start recording code coverage data and Stop Recording to stop recording. Select Pause Recording to pause recording code coverage data and Resume Recording to resume recording. The buttons are disabled if not in a Coverage Recording session. Generate Report Select Generate Report to generate a coverage report from the last set of tests that were run in the Test Runner or from the last Coverage Recording session. Note that Generate Report is disabled if no tests ran, there is no Coverage Recording data or all HTML Report, Additional Reports and Summary Badges checkboxes are unchecked. Clear Results Select Clear Results to clear the coverage data from previous test runs or from previous Coverage Recording sessions. Clear Results is disabled if the coverage results are cleared, if no tests ran, or if there is no Coverage Recording data. Clear History Select Clear History to clear the coverage report history. Clear History is disabled if the history is cleared or if no reports were generated. Select Help to open the Documentation Reference for Code Coverage in the web browser. Settings Description Enable Code Coverage Check this to enable Code Coverage. This is required in order to generate Coverage data and reports. Note that Code Coverage can affect the Editor performance. Results Location Select the dropdown to open or specify the folder where the coverage results and report are saved to, and to reset to the default location. The default location is the Project's folder. Report History Location Select the dropdown to open or specify the folder where the coverage report history is saved to, and to reset to the default location. The default location is the Project's folder. Included Assemblies Specify assemblies to be included in the coverage results. This is a dropdown list containing the available assemblies. Click the dropdown to view, select or deselect the assemblies. Select All to select all the assemblies in the project. Select Assets to select only the assemblies under the Assets folder. Select Packages to select only the Packages' assemblies. Select Deselect All to deselect all the assemblies. Note: If searching, the buttons will apply only to the assemblies visible in the list. Included Paths Select Add (+) to specify individual folders and files to include in coverage results. You can use globbing to filter the paths. If the list is empty, Unity includes all files in the Included Assemblies. To remove an individual list entry, select the entry and then select Remove (-). Excluded Paths Select Add (+) to specify individual folders and files to exclude from coverage results. You can use globbing to filter the paths. To remove an individual list entry, select the entry and then select Remove (-). Log Verbosity Level Click the dropdown to set the verbosity level for the editor and console logs. The default level is Info. Levels: Verbose All logs will be printed. Info Logs, Warnings and Errors will be printed. Warning Warnings and Errors will be printed. Error Only Errors will be printed. Off No logs will be printed. Report Options Description HTML Report Check this to generate an HTML report. Additional Reports Check this to generate SonarQube, Cobertura and LCOV reports. Report History Check this to generate and include the coverage history in the HTML report. Summary Badges Check this to generate coverage summary badges in SVG and PNG format. Additional Metrics Check this to generate and include additional metrics in the HTML report. These currently include Cyclomatic Complexity and Crap Score calculations for each method. See the Risk Hotspots section for more information. Test Runner References Check this to include test references to the generated coverage results and enable the Coverage by test methods section in the HTML report. This shows how each test contributes to the overall coverage. Auto Generate Report Check this to generate the report automatically after the Test Runner finishes running or the Coverage Recording session is complete. Auto Open Report Check this to open the coverage report automatically after it has been generated."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageBatchmode.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageBatchmode.html",
    "title": "Using Code Coverage in batchmode | FSM Unity Framework",
    "keywords": "Using Code Coverage in batchmode You can pass the following arguments in batchmode: -enableCodeCoverage, to enable code coverage. -coverageResultsPath (optional), to set the location where the coverage results and report are saved to. The default location is the project's path. -coverageHistoryPath (optional), to set the location where the coverage report history is saved to. The default location is the project's path. -coverageOptions (optional), to pass extra options. Options are separated by semicolon. Some shells use semicolons to separate commands. Therefore, to ensure that coverage options are parsed correctly, enclose them in quotation marks. Coverage Option Description generateHtmlReport Add this to generate a coverage HTML report. generateHtmlReportHistory Add this to generate and include the coverage history in the HTML report. generateAdditionalReports Add this to generate SonarQube, Cobertura and LCOV reports. generateBadgeReport Add this to generate coverage summary badges in SVG and PNG format. generateAdditionalMetrics Add this to generate and include additional metrics in the HTML report. These currently include Cyclomatic Complexity and Crap Score calculations for each method. See the Risk Hotspots section for more information. generateTestReferences Add this to include test references to the generated coverage results and enable the Coverage by test methods section in the HTML report. This shows how each test contributes to the overall coverage. verbosity Add this to set the verbosity level for the editor and console logs. The default value is info. Values: verbose, info, warning, error, off useProjectSettings Add this to use the settings specified in ProjectSettings/Packages/com.unity.testtools.codecoverage/Settings.json instead. Any options passed in the command line will override this. This option can only be used in batchmode and it does not take effect when running the editor from the command line in non-batchmode. dontClear Add this to allow coverage results to be accumulated after every code coverage session. If not passed the results are cleared before a new session. For more information see Generate combined report from EditMode and PlayMode tests. sourcePaths Add this to specify the source directories which contain the corresponding source code. The source directories are used by the report generator when the path information of classes cannot be determined. This is a comma separated string. Globbing is not supported. Example: See Generate combined report from separate projects. assemblyFilters Add this to specify the assemblies to include or exclude in the coverage calculation and/or report. This is a comma-separated string. Prefix assemblies with + to include them or with - to exclude them. Globbing can be used to filter the assemblies. Available aliases: <all> maps to all the assemblies in the project. <assets> maps to the assemblies under the Assets folder. <packages> maps to the Packages' assemblies in the project, including the built-in packages. By default, if there are no included assemblies specified, only the assemblies under the Assets folder will be included. Examples: assemblyFilters:+<all> will include code from all the assemblies in the project. assemblyFilters:+my.assembly will only include code from the assembly called my.assembly. assemblyFilters:+unity.* will include code from any assembly whose name starts with unity. assemblyFilters:-*unity* will exclude code from all assemblies that contain the word unity in their names. assemblyFilters:+my.assembly.*,-my.assembly.tests will include code from any assembly whose name starts with my.assembly., but will explicitly exclude code from the assembly called my.assembly.tests. assemblyFilters:+my.locale.?? will only include code from assemblies whose names match this format, e.g. my.locale.en, my.locale.99, etc. assemblyFilters:+my.assembly.[a-z][0-9] will only include code from assemblies whose names match this format, e.g. my.assembly.a1, my.assembly.q7, etc. pathFilters Add this to specify the paths that should be included or excluded in the coverage calculation and/or report. This is a comma-separated string. Prefix paths with + to include them and with - to exclude them. Globbing can be used to filter the paths. Both absolute and relative paths are supported. Absolute paths can be shortened using globbing e.g. **/Assets/Scripts/. Relative paths require the sourcePaths option to be set. See Using relative paths in path filters. Note: If pathFilters are specified and there are no included assemblies specified in assemblyFilters, then all the assemblies in the project are included in order for path filtering to take precedence over assembly filtering. Examples: pathFilters:+C:/MyProject/Assets/MyClass.cs will only include the MyClass.cs file. pathFilters:+C:/MyProject/Assets/Scripts/* will include all files in the C:/MyProject/Assets/Scripts folder. Files in subfolders will not be included. pathFilters:-C:/MyProject/Assets/AutoGenerated/** will exclude all files under the C:/MyProject/Assets/AutoGenerated folder and any of its subfolders. pathFilters:+**/Assets/Editor/** will include just the files that have /Assets/Editor/ in their path. pathFilters:+C:/MyProject/Assets/**/MyClass.cs will include any file named MyClass.cs that is under the C:/MyProject/Assets folder and any of its subfolders. pathFilters:+C:/MyProject/**,-**/Packages/** will only include files under C:/MyProject/ folder and exclude all files under any Packages folder. pathFilters:+**/MyGeneratedClass_??.cs will include only files with filenames that match this format, i.e. MyGeneratedClass_01.cs, MyGeneratedClass_AB.cs, etc. pathFilters:+**/MyClass_[A-Z][0-9].cs will include only files with filenames that match this format, i.e. MyClass_A1.cs, MyClass_Q7.cs, etc. pathFiltersFromFile Add this to specify the file to read path filtering rules from. Instead of defining all path filtering rules directly in the command line, as you would with the pathFilters option, this allows you to store them in a separate file, making your commands clearer and easier to manage. Like with the pathFilters option, pathFiltersFromFile also supports relative paths. See Using relative paths in path filters. Examples: pathFiltersFromFile:C:/MyProject/FilteringRules.txt will read rules from a file located in C:/MyProject/FilteringRules.txt pathFiltersFromFile:FilteringRules.txt will read rules from FilteringRules.txt located in the root of your project. Syntax of the rules is the same as with the pathFilters option, however, rules should be listed in separate lines in the file. File example: This will include all the files in the Scripts folder and exclude all the files in the Scripts/Generated folder +/Scripts/ -/Scripts/Generated/ Note: The pathFiltersFromFile option will be deprecated in the next package major release. Please use the filtersFromFile option instead. filtersFromFile Add this to specify the json file to read path and assembly filtering rules from. Instead of defining all filtering rules directly in the command line, as you would with pathFilters and assemblyFilters options, this allows you to store them in a separate file, making your commands clearer and easier to manage. Like with the pathFilters option, filtersFromFile also supports relative paths. See Using relative paths in path filters. Examples: filtersFromfile:C:/MyProject/FilteringRules.json will read rules from a file located in C:/MyProject/FilteringRules.json. filtersFromFile:FilteringRules.json will read rules from FilteringRules.json located in the root of your project. File example: This will include the my.included.assembly, exclude my.excluded.assembly and all assemblies with unity in their name. It will also include all files in the Scripts folder, and exclude all files in the Scripts/Generated folder { \"assembliesInclude\": [ \"my.included.assembly\" ], \"assembliesExclude\": [ \"my.excluded.assembly\", \"unity\" ], \"pathsInclude\": [ \"/Scripts/\" ], \"pathsExclude\": [ \"/Scripts/Generated/\" ] } Note: The pathFiltersFromFile option will be deprecated in the next package major release. Please use the filtersFromFile option instead. pathReplacePatterns Add this to replace specific sections from the paths that are stored in the coverage results xml files. This is a comma separated string and requires elements to be passed in pairs i.e. pathReplacePatterns:from,to,from,to. Globbing is supported. You can change the file paths in the coverage results xml to relative paths so that coverage data generated on different machines can be merged into a single report. Use the pathReplacePatterns option in conjunction with the sourcePaths option to specify the source directories which contain the corresponding source code. For more information see Generate combined report from separate projects. Note: The OpenCover results xml format specifies file paths as absolute paths (fullPath). Changing these paths to relative paths will invalidate the OpenCover standard format. When the results xml files are fed into other tools, these may not work as expected if the paths are relative. Examples: pathReplacePatterns:C:/MyProject,C:/MyOtherProject will store the path as C:/MyOtherProject/Assets/Scripts/MyScript.cs, when the original path is C:/MyProject/Assets/Scripts/MyScript.cs pathReplacePatterns:@*,,**/PackageCache/,Packages/ will store the path as Packages/com.unity.my.package/Editor/MyScript.cs, when the original path is C:/Project/Library/PackageCache/com.unity.my.package@12345/Editor/MyScript.cs pathReplacePatterns:C:/MyProject/, will store the path as Assets/Scripts/MyScript.cs, when the original path is C:/MyProject/Assets/Scripts/MyScript.cs pathReplacePatterns:**Assets/, will store the path as Scripts/MyScript.cs, when the original path is C:/MyProject/Assets/Scripts/MyScript.cs pathReplacePatterns:C:/*/Assets/, will store the path as Scripts/MyScript.cs, when the original path is C:/MyProject/Assets/Scripts/MyScript.cs pathReplacePatterns:C:/MyProject??/, will store the path as Assets/Scripts/MyScript.cs, when the original path is C:/MyProject01/Assets/Scripts/MyScript.cs pathReplacePatterns:**/MyProject[A-Z][0-9]/, will store the path as Assets/Scripts/MyScript.cs, when the original path is C:/MyProjectA1/Assets/Scripts/MyScript.cs Example Unity.exe -projectPath <path-to-project> -batchmode -testPlatform editmode -runTests -testResults <path-to-results-xml> -debugCodeOptimization -enableCodeCoverage -coverageResultsPath <path-to-coverage-results> -coverageHistoryPath <path-to-coverage-history> -coverageOptions \"generateAdditionalMetrics;generateHtmlReport;generateHtmlReportHistory;generateBadgeReport; assemblyFilters:+my.assembly.*,+<packages>; pathFilters:-**/Tests/**,-**/BuiltInPackages/**\" The example above opens the project at \\<path-to-project\\>, runs the EditMode tests and produces an HTML coverage report and coverage summary badges in \\<path-to-coverage-results\\>. The report includes the coverage history, Cyclomatic Complexity and Crap Score calculations. The coverage history files are saved in \\<path-to-coverage-history\\>. Additionally, the report includes code from any assembly whose name starts with my.assembly., and includes code from all the Packages' assemblies. It excludes files that have /Tests/ in their path (i.e. all the files under the Tests folder) and also excludes files that have /BuiltInPackages/ in their path (i.e. all the built-in packages). Note: -debugCodeOptimization is passed above to ensure Code optimization is set to Debug mode. See Using Code Coverage with Code Optimization. Generate combined report from EditMode and PlayMode tests To get coverage information for both EditMode and PlayMode tests, run the editor three times as shown in the example below: Unity.exe -projectPath <path-to-project> -batchmode -testPlatform editmode -runTests -debugCodeOptimization -enableCodeCoverage -coverageResultsPath <path-to-coverage-results> -coverageOptions \"generateAdditionalMetrics;assemblyFilters:+my.assembly.*;dontClear\" Unity.exe -projectPath <path-to-project> -batchmode -testPlatform playmode -runTests -debugCodeOptimization -enableCodeCoverage -coverageResultsPath <path-to-coverage-results> -coverageOptions \"generateAdditionalMetrics;assemblyFilters:+my.assembly.*;dontClear\" Unity.exe -projectPath <path-to-project> -batchmode -debugCodeOptimization -enableCodeCoverage -coverageResultsPath <path-to-coverage-results> -coverageOptions \"generateHtmlReport;generateBadgeReport;assemblyFilters:+my.assembly.*\" -quit The first generates the coverage results for the EditMode tests, the second generates the coverage results for the PlayMode tests and the third generates the coverage report and summary badges based on both coverage results. Note: In Unity Test Framework 2.0 and above the coverage results from both the EditMode and PlayMode test runs are stored in the Automated folder. In this example, passing the dontClear coverage option ensures that the results from the EditMode test run are not cleared during the PlayMode test run. Generate combined report from separate projects To get a coverage report for your shared code which is used on separate projects, run the tests for each project making sure the -coverageResultsPath points to a separate location inside a shared root folder as shown in the example below: Unity.exe -projectPath C:/MyProject -batchmode -testPlatform playmode -runTests -debugCodeOptimization -enableCodeCoverage -coverageResultsPath C:/CoverageResults/MyProject -coverageOptions \"generateAdditionalMetrics;assemblyFilters:+my.assembly.*;pathReplacePatterns:C:/MyProject/,\" Unity.exe -projectPath C:/MyOtherProject -batchmode -testPlatform playmode -runTests -debugCodeOptimization -enableCodeCoverage -coverageResultsPath C:/CoverageResults/MyOtherProject -coverageOptions \"generateAdditionalMetrics;assemblyFilters:+my.assembly.*;pathReplacePatterns:C:/MyOtherProject/,\" Unity.exe -projectPath C:/MyProject -batchmode -debugCodeOptimization -enableCodeCoverage -coverageResultsPath C:/CoverageResults -coverageOptions \"generateHtmlReport;generateBadgeReport;assemblyFilters:+my.assembly.*;sourcePaths:C:/MyProject\" -quit The first run generates the coverage results for the PlayMode tests for MyProject and stores these in C:/CoverageResults/MyProject. The second run generates the coverage results for the PlayMode tests for MyOtherProject and stores these in C:/CoverageResults/MyOtherProject. The third run generates the coverage report and summary badges based on the results found under the common C:/CoverageResults folder. Using relative paths in path filters When the sourcePaths option is specified, the path filtering rules set by the pathFilters, pathFiltersFromFile and filtersFromFile options can be defined as relative paths. Example: Unity.exe -projectPath C:/MyProject -batchmode -testPlatform playmode -runTests -debugCodeOptimization -enableCodeCoverage -coverageResultsPath C:/CoverageResults/MyProject -coverageOptions \"generateHtmlReport;generateAdditionalMetrics;assemblyFilters:+<all>;pathFiltersFromFile:FilteringRules.txt;sourcePaths:C:/MyProject/Assets\" FilteringRules.txt +Scripts/Animation/** -**/Generated/** +C:/MyPackages/com.my.company.mypackage/** This example contains three rules: +Scripts/Animation/** - because the sourcePaths option was set and this is a relative path, this rule will include all the scripts in the C:/MyProject/Assets/Scripts/Animation folder and its subfolders. -**/Generated/** - excludes all the files that have /Generated/ in their path. This is not a relative path so the sourcePaths option has no effect. +C:/MyPackages/com.my.company.mypackage/** - includes all the scripts located in the package outside of the project. This is an absolute path so the sourcePaths option has no effect."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageRecording.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageRecording.html",
    "title": "On-demand coverage recording | FSM Unity Framework",
    "keywords": "On-demand coverage recording With Coverage Recording you can capture coverage data on demand and generate an HTML report which shows which lines of your code run while recording. It supports capturing in EditMode as well as in PlayMode, and you can switch between the two. To start recording coverage data, select Start Recording. While recording, use the Editor as usual, for example to enter PlayMode. To stop recording coverage data, select Stop Recording. If Auto Generate Report is checked, then an HTML report is generated and a file viewer window opens (if Auto Open Report is checked too). It contains the coverage results and the report. Otherwise, select Generate Report to generate the report. The results are based on the assemblies specified in Included Assemblies. You can also control Coverage Recording via the CodeCoverage ScriptingAPI. Steps Open the Code Coverage window (go to Window > Analysis > Code Coverage). Select Enable Code Coverage if not already selected, to be able to generate Coverage data and reports. Note: Enabling Code Coverage adds some overhead to the Editor and can affect the performance. Select the Assembly Definitions you would like to see the coverage for. In this example we selected Assembly-CSharp and Assembly-CSharp-Editor. By default, Unity compiles almost all project scripts into the Assembly-CSharp.dll managed assembly and all Editor scripts into the Assembly-CSharp-Editor.dll managed assembly. Select Start Recording. Continue using the Editor as normal, for example enter PlayMode to test your application or run some manual testing. You can also select Pause Recording to pause recording and Resume Recording to resume recording. When you have finished your testing and have collected enough coverage data, select Stop Recording. If Auto Open Report is checked a file viewer window opens containing the coverage report. Alternatively, select the Results Location dropdown to open it in the file viewer. Note: To generate the report automatically after you stop recording, select Auto Generate Report in the Code Coverage window. Alternatively, you can select Generate Report. Select index.htm. This opens the HTML coverage report."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageTestRunner.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/CoverageTestRunner.html",
    "title": "Using Code Coverage with Test Runner | FSM Unity Framework",
    "keywords": "Using Code Coverage with Test Runner When running your tests in the Test Runner you can generate an HTML report which shows which lines of your code the tests cover. This includes both EditMode and PlayMode tests. If Auto Generate Report is checked, then an HTML report is generated and a file viewer window opens (if Auto Open Report is checked too). It contains the coverage results and the report. Otherwise, select Generate Report to generate the report. The results are based on the assemblies specified in Included Assemblies. Steps Open the Code Coverage window (go to Window > Analysis > Code Coverage). Select Enable Code Coverage if not already selected, to be able to generate Coverage data and reports. Note: Enabling Code Coverage adds some overhead to the Editor and can affect the performance. Select the Assembly Definitions you would like to see the coverage for. In this example we selected Assembly-CSharp and Assembly-CSharp-Editor. By default, Unity compiles almost all project scripts into the Assembly-CSharp.dll managed assembly and all Editor scripts into the Assembly-CSharp-Editor.dll managed assembly. Switch to the Test Runner and run your EditMode and/or PlayMode test(s). Example test: using NUnit.Framework; using Assert = UnityEngine.Assertions.Assert; public class EditorTests { [Test] public void MyPublicClass_PublicFunctionCanBeCalled() { MyPublicClass myPublicClass = new MyPublicClass(); Assert.IsTrue(myPublicClass.MyPublicFunction()); } } When the test(s) finish running, a file viewer window opens containing the coverage report. Alternatively, select the Results Location dropdown to open it in the file viewer. Note: To generate the report automatically after the Test Runner has finished running the tests, select Auto Generate Report in the Code Coverage window. Alternatively, you can select Generate Report. Select index.htm. This opens the HTML coverage report. Get results for EditMode and PlayMode tests Coverage data are generated from the last set of tests that were run in the Test Runner. Note: Currently the Test Runner does not support EditMode and PlayMode tests running at the same time. In version 2.0 of the Test Framework this will be possible. In the meantime, to include coverage for both EditMode and PlayMode tests, you must run these separately. In this case, the last Coverage Report generated will include the combined coverage of EditMode and PlayMode tests. If a fresh start is required, select Clear Results to clear the Coverage data from all previous test runs for both EditMode and PlayMode tests. Get coverage by test methods To see how each test contributes to the overall coverage check Test Runner References. For more details see Coverage by test methods."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/DocumentArchive.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/DocumentArchive.html",
    "title": "Document revision history - Archive | FSM Unity Framework",
    "keywords": "Document revision history - Archive Date Reason Feb 26, 2021 Added information about the Enable Code Coverage setting in the Code Coverage window Updated Installing Code Coverage section Added verbosity section in -coverageOptions Added Generate combined report from separate projects section Matches package version 1.0.0-pre.4 Jan 13, 2021 Added information about assemblyFilters aliases and note about how path filtering can take precedence over assembly filtering in Using Code Coverage in batchmode section Matches package version 1.0.0-pre.2 Nov 12, 2020 Matches package version 1.0.0-pre.1 Nov 11, 2020 Added information about Included Paths and Excluded Paths settings in the Code Coverage window Updated information about Included Assemblies setting in the Code Coverage window Matches package version 0.4.0-preview Aug 3, 2020 Matches package version 0.3.1-preview May 20, 2020 Added information about the History Location and Generate History settings in the Code Coverage window and coverageHistoryPath and generateHtmlReportHistory in -coverageOptions Added information about the Generate Additional Metrics setting in the Code Coverage window and generateAdditionalMetrics in -coverageOptions Added information about Coverage History and Crap Score in How to interpret the results page Updated Installing Code Coverage section Matches package version 0.3.0-preview Feb 18, 2020 Added Generate combined report from EditMode and PlayMode tests section Matches package version 0.2.3-preview Dec 11, 2019 Matches package version 0.2.2-preview Dec 3, 2019 Added pathFilters section Added examples for assemblyFilters and pathFilters Added a reference to the Coverage Recording ScriptingAPI Matches package version 0.2.1-preview Nov 10, 2019 Split documentation into separate pages Matches package version 0.2.0-preview Nov 5, 2019 Updated UX design Sep 27, 2019 Added Using Code Coverage with Burst compiler section Added Using Code Coverage with Code Optimization section Matches package version 0.1.0-preview.3 Sep 23, 2019 Added Coverage Recording section Matches package version 0.1.0-preview.2 Aug 16, 2019 Added How to interpret the results section Added How it works section Aug 7, 2019 Added Clear Coverage Data setting to Settings table Added Note in Using Code Coverage section about combined coverage of EditMode and PlayMode tests Aug 5, 2019 Updated About Code Coverage section and Settings table May 15, 2019 Document created. Matches package version 0.1.0-preview.0"
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/DocumentRevisionHistory.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/DocumentRevisionHistory.html",
    "title": "Document revision history | FSM Unity Framework",
    "keywords": "Document revision history Date Reason Jun 02, 2023 Corrected Settings.json path in useProjectSettings section in -coverageOptions. Matches package version 1.2.4. Apr 14, 2023 Added filtersFromFile section in -coverageOptions. Updated Using Code Coverage with Burst compiler section with the correct number of dashes for --burst-disable-compilation. Matches package version 1.2.3. Nov 18, 2022 Added Uncoverable lines definition in How to interpret the results page. Matches package version 1.2.2. Oct 27, 2022 Matches package version 1.2.1. Aug 01, 2022 Updated What's new and Upgrade guide pages. Added information about the buttons in the toolbar in the Code Coverage window. Added information about the Log Verbosity Level setting in the Code Coverage window. Added information about the Additional Reports, Test Runner References and Auto Open Report options in the Code Coverage window. Introduced new selection buttons under the Included Assemblies dropdown in the Code Coverage window. Added generateTestReferences, generateAdditionalReports, pathFiltersFromFile and dontClear sections in -coverageOptions. Added pathReplacePatterns section in -coverageOptions. Removed pathStrippingPatterns section from -coverageOptions (replaced with pathReplacePatterns). Renamed the aliases in the assemblyFilters command line option. Updated the examples in the pathFilters command line option to follow the standard globbing paradigm. Added Using relative paths in path filters section. Added Coverage by test methods section. Updated the Quickstart - Code Coverage Tutorial page. Matches package version 1.2.0. Dec 17, 2021 Added What's new and Upgrade guide pages. Matches package version 1.1.1. Jun 09, 2021 Added Subscribing to Code Coverage session events section. Added Ignoring tests for Code Coverage section. Added useProjectSettings, pathStrippingPatterns and sourcePaths sections in -coverageOptions. Updated the Generate combined report from separate projects section. Matches package version 1.1.0. Mar 09, 2021 Added Quickstart guide. Matches package version 1.0.0."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/HowToInterpretResults.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/HowToInterpretResults.html",
    "title": "How to interpret the results | FSM Unity Framework",
    "keywords": "How to interpret the results This section assumes that you checked HTML Report in the Code Coverage window or passed the generateHtmlReport option in -coverageOptions on the command line. If you're running the tests in the Editor, a file viewer window opens up containing the coverage report once the test run has been finished. If you're running the tests from the command line, navigate to the -coverageResultsPath location with your file viewer. Open the Report folder then open index.htm in a web browser. This shows a summary of the coverage results from the tests. Summary view The summary view is divided into several sections: Summary, Coverage History, Risk Hotspots and Coverage. Summary This section shows a brief summary of the coverage results including the number of assemblies, classes, files and lines that were processed. The most important value is the Line Coverage which shows the current coverage percentage of all coverable lines. Coverable lines only include the lines that can be executed and are colored either green or red in the File(s) section depending on whether the line was covered or not. Uncoverable lines are the lines that cannot be executed and are not colored in the File(s) section; these include lines containing: Assemblies, Classes, Constructors, Methods and Structs that are marked to be excluded from coverage Directives Attributes Method signatures Class definitions Unassigned variable declarations Constant variable declarations else/case/default keywords Lines from other classes, when multiple classes are included in the same file Blank lines Coverage History This section only appears if you checked Report History in the Code Coverage window or passed the generateHtmlReportHistory option in -coverageOptions on the command line. The Coverage History displays a graph showing the total percentage coverage for every test run for this project. Aim to keep this percentage as high as possible. If it is decreasing, consider writing more tests to improve your coverage. Risk Hotspots This section only appears if you checked Additional Metrics in the Code Coverage window or passed the generateAdditionalMetrics option in -coverageOptions on the command line. The Risk Hotspots display information about any methods that have a Cyclomatic Complexity score that is greater than 15. The Cyclomatic Complexity score is a value that is based on the number of paths that can be taken through a method. The score will tend to be higher if a method has a large number of if or switch statements. For more detailed information see the Wikipedia entry on Cyclomatic Complexity. You will also see information about any methods with a high Crap Score. CRAP stands for Change Risk Anti-Patterns. For more detailed information see this article. If there are any methods with a very high Cyclomatic Complexity or Crap Score, consider refactoring the method to reduce its complexity. Note: NPath Complexity calculation and Branch Coverage are not implemented at present so will always appear as zero. Coverage By default, this shows a list of the assemblies that have been covered together with some stats showing how well covered they are. Select Expand (+) next to the assembly name to see a list of the classes or structs within the assembly and their associated coverage data. To see more detailed information for a particular class, select its name in the list. Class/Struct view Summary Similar to the Summary section of the previous page, this section shows some brief statistics for the selected class. The most important value is the Line Coverage percentage. Select Back (<) in the top left hand corner to return to the previous screen. Coverage History This section only appears if you checked Report History in the Code Coverage window or passed the generateHtmlReportHistory option in -coverageOptions on the command line. It shows a graph of the coverage percentage of the class/struct over time. Try to keep this value as high as you can. Make sure that as you add new code the coverage percentage is maintained by adding more tests. Metrics The metrics section displays a list of the methods and properties of the class, along with each method's Cyclomatic Complexity, Crap Score and Sequence Coverage scores. Currently, the NPath Complexity and Branch Coverage aren't calculated, so will always appear as zero. File(s) The File(s) section shows the C# source code for the selected class. Each line is colored either green or red depending on whether the line was covered or not. The number in the left column indicates the number of times that the line was executed during a Test Runner or Coverage Recording session. Coverage by test methods This section only appears if you checked Test Runner References in the Code Coverage window or passed the generateTestReferences option in -coverageOptions on the command line. It shows a list of test methods allowing you to see how each test contributes to the overall coverage. Select a test method to view the relevant code or hover over the code to see which test method executed it."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/InstallingCodeCoverage.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/InstallingCodeCoverage.html",
    "title": "Installing Code Coverage | FSM Unity Framework",
    "keywords": "Installing Code Coverage Before you install the package, make sure you have no errors in the Console window (in red text). From the Unity Package Manager Use the Unity Package Manager to find and install the Code Coverage package. Alternatively, use the Add (+) dropdown and select Add package from git URL... or Add package by name... and type com.unity.testtools.codecoverage. To verify that Code Coverage has been installed correctly, open the Code Coverage window (go to Window > Analysis > Code Coverage). If you don't see the Code Coverage menu item, then Code Coverage did not install correctly. Manually from the Package Manifest You can also install the Code Coverage package manually. To do this, add a reference to Code Coverage in your project's Packages/manifest.json file. There are two ways you can reference a specific version of the Code Coverage package, depending on how you use it. Using a production version of the package You can point the Package Manager at a publicly available version. To do this manually, add it to manifest.json: \"dependencies\": { //... \"com.unity.testtools.codecoverage\":\"<full version number>\" } Using a local clone of the package If you want to use a cloned version of the package, you can point the Package Manager at a local folder as the package location: \"dependencies\": { //... \"com.unity.testtools.codecoverage\":\"file:path/to/package/root\" } To verify that Code Coverage has been installed correctly, open the Code Coverage window (go to Window > Analysis > Code Coverage). If you don't see the Code Coverage menu item, then Code Coverage did not install correctly."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/Quickstart.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/Quickstart.html",
    "title": "Quickstart - Code Coverage tutorial | FSM Unity Framework",
    "keywords": "Quickstart - Code Coverage tutorial The Quickstart guide will give you an insight into what Code Coverage is and how you can identify areas of your code that need more testing, even if you haven't written any automated tests. It takes about 30 minutes to complete. Tasks What is Code Coverage (2 min) Install the Code Coverage package (2 min) Install the Asteroids sample project (1 min) Enable Code Coverage (1 min) Understanding the game code: Shoot() function (4 min) Generate a Coverage report from PlayMode tests (3 min) Add Weapon tests to improve coverage (3 min) Add a test for the LaserController (4 min) Clear the coverage data (1 min) Generate a Coverage report using Coverage Recording (4 min) Note: Estimated times are shown for each task to give you a better understanding of the time required. These times are rough guidelines - it is fine to take as much or as little time as needed. 1. What is Code Coverage (2 min) Code Coverage is a measure of how much of your code has been executed. It is normally associated with automated tests, but you can gather coverage data in Unity at any time when the Editor is running. It is typically presented as a report that shows the percentage of the code that has been executed. For automated testing the report does not measure the quality of tests, only whether your code is executed by PlayMode and EditMode tests. It is especially useful to check that critical or high risk areas of your code are covered, because they should receive the most rigorous testing. It is much easier to accidentally introduce bugs into code that is not covered by tests, because those bugs are not detected straight away by the tests and can instead cause problems later — such as after you have published your game or app. Additionally, the Code Coverage package offers a Coverage Recording feature which allows capturing coverage data on demand, in case you do not have tests in your project or doing manual testing. 2. Install the Code Coverage package (2 min) Note: Skip this task if the package is already installed. Use the Unity Package Manager to find and install the Code Coverage package. Alternatively, use the Add (+) dropdown and select Add package from git URL... or Add package by name... and type com.unity.testtools.codecoverage. To verify that Code Coverage has been installed correctly, open the Code Coverage window (go to Window > Analysis > Code Coverage). If you don't see the Code Coverage menu item, then Code Coverage did not install correctly. 3. Install the Asteroids sample project (1 min) In the Unity Package Manager (Window > Package Manager) select the Code Coverage package, if not already selected. Find the Samples section in the package details (right hand side) and select Import next to Code Coverage Tutorial. 4. Enable Code Coverage (1 min) To enable Code Coverage open the Code Coverage window (go to Window > Analysis > Code Coverage) and select Enable Code Coverage if not already selected, to be able to generate Coverage data and reports. Note: Enabling Code Coverage adds some overhead to the editor and can affect the performance. 5. Understanding the game code: Shoot() function (4 min) Go to Asteroids/Scenes in Project View and open the Asteroids scene. This is located in Assets/Samples/Code Coverage/<version>/Code Coverage Tutorial. Hit Play and play the game for a minute or two. Use the arrow keys to move and the spacebar to shoot. Exit PlayMode. Open the Scripts/Controllers/SpaceshipController.cs script. Study the Shoot function. If Weapon is Basic, the Prefabs/Weapons/Projectile prefab is instantiated If Weapon is Laser, the Prefabs/Weapons/Laser prefab is instantiated 6. Generate a Coverage report from PlayMode tests (3 min) Open the Code Coverage window (go to Window > Analysis > Code Coverage). If you see this warning select Switch to debug mode. Code Optimization was introduced in Unity 2020.1; in Release mode the code is optimized and therefore not directly represented by the original code. Therefore, Debug mode is required in order to obtain accurate code coverage information. Click the Included Assemblies dropdown to make sure only Unity.TestTools.CodeCoverage.Sample.Asteroids and Unity.TestTools.CodeCoverage.Sample.Asteroids.Tests are selected. Make sure HTML Report, Report History, Auto Generate Report and Auto Open Report are all checked. Switch to the Test Runner window, select the PlayMode tab and hit Run All tests. When the tests finish running, a file viewer window will open up containing the coverage report. Select index.htm. Look for the classes with low coverage, especially LaserController, BaseProjectile and ProjectileController. You can sort the results by Line coverage. See also How to interpret the results. 7. Add Weapon tests to improve coverage (3 min) Open the Tests/WeaponTests.cs script. Uncomment all the tests (from line 35 down to line 237). Back in the Test Runner, hit Run All tests again. When the tests finish running, a file viewer window will open up containing the coverage report. Select index.htm. Notice that now BaseProjectile and ProjectileController coverage is considerably higher, but LaserController has not improved much. 8. Add a test for the LaserController (4 min) Open the Tests/WeaponTests.cs script. Go to the _18_LaserFiresSuccessfully test in line 225. Uncomment and study the code. Back in the Test Runner, hit Run All tests again. When the tests finish running, a file viewer window will open up containing the coverage report. Select index.htm. Notice how the coverage for LaserController has improved. Select the LaserController class to enter the class view and notice that about 2/3 (65%) of the code is now covered (green). Complete the Bonus Task at the end of the tutorial to get 100% coverage! 9. Clear the coverage data (1 min) Open the Code Coverage window (go to Window > Analysis > Code Coverage). Select Clear Results and confirm. Select Clear History and confirm. 10. Generate a Coverage report using Coverage Recording (4 min) Go to Asteroids/Scenes in Project View and open the Asteroids scene, if not opened already. Open the Code Coverage window. Make sure HTML Report, Report History, Auto Generate Report and Auto Open Report all are checked. Select Start Recording. Hit Play to play the game and exit PlayMode before you get 8000 points. Select Stop Recording. A file viewer window will open up containing the coverage report. Select index.htm. Notice that LaserController has 0% coverage. Go back to the Code Coverage window. Select Start Recording. Now hit Play to play the game again but this time exit PlayMode when you get 8000 points. Select Stop Recording. Notice that LaserController coverage is now 100%. See also How to interpret the results. 11. Bonus task (5-8 min) Write a new test that checks that the laser gets destroyed after 2 seconds, which will also cover the rest of the code in LaserController. Suggested name: _19_LaserFiresAndIsDestroyedAfterTwoSeconds. Hint: You can use yield return new WaitForSeconds(2f); to wait for 2 seconds. Well done for finishing the Code Coverage tutorial! For questions and feedback please visit the Testing & Automation forum section to browse current conversations or start a new thread. Please use the code coverage tag."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Code Coverage What's new Upgrade guide Code Coverage package Coverage HTML Report Other Reports Coverage Summary Badges Quickstart What is Code Coverage Install the Code Coverage package Install the Asteroids sample project Enable Code Coverage Understanding the game code: Shoot() function Generate a Coverage report from PlayMode tests Add Weapon tests to improve coverage Add a test for the LaserController Clear the coverage data Generate a Coverage report using Coverage Recording Installing Code Coverage From the Unity Package Manager Manually from the Package Manifest Using Code Coverage Code Coverage window Using Code Coverage with Test Runner On demand coverage recording Using Code Coverage in batchmode Using Code Coverage with Burst compiler Using Code Coverage with Code Optimization Excluding code from Code Coverage Ignoring tests for Code Coverage Subscribing to Code Coverage session events How to interpret the results Summary View Class/Struct View Technical details How it works Requirements 3rd party libraries used Known limitations Document revision history Archive"
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/TechnicalDetails.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/TechnicalDetails.html",
    "title": "Technical details | FSM Unity Framework",
    "keywords": "Technical details How it works The package is a client of the coverage API. For more information, see the coverage API's documentation. The package uses a combination of this API and C# reflection to output the test coverage data in the OpenCover format. Optionally, a third-party report generator will then parse the OpenCover data and generate a report (HTML, SonarQube, Cobertura, LCOV). Requirements This version of the Code Coverage package is compatible with the following versions of the Unity Editor: 2019.3 and later Third-party libraries used ReportGenerator - v5.0.4 Known limitations Code Coverage includes the following known limitations: Code Coverage currently only supports the OpenCover format. Code Coverage currently only supports code run in the Editor and not in Standalone/Player. NPath Complexity calculation and Branch Coverage are not implemented at present so they will always appear as zero in the coverage report."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/UsingCodeCoverage.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/UsingCodeCoverage.html",
    "title": "Using Code Coverage | FSM Unity Framework",
    "keywords": "Using Code Coverage Using Code Coverage with Burst compiler If you use the Burst package and have jobs compiled with Burst, you will need to disable Burst compilation in order to get full coverage. To disable Burst compilation you can do one of the following: Uncheck Enable Compilation under Jobs > Burst > Enable Compilation. Pass --burst-disable-compilation to the command line. Using Code Coverage with Code Optimization Code Optimization was introduced in 2020.1. Code Optimization mode defines whether Unity Editor compiles scripts in Debug or Release mode. Debug mode enables C# debugging and it is required in order to obtain accurate code coverage. To ensure Code optimization is set to Debug mode you can do one of the following: Switch to Debug mode in the Editor (bottom right corner, select the Bug icon > Switch to debug mode). Using the CompilationPipeline api, set CompilationPipeline.codeOptimization = CodeOptimization.Debug. Pass -debugCodeOptimization to the command line. Excluding code from Code Coverage Any code that should not be contributing to the Code Coverage calculation can be excluded by adding the ExcludeFromCoverage attribute. This attribute can be added to Assemblies, Classes, Constructors, Methods and Structs. Note that you can also use the .NET ExcludeFromCodeCoverage attribute. Ignoring tests for Code Coverage To ignore tests when running with Code Coverage, use the ConditionalIgnore attribute, passing the \"IgnoreForCoverage\" ID. Example public class MyTestClass { [Test, ConditionalIgnore(\"IgnoreForCoverage\", \"This test is disabled when ran with code coverage\")] public void TestNeverRunningWithCodeCoverage() { Assert.Pass(); } } Subscribing to Code Coverage session events Use the Events API to subscribe to events invoked during a Code Coverage session."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/index.html",
    "title": "About Code Coverage | FSM Unity Framework",
    "keywords": "About Code Coverage Code Coverage is a measure of how much of your code has been executed. It is normally associated with automated tests, but you can gather coverage data in Unity at any time when the Editor is running. It is typically presented as a report that shows the percentage of the code that has been executed. For automated testing the report does not measure the quality of tests, only whether your code is executed by PlayMode and EditMode tests. It is especially useful to check that critical or high risk areas of your code are covered, because they should receive the most rigorous testing. Code Coverage package Use the Code Coverage package with the Test Runner to gather and present test coverage information. When you run your tests with code coverage enabled you can see exactly which lines of your code are executed when the tests run in addition to whether the tests passed or failed. See Using Code Coverage with Test Runner. Once a test run has completed, the Code Coverage package will generate an HTML coverage report showing which lines of your code are covered by tests. Code Coverage currently supports PlayMode and EditMode tests. It also allows you to track the code coverage changes through time. Additionally, the Code Coverage package offers a Coverage Recording feature which allows capturing coverage data on demand, in case you do not have tests in your project or doing manual testing. The Quickstart guide will give you an insight into the package. Coverage HTML report Shown below is an example of the top level page of an HTML report generated by the package. Class view This view shows some brief statistics for the selected class as well as the C# source code. Each line will be colored either green or red depending on whether the line was covered or not. For more information see How to interpret the results. Other reports The package can produce SonarQube, Cobertura and LCOV reports. Coverage summary badges Additionally, the package can produce simple badges in SVG and PNG format, showing the current percentage of code that is covered."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/upgrade-guide.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/upgrade-guide.html",
    "title": "Upgrading to Code Coverage package version 1.2 | FSM Unity Framework",
    "keywords": "Upgrading to Code Coverage package version 1.2 To upgrade to Code Coverage package version 1.2, you need to do the following: Update assembly filtering aliases in batchmode Rename pathStrippingPatterns to pathReplacePatterns in batchmode Note: If you're upgrading from a version older than 1.1, follow the upgrade guide for version 1.1 first. Update assembly filtering aliases in batchmode Rename assembly filtering aliases when running in batchmode. <user> alias was renamed to <assets> and <project> was renamed to <all>. Rename pathStrippingPatterns to pathReplacePatterns in batchmode Rename pathStrippingPatterns to pathReplacePatterns in batchmode. Example: Change pathStrippingPatterns:C:/MyProject/ to pathReplacePatterns:C:/MyProject/,. This is equivalent to stripping C:/MyProject/ by replacing C:/MyProject/ with an empty string. Upgrading to Code Coverage package version 1.1 To upgrade to Code Coverage package version 1.1, you need to do the following: Update path filtering globbing rules Update path filtering globbing rules Update the path filtering globbing rules in your batchmode commands and code coverage window. To keep the current behavior when using globbing to match any number of folders, the * character should be replaced with **. A single * character can be used to specify a single folder layer. Examples: pathFilters:+C:/MyProject/Assets/Scripts/* will include all files in the C:/MyProject/Assets/Scripts folder. Files in subfolders will not be included. pathFilters:+C:/MyProject/Assets/Scripts/** will include all files under the C:/MyProject/Assets/Scripts folder and any of its subfolders. For a full list of changes and updates in this version, see the Code Coverage package changelog."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/whats-new.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Documentation~/whats-new.html",
    "title": "What's new in version 1.2 | FSM Unity Framework",
    "keywords": "What's new in version 1.2 Summary of changes in Code Coverage package version 1.2 The main updates in this release include: Added Added Pause Recording and Resume Recording buttons in the toolbar in the Code Coverage window. Added Test Runner References coverage report option in the Code Coverage window. When you check this option, the generated coverage results include references to the triggering tests enabling the Coverage by test methods section in the HTML report. This section allows you to see how each test contributes to the overall coverage. In batchmode, you can generate test references by adding the generateTestReferences option in -coverageOptions. Added Log Verbosity Level setting in the Code Coverage window which allows setting the verbosity level for the editor and console logs. Added Additional Reports option in the Code Coverage window which if checked SonarQube, Cobertura and LCOV reports will be generated. Added generateAdditionalReports in -coverageOptions for batchmode. Added filtersFromFile in -coverageOptions for batchmode. This allows you to specify an external Json file which contains path and assembly filtering rules. Added dontClear in -coverageOptions for batchmode which allows coverage results to be accumulated after every code coverage session. If not passed the results are cleared before a new session. Updated Updated the UI of the Code Coverage window moving the action buttons into a toolbar at the top. Introduced new selection buttons under the Included Assemblies dropdown in the Code Coverage window. Renamed assemblyFilters aliases in batchmode; <user> was renamed to <assets> and <project> was renamed to <all>. Replaced pathStrippingPatterns with pathReplacePatterns in batchmode. The pathReplacePatterns option allows stripping and replacing specific sections from the paths that are stored in the coverage results xml files. The size of the coverage result files and the Code Coverage session duration have been optimized. Improved the editor and console logs. Fixed Ensure assemblies are removed from the Included Assemblies field if they no longer exist (case 1318668). Ensure hidden sequence points are ignored (case 1372305). For a full list of changes and updates in this version, see the Code Coverage package changelog."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Code Coverage copyright © 2020 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/README.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/README.html",
    "title": "About Code Coverage | FSM Unity Framework",
    "keywords": "About Code Coverage Use the Code Coverage package with the Test Runner to gather and present test coverage information. When you run your tests with code coverage enabled you can see exactly which lines of your code are executed when the tests run in addition to whether the tests passed or failed. Once a test run has completed, the Code Coverage package will generate an HTML coverage report showing which lines of your code are covered by tests. Code Coverage currently supports EditMode and PlayMode tests. It also allows you to track the code coverage changes through time. Additionally, the Code Coverage package offers a Coverage Recording feature which allows capturing coverage data on demand, in case you do not have tests in your project or doing manual testing. The Code Coverage package is available as a released package via the Package Manager for Unity 2019.3 and above. For more information see the Code Coverage package documentation."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Samples~/Tutorial/README.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Samples~/Tutorial/README.html",
    "title": "Code Coverage Tutorial | FSM Unity Framework",
    "keywords": "Code Coverage Tutorial The Code Coverage Tutorial will give you an insight into what Code Coverage is and how you can identify areas of your code that need more testing, even if you haven't written any automated tests. It takes about 30 minutes to complete. To start open the Worksheet pdf file located under the Code Coverage Tutorial folder, and work through the tasks. Tutorial tasks What is Code Coverage (2 min) Install the Code Coverage package (2 min) Enable Code Coverage (1 min) Understanding the game code: Shoot() function (4 min) Generate a Coverage report from PlayMode tests (3 min) Add Weapon tests to improve coverage (3 min) Add a test for the LaserController (4 min) Clear the coverage data (1 min) Generate a Coverage report using Coverage Recording (4 min) For questions and feedback please visit the Testing & Automation forum section to browse current conversations or start a new thread. Please use the code coverage tag."
  },
  "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Third Party Notices.html": {
    "href": "Library/PackageCache/com.unity.testtools.codecoverage@1.2.4/Third Party Notices.html",
    "title": "| FSM Unity Framework",
    "keywords": "This package contains third-party software components governed by the licenses indicated below: Component Name: ReportGenerator License Type: Apache 2.0 Copyright (c) 2018 Daniel Palme ReportGenerator is licensed under the Apache License 2.0. This means you may use this program in any project. You are allowed to modify the program as you like. For further details visit https://github.com/danielpalme/ReportGenerator/blob/master/LICENSE.txt Component Name: OpenCover License Type: MIT Copyright (c) 2011-2019 Shaun Wilde OpenCover is released under the following MIT compatible software licence this does not apply to any other software, be that source code, compiled libraries or tools, that OpenCover may rely on or use and that that software will continue to retain whatever licence they were released under. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For further details visit https://github.com/OpenCover/opencover/blob/master/LICENSE Component Name: Cyclomatic Complexity calculation from MonoTools License Type: MIT Copyright (c) 2001, 2002, 2003 Ximian, Inc and the individuals listed on the ChangeLog entries. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For further details visit https://github.com/mono/mono-tools/blob/master/LICENSE Component Name: Mono.Reflection License Type: MIT Author: Jb Evain (jbevain@novell.com) Copyright (c) 2009-2010 Novell, Inc. (http://www.novell.com) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For further details visit https://github.com/jbevain/mono.reflection/tree/master/Mono.Reflection Component Name: SixLabors.Fonts, SixLabors.ImageSharp License Type: Apache 2.0 Copyright (c) 2017 Six Labors Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. For further details visit: SixLabors.Fonts - https://github.com/SixLabors/Fonts/blob/master/LICENSE SixLabors.ImageSharp - https://github.com/SixLabors/ImageSharp/blob/master/LICENSE"
  },
  "Library/PackageCache/com.unity.textmeshpro@3.0.6/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.textmeshpro@3.0.6/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog These are the release notes for the TextMesh Pro UPM package which was first introduced with Unity 2018.1. Please see the following link for the Release Notes for prior versions of TextMesh Pro. http://digitalnativestudios.com/forum/index.php?topic=1363.0 [3.0.6] - 2021-04-23 [2.1.6] [1.5.6] Changes Added compiler conditional to exclude reference to PS5 in Unity 2019.4.22f1 or older and similar for Unity 2020.2.2f1 or older. [3.0.5] - 2021-04-09 [2.1.5] [1.5.5] Changes Added compiler conditional to address error related to missing RectMask2D padding property which was added in Unity 2019.4.12f1. See forum post for details. Fixed GetPreferredValues(string text) and GetPreferredValues(string text, float width, float height) incorrectly changing the text. See forum post for details. Fixed potential crash when FontEngine.GetGlyphIndex is called on a font asset that was previously unloaded or deleted. See forum post for details. Fixed potential crash when trying to add new glyphs to a dynamic font asset whose atlas texture is set to non readable. Case #1319567 Fixed Format Exception error when using the Project Text Spacing Conversion Tool when the Language Region Format is not English. Case #1320544 Fixed text rendering issue due to incorrectly SDF scaling when using a CanvasScaler and resizing the game view. Fixed TextMeshPro component Sorting Layer field in the Inspector's Extra Settings not showing the correct layer. Case #1326985 Fixed m_AlphaTweenRunner not initialized in TMP_Dropdown when Reload Domain is disabled in the Editor Enter Play Mode Settings. See forum post for details. Added support for PS4 and PS5 to TMP Input Field. [3.0.4] - 2021-02-19 [2.1.4] [1.5.4] Changes Improved sprite tag anim functionality to take into consideration the sprite character and glyph scale. Case #1309707 Improved Ellipsis character insertion handling to prevent potential issues when the Ellipsis glyph ascender and descender exceed those of the primary font asset. See forum post for details. Fixed text object margin handles in Scene view not behaving correctly as a result of lossy scale or object rotation. Case #1295523 The <mark> tag padding attribute can now be defined using font units (em). Fixed text parsing issue related to recent memory overhead optimizations. Case #1295755 Updated TMP Essential Resources and TMP Examples & Extras. Updated TMP Sprite shader to add support for Single Pass Stereo rendering. Fixed potential iOS build failure. Case #1298753 Fixed a few missing Profiler.EndSample() in the TMP_FontAsset.cs file. See forum post for details. Fixed SetText() with formatting issue where large numbers would show a leading zero. See forum post for details. Updated profiling code to use the new and more efficient ProfilerMarker. Fixed incorrect text bounds. See forum post for details. Fixed OutOfRangeException error that could occur in the TMP Input Field when selecting all and inserting characters using IME. Case #1301059 Fixed incorrect handling of Surrogate Pairs in the TMP Input Field. Case #1299798 Fixed Font Asset Creator incorrectly leaving the Readable state of font asset atlas textures to readable where it should be set to non readable for static font assets. Case #1305520 Added Multi Select functionality to the \"Create - TextMesh Pro - Font Asset\" context menu option. Case #1303074 Revised internal handling of the various text input methods to ensure the text property is always reflective of the text content in the Inspector Text Input Box in the Editor and via the text property getter even when using a combination of the various SetText methods or the text property setter. Case #1294998 Please note that using the text property getter when the text was updated via one of the SetText methods will results a string allocation. Fixed incorrect line spacing caused by preceding <size=x.x> tag. See forum post for details. Revised how the Bold Spacing which is defined per font asset will affect spacing between bold characters to ensure more uniform spacing. This change may require users to manually adjust the bold spacing value of their font assets to maintain similar spacing / layout results. Fixed linked text components not updating correctly when setting the text to null or empty. Case #1305832 The vertexBufferAutoSizeReduction property will now be set to false by default. This property is used to determine if the internal data structures used in the parsing of the text should be resized when the text content shrinks by more than 256 characters which results in CG. Case #1305311 Fixed animated sprites not behaving correctly when using text overflow mode Ellipsis and Truncate. Case #1303672 Fixed TMP Resource Importer window stealing focus when Inspector Layout Property window is open when TMP Essential Resources have not been imported into the project. Case #1300462 Fixed minor UI cosmetic issue affecting text spacing properties alignment in the Quick Search window. Case #1299587 Fixed minor UI cosmetic issue in the Font Asset inspector related to the positioning of the warning when changing Generation Settings. Fixed issue where the material properties of fallback font assets are not updated when changing the material properties of the primary font asset via code. Case #1271468 Fixed an issue with Text Overflow Linked mode where text would not flow correctly from one component to the other when the last character present at the break point was a linefeed \"\\n\" or vertical tab \"\\v\". See forum post for details. [3.0.3] - 2020-10-27 [2.1.3] [1.5.3] Changes Fixed potential null reference exception in the Input Field that can occur as a result of using a workflow that involves enabling and disabling Canvases. See forum post for details. Fixed potential Invalid AssetDatabase path warning that can be issued when assets are imported from outside the project. See forum post for details. Fixed <TextMeshProUGUI> objects not being created correctly in Prefab isolation mode when using the Create context menu. See forum post for details. Case #1266096 Fixed an issue where nesting <uppercase> and <lowercase> tags didn't behaves as expected. See forum post for details. Fixed Input Field incorrect handling of validation with text selection. Case #1267777 Fixed potential null reference exception that could occur in the Input Field when hiding the soft keyboard on iOS or Android. Case #1273631 Fixed OnScroll event not getting passed to potential parent ScrollRect when the Input Field is in Single Line mode. Case #1270241 Fixed Prefab override context menu to override or revert changes not being available for some text object properties. Case #1271420 The sampling point size in the Font Asset Creator will now be limited to a maximum of 16,384 points for SDF over-sampled modes. This means a maximum point size of 2048 for SDF8, 1024 for SDF16 and 512 for SDF32. Case #1253370 Fixed Margin widget in the scene view not working correctly when the text object is rotated on the z-axis. Case #1263001 Fixed Input Field Scrollbar not behaving correctly when set to Bottom to Top direction. Case #1179982 Fixed minor UI cosmetic issue in the StyleSheet inspector. Case #1258771 Fixed minor UI cosmetic issue in Material inspector texture properties. Case #1163983 Fixed potential IndexOutOfRangeException that could occur when duplicating text objects that have more than 8 sub text objects. Revised and improved Input Field with Scrollbar behavior with respect to text alignment. Case #1272647 Improved Input Field Name validation including adding the ability to use Hyphens. Case #1277951 Fixed state of MeshRenderer potentially not being mirrored on sub text objects. Case #1278329 Fixed GetPreferredValues() function returning incorrect values when called consecutively. See forum post for details. Initial pass at revising some of the data structures used in the text parsing and layout process to reduce text object memory overhead. Fixed incorrect positioning of IME window when using a canvas in World Space when no camera is assigned to the canvas. Case #1043535 Added new option to Font Asset Generation Settings to automatically clear dynamic data and atlas texture when creating a build. Replaced the automatic removal of the CanvasRenderer from <TextMeshPro> components with a warning to manually remove this now unnecessary component. Fixed text object properties not being applied correctly when instantiating a text prefab prior to importing TMP Essential Resources. Case #1271192 Fixed default text object properties potentially not being set correctly when instantiating a prefab. Case #1286412 Fixed incorrect parsing and display of UTF32 characters. See forum post for details. Fixed potential material error when updating a font asset generation settings when the font asset is using a non SDF shader. Case #1286132 Fixed minor UI cosmetic issue in the Sprite Asset Sprite Glyph Table inspector. Case #1285022 [3.0.1] - 2020-07-26 [2.1.1] [1.5.1] Changes Addressed compiler warning related to the new virtual event OnPreRenderText. Added one additional layer of missing character search where in the even the missing glyph character \\u0000 or space character \\u0020 is not available in any font asset or potential fallbacks, the End of Text (ETX) \\u0003 will be used instead. Input Field Integer or Decimal validation will now take into account the current culture. See forum post for details. Added Editor only font asset post processor to handle font assets being modified outside of the Unity Editor. Fixed potential Array Out of Bounds error that could occur when using </style> without first using a valid <style>. Case #1263787 and See forum post for details. Fixed potential issue when using multiple <font> tag in the same text object where these referencing several font assets derived from the same font file. Since their Default Material all have the same name, this was causing an issue in the Material Reference Manager. See forum post for details. Case #1264596. [3.0.0] - 2020-06-30 [2.1.0] [1.5.0] Changes Added support to control if a text object is Maskable and affected by UI Mask components. The new setting is found in the Extra Settings section of the <TextMeshProUGUI> component inspector. Fixed potential Null Reference Exception when trying to add characters and / or glyphs to a font asset via scripting and before it has been initialized or ReadFontAssetDefinition() has been called. Fixed incorrect preferred width values when using alternative font weight font assets. Case #1255336 Enabling or disabling the Mesh Renderer of a <TextMeshPro> text object should now also mirror that state on any sub text object renderers as well. Fixed <sprite> incorrect position when this sprite is the only character in the text and when the sprite asset face info has not been defined. Fixed potential Null Reference Exception related to culling when entering play mode. Added OnPreRenderText event delegate to allow potential modification of the text geometry before it is uploaded to the mesh and rendered. Fixed missing warning when the requested character is missing from the font asset or any potential fallbacks. Case #1256879 Fixed potential issue with Underline and StrikeThrough when using alternative typeface. Case #1255336 Fixed potential errors in the Text StyleSheet Inspector when adding or removing Text Styles after resetting the asset. Case #1254602 Fixed text Margin property values not being draggable in the Extra Settings section of the text inspector. Case #1253447 It will no longer be possible to create Editor Presets for the TMP_FontAsset, TMP_SpriteAsset, TMP_StyleSheet, TMP_ColorGradient and TMP_Settings as these are persistent and runtime assets. Case #1251229 [3.0.0-preview.14] - 2020-06-08 [2.1.0-preview.14] [1.5.0-preview.14] Changes Fixed sprite character and sprite glyph scale not being reflected in the text layout. See forum post for details. Fixed potential null reference exception in the CrossFadeColor or CrossFadeAlpha functions. See forum post for details. Minor improvements to the Sprite Asset Importer to improve allocations and address potential error encountered when creating multiple sprite assets. TMP GUID Remapping Tool - Removed \"UnityEditor.Animations.AnimatorController\" from the exclusion search list. Fixed potential culling issue when dynamically updating the content of child text objects of RectMask2D components. Case #1253625 Fixed InvalidOperationException that could occur when changing text Overflow linked components via code. Case #1251283 [3.0.0-preview.13] - 2020-05-22 [2.1.0-preview.13] [1.5.0-preview.13] Changes Fixed potential issue where the Font Asset Creator could get stuck in the packing phase of the atlas generation process. See forum post for details. Fixed issue potentially affecting text layout as a result of the width of the RectTransform being incorrectly reported. See forum post for details. Previously created prefabs containing sub text objects will now have their HideFlags updated to HideFlags.DontSave to be consistent with newly created prefabs whose sub text objects are no longer serialized. Case #1247184 Fixed culling issue where lossy scale was not considered in the determination of the bounds of the text geometry. [3.0.0-preview.12] - 2020-05-09 [2.1.0-preview.12] [1.5.0-preview.12] Changes Added synchronization of the RaycastTarget property of the parent <TextMeshProUGUI> with potential child sub text objects. Case #1240784 Fixed Font Asset Bold Spacing adjustment scaling based on the text object point size instead of current point size. Case #1241132 Improved text alignment when using RTL in conjunction with character, word and other spacing adjustments. Fixed TMP Input Field caret potentially not being visible when reaching the right side of the viewport. See forum post for more details. Fixed TMP Input Field incorrect text RectTransform horizontal adjustment when using the Backspace key. See forum post for more details. Fixed potential null reference in the TextMeshProUGUI.Cull function when using a workflow that involves enabling / disabling Canvases in the scene. Fixed ArgumentOutOfRangeException when using the \"Update Sprite Asset\" inspector option on a sprite asset that does not contain any sprites. Case #1242936 Fixed incorrect culling of the text geometry by the RectMask2D component on newly created text objects. Case #1245445 It is now possible to use the Material Context Menu options to Copy / Paste Material Properties or Atlas Texture originally created for TMP with all other non TMP specific materials. Case #1242671 Fixed NullReferenceException when setting the Atlas Texture to None in the Debug Settings of the Material Inspector of a text object. Case #1245104 [3.0.0-preview.11] - 2020-04-22 [2.1.0-preview.11] [1.5.0-preview.11] Changes Fixed incorrect culling of text object by RectMask2D component when the parent Canvas Render Mode is set to Screen Space - Camera or World Space. Case #1240595 Added special handling to ForceMeshUpdate() when the parent Canvas is disabled. [3.0.0-preview.10] - 2020-04-21 [2.1.0-preview.10] [1.5.0-preview.10] Changes Revised caching of Preferred Width and Height to further reduce the amount of time it has to be recomputed when using a complex structure of Layout components. Fixed potential issue when using Text Overflow Ellipsis and Truncate modes when the text contains characters using superscript, subscript or using the <voffset> tag. Revised culling of text objects when using a RectMask2D where the bounds of the text geometry instead of the RectTransform define the culling rect. Added HDR support to material preset colors. Fixed various formatting issues in this ChangeLog. Added the ability to define a unicode value for a missing sprite character in the TMP Settings. Added support for displaying a missing sprite character when the requested sprite character is not present in the sprite asset or potential fallback(s). This new functionality is only available when trying to reference a sprite by name. Sprite Characters will now have a default Unicode value of 0xFFFE (Private NonCharacter) instead of a Unicode value of 0x0 (default unicode value for missing character). Using the sprite asset context menu option \"Update Sprite Asset\" will now remap sprite characters with unicode value of 0x0 to 0xFFFE in addition to its currently functionality. Updating TMP Essential Resources via the \"Window - TextMeshPro - Import TMP Essential Resources\" menu option will no longer override existing TMP Settings. Minor optimization where SDF Scale on some text objects could be unnecessarily updated due to floating point rounding errors in their lossy scale. Case #1230799 Fixed minor issue where text objects created before importing the required TMP Essential Resources would have no default text. Improvements to line breaking for CJK and mixed Latin and CJK characters. See the following forum post for more details. Fixed potential NullReferenceException that could occur in the TMP InputField on some platforms if the InputSystem reference is null. Case #1232433 Added small padding to bitmap character geometry to prevent potential clipping. Added optimization to ignore very small RectTransform pivot changes that are usually the result of rounding errors when using Layout Components. Case #1237700 Sorting Layer ID and Sorting Order properties located in the Extra Settings of <TextMeshPro> text objects will now serialized when creating Editor Presets. Case #1215750 TextMeshProUGUI sub text objects will now be set as first sibling of their parent to prevent them from being rendered over other non text object child in the scene hierarchy. Fixed text objects becoming visible when set to empty or null as a result of a scale change. Case #1238408 Fixed useMaxVisibleDescender property now getting set properly via scripting. Case #1218526 Fixed SortingLayerID and SortingOrder not getting set correctly when multiple <TextMeshPro> objects are selected. Case #1171272 Fixed default settings getting applied to disabled text objects in the scene hierarchy whose text property was set to null. Case #1151621 Fixed mouse cursor flickering when hovering the Text Input Box of a text prefab with RTL enabled. Case #1206395 [3.0.0-preview.8] - 2020-03-14 [2.1.0-preview.8] [1.5.0-preview.8] Changes Fixed a minor issue where the preferred width of a text object can be incorrect when using multiple font assets, fallbacks and sprites in the same line of text. Added Alpha Fade Speed property to the TMP_DropDown inspector. Minor improvements to the LogWarning related to missing characters in a font asset or fallback being replaced by a space character. Fixed text object geometry not getting clipped when object is outside of RectMask2D. Improved search for potential missing character to include the primary font asset and potential fallbacks when the current font asset is not the primary. Ignorable / Synthesized characters in font assets will only be created if they do not exist in the source font file. Trying to use Text Overflow Ellipsis mode when no Ellipsis character is available in the primary font asset or potential fallbacks will now issue a warning and switch Text Overflow mode to Truncate. Added &ltcolor=lightblue&gt and &ltcolor=grey&gt to pre-defined rich text tag colors. Fixed compatibility issue when using TexturePacker - JSON (Array) mode and the TMP Sprite Asset Importer to create SpriteAssets. Simple fix to prevent the underline rich text tag becoming visible in the TMP Input Field when in IME composition mode with Rich Text disabled on the TMP Input Field. This is a temporary fix until a more robust and flexible solution is implemented. Case #1219969 Sub Text Objects which are created when the text requires the use of a fallback font asset or sprite asset will now use HideFlags.DontSave to prevent them from being save with Prefabs as they are created on demand. Fix incorrect material reference when current font asset is not the primary or a fallback that is missing a character which is present in the primary font asset. [3.0.0-preview.7] - 2020-03-07 [2.1.0-preview.7] [1.5.0-preview.7] Changes Reverted recent change to the TMP_SubMeshUI OnDisable() function that could result in a Missing Reference Exception in the GraphicRaycaster.cs script. See the following forum post. Fixed glyph drawing issue in the Font Asset Inspector Glyph Adjustment Table when Multi Atlas Texture is enabled and the glyph is not located in the main atlas texture or at atlasTextures[0]. Added support for &ltZWSP&gt tag which is internally replaced by a zero width space or \\u200B. Improved line breaking handling when using &ltNBSP&gt and / or &ltNOBR&gt tags where instead of breaking these line segments per character, they will break at any possible soft breaking space when these segments exceed the width of the text container. Improved PreferredHeight calculations and handling when using Text Auto Size. Fixed incorrect color being applied to the underline or strikethrough line segments when using and / or tags along with a tag while at the same time applying an Underline or Strikethrough font style on the whole text object. Fixed SDF Scale not getting updated when using SetText() with StringBuilder when the lossyScale of the text object changes. Case #1216007 Added Non Breaking Space \\u00A0 and Soft Hyphen \\u00AD to list of synthesized characters in the event they are not present in the source font file. Fixed stack overflow issue when using TMP_FontAsset.HasCharacter and TMP_FontAsset.HasCharacters function on font assets that have circular fallback references. Case #1222574 Fixed atlas texture not getting set correctly to IsReadable when switching a static font asset to dynamic in the Generation Settings of the Font Asset Inspector. Added check for RectTransform.sizeDelta change when OnRectTransformDimensionsChange() is called by the Canvas system to get around potential rounding error that erroneously trigger this callback when the RectTransform is using Stretch Anchor mode. As requested by a few users, TMP_FontAsset.faceInfo setter is now public instead of internal. [3.0.0-preview.5] - 2020-02-25 [2.1.0-preview.5] [1.5.0-preview.5] Changes Revised SetText function formatting options to including ability to specify padding for integral part of the value. Revised format is as follows: {Arg Index:Integral Padding.Decimal Precision} Example: TMP_Text.SetText(\"Value = {0:000.00}\", 10.375f); result in \"Value = 010.38\". Fixed issue where text objects isTextObjectScaleStatic property would be ignored when OnCanvasHierarchyChanged() is called. Added a Character, Glyph and Record count to those respective tables in the Font Asset Inspector. Fixed potential Null Reference Exception that would occur when using text Overflow Ellipsis mode with a primary font asset that doesn't contain the Ellipsis character. Case #1209771 Fixed a potential Editor lockup when using text Overflow Page mode. Case #1219055 Fixed Input Field incorrect caret vertical alignment when using the Midline / Vertical Geometry alignment option. Added initial / minimal support for the New Input System. Please use with caution and report any issues. Changes to Font Asset Generation Settings via the Font Asset Inspector will now update the existing glyphs and characters for the new settings instead of clearing them. Text object InternalUpdate() used to handle potential scale changes of text objects now uses willRenderCanvases event instead of onPreCull. This avoids a potential one frame delay in updating of objects and no impact on objects. Case #1216007 [3.0.0-preview.4] - 2020-01-31 [2.1.0-preview.4] [1.5.0-preview.4] Changes Fixed Input Field issue where scrolling events could prevent OnEndEdit event from firing. See forum post for details. Improved Input Field handling of Vertical Scrollbar in conjunction with the ResetOnDeActivation property. Using the Vertical Scrollbar no longer triggers OnEndEdit event. Fixed potential Missing Component Exception that could occur when a TMP_SubMeshUI object is created. Fixed MissingReferenceException when deleting a TMP prefab that is part of a nested prefab. Case #1207793 Improved handling of allocations of newly created text objects with large amount of text. As a result of these revisions, allocations will potentially be reduce by 10X. See #1205923 Fixed potential Null Reference Exception with the TMP DropDown that could occur when using the experimental Editor \"Enter Play Mode\" feature. Case #1207915 Fixed potential issue with the assignment of sub text object materials. Add support for hiding the soft keyboard for Switch in the TMP Input Field. Fixed incorrect Preferred Height when Word Wrapping is disabled on text objects. See forum post for details. Added support for the new Selected state and color to the TMP Input Field. Case #1210496 Fixed additional instances of TMP Resource Importer window being created when deleting the \"TextMesh Pro\" folder just after having imported them. Case #1205848 Added public ITextPreprocessor textPreprocessor; property to allow setting the text preprocessor for a given text component. [3.0.0-preview.3] - 2019-12-16 [2.1.0-preview.3] [1.5.0-preview.3] Changes Fixed potential issue with TMP Dropdown where calling Show() and Hide() in very short interval could result in additional Blockers. Case #1194114 Fixed potential issues that could occur when upgrading to version 1.5.0-preview.2 or newer of the TMP package without also updating the TMP Essential Resources in the project. Added check and warning when trying to create a font asset whose source font file has \"Incl. Font Data\" disabled in the Font Import Settings. Case #1198587 and #1198112 Fixed Ellipsis overflow mode issue when using small caps. Case #1198392 Fixed potential layout issue when adding a Layout Group to the text object itself. Case #1197614 Fixed Font Asset Creator issue where too many adjustment records with adjustment value of zero were added to the font asset. Added support for Line Separator \\u2028 and Paragraph Separator \\u2029. TMP shaders have been moved from \"TextMesh Pro/Resources/Shaders\" folder to \"TextMesh Pro/Shaders\" folder. See the following post for details. Added new experimental SDF and Mobile SDF Shaders that use Screen Space Derivatives (SSD) where these shaders no longer require SDF Scale to be passed via Vertex Attributes. These shaders have higher performance overhead but are more flexible. This overhead should only be noticeable on mobile platforms. Fixed potential text alignment issue where upgrading from package version 1.4.1 to 1.5.0-preview.2 would result in incorrect alignment on prefabs. Case #1198833 Added \\u061C Arabic Letter Mark, \\u200E Left-to-Right Mark and \\u200F Right-to-Left Mark to list of control and non renderable characters. Fixed Missing Reference Exception that would appear when expanding the Extra Settings of a TextMeshPro Preset asset. Case #1201072 Fixed Missing Reference Exception that would appear when editing the Vertex Color or Color Gradient of a TMP component Preset asset. Case #1201069 Fixed Inspector layout issue preventing enabling or disabling the Outline, Underlay, Lighting and Glow features when selecting a Preset asset material. Case #1196963 Revised the Create Material Preset context menu option to issue a warning and ignore materials outside the project. Case #1200109 Added experimental ITextPreprocessor interface to allow users to create custom components to handle text preprocessing and shaping. This interface includes a PreprocessText(string text) function that is called when the object contains a component that inherits from this interface. Added support for Unity Presets in the Editor for both and components. Case #1191793 Fixed missing CanvasRenderer component issue on the Input Field Caret object. Added padding to the 2DRectMask on the TMP Input Field - Text Area object. Optimization to ensure the TMP Update Manager only rebuilds text objects once per frame regardless of the number of cameras in the scene. [2.1.0-preview.2] - 2019-10-30 [1.5.0-preview.2] Changes Fixed Input Field issue when Read Only flag is set preventing the initial setting of the text. Also fixed Read Only flag not being respected when using IME input. Fixed potential infinite loop when using text overflow mode ScrollRect. See Case #1188867 Fixed Input Field culling related issue(s) where text would be incorrectly culled. See https://forum.unity.com/threads/version-1-5-0-2-1-0-preview-1-now-available-for-testing.753587/#post-5023700 Revised handling and referencing of the CanvasRenderer in anticipation of an incoming change to the MaskableGraphic class where it will no longer automatically add a CanvasRenderer to components inheriting from it. As a result, objects will no longer have a CanvasRenderer. Fixed potential NRE when using Overflow Truncate mode with sprites. See https://forum.unity.com/threads/tmpro-stackoverflow-caused-by-tmpro-textmeshprougui-generatetextmesh.750398/page-2#post-5042822 Fixed issue when using font weights in combination of font styles in the editor. Fixed for potential incorrect preferred height. Improved handling of StyleSheet options to reorder, add or delete styles. Fixed Input Field Caret & Selection Highlight potential culling issue when the object was instantiated outside the culling region. Fixed potential issue with registration of text objects in the TMP_UpdateManager. Optimization to suppress callback to InternalUpdate when parent Canvas is disabled. Case #1189820 Fixed Fallback material not getting updated correctly when changing Generation Settings on the Fallback Font Asset. Fixed a typo in the Font Weight section of the Font Asset Editor. Fixed potential ArgumentOutOfRangeException in the Input Field when using Hide Mobile Input and deleting a long string. Case #1162514 Added \"Is Scale Static\" option in the Extra Settings to exclude text objects from InternalUpdate callbacks to improve performance when the object's scale is static. This InternalUpdate callback is used to track potential changes to the scale of text objects to update their SDF Scale. Added the ability to control culling modes for the TMP Shaders. This new option is available in the Debug section of the Material Inspector. New feature requires updating the TMP Essential Resources. See the following post https://forum.unity.com/threads/not-see-textmeshpro-rendering-from-the-back.767510/#post-5112461. Fixed Material Inspector issue when toggling the Record button in the Animation window. Case #1174960 Improved Line Breaking handling for CJK. This also addresses a few reported issues. Case #1171603 Added support for &ltNBSP&gt tag which is internally replaced by a non-breaking space or \\u00A0. Improved performance when retrieving glyph adjustment records when using dynamic font assets. Fixed potential Null Reference Exception in the Editor when assigning new font asset to disabled game object when no previous font asset was assigned. [2.1.0-preview.1] - 2019-09-30 [1.5.0-preview.1] Changes Fixed an issue when using Overflow Ellipsis mode where the Ellipsis character would not be displayed correctly when the preceding character is a sprite. Added the ability to define the Resource path for Style Sheets in the TMP Settings. TMP Style Sheets can now be assigned to text objects in the Extra Settings section of the text object inspector. Added the ability to assign a Style to text objects using the new Text Style property in the text object inspector. A new public property TMP_Text.textStyle was also added. Improved Style Sheet editor to allow sorting of styles in the style sheet. Improved handling of nested styles. Added public TMP_Style GetStyle(string name) to get the potential style by name. Revised the ForceMeshUpdate() function as follows: public void ForceMeshUpdate(bool ignoreActiveState = false, bool forceTextReparsing = false). Fixed SubMeshUI objects text disappearing when saving a scene. Creating Material Presets via the Material Context menu with multi selection will now work as expected and assign the newly created material preset to all selected text objects. Fixed minor issue when changing Material Preset in prefab isolation mode with multiple text objects selected where the new material preset would not be assigned to disabled text objects. Revised Character, Word, Line and Paragraph spacing adjustments to be in font units (em) where a value of 1 represents 1/100 em. Added TMP_Text.onFontAssetRequest and TMP_Text.onSpriteAssetRequest events to allow users to implement custom asset loading when using the &ltfont=\"Font Asset Name\"&gt and &ltsprite=\"Sprite Asset Name\"&gt tags. Additional Shader Channels on the Canvas will be set to TexCoord1, Normal and Tangents or Mixed when using TMP Surface Shaders. Otherwise it will be set to TexCoord1 only. Case #1100696 Added new attribute to the &ltmark&gt tag to allow users to define a padding value for the mark / highlight region. Example: &ltmark color=#FFFF0080 padding=\"1.0,1.0,0.0,0.0\"&gt where padding=\"Left, Right, Top, Bottom\". Fixed an issue which could result in out of range exception when referencing sprites contained in fallback sprite assets using unicode values. Fixed an issue in the Font Asset Creator where the source font file property of the newly created font asset was not getting set. Added .blend files to exclusion asset scan list of the Project GUID Remapping tool. Fixed issue where Caret position would be incorrect when using IME. Case #1146626 Clamped Outline Softness to a value of 0-1 in the TMP Distance Field shader which makes it consistent with other SDF Shaders. Case #1136161 Text Auto-Sizing Min and Max values are now clamped between 0 and 32767. Case #1067717 Text Font Size Min and Max values are now clamped between 0 and 32767. Case #1164407 Rich Text Tag values are now limited to a maximum value of 32767. Added Placeholder option to TMP Dropdown. Placeholder text is displayed when selection value is -1. Also added example scene in the TMP Examples & Extras. Added the ability to define Face Info metrics per Sprite Assets. This will provide for more consistent scaling of the sprites regardless of the font asset used. Sprite Assets with undefined Face Info will continue to inherit the Face Info metrics of the current font asset. Added Update Sprite Asset option in the header of the Sprite Asset inspector. This increases the discoverability of this option already available via the Sprite Asset Context Menu. Revised the text auto-sizing handling in regards to maximum iteration threshold which could result in a crash on some Android devices. Case #1141328 Font Asset Generation Settings are now disabled in the inspector if the Source Font File is missing or if the Atlas Population Mode is set to static. Fixed vertical alignment issue when using Overflow Page mode. Improved handling of text auto-size line adjustment spacing resulting in fewer iterations and more accurate resulting point size. Added support for Layout Elements to the TMP Input Field. = Fixed text alignment issue with TMP Input Field when using Center alignment on the underlying text component. Setting ContentType.Custom on the TMP Input Field will no longer hide the Soft Keyboard. The Soft Keyboard can now be control independently via the shouldHideSoftKeyboard property. Added new Font Asset Context Menu option \"Force Upgrade To Version 1.1.0\" for convenience purposes in case a font asset didn't get upgraded automatically when migrating from version 1.3.0 to 1.4.x or 2.0.x. The &ltgradient&gt tag now as an optional attribute \"tint=0\" or \"tint=1\" controlling whether or not the gradient will be affect by vertex color. The alpha of the gradient will continue to be affected by the vertex color alpha. Added new angle=x attribute to the &lti&gt tag where the value of x define the italic slant angle. Since the legacy TextContainer used by TMP has been deprecated, it was removed from the Layout Context Menu options. Improved character positioning when using italic text where large angle / slant would potentially result in uneven spacing between normal and italic blocks of text. Fixed an issue where &ltmspace&gt and &ltcspace&gt tags would not be handled correctly in conjunction with word wrapping. Fixed issue in the TMP_Dropdown.cs that was affecting navigation. Case 1162600. See https://forum.unity.com/threads/huge-bug-missing-a-code-line-since-1-4-0.693421/ Fixed an issue related to kerning where the glyph adjustment values did not account for the upsampling of the legacy SDF modes like SDF8 / SDF16 and SDF32. Made the TMP_Text.text property virtual. Fixed Material Preset of fallback materials getting modified when the TMP Settings Match Material Preset option is disabled. Added ShaderUtilities.ID_GlowInner to list of material property IDs. Fixed potential null reference exception when creating new text objects when no default font asset has been assigned in the TMP Settings and the LiberationSans SDF font asset has been deleted from the project. Case #1161120 Fixed import TMP Essential Resources button being disabled when importing the TMP Examples & Extras first. Case #1163979 Fixed potential ArgumentOutOfRangeException when Hide Mobile Input is enabled and deleting the last character in the text. Case #1162514 Improved handling of manual addition of glyph positional adjustment pairs for both dynamic and static font assets. Case #1165763 Fixed issue where text in the TMP_InputField would disappear due to incorrect culling. Case #1164096 Fixed potential IndexOutOfRangeException that could be thrown when using the Pinyin IME interface and typing very fast to enter Chinese text. Case #1164383 Added support for Vertical Tab \\v which inserts a line break but not a paragraph break. Added support for Shift Enter in the TMP Input Field which inserts a Vertical Tab in the text in Multi Line mode. Fixed text horizontal alignment when lines of text only contain the Ellipsis \\u2026 Unicode character. Case #1162685 Text alignment is now serialized into separate fields for horizontal and vertical alignment and can now be get / set independently via TMP_Text.horizontalAlignment and TMP_Text.verticalAlignment. The TMP_Text.alignment property remains and uses the new serialized fields for horizontal and vertical alignment. Improved handling of Soft Hyphens when using Text Auto-Size. Fixed Null character being passed to Validate method of the TMP_InputField. Case #1172102 Fixed an issue where the Preferred Width and Height were not correct when using Tabs. The Cull Transparent Mesh flag on TMP_SubMeshUI objects will now mirror the settings on the parent text object's CanvasRenderer. Updated Sprite Importer to improve compatibility with Texture Packer Json Array export format. Newly created StyleSheets will be pinged in the project tab. Case #1182117 Added new option in the TMP Settings to control line breaking rules for Hangul to enabled Modern line breaking or traditional line breaking. Fixed potential issue related to SDF Scaling when the scale of the text object is negative. See https://forum.unity.com/threads/version-1-4-1-preview-1-with-dynamic-sdf-for-unity-2018-3-now-available.622420/page-5#post-4958240 for details. Added validation check for Sprite Data Source file in the Sprite Asset Importer. Case #1186620 Added warning when using Create - TextMeshPro - Sprite Asset menu when no valid texture is selected. Case #1163982 Fixed potential cosmetic issue in the text component inspector when using Overflow Linked mode. Case #1177640 [1.4.1] - 2019-04-12 Changes Improved handling of font asset automatic upgrade to version 1.1.0 which is required to support the new Dynamic SDF system. Made release compatible with .Net 3.5 scripting runtime. [1.4.0] - 2019-03-07 Changes Same release as 1.4.0-preview.3a. [1.4.0-preview.3a] - 2019-02-28 Changes Improved performance of the Project Files GUID Remapping Tool. Fixed an issue with the TMP_FontAsset.TryAddCharacters() functions which was resulting in an error when added characters exceeded the capacity of the atlas texture. Updated TMP_FontAsset.TryAddCharacters functions to add new overloads returning list of characters that could not be added. Added function in OnEnable of FontAsset Editor's to clean up Fallback list to remove any null / empty entries. Added support for Stereo rendering to the TMP Distance Field and Mobile Distance Field shaders. [1.4.0-preview.2a] - 2019-02-14 Changes Fixed an issue with SDF Scale handling where the text object would not render correctly after the object scale had been set to zero. Fixed an issue with the TMP_UpdateManager where text objects were not getting unregistered correctly. Any changes to Font Asset Creation Settings' padding, atlas width and / or atlas height will now result in all Material Presets for the given font asset to also be updated. Added new section in the TMP Settings related to the new Dynamic Font System. Added new property in the Dynamic Font System section to determine if OpenType Font Features will be retrieved from source font files at runtime as new characters are added to font assets. Glyph Adjustment Data (Kerning) is the only feature currently supported. Fix an issue where font assets created at runtime were not getting their asset version number set to \"1.1.0\". Improved parsing of the text file used in the Font Asset Creator and \"Characters from File\" option to handle UTF16 \"\\u\" and UTF32 \"\\U\" escape character sequences. Fixed a Null Reference Error (NRE) that could occur when using the &ltfont&gt tag with an invalid font name followed by the &ltsprite&gt tag. The Glyph Adjustment Table presentation and internal data structure has been changed to facilitate the future addition of OpenType font features. See https://forum.unity.com/threads/version-1-4-0-preview-with-dynamic-sdf-for-unity-2018-3-now-available.622420/#post-4206595 for more details. Fixed an issue with the &ltrotate&gt tag incorrectly affecting character spacing. [1.4.0-preview.1] - 2019-01-30 Changes Renamed TMPro_FontUtilities to TMP_FontAssetCommon to more accurately reflect the content of this file. Accessing the TextMesh Pro Settings via the new Edit - Settings menu when TMP Essential Resources have not yet been imported in the project will no longer open a new window to provide the options to import these resources. Fixed an issue where using int.MaxValue, int.MinValue, float.MaxValue and float.MinValue in conjunction with SetText() would display incorrect numerical values. Case #1078521. Added public setter to the TMP Settings' missingGlyphCharacter to allow changing which character will be used for missing characters via scripting. Fixed a potential Null Reference Exception related to loading the Default Style Sheet. Added compiler conditional to TMP_UpdateManager.cs to address changes to SRP. Improved the &ltmargin&gt tag to make it possible to define both left and right margin values. Example: &ltmargin left=10% right=10px&gt. Added new menu option to allow the quick creation of a UI Button using TMP. New menu option is located in Create - UI - Button (TextMeshPro). Renamed TMP related create menu options. Fixed TMP object creation handling when using Prefab isolation mode. Case #1077392 Fixed another issue related to Prefabs where some serialized properties of the text object would incorrectly show up in the Overrides prefab options. Case #1093101 Fixed issue where changing the Sorting Layer or Sorting Order of a object would not dirty the scene. Case #1069776 Fixed a text alignment issue when setting text alignment on disabled text objects. Case #1047771 Fixed an issue where text object bounds were not set correctly on newly created text objects or in some cases when setting the text to null or string.empty. Case #1093388 Fixed an issue in the IntToString() function that could result in Index Out Of Bounds error. Case #1102007 Changed the TMP_InputField IsValidChar function to protected virtual. The \"Allow Rich Text Editing\" property of the TMP_InputField is now set to false by default. Added new option to the Sprite Asset context menu to make it easier to update sprite glyphs edited via the Unity Sprite Editor. Added new Sharpness slider in the Debug section of the SDF Material inspector. Fixed an error that would occur when using the context menu Reset on text component. Case #1044726 Fixed issue where CharacterInfo.index would be incorrect as a result of using Surrogate Pairs in the text. Case #1037828 The TMP_EditorPanel and TMP_UiEditorPanel now have their \"UseForChildren\" flag set to true to enable user / custom inspectors to inherit from them. Fixed an issue where rich text tags using pixel (px) or font units (em) were not correctly accounting for orthographic camera mode. This change only affects the normal TMP text component. Fixed an inspector issue related to changes to the margin in the TMP Extra Settings panel. Case #1114253 Added new property to Glyph Adjustment Pairs which determines if Character Spacing Adjustments should affect the given pair. Updated the Glyph Adjustment Table where ID now represents the unicode (hex) value for the character instead of its decimal value. Added new SetValueWithoutNotify() function to TMP_DropDown and SetTextWithoutNotify() function to TMP_InputField allowing these to be set without triggering OnValueChanged event. Geometry buffer deallocation which normally takes place when current allocations exceed those of the new text by more than 256 characters will no longer occur if the new text is set to null or string.empty. Fixed a minor issue where the underline SDF scale would be incorrect when the underline text sequence contained normal size characters and ended with a subscript or superscript character. Fixed an error that would occur when using the Reset Context menu on a Material using the SDF Surface or Mobile SDF Surface Shaders. Case #1122279 Resolved a Null Reference Error that would appear when cycling through the text overflow modes. Case #1121624 [1.3.0] - 2018-08-09 Changes Revamped UI to conform to Unity Human Interface Guidelines. Updated the title text on the Font Asset Creator window tab to \"Font Asset Creator\". Using TMP_Text.SetCharArray() with an empty char[] array will now clear the text. Made a small improvement to the TMP Input Field when using nested 2d RectMasks. Renamed symbol defines used by TMP to append TMP_ in front of the define to avoid potential conflicts with user defines. Improved the Project Files GUID Remapping tool to allow specifying a target folder to scan. Added the ability to cancel the scanning process used by the Project Files GUID Remapping tool. Moved TMP Settings to universal settings window in 2018.3 and above. Changing style sheet in the TMP Settings will now be reflected automatically on existing text objects in the editor. Added new function TMP_StyleSheet.UpdateStyleSheet() to update the internal reference to which style sheet text objects should be using in conjunction with the style tag. [1.2.4] - 2018-06-10 Changes Fixed a minor issue when using Justified and Flush alignment in conjunction with \\u00A0. The Font Asset creationSettings field is no longer an Editor only serialized field. [1.2.3] - 2018-05-29 Changes Added new bitmap shader with support for Custom Font Atlas texture. This shader also includes a new property \"Padding\" to provide control over the geometry padding to closely fit a modified / custom font atlas texture. Fixed an issue with ForceMeshUpdate(bool ignoreActiveState) not being handled correctly. Cleaned up memory allocations from repeated use of the Font Asset Creator. Sprites are now scaled based on the current font instead of the primary font asset assigned to the text object. It is now possible to recall the most recent settings used when creating a font asset in the Font Asset Creator. Newly created font assets now contain the settings used when they were last created. This will make the process of updating / regenerating font assets much easier. New context menu \"Update Font Asset\" was added to the Font Asset inspector which will open the Font Asset Creator with the most recently used settings for that font asset. New Context Menu \"Create Font Asset\" was added to the Font inspector panel which will open the Font Asset Creator with this source font file already selected. Fixed 3 compiler warnings that would appear when using .Net 4.x. Modified the TMP Settings to place the Missing Glyph options in their own section. Renamed a symbol used for internal debugging to avoid potential conflicts with other user project defines. TMP Sprite Importer \"Create Sprite Asset\" and \"Save Sprite Asset\" options are disabled unless a Sprite Data Source, Import Format and Sprite Texture Atlas are provided. Improved the performance of the Project Files GUID Remapping tool. Users will now be prompted to import the TMP Essential Resources when using the Font Asset Creator if such resources have not already been imported. [1.2.2] - 2018-03-28 Changes Calling SetAllDirty() on a TMP text component will now force a regeneration of the text object including re-parsing of the text. Fixed potential Null Reference Exception that could occur when assigning a new fallback font asset. Removed public from test classes. Fixed an issue where using nested links (which doesn't make sense conceptually) would result in an error. Should accidental use of nested links occurs, the last / most nested ends up being used. Fixed a potential text alignment issue where an hyphen at the end of a line followed by a new line containing a single word too long to fit the text container would result in miss alignment of the hyphen. Updated package license. Non-Breaking Space character (0xA0) will now be excluded from word spacing adjustments when using Justified or Flush text alignment. Improved handling of Underline, Strikethrough and Mark tag with regards to vertex color and Color tag alpha. Improved TMP_FontAsset.HasCharacter(char character, bool searchFallbacks) to include a recursive search of fallbacks as well as TMP Settings fallback list and default font asset. The &ltgradient&gt tag will now also apply to sprites provided the sprite tint attribute is set to a value of 1. Ex. &ltsprite=\"Sprite Asset\" index=0 tint=1&gt. Updated Font Asset Creator Plugin to allow for cancellation of the font asset generation process. Added callback to support the Scriptable Render Pipeline (SRP) with the normal TextMeshPro component. Improved handling of some non-breaking space characters which should not be ignored at the end of a line. Sprite Asset fallbacks will now be searched when using the &ltsprite&gt tag and referencing a sprite by Unicode or by Name. Updated EmojiOne samples from https://www.emojione.com/ and added attribution. Removed the 32bit versions of the TMP Plugins used by the Font Asset Creator since the Unity Editor is now only available as 64bit. The isTextTruncated property is now serialized. Added new event handler to the TMP_TextEventHandler.cs script included in Example 12a to allow tracking of interactions with Sprites. [1.2.1] - 2018-02-14 Changes Package is now backwards compatible with Unity 2018.1. Renamed Assembly Definitions (.asmdef) to new UPM package conventions. Added DisplayName for TMP UPM package. Revised Editor and Playmode tests to ignore / skip over the tests if the required resources are not present in the project. Revised implementation of Font Asset Creator progress bar to use Unity's EditorGUI.ProgressBar instead of custom texture. Fixed an issue where using the material tag in conjunction with fallback font assets was not handled correctly. Fixed an issue where changing the fontStyle property in conjunction with using alternative typefaces / font weights would not correctly trigger a regeneration of the text object. [1.2.0] - 2018-01-23 Changes Package version # increased to 1.2.0 which is the first release for Unity 2018.2. [1.1.0] - 2018-01-23 Changes Package version # increased to 1.1.0 which is the first release for Unity 2018.1. [1.0.27] - 2018-01-16 Changes Fixed an issue where setting the TMP_InputField.text property to null would result in an error. Fixed issue with Raycast Target state not getting serialized properly when saving / reloading a scene. Changed reference to PrefabUtility.GetPrefabParent() to PrefabUtility.GetCorrespondingObjectFromSource() to reflect public API change in 2018.2 Option to import package essential resources will only be presented to users when accessing a TMP component or the TMP Settings file via the project menu. [1.0.26] - 2018-01-10 Added Removed Tizen player references in the TMP_InputField as the Tizen player is no longer supported as of Unity 2018.1. [1.0.25] - 2018-01-05 Added Fixed a minor issue with PreferredValues calculation in conjunction with using text auto-sizing. Improved Kerning handling where it is now possible to define positional adjustments for the first and second glyph in the pair. Renamed Kerning Info Table to Glyph Adjustment Table to better reflect the added functionality of this table. Added Search toolbar to the Glyph Adjustment Table. Fixed incorrect detection / handling of Asset Serialization mode in the Project Conversion Utility. Removed SelectionBase attribute from TMP components. Revised TMP Shaders to support the new UNITY_UI_CLIP_RECT shader keyword which can provide a performance improvement of up to 30% on some devices. Added TMP_PRESENT define as per the request of several third party asset publishers. [1.0.23] - 2017-11-14 Added New menu option added to Import Examples and additional content like Font Assets, Materials Presets, etc for TextMesh Pro. This new menu option is located in \"Window -> TextMeshPro -> Import Examples and Extra Content\". New menu option added to Convert existing project files and assets created with either the Source Code or DLL only version of TextMesh Pro. Please be sure to backup your project before using this option. The new menu option is located in \"Window -> TextMeshPro -> Project Files GUID Remapping Tool\". Added Assembly Definitions for the TMP Runtime and Editor scripts. Added support for the UI DirtyLayoutCallback, DirtyVerticesCallback and DirtyMaterialCallback."
  },
  "Library/PackageCache/com.unity.textmeshpro@3.0.6/Documentation~/TextMeshPro.html": {
    "href": "Library/PackageCache/com.unity.textmeshpro@3.0.6/Documentation~/TextMeshPro.html",
    "title": "TextMesh Pro User Guide | FSM Unity Framework",
    "keywords": "TextMesh Pro User Guide Overview This User Guide was designed to provide first time users of TextMesh Pro with a basic overview of the features and functionality of the tool. Installation The TextMesh Pro UPM package is already included with the Unity Editor and as such does not require installation. TextMesh Pro \"TMP\" does however require adding resources to your project which are essential for using TextMesh Pro. To import the \"TMP Essential Resources\", please use the \"Window -> TextMeshPro -> Import TMP Essential Resources\" menu option. These resources will be added at the root of your project in the \"TextMesh Pro\" folder. The TextMesh Pro package also includes additional resources and examples that will make discovering and learning about TextMesh Pro's powerful features easier. It is strongly recommended that first time users import these additional resources. To import the \"TMP Examples & Extras\", please use the \"Window -> TextMeshPro -> Import TMP Examples & Extras\" menu option. These resources will also be added in the same \"TextMesh Pro\" folder inside your project. Quick Start There are two TextMesh Pro components available. The first TMP text component is of type <TextMeshPro> and designed to work with the MeshRenderer. This component is an ideal replacement for the legacy TextMesh component. To add a new <TextMeshPro> text object, go to: �GameObject->3D Object->TextMeshPro Text�. The second TMP text component is of type <TextMeshProUGUI> and designed to work with the CanvasRenderer and Canvas system. This component is an ideal replacement for the UI.Text component. To add a new <TextMeshProUGUI> text object, go to: �GameObject->UI->TextMeshPro Text�. You may also wish to watch this Getting Started short video which covers this topic. We strongly recommend that you also watch the Font Asset Creation video as well as the Working with Material Presets as these two topics is also key to working and getting the most out of TextMesh Pro. As mentionned in the Installation section of this guide, it is recommended that you import the \"TMP Examples & Extras\" and take the time to explore each of the examples as they provide a great overview of the functionality of the tool and the many text layout and rich text tags available in TextMesh Pro. Support & API Documentation Should you have questions or require assistance, please visit the Unity UI & TextMesh Pro section of the Unity forum as well as the TextMesh Pro User Forum where you will find additional information, Video Tutorials and FAQ. In the event you are unable to find the information you seek, always feel free to post on the Unity UI & TextMesh Pro section user forum. Online Documentation is also available on TextMesh Pro including Rich Text tags, Shaders, Scripting API and more."
  },
  "Library/PackageCache/com.unity.textmeshpro@3.0.6/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.textmeshpro@3.0.6/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "TextMesh Pro copyright © 2021 Unity Technologies ApS Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog All notable changes to this package will be documented in this file. The format is based on Keep a Changelog [1.7.6] - 2023-10-05 Fixed Fixed issue where a warning would be logged in the console for TrackAsset (TB-229). Fixed issue where changing the name of a group track was not undoable (TB-218). Fixed performance regression when rebuilding the playable graph. [1.7.5] - 2023-06-15 Fixed Fixed issue where exceptions were thrown when different ControlTracks are referencing the same TimelineAsset (IN-21163). The Text Track sample has been updated to use the com.unity.ugui package. Removed usage of deprecated API: UnityEditor.MemoryProfiler [1.7.4] - 2023-03-08 Fixed Fixed issue where previewing the Timeline would create prefab property modifications [1.7.3] - 2023-01-31 Fixed Fixed issue where modifying curves on an animation clip did not trigger an evaluation of the graph when the Timeline Window is hidden. (TB-117) [1.7.2] - 2022-09-12 Fixed Fixed an issue where menu items related to track, marker and clip types in contextual menus would be in arbitrary order in some versions of Unity. Menu items related to types will now be sorted based on their full names, including the assembly name. Fix post-extrapolation mode change not recalculating previous clip pre-extrapolation time. ([ATL-1291]) Fixed an issue where prefab overrides would be created when keyframing a prefab instance in Timeline. ([TB-108]) Fixed an issue where a warning would be raised when using the undo history to undo multiple timeline interactions([TB-119]) Fixed an issue where in some cases a NullReferenceException would be thrown in the PlayableDirector inspector after a TimelineAsset would be unloaded in the Editor([TB-129]) Fixed an issue with the NoFoldOut attribute drawer, which was behaving incorrectly when used outside of the sample context. ([TB-132]) [1.7.1] - 2022-03-07 Fixed Fixed warnings related to meta files related to missing folders. [1.7.0] - 2022-02-21 Added Added TimelinePlaybackControls Editor API: The Playback controls API lets you drive the Timeline Window playback controls from code. From this API, you can Change the current Time/Frame Query the current Time/Frame Start/Stop playback of the currently shown Timeline Go to First or Last frame Go to previous or next frame. Use it to accelerate your workflow, or build your own workflows on top of Timeline. Changed [Requires Unity 2021.2] Fixed an issue where the last frame of a Timeline was not guaranteed to be executed when the Playable Director had Wrap Mode None. License file header changed from \"Timeline copyright © 2021 Unity Technologies ApS\" to \"Timeline copyright © 2021 Unity Technologies\" Fixed Fixed an issue where unused TrackAssets would be saved in the TimelineAsset file after removing tracks. Fixed an issue where grouped markers at time zero would sometimes disappear after clicking on them (https://issuetracker.unity3d.com/issues/timeline-markers-disappear-when-double-clicking-on-stacked-markers-at-0-frames) Fixed an issue where selecting a prefab in the project view could trigger an exception when parenting the prefab to a prefab sub-object. (1386125) Fixed an issue where duplicated or pasted tracks that were part of group tracks would lose their associated bindings (https://issuetracker.unity3d.com/issues/duplicated-track-groups-lose-their-nester-tracks-game-object-assignments) Fixed an issue where pasting a track after changing scenes would lose PlayableAsset references in clips (https://issuetracker.unity3d.com/issues/animation-tracks-copy-loses-its-properties-when-its-pasted-from-another-scene) Fixed an issue where the Timeline Window play range would not be serialized and persisted. Fixed an issue where clicking on a clip during Play Mode would evaluate the Timeline unnecessarily. (https://issuetracker.unity3d.com/issues/timeline-rebuilds-playable-graph-when-selecting-a-timeline-clip-during-play-mode) Fixed an issue where control clips would behave inconsistently if the clip was set to hold, but the PlayableDirector was set to not extrapolate. (https://issuetracker.unity3d.com/product/unity/issues/guid/1375771) Fixed issue where a warning would appear in 2022.1 regarding AnimationWindowState.SnapMode. [Requires Unity 2021.2] Fixed an issue where the last frame of a Timeline was not guaranteed to be executed when the Playable Director had Wrap Mode None. Fixed an issue where the Timeline Window's UI would not update until the user clicked in the window if the TimelineAsset's file contents were changed on disk, such as during a version control operation 1357110 [1.6.3] - 2021-10-20 Fixed Fixed an issue where the Timeline Window would not work correctly with read-only source controlled files. Fixed an issue where the a MissingReferenceException would be thrown when an IAnimationWindowPreview component previewed by Timeline would be destroyed. (https://issuetracker.unity3d.com/issues/missingreferenceexception-is-thrown-when-using-rigbuilder-inside-a-prefab) Fixed an issue where the \"Match Content\" action would not apply on all selected clips. (1368028) [1.6.2] - 2021-08-05 Fixed Fixed an issue where copy-pasting Timeline Clips that contain Generic Lists of ExposedReferences would cause a NullReferenceException (1332377) [1.6.1] - 2021-06-22 Added ClipDrawOptions.hideScaleIndicator can now be used to disable the clip scale indicator. Added an asterisk to the Timeline Window when the currently edited Timeline Asset is dirty (has unsaved changes). (1024230) Added the IInspectorChangeHandler interface to change what happens when a UI component in the inspector is modified. (1283486) (Unity 2020.2+ only) The Timeline window title displays an asterisk when there are unsaved changes. Double click now toggles the collapsed state of group tracks. A keyboard shortcut can now be mapped to expand or collapse group tracks. Added displayClipName property to ClipDrawOption. Use displayClipName to display (true) or hide (false) the clip name. New API added to TimelineEditorWindow: TimelineNavigator. Enables navigation between timelines and nested timelines through code for automation purposes. Gives access to Timeline window breadcrumbs. (Unity 2021.2+ only) Added Framelocked preview option in Timeline preferences. Added framerate display with standard framerates. TimelineAsset framerate can be set with a StandardFramerate value. (TimelineAsset.SetStandardFramerate) Changed Removed non-working PlayRange options (Loop/Hold) as both were actually mapping to Loop behaviour and always have been. Timeline settings menu has been modified to use standard framerates in framerate submenu. TimelineAsset.fps is obsolete and is replaced by TimelineAsset.frameRate. TimelineProjectSettings.assetDefaultFramerate is obsolete and is replaced by TimelineProjectSettings.defaultFramerate. Fixed Removed GC allocations in PlayableDirector.duration when a timeline asset is assigned. (1298818) Removed warnings with AnimationWindowState snap mode. (1306205) Fixed issue where the \"Navigate Right\" (default key: Right Arrow ▶) would not behave consistently. The correct order of operations should now always be, in order: expand group, select first track of group, then select first item of the track. Fixed frame display not rounding up correctly. (1333009) Fixed an issue where TimelinePlayable duration would not be initialized if the playable is not created from the PlayableDirector. (1329151) Fixed memory leak in custom playable inspectors. (1332377) Fixed exception when using the Key All Animated shortcut with no Timeline selected. (1334339) Fixed issue where a warning would appear regarding obsolete AnimationWindowState.SnapMode values. [1.5.5] - 2021-04-30 Fixed Fixed an issue in the Curves view where the color indicator was sized incorrectly on high-res displays. (1318782) Fixed a rare issue where keyframes were created for Playable Curves when switching to play mode. (1319124) Fixed an issue where clearing the Unity selection did not refresh the Timeline window. (1320260) Fixed an issue with IAnimationWindowPreview.StartPreview not getting called for sub timelines. (1322571) Fixed an issue where the curve color identifiers would overlap property names when the Timeline window was resized. (1323591) Fixed a regression where changes made to clip curves would not be processed until another modification caused a graph rebuild. Fixed compilation issue on 2020.1 due to incorrect version checks. Fixed issue where text labels were incorrectly displayed when the mouse pointer was located above a clip. [1.5.4] - 2021-03-10 Fixed Fixed issue where the horizontal scrollbar could not be moved or resized. [1.5.3] - 2021-03-05 Changed Disabled edition of Track Asset Inspector Script field as it could break Timeline Assets. Fixed Fixed issue where the timeline header track would automatically open during a drag and drop operation. (1305436) Fixed a rare issue where some broken tracks could not be removed. (1305388) Fixed rare issue where the time field could not be edited after opening a timeline. (1312198) Fixed cosmetic issue where the duration marker was drawn over the scroll bar. Fixed issue where times without a decimal separator (. or , depending on locale) would not be interpreted correctly by the time field. (1315605) Fixed issue where a selection rectangle could not be made when started inside a track. (1315840) Performing Undo/Redo will not affect Timeline window selection when the window is locked. (Selecting sub-timelines can still be undone). (1313515) Fixed an issue where text would be clipped in the track header binding. (1302401) Fixed issue where clicking in the Timeline window while there is no active timeline would throw an exception. [1.5.2] - 2021-01-08 Added During recording, there are new ways to key animated properties: A new Inspector context menu has been added (Key All Animated) that sets a key to all currently animated properties. It is possible to make a multi-selection of tracks to set a keyframe to all currently animated properties. If no track is selected, all recording tracks are keyed. If properties are selected in the curve editor, only those properties are keyed. TimelineEditor.GetWindow and TimelineEditor.GetOrCreateWindow to get the current Timeline window or create a Timeline window. TimelineEditorWindow.SetCurrentTimeline to change which timeline asset is opened in the Timeline window. TimelineEditorWindow.lock to lock or unlock the Timeline window. TrackExtensions.GetCollapsed, TrackExtensions.SetCollapsed, TrackExtensions.IsVisibleRecursive to get and change the visibility state of a track. AnimationTrackExtensions.IsRecording, AnimationTrackExtensions.SetRecording, AnimationTrackExtensions.SupportsRecording to get or change the recording state of an Animation track. Added two methods in TrackEditor to control how an object is bound to a track: IsBindingAssignableFrom and GetBindingFrom. Added Japanese translation. The Timeline window will automatically rebuild the graph when a notifications's properties are changed. The Timeline window will be automatically refreshed when a marker's properties are changed. Added TimelineEditor.GetInspectedTimeFromMasterTime and TimelineEditor.GetMasterTimeFromInspectedTime to convert time from master to inspected timeline and vice versa when using sub-timelines. Added API to improve how to get/set a TimelineClip's parent track: TimelineClip.GetParentTrack (replaces obsolete property getter) ItemsUtils.SetParentTrack (extension method thar replaces obsolete property setter) Added a new Seconds time display mode and renamed previous Seconds mode to Timecode. TimelinePreferences.timeFormat field, UnityEditor.Timeline.TimeFormat enum. Added API for the user to clip to the track area: API: Relevant member to MarkerOverlayRegion, API: MarkerOverlayRegion.trackRegion, API: MarkerOverlayRegion constructor. Added Gameplay sequence sample. This sample demonstrates how Timeline can be used to create a small in-game moment, using built-in tracks. Added Customization sample. This sample demonstrates how to create custom tracks, clips, markers and actions. Changed The binding field on a track header will change its background color when dragging a valid object on it. Timeline marker track is now selectable. TimelineClip property parentTrack is now obsolete. TimelinePreferences.timeUnitInFrames is now obsolete. Fixed Fixed a bug affecting the conversion between seconds and frames in the inspector. Fixed issue where KeyAllAnimated was available when right-clicking on markers and tracks that were not in record mode. (1270304) Fixed issue where the mouse cursor would stay stuck to a resize icon when resizing the track header. (1076031) Fixed case where an animation event at time 0 would not fire on a timeline loop. (1184106) Fixed issue where Timeline objects (ie. TrackAsset, ControlTrack, SignalAsset, etc.) would have incorrect links to the documentation pages. Available starting from Unity 2021.1. (1082941) Fixed multiple issues related to blends Fix display of blends when clips have ease-in/ease-out (1178066) Fix clip disappearing when dragging it from left to right completely inside another clip. Fix select and drag clip discarding foreground display rule of selected clip after releasing the drag. Fix fully blended clips selection not available. (1289912) Fixed issue where the clip display would flicker when moving two clips that are completely overlapped. (1085679) The Timeline window will no longer revert to editing only the asset if the user uses the Timeline selector to pick a game object and switches focus. (1291455) Create button on timeline panel no longer defaults to an invalid path. (1289923) Fixed issue where Timeline's bindings field would loses names and bindings when selecting clips. (1293941) Make Timeline's duration result displayed in the Inspector, when switching from duration mode: Based On Clips to Fixed Length, closer to the actual duration. (1156920) Copy/Paste of clips in the Timeline Window will no longer paste clips at an invalid time in mix-mode. (1289925) [1.4.5] - 2020-11-19 Fixed Fixed issue where changing a clip's extrapolation values would clear the current clip selection. (936046) Fixed multiple issues related to the curves view: Fixed curve removal not functioning with PlayableAssets (clips & tracks curves). (1231002) Fixed inconsistent icon display on curves. Fixed incorrect ordering of properties. Properties now have a object/type/property ordering. Fixed unnecessary grouping of fields. Changed context menu from Remove Properties to Remove Curves to better reflect the change in functionality between curves for GameObjects and curves for PlayableAssets. Fixed behaviour where removing a single field in a Position, Rotation or Scale group would remove the entire group. Fixed case where pausing in Playmode and switching the active director in editor could pause the director. (1263707) Material properties are now displayed by their shader name in the curves view when possible. (1115961) Fixed issue where a signal could be pasted on a track that doesn't support notifications. (1283763) Fixed issue where a clip could be paseted on an incompatible track. (1283763) Fixed errors when leaving prefab mode when a timeline is opened. (1280331) No preview will be shown when the PlayableDirector is disabled. (1286198) Fixed issue where an infinite clip's Foot Ik property was not visible in the Inspector when selecting its track. (1279824) Fixed issue where child particle systems were not controlled correctly when they are not subemitters. (1212943) Fixed inconsistent recording behaviour on audio tracks and PlayableAssets. Default values are changed when a value is not recorded, and the key added/updated when a value is already animated. (1283453) Fixed issue where the curves view for tracks and PlayableAssets would not update when changed externally (such as from the Animation window). Fixed Add Key/Remove Key context menus not being properly enabled in some cases when using tracks and PlayableAssets. Fixed simulation of subemitters when scrubbing a timeline. (1142781) Fixed choppy playback of particles with a large fixed time step. (1262234) [1.4.4] - 2020-10-09 Fixed Disable drag and drop of Signal asset on Control Track. (1222760) Fixed system locale causing issues when keying float values on custom clips. (1190877) Fixed issue where recording to a clip would place keys on the frame. (1274892) Fixed keyboard clip selection from locked tracks. (1233612) Fixed issue where the Timeline window would stay locked even when no timeline asset is shown. (1278598) Fixed issue where invoking SelectLeft or SelectRight shortcuts on a group track, the group would not collapse or expand. (1279379) Fixed Blend Curve Editor from the clip's inspector that was not responding correctly to undo and redo commands. (978673) Fixed issue where the Frame All action would not frame keys outside of clips when the curve display is collapsed. (1273725, #295) Scrolling the horizontal scrollbar of the timeline to the right edge will no longer prevent the user from dragging left again. (1127199, #301) Splitting a clip with an ease in or out value now ensures ease duration stays on correct side of split. (1279350) Fixed delay when zooming in after reaching Timeline window's maximum and then zooming back. (1214228) Prevent creation of presets with Group Tracks. (1281056) Fixed issue where markers placed on top of clips could not be selected. (1284807, #314) Fixed issue where multiple markers placed on top of each other could not be selected. (1284801, #314) [1.4.3] - 2020-08-26 Fixed Fixed incorrect selection when clicking on a clip's blend. (1178052) Fixed issue where an exception was thrown when drawing an Audio clip's waveform when that clip wasn't in the AssetDatabase. (1268868) When choosing Add Signal Emitter from Signal Asset, closing the Object Selector window will not add an empty Signal Emitter. (1261553) Fixed issue where an error would appear when editing keys in the Animation window if the Timeline window is opened. (1269829) Fixed issue where the Frame All operation would continually increase the zoom value when only empty tracks are added to the timeline (1273540). [1.4.2] - 2020-08-04 Fixed Fixed double-click not opening the AnimationWindow on clips with animated parameters. (1262950) Fixed issue where the Timeline window would rebuild its Playable Graph every time an AnimationClip would be added, changed or deserialized. (1265314, 1267055) [1.4.1] - 2020-07-15 Fixed Fixed IndexOutOfRangeException exception being thrown when editing inspector curves. (1259902) Fixed IndexOutOfRangeException exception being thrown when the New Signal dialog replaces an existing signal. (1241170) Fixed signal state being reset on paused timelines. (1257208) Fixed nested custom types not updating animation values in the inspector. (1239893) Fixed AnimationTracks SceneOffset mode incorrectly overriding root transform on tracks without root transform in editor. (1237704) The DisplayName attribute is now supported when used with TrackAssets. (1253397) Fixed NullReference exception being thrown when clicking on the Scene Preview checkbox if the Timeline window was closed. (1261543) [1.4.0] - 2020-06-26 Added Added ClipCaps.AutoScale to automatically change the speed multiplier value when the clip is trimmed in the Timeline window. Added a DeleteClip method in TrackAsset. Added dependency on Animation, Audio, Director and Particle System modules. (1229825) Added an option in TimelineAsset.EditorSettings to disable scene preview. Added base classes to define custom actions: TimelineAction TrackAction ClipAction MarkerAction Added the following attributes that can be used with action classes: ApplyDefaultUndo to automatically manage undo operations. ActiveInMode to control in which Timeline mode the action is valid. MenuEntry to add the action to the context menu. TimelineShortcut can be added to a static method to invoke the action with a shortcut. Invoker to invoke actions using Timeline's selection or context. MenuOrder contains menu priority values, to be used with MenuEntry. TimelineModes to specify in which mode an action is valid, to be used with MenuEntry. ActionContext to provide a context to invoke TimelineActions. ActionValidity to specify is an action is valid for a given context. UndoExtension to manage undo operations with common Timeline types. Changed Improved performance with ControlTracks in preview mode for cases where multiple Control Tracks are assigned to the same PlayableDirector. Improved layout and appearance of track header buttons. Reduced icons' file size without any quality loss. A track's binding will be duplicated when pasting or duplicating a track. When creating a new timeline asset, the \"Timeline\" suffix will not be added to the file name twice. ClipCaps.All now includes the new Autoscale feature. To get the previous ClipCaps.All behaviour on clips, use ClipCaps.Looping | ClipCaps.Extrapolation | ClipCaps.ClipIn | ClipCaps.SpeedMultiplier | ClipCaps.Blending Inline curve selection is now synced with the clip's selection. Selecting a curve view property will also select the corresponding curve view. Clicking and holding the Command or Control key on a curve view will deselect it if it was already selected. Improved Timeline window UI performance. Fixed Selecting clips from locked tracks is not allowed anymore when using the playhead's context menu. Inserting gaps in locked tracks is not allowed anymore. When adding an Activation track, the viewport is adjusted to show the new Activation clip. Fixed issue where trimming AnimationClips would also change the speed multiplier. [1.3.4] - 2020-06-09 Fixed Fix a Control Track bug that caused the first frame of an animation to evaluated incorrectly when scrubbing forwards and backwards. (1253485) Fixed memory leak where the most recently played timeline would not get unloaded. (1214752 and 1253974) [1.3.3] - 2020-05-29 Fixed Fixed regression where animation tracks were writing root motion when the animation clip did not contain root transform values (1249355) [1.3.2] - 2020-04-02 Fixed Fixed issue where the clip Inspector's curve preview would close when clicking on the curve. (1228127) Fixed issue where the curves view was not synced between Animation and Timeline windows. (1213937) Fixed issue where play range didn't loop when range ends on the final frame. (1215926) Fixed issue where displaying an array in the curves view generated errors. (1178251) [1.3.1] - 2020-03-13 Fixed Fixed issue where the curves view would flicker when editing multiple keys. (1217326) Fixed issue where adding a keyframe in the curves view at the end of a clip would not place the keyframe at the correct position. (1221337) [1.3.0] - 2020-02-26 Added Inline Curve Properties can be removed. Tracks can be individually resized. Changed Creating a new Timeline will no longer automatically add an Animation Track and an Animator to the target GameObject. Ease-in and ease-out values for clips are no longer restricted to 50% of the clip's duration. The resize handle for inline curves has been moved to the track header area. Reduced the minimum width of the track header area. Trimming the left edge of a clip while pressing the Shift key will change the Speed Multiplier value. Fixed Fixed humanoid characters going to default pose during initial root motion recording. (1174752) Fixed Override Tracks not masking RootTransform when an AvatarMask without the Root Node is applied. (1190600) Fixed preview of Avatar Masks on base level Animation Tracks. (1190600) [1.2.13] - 2020-02-24 Fixed Fixed Performance issue where Control Tracks would resimulate during the tail of a non-looping particle clip. (1216702) Fixed adjacent recording clips highlighting the wrong clip. (1210312) Fixed timescale drawing to only draw visible lines which avoids a hang with very large clips. (1213189) Fixed SignalReceiver.ChangeSignalAtIndex incorrectly throwing exception when multiple entries are set to null. (1210877) Fixed a memory leak with Animation Clips in Edit mode. Fixed issue where changes to a Signal Receiver component in a prefab were reverted. (1210883) Fixed avatar mask reassignment not causing immediate re-evaluation. (1219326) Fixed issues related to recursive control tracks. (1178423) Fixed issue where using the HideInMenu attribute in combination with a class inheriting from Marker would not hide the marker from the Timeline context menus. (1221054) [1.2.12] - 2020-02-21 Fixed Fixed issue where the curves view would change its framing when moving a clip. (1217353) [1.2.11] - 2020-01-22 Fixed Fixed Control Track inspector dropdown not opening. (1208943) Fixed issue where applying the Match content command on subtimeline clip with a newly created subtimeline with no duration makes the clip disappear. (1203662) Fixed issue where the opened timeline is changed to another timeline when switching focus from Unity to a different application. (1087348) Fixed issue where the keys in the inline curves view were incorrectly positioned (1205835) Changed ControlPlayableAsset.searchHierarchy (a.k.a. Control Children) now defaults to false. [1.2.10] - 2019-12-08 Fixed Fixed issue where object selectors on tracks did not show bound objects. (1202853) Fixing inspector blend graph display for animation clips. (1201474) Fixed Timeline Window lock state when restarting Unity and no timeline are selected. (1201405) [1.2.9] - 2019-12-06 Fixed Added missing high-resolution icons for Personal Skin. [1.2.8] - 2019-11-21 Fixed Fixed issue where recording couldn't be turned on for override tracks. (1199389) Fixed overlay bug when panning. (1198348) Fixed Foot IK being applied in Editor when option is disabled. (1197426) Fixed issue where the Animation Track's inline curves were not properly aligned when panning the timeline. (1198364) [1.2.7] - 2019-11-15 Fixed Fixed inline curves to display PlayableBehaviour array properties. (1178251) Fixed clip selection from playhead. (1187495) Fixed recorded clips dirtying the scene on copy/paste. (1181492) [1.2.6] - 2019-10-25 Added Added Timeline manual. [1.2.5] - 2019-10-16 Changed Added tooltips that were missing for Timeline selector and settings buttons. (1152790) Removed Undo menu entry that was added when clicking on the Inline curves button. (1187402) Fixed Fixed issue where recording couldn't be turned off when an object is deactivated. (1187174) Timelines listed in the Timeline selector will now be sorted alphabetically. (1190514) Fixed Insert Frames options from Trackhead context menu not applying to markers. (1187895) Fixed incorrect display when a large number of nested group tracks was added to a Timeline. (1157367) [1.2.4] - 2019-10-03 Changed Properties in the Inline Curve editor will now be listed in the same order as the Animation window. (1184058) Updated the appearance of the Timeline window to conform to the editor's UX redesign Improved the appearance of clip blends. Fixed Adding a PlayableDirector with no Playable Asset will no longer trigger a repaint of the Timeline Window on each frame. (1172707) Fixed issue where a clip's blend selection border was not drawn correctly when there was a previous clip. (1178173) Fixed issue where Animation Events were fired twice when the Playable Director Wrap mode is set to Loop. (1173281) Fixed issue where double-clicking on a Timeline Asset would not open it in the Timeline window. (1182159) Fixed issue where the paste shortcut would not work when copying and pasting between two different timelines. (1184967) Fixed audio stutter when going into playmode. (1167289) Fixed PreviousFrame and NextFrame controls in subtimelines with large offsets. (1175320) Fixed issue where exceptions were thrown when resetting a Signal Receiver component. (1158227) Increased font size of clip labels (1179642) [1.2.3] - 2019-10-03 Fixed Removed unnecessary directories from the package. [1.2.2] - 2019-08-20 Fixed Fixed issue where fields for custom clips were not responding to Add Key commands. (1174416) Fixed issue where a different track's bound GameObject is highlighted when clicking a track's bound GameObject box. (1141836) Fixed issue where a clip locks to the playhead's position when moving it. (1157280) [1.2.1] - 2019-08-01 Fixed Fixed appearance of a selected clip's border. Fixed non-transform properties from AnimationClips not being correctly put into preview mode when the avatar root does not contain the animator component. (1162334) Fixed an issue where the context menu for inline curves keys would not open on MacOS. (1158584) Fixed recording state being incorrect after toggling preview mode (1146551) Fixed copying clips without ExposedReferences causing the scene to dirty (1144469) [1.2.0] - 2019-07-16 Compatible with Unity 2019.3 Added Added ILayerable interface. Implementing this interface on a custom track will enable support for multiple layers, similar to the AnimationTracks override tracks. Added \"Pan\" autoscrolling option in the Timeline window. Enabled rectangle tool for inline curves. Changed Scrolling horizontally with the mouse wheel or trackpad now pans the timeline view horizontally, instead of zooming. Scrolling vertically with the mouse wheel or trackpad on the track headers or on the vertical scroll bar now pans the timeline view vertically, instead of zooming. Fixed Fixed an issue causing info text to overlap when displaying multiple lines (1150863). Fixed duration mode not reverting from \"Fixed Length\" to \"Based On Clips\" properly. (1154034) Fixed playrange markers being drawn over horizontal scrollbar (1156023) Fixed an issue where a hotkey does not autofit all when Marker is present (1158704) Fixed an issue where an exception was thrown when overwriting a Signal Asset through the Signal Emitter inspector. (1152202) Fixed Control Tracks not updating instances when source prefab change. (case 1158592) An exception will be thrown when calling TrackAsset.CreateMarker() with a marker that implements INotification if the track does not support notifications. (1150248) Fixed preview mode being reenabled when warnings change on tracks. (case 1151381) Fixed minimum clip duration to be frame aligned. (case 1156602) Fixed playhead being moved when applying undo while recording.(case 1154802) Fixed warnings about localEulerAnglesRaw when using RectTransform. (case 1151100) Fixed precision error on the duration of infinite tracks. (case 1156723) Fixed issue where two GatherProperties call were made when switching between two PlayableDirectors. (1159036) Fixed issue where inspectors for clips, tracks and markers would get incorrectly displayed when no Timeline Window is opened. (1158242, 1158283) Fixed issue with clip connectors that were incorrectly drawn when the timeline was panned or zoomed. (1141960) Fixed issue where evaluating a Playable Graph inside a Notification Receiver would cause an infinite recursion. (1149930) Fixed Trim and Move operations to ensure playable duration is updated upon completion. (1151894) Fixed options menu icon that was blurry on high-dpi screens. (1154623) Track binding field is now larger. (1153446) Fixed issue where an empty Timeline window would create new objects on each repaint. (1142894) Fixed an issue causing info text to overlap when displaying multiple lines (when trimming + time scaling, for example). (1150863) Fixed duration mode not reverting from \"Fixed Length\" to \"Based On Clips\" properly. (1154034) Prevented the PlayableGraph from being created twice when playing a timeline in play mode with the Timeline window opened. (1147247) Fixed issue where an exception was thrown when clicking on a SignalEmitter with the Timeline window in asset mode. (1146261) A timeline will now be played correctly when building a player with Mono and Managed Stripping Level set higher than Low. (1133182) The Signal Asset creation dialog will no longer throw exceptions when canceled on macOS. (1141959) Fixed issue where the Emit Signal property on a Signal Emitter would not get saved correctly. (1148709) Fixed issue where a Signal Emitter placed at the start of a timeline would be fired twice. (1149653) Fixed record button state not updating when offset modes are changed. (1142747) Cleared invalid assets from the Timeline Clipboard when going into or out of PlayMode. (1144473) Copying a Control Clip during play mode no longer throws exceptions. (1141581) Going to Play Mode while inspecting a Track Asset will no longer throw exceptions. (1141958) Resizing Timeline's window no longer affects the zoom value. (1147150) Snap relaxing now responds to Command on Mac, instead of Control. (1149144) Clips will no longer randomly disappear when showing or hiding inline curves. (1141661) The global/local time referential button will no longer be shown for a top-level timeline. (1080872) Playhead will not be drawn above the bottom scrollbar anymore. (1134016) Fixed moving a marker on an Infinite Track will keep the track in infinite mode (1141190) Fixed zooming in/out will keep the padding at the beginning of the timeline (1030689) Fixed marker UI is the same color and size on infinite track (1139370) Fixed Disable the possibility to add Markers to tracks of a Timeline that is ReadOnly (1134463) Fixed wrong context menu being shown when right-clicking a marker (1133592) Fixed creation of override track to work with multiselection (1133592) [1.1.0] - 2019-02-14 Compatible with Unity 2019.2 Added ClipEditor, TrackEditor and MarkerEditor classes users can derive from to control visual appearance of custom timeline clips, tracks and markers using the CustomTimelineEditor attribute. ClipEditor.GetSubTimelines to allow user created clips that support sub-timelines in editor TimelineEditor.selectedClip and TimelineEditor.selectedClips to set and retrieve the currently selected timeline clips IPropertyCollector.AddFromName override that takes a component. Warning icons to SignalEmitters when they do not reference an asset Ability to mute/unmute a Group Track. Mute/Unmute only selected track command added for tracks with multiple layers. Animate-able Properties on Tracks and Clips can now be edited through inline curves. Added loop override on AnimationTrack clips (1140766) ReadOnly/Source Control Lock support for Timeline Scene Changed Control Track display to show a particle system icon when particle systems are being controlled Animate-able Properties for clips are no longer edited using by \"recording\"; they are edited through the inline curves just like tracks. AudioTrack properties can now be animated through inline curves. Changed Marker show/hide to be undoable. Hide will also unselect markers. (1124661) Changed SignalReceivers show their enabled state in the inspector. (1131163) Changed Track Context Menu to show \"Add Signal Emitter\" at the top of the list of Marker commands. (1131166) Moved \"Add Signal Emitter\" and \"Add Signal Emitter From Asset\" commands out of their sub-menu. (1131166) Fixed Fixed markers being drawn outside their pane. (1124381) Fixed non-public tracks not being recognized by the Timeline Editor. (1122803) Fixed keyboard shortcuts for Frame All (default: A) and Frame Selected (default: F) to also apply horizontally (1126623) Fixed recording getting disabled when selecting a different GameObject while the Timeline Window is not locked. (1123119) Fixed time sync between Animation and Timeline windows when clips have non-default timescale or clip-in values. (930909) Fixed animation window link not releasing when deleting the timeline asset. (1127425) Fixed an exception being raised when selecting both a Track marker and a Timeline marker at the same time. (1113006) Fixed the header marker area will so it no longer opens its context menu if it's hidden. (1124351) Fixed Signal emitters to show the Signals list when created on override tracks. (1102913) Fixed a crash on IL2CPP platforms when the VideoPlayer component is not used. (1129572) Fixed Timeline Duration changes in editor not being undoable. (1109279) Fixed Match Offsets commands causing improper animation defaults to be applied. (911678) Fixed Timeline Inspectors leaving EditorGUI.showMixedValue in the wrong state. (1123895) Fixed issue where performing undo after moving items on multiple tracks would not undo some items. (1131071) Fixed cog icon in the Signal Receiver inspector being blurry. (1130320) Fixed Timeline marker track hamburger icon not being centered vertically. (1131112) Fixed detection of signal receivers when track is in a group. (1131811) Fixed exception being thrown when deleting Signal entries. (1131065) Fixed Markers blocking against Clips when moving both Clips and Markers in Ripple mode. (1102594) Fixed NullReferenceException being thrown when muting an empty marker track. (1131106) Fixed SignalEmitter Inspector losing the Receiver UI when it is locked and another object is selected. (1116041) Fixed Marker and Clip appearing to be allowed to move to another track in Ripple mode. (1131123) Fixed issue where the Signal Emitter inspector did not show the Signal Receiver UI when placed on the timeline marker track. (1131811) Fixed Replace mode not drawing clips when moved together with a Marker. (1132605) Fixed inline curves to retain their state when performing undo/redo or keying from the inspector. (1125443) Fixed an issue preventing Timeline from entering preview mode when an Audio Track is present an a full assembly reload is performed. (1132243) Fixed an issue where the Marker context menu would show a superfluous line at the bottom. (1132662) Fixed an issue preventing Timeline asset to be removed from a locked Timeline Window when a new scene is loaded. (1135073) Fixed EaseIn/Out shortcut for clips [1.0.0] - 2019-01-28 Compatible with Unity 2019.1 Added This is the first release of Timeline, as a Package Added API calls to access all AnimationClips used by Timeline. Added support in the runtime API to Animate Properties used by template-style PlayableBehaviours used as Mixers. Added Markers. Markers are abstract types that represent a single point in time. Added Signal Emitters and Signal Assets. Signal Emitters are markers that send a notification, indicated by a SignalAsset, to a GameObject indicating an event has occurred during playback of the Timeline. Added Signal Receiver Components. Signal Receivers are MonoBehaviour that listen for Signals from Timeline and respond by invoking UnityEvents. Added Signal Tracks. Signal Tracks are Timeline Tracks that are used only for Signal Emitters. Fixed Signal Receiver will no longer throw exceptions when its inspector is locked (1114526) Context menu operations will now be applied on all selected tracks (1089820) Clip edit mode clutch keys will not get stuck when holding multiple keys at the same time (1097216) Marker inspector will be disabled when the marker is collapsed (1102860) Clip inspector will no longer throw exceptions when changing values when the inspector is locked (1115984) Fixed appearance of muted tracks (1018643) Fixed multiple issues where clips and markers were selectable when located under the time ruler and the marker header track (1117925, 1102598) A marker aligned with the edge of a clip is now easier to select (1102591) Changed behaviour of the Timeline Window to apply modifications immediately during Playmode (922846, 1111908) PlayableDirector.played event is now called after entering or exiting Playmode (1088918) Undoing a paste track operation in a group will no longer corrupt the timeline (1116052) The correct context menu will now be displayed on the marker header track (1120857) Fixed an issue where a circular reference warning appeared in the Control Clip inspector even if there was no circular reference (1116520) Fixed preview mode when animation clips with root curves are used (case 1116297, case 1116007) Added option to disable foot IK on animation playable assets (case 1115652) Fixed unevaluated animation tracks causing default pose (case 1109118) Fixed drawing of Group Tracks when header is off-screen (case 876340) Fixed drag and drop of objects inside a group being inserted outside (case 1011381, case 1014774)"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "About Timeline Timeline overview Using the Timeline window Creating a Timeline Asset and Timeline instance Recording basic animation with an Infinite clip Converting an Infinite clip to an Animation clip Animating a humanoid Using an Animation Override track and Avatar masking Nesting Timeline instances Timeline window Timeline Preview and Timeline Selector Timeline Playback Controls Track List and Track Headers Adding Tracks Selecting Tracks Duplicating Tracks Deleting Tracks Locking Tracks Muting Tracks Reordering Tracks and Rendering Priority Using Track Groups Collapsing and Expanding Track Groups Locking Track groups Clip Edit modes and the Clips view Panning and Zooming the Clips View Adding Clips Inserting Clips Selecting Clips Positioning Clips Tiling Clips Duplicating Clips Trimming Clips Splitting Clips Resetting Clips Changing Clip Play Speed Setting Gap Extrapolation Easing-in and Easing-out Clips Blending Clips Matching clip offsets Curves View Hiding and Showing Curves Navigating the Curves View Selecting Keys Adding Keys Editing Keys Changing Interpolation and Shape Deleting Keys Timeline Settings Timeline Inspector Setting Timeline Properties Setting Track Properties Activation Track Properties Animation Track Properties Setting Clip Properties Activation Clip Properties Animation Clip Common Properties Animation Clip Playable Asset Properties Audio Clip Properties Control Clip Common Properties Control Clip Playable Asset Properties Playable Director Component Samples Annotation marker Video track Time dilation track Tween track Text track Defining custom USS styles Timeline Glossary"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_about.html",
    "title": "Clip Edit modes and the Clips view | FSM Unity Framework",
    "keywords": "Clip Edit modes and the Clips view Use the Clips view to add, position, and manipulate clips on each track in the Track list. The selected Clip Edit mode determines how clips interact when you add, move, or delete them. The Clip Edit modes (green) and the Clips view (red) Clips and the Clips view In the Clips view, each clip has a colored accent line that identifies the type of clip: Activation clips are green. Animation clips are blue. Audio clips are orange. Control clips are turquoise. Playable clips are white. A clip based on data, such as an Animation clip or an Audio clip, displays arrows that indicate when the clip has been trimmed to exclude part of its source animation, waveform, or other data. For example, if an Animation clip uses only part of its full key animation, white arrows indicate that key animation exists before the start or after the end of the clip. Small arrows (circled) indicate that data exists before the start or after the end of the area defined by the clip To resize a clip and view its hidden data, either right-click the clip and select Match Content from the context menu, or select the clip and modify its clip timing properties in the Inspector window. When you resize a clip, the selected Clip Edit mode determines how the surrounding clips are affected. Clip Edit modes Select a Clip Edit mode to choose how clips are added, positioned, and trimmed within the Clips view, or when modifying clip timing properties in the Inspector window. There are three Clip Edit modes that affect most clip editing features: Mix mode (default), Ripple mode, and Replace mode. Clip Edit modes are Mix (default and selected), Ripple, and Replace mode You can also temporarily switch between Clip Edit modes. This is useful if, for example, you want to temporarily use Ripple mode to offset the content of a track while you position clips. To temporarily switch between Clip Edit modes, hold down the following keyboard keys: Hold 1 to temporarily switch to Mix mode. Hold 2 to temporarily switch to Ripple mode. Hold 3 to temporarily switch to Replace mode. Mix mode Use Mix mode to add, position, and trim clips without moving or replacing adjacent clips. Mix mode creates blends between intersecting clips. Mix mode is the default Clip Edit mode. Timeline window with Mix mode as the selected Clip Edit mode. The position cursor (circled) indicates where you drag to position the clip. In Mix mode, when you hover over a selected clip in the Clips view, the cursor changes to indicate the action that you can perform. The action depends on the part of the clip that you hover over: When you hover over the start of a selected clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the start of the clip. When you hover over the middle of a selected clip, the cursor changes to a position cursor and indicates the area to drag to position the clip. When you hover over the end of a selected clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the end of the clip. In Mix mode, if you drag to trim or position a clip and it intersects another clip, the cursor changes to a white arrow that points towards the blend being created. There are three possible cursors depending on whether the blend is created at the beginning of the clip, at the end of the clip, or at both the beginning and end of the clip. The white arrow cursor indicates that dragging Clip 2A to the right creates a blend, at the end of the clip, between Clip 2A and Clip 2B. Ripple mode Use Ripple mode to add, position, and trim a clip while affecting the subsequent clips on the same track. Positioning or trimming clips in Ripple mode preserves the gaps between subsequent clips. Timeline window with Ripple mode as the selected Clip Edit mode. The position cursor (circled) indicates where you drag to position the clip. In Ripple mode, when you hover over a selected clip in the Clips view, the cursor changes to indicate the action that you can perform. The actions and areas are similar to Mix mode: When you hover over the start of a selected clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the clip relative to its start. When you hover over the middle of a clip, the cursor changes to a position cursor and indicates the area to drag to position the clip. When you hover over the end of a clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the clip relative to its end. In Ripple mode, when you click and drag to trim or position a clip, the cursor switches to a yellow arrow that points towards the affected clips and gaps. A yellow line indicates the ripple point. When you drag to trim a clip, dragging left and right changes the duration of the selected clip and repositions subsequent clips and gaps after the ripple point. For example, the yellow arrow cursor indicates that trimming the start of Clip 2A in Ripple mode changes the clip duration and affects the clips and gaps after the ripple point: Clip 2B and Clip 2C.] Replace mode Use Replace mode to add, position, and trim a clip while cutting or replacing intersecting clips. Timeline window with Replace mode as the selected Clip Edit mode. The position cursor (circled) indicates where you drag to position the clip. In Replace mode, when you hover over a selected clip in the Clips view, the cursor changes to indicate the action that you can perform. The actions and areas are similar to Mix mode: When you hover over the start of a selected clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the clip relative to its start. When you hover over the middle of a clip, the cursor changes to a position cursor and indicates the area to drag to position the clip. When you hover over the end of a clip, the cursor changes to a trim cursor. The trim cursor indicates the area to drag to trim the clip relative to its end. In Replace mode, when you drag to position a clip, the clip becomes translucent so that you can view overlapping clips. If the clip being positioned overlaps other clips, the cursor changes to a red arrow and red replacement lines indicate where each overlap occurs. Releasing the clip cuts the underlying clip at each red overlap. For example, the red arrow cursor indicates that dragging Clip 2A to the right overlaps Clip 2B. Releasing the clip cuts Clip 2B at the point where the overlap occurs. In Replace mode, trimming a clip is similar to positioning a clip. When you drag to trim a clip and it intersects another clip, the cursor changes to a red arrow and a red replacement line indicates where the overlap occurs. Releasing the trim cuts the intersecting clip at the red replacement line."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_add.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_add.html",
    "title": "Adding clips | FSM Unity Framework",
    "keywords": "Adding clips The Timeline window supports different methods of adding clips to tracks, depending on the type of track, where you click, and whether a clip or track is already selected. The quickest method to add a clip is to right-click on an empty area within a track and select the appropriate Add option from the context menu. Depending on the track, the options for adding a clip change. Context menu for adding an Activation clip. There are other ways to add clips: Select a clip option from the Track menu in the Track Header to add a clip at the location of the Timeline Playhead. Drag an animation Source Asset from the Project window to an empty area in the Timeline window to automatically create an Animation track and add an Animation clip. Drag an animation Source Asset from the Project window to an existing track in the Timeline window to add an Animation clip to the same track. Drag an audio Source Asset from the Project window to an empty area in the Timeline window to automatically create an Audio track and add an Audio clip. Drag a GameObject with a PlayableDirector component to create a nested Timeline instance. This automatically creates a Control track and adds a Control clip for the nested Timeline instance. Drag a Prefab from the Project window to an empty area in the Timeline window to add a Prefab instance to your Timeline instance. This automatically creates a Control track and adds a Control clip for the Prefab instance. Drag a GameObject with a Particle component to add a particle effect to your Timeline instance. This automatically creates a Control track and adds a Control clip for the duration of the Particle effect. When you add a clip, the selected Clip Edit mode determines how the added clip interacts with surrounding clips. For example, if you add an Animation clip or an Audio clip in Mix mode and the added clip intersects a clip on the same track, Timeline creates a blend."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_blend.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_blend.html",
    "title": "Blending clips | FSM Unity Framework",
    "keywords": "Blending clips Blend two clips on the same track to create a smooth transition between two Animation clips, two Audio clips, or two Playable clips. To blend two clips, select the Mix Clip Edit mode and position or trim one clip until it overlaps an adjacent clip. In a blend, the first clip is referred to as the outgoing clip and the second clip is referred to as the incoming clip. The area where the outgoing clip transitions to the incoming clip is referred to as the blend area. The blend area sets the duration of the transition. The blend area shows the transition between the outgoing clip and incoming clip Although the Clips view represents a blend area as a single linear curve, the transition between clips is actually comprised of two blend curves. The blend curve for the outgoing clip is referred to as the Blend Out curve. The blend curve for the incoming clip is referred to as the Blend In curve. By default, each blend curve is automatically set to an ease-in and ease-out curve. Use Blend Curves to customize the blend area Use the Blend Curves in the Inspector window to change the shape for either the Blend In or Blend Out curve of the selected clip. However, the Inspector window only allows you to edit the properties of one clip at a time. You cannot simultaneously customize both blend curves from the same blend area. To customize the Blend Curves for the transition between two clips: Select the outgoing clip to customize its Blend Out curve (labelled Out). Select the incoming clip to customize its Blend In curve (labelled In). To customize either the Blend Out curve or Blend In curve, use the drop-down menu to switch from Auto to Manual. With Manual selected, the Inspector window shows a preview of the blend curve. Click the curve preview to open the Curve Editor below the Inspector window. Select Manual and click the curve preview to open the Curve Editor Use the Curve Editor to customize the shape of the blend curve. By default, the blend curve includes a key at the beginning of the curve and a key at the end of the curve. The Curve Editor provides the following different methods of modifying the blend curve: Select the key at the start or end of the blend curve and use the tangent handles to adjust the interpolation between keys. Add additional keys to change the shape of the blend curve by adding more interpolation points. Adding keys in the Curve Editor is the same as adding keys in the Curves view. Right-click a key to delete or edit the key. Editing keys in the Curve Editor is the same as editing keys in the Curves view. Note that you cannot delete the first and last keys. Select a shape template from the bottom of the Curve Editor. The Curve Editor also includes shape templates based on whether you are modifying the Blend In curve or the Blend Out curve. Select a shape template to change the blend curve to the selected shape template."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_dup.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_dup.html",
    "title": "Duplicating clips | FSM Unity Framework",
    "keywords": "Duplicating clips There are many ways to duplicate clips in the Clips view: Select a clip or multiple clips. Right-click in the Clips view and select Duplicate from the context menu. Select a clip or multiple clips. Hold Command/Control and press D. Right-click an unselected clip and choose Duplicate from the context menu. Duplicating clips copies each selected clip and places the duplicates after the last clip on the same track. If you duplicate clips used in a blend or clips separated by a gap, the blend or gap is also duplicated. If you duplicate an Animation clip that uses a recorded clip as its Source Asset, the recorded clip is also duplicated. The duplicate of the recorded clip only appears in your Project after you save the Scene or Project. For example, the following images demonstrates what happens if you duplicate an Animation clip named \"Clip 2B\" that uses the recorded clip named \"Recorded (3)\". Select the\"Clip 2B\", hold Command/Control and press D to duplicate A duplicate Animation clip is placed at the end of the same track. The recorded clip associated with \"Clip 2B\" is also duplicated. The new \"Recorded (6)\" recorded clip appears in the Project window after you save the Scene or Project"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_ease.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_ease.html",
    "title": "Easing-in and easing-out clips | FSM Unity Framework",
    "keywords": "Easing-in and easing-out clips Ease-in and ease-out a clip to create a smooth transition between a clip and its surrounding gaps. To create an ease-in or ease-out transition, select a clip and, in the Inspector window, set either the Ease In Duration or the Ease Out Duration. Use Ease In Duration and Ease Out Duration to smoothly transition into and out of the selected clip. Ease-in and ease-out transitions create different effects, depending on the track: On an Animation track or an Animation Override track, ease-in to an Animation clip to create a smooth transition between the animation in the gap before the clip and the Animation clip. Ease-out of an Animation clip to create a smooth transition between the Animation clip and the animation in the gap after the clip. For information on the factors that determine what animation occurs in the gap before and after an Animation clip, see Setting gap extrapolation. On an Audio track, ease-in to an Audio clip to fade in the volume of the audio waveform. Ease-out of an Audio clip to fade out the volume of the audio waveform specified by the Audio clip. On a Playable track, ease-In to a Playable clip to fade in the effect or script in the Playable clip. Ease-out of a Playable clip to fade out the effect or script in the Playable clip. Ease-in and ease-out an Animation clip to transition between its animation and its gaps. Timeline represents ease-in and ease-out transitions as a linear curve. Although the Clips view represents an ease-in or ease-out transition as a single linear curve, every ease-in or ease-out transition is actually set to a gradually easing-in or easing-out curve by default. To change the shape of either the ease-in curve (labelled In) or the ease-out (labelled Out) curve, use the Blend Curves in the Inspector window. Use the Blend Curves to customize ease-in or ease-out transitions Note that the Blend Curves might affect the blend area used for blending between two clips. The Ease In Duration and Ease Out Duration properties indicate whether the Blend Curves affect an ease-in or ease-out transition, or a blend. For example, If the Ease Out Duration is editable, then the Blend Out curve (labelled Out) affects the curve used by an ease-out transition. If the Ease Out Duration is not editable, then the Blend Out curve (labelled Out) affects the outgoing clip in a blend between two clips. Ease Out Duration is not editable, therefore the Out curve affects the blend area between two clips To customize either the ease-in or ease-out transition, use the drop-down menu to switch from Auto to Manual. With Manual selected, the Inspector window shows a preview of the blend curve. Click the curve preview to open the Curve Editor below the Inspector window. Select Manual and click the preview to open the Curve Editor The Curve Editor is the same editor that is used to customize the shape of the blend curves when blending between clips. When creating an ease-in or an ease-out transition with Animation clips, the Animation clip blends between its gaps and the Animation clip. The following factors affect the values of animated properties in the gaps surrounding an Animation clip: The pre-extrapolate and post-extrapolate settings for the Animation clip and for other Animation clips on the same track. Animation clips on other Animation tracks that are bound to the same GameObject. The position or animation of the GameObject in the Scene, outside the Timeline Asset. Gap extrapolation and easing clips To successfully ease-in or ease-out an Animation clip, gap extrapolation must not be set based on the Animation clip being eased-in or eased-out. Gap extrapolation must either be set to None or set by another Animation clip. For example, the following ease-in transition has no effect because the Pre-Extrapolate for the Victory_Dance clip is set to Hold. This means that the ease-in creates a transition between the first frame of the Animation clip and the rest of the Animation clip. The gap is set to Hold from the Animation clip (circled). The ease-in transition has no effect. To ease-in from the Idle clip, set pre-extrapolate for the Victory_Dance clip to None. The ease-in gap uses the post-extrapolate mode from the Idle clip (circled). Overriding Animation tracks with ease-in and ease-out transitions Use two Animation tracks bound to the same GameObject to create a smooth transition between two Animation clips. For example, if two Animation tracks are bound to the same GameObject and a clip on the second track contains an ease-in transition, the ease-in transition creates a smooth transition between the animation on the previous track and the animation on the second track. Example of using two Animation tracks, bound to the same GameObject, to create smooth transitions between Animation clips. In this example, the Animation clip on the first track is a repeated idle cycle where the humanoid GameObject stands still. The Animation clip in the second track eases-in the Victory_Dance motion and eases-out to return back to the idle cycle To successfully override animation on a previous track, the gap extrapolation for the second track must be set to None so that the animation data in the gap is taken from the previous track bound to the same GameObject. The ease-in and ease-out transitions use this animation data."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_gap_extrap.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_gap_extrap.html",
    "title": "Setting gap extrapolation | FSM Unity Framework",
    "keywords": "Setting gap extrapolation Gap extrapolation refers to how an Animation track approximates animation data in the gaps before and after an Animation clip. The main purpose for extrapolating animation data in the gaps between Animation clips is to avoid animation anomalies. Depending on the GameObject bound to the Animation track, these anomalies could be a GameObject jumping between two transformations, or a humanoid jumping between different poses. Each Animation clip has two gap extrapolation properties: Pre-Extrapolate, which controls how animation data is approximated in the gap before an Animation clip, and Post-Extrapolate, which controls how animation data extends in the gap after an Animation clip. By default, Timeline sets both extrapolation properties to Hold. This sets the gap before the Animation clip to the animation on the first frame, and the gap after the Animation clip to the animation on the last frame. Each gap \"holds\" the animation at a certain frame. Icons before and after an Animation clip indicate the selected extrapolation modes. Icons indicate the pre-extrapolate and post-extrapolate modes When an Animation track contains a gap between two Animation clips, the Post-Extrapolate property of the left clip sets the gap extrapolation. If the Post-Extrapolate property of the clip to the left of a gap is set to None, the Pre-Extrapolate property of the right clip sets the gap extrapolation. Icons before and after Animation clips indicate whether the extrapolation for a gap is taken from the Post-Extrapolate property of the clip to the left or from the Pre-Extrapolate property of the clip to the right. First track (red box): gap extrapolation from Post-Extrapolate of the left clip. Third track (blue box): gap extrapolation from Pre-Extrapolate of the right clip. To change the Pre-Extrapolate and Post-Extrapolate properties, select the Animation clip and use the Animation Extrapolation properties in the Inspector window. Use Pre-Extrapolate and Post-Extrapolate to set the extrapolation modes for the selected Animation clip The Pre-Extrapolate property is hidden when one of the following is true: The gap before the Animation clip is set by the Post-Extrapolation mode of the previous clip. There is no gap before the Animation clip. Use the Pre-Extrapolation property to set the gap extrapolation of the gap before the selected Animation clip to one of the following options: None: Turns off pre-extrapolation. In the gap before the selected Animation clip, the GameObject uses its transform, pose, or state from the Scene. Select None if, for example, you want to create an ease-in between the motion of a GameObject in the Scene and an Animation clip. See Easing-in and Easing-out Clips for details. Hold (default): In the gap before the selected Animation clip, the GameObject bound to the Animation track uses the values assigned at the start of the Animation clip. Loop: In the gap before the selected Animation clip, the GameObject bound to the Animation track repeats the entire animation as a forward loop: from start to end. To offset the start of the loop, use the Clip In property. Ping Pong: In the gap before the selected Animation clip, the GameObject bound to the Animation track repeats the entire animation forwards, then backwards. Use the Clip In property to offset the start of the loop. Changing the Clip In property affects the start of the loop when looping forward, and the end of the loop when looping backwards. Continue: In the gap before the selected Animation clip, the GameObject bound to the Animation track either holds or loops the animation based on the settings of the Source Asset. For example, if the selected Animation clip uses the motion file \"Recorded(2)\" as its Source Asset and \"Recorded(2)\" is set to Loop, then selecting Continue loops the animation according to the \"Recorded(2)\" Loop Time settings. Use the Post-Extrapolate property to set the gap extrapolation of the gap after the selected Animation clip to one of the following options: None: Turns off post-extrapolation. In the gap after the selected Animation clip, the GameObject uses its transform, pose, or state from the Scene. Selecting None is useful if, for example, you want to create an ease-out between an Animation clip and the motion of a GameObject in the Scene. See Easing-in and Easing-out Clips for details. Hold (default): In the gap after the selected Animation clip, the GameObject bound to the Animation track uses the values assigned at the end of the Animation clip. Loop: In the gap after the selected Animation clip, the GameObject bound to the Animation track repeats the entire animation as a forward loop: from start to end. To offset the start of the loop, use the Clip In property. Ping Pong: In the gap after the selected Animation clip, the GameObject bound to the Animation track repeats the entire animation forwards, then backwards. Use the Clip In property to offset the start of the loop. Changing the Clip In property affects the start of the loop when looping forward, and the end of the loop when looping backwards. Continue: In the gap after the selected Animation clip, the GameObject bound to the Animation track either holds or loops the animation based on the settings of the Source Asset. For example, if the selected Animation clip uses the motion file \"Recorded(2)\" as its Source Asset and \"Recorded(2)\" is set to Loop, then selecting Continue loops the animation according to the \"Recorded(2)\" Loop Time settings."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_insert.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_insert.html",
    "title": "Inserting clips | FSM Unity Framework",
    "keywords": "Inserting clips The Timeline window supports different methods of inserting clips depending on the type of track, where you click, and whether a clip or track is already selected. In the Timeline window, inserting clips refers to adding and making space for a clip without blending or replacing intersecting clips. To accurately insert a clip, select Ripple mode as the Clip Edit mode, and position the Timeline Playhead to set the insertion point. Select Add From Animation Clip from the Track menu for the track where you want to insert the clip. Accurately insert a clip with the Ripple mode (red circle), the Timeline Playhead (green box), and the Add From Animation Clip in the Track menu In the above example, the Timeline Playhead is the insertion point. You can specify the insertion point using these other methods: Right-click within a gap and add a clip with the context menu. The insertion point is where you right-click. Drag a Source Asset (animation or audio) to a track in the Clips view. The insertion point is where you stop dragging. The location of the insertion point determines where the clip is inserted and how it affects the other clips and gaps on the same track: If the insertion point intersects a clip, the inserted clip is added at the insertion point. The intersected clip, and all subsequent clips and gaps, are rippled after the inserted clip. If the insertion point is within a gap and there is enough space between the insertion point and the next clip, then the inserted clip is added to the gap. The other clips on the track are not affected. If the insertion point is within a gap and the inserted clip overlaps the next clip, the inserted clips is added at the insertion point. The next clip, and all subsequent clips and gaps, are rippled to accommodate the inserted clip. For example, inserting a clip at the Timeline Playhead ripples Clip 1B to accommodate the 36 frame Run clip."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_match.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_match.html",
    "title": "Matching clip offsets | FSM Unity Framework",
    "keywords": "Matching clip offsets Every Animation clip contains key animation, or motion, that animates the GameObject, or humanoid, bound to the Animation track. When you add an Animation clip to an Animation track, its key animation or motion does not automatically begin where the previous clip ends. The key animation or motion also does not end where the next clip begins. By default, each Animation clip begins at the position and rotation of the GameObject, or humanoid, at the beginning of the Timeline instance. An animation sequence of three Animation clips. For example, three Animation clips create an animation sequence that starts with a clip of a standing humanoid that starts to run, then turns left, and finally comes to a stand still. Each Animation clip begins at the position and rotation of the humanoid at the start of the Timeline instance, indicated by a red arrow in the Scene view below. The three Animation clips, Stand2Run, RunLeft, and Run2Stand, end at the green, blue, and yellow arrows, respectively. For an animation sequence to flow seamlessly between adjacent Animation clips, you must match each Animation clip with its previous clip or next clip. Matching clips adds a position and rotation offset for each Animation clip. The position and rotation offsets are named Clip Transform Offsets and they can be set manually or automatically. The following sections describe how to automatically match two or many Animation clips. Matching two clips To match the clip offsets between two clips, right-click the Animation clip that you want to match. From the context menu, select either Match Offsets to Previous Clip or Match Offsets to Next Clip. Matching an Animation clip with the next clip For example, right-click the middle Animation clip, named \"RunLeft\", and select Match Offsets To Next Clip to match its offsets to the next clip When you are matching offsets for a single Animation clip, you don’t need to select the Animation clip first, but you must right-click the Animation clip that you want to match. For example, if you right-click an Animation clip that is not selected, Timeline matches the clicked clip and ignores the selected Animation clips. The context menu only displays the match options available for the clicked Animation clip. For example, if there is a gap before the clicked Animation clip, only the Match Offsets to Next Clip menu item is available. Matching many clips To match the clip offsets of many clips, select the adjacent Animation clips that you want to match and right-click one of the selected clips. From the context menu, select either Match Offsets to Previous Clip or Match Offsets to Next Clip. Matching many clips with previous clips For example, select the \"RunLeft\" and \"Run2Stand\" clips. Right-click one of the selected clips, and select Match Offsets to Previous Clips, to match the \"RunLeft\" clip with the previous \"Stand2Run\" clip, and to match \"Run2Stand\" with the previous \"RunLeft\" clip."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_pan_zoom.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_pan_zoom.html",
    "title": "Panning and zooming the Clips view | FSM Unity Framework",
    "keywords": "Panning and zooming the Clips view Use either the keyboard or the zoombar to pan and zoom the contents of the Clips view. There are many ways to pan, zoom, or frame clips in the Clips view with the keyboard: To pan, either middle-drag, or hold Alt and drag. To frame all selected clips, select clips then press F. To frame all clips, press A. To zoom horizontally, move the scroll-wheel. To zoom vertically, hold Command/Control and move the scroll-wheel. When you horizontally zoom the Clips view, the zoombar indicates the level of zoom. The zoombar is the horizontal bar at the bottom of the Clips view that zooms and pans the section of the Timeline instance or Timeline Asset that is shown in the Clips view. The zoombar (inside the red box) and the zoombar handles (shown by the green arrows). The zoombar thumb is the area between the two zoombar handles. There are many ways to pan and zoom with the zoombar: To pan, drag the zoombar thumb left or right. To jump to a section of the Timeline instance or Timeline Asset, click on an empty area of the scrollbar, on either side of the zoombar. To zoom in or zoom out, drag either zoombar handle. Dragging a zoombar handle also resizes the zoombar thumb. On the zoombar thumb, a white line indicates the location of the Timeline Playhead. Use this line to see where the Timeline Playhead is in relation to the zoom level and the part of the Timeline instance shown in the Clips view."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_position.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_position.html",
    "title": "Positioning clips | FSM Unity Framework",
    "keywords": "Positioning clips To position a clip, select Mix mode as the Clip Edit mode. Select a clip and hover over the middle of the clip. When the cursor changes to a position cursor, click and drag the clip to its new position. While dragging, black lines indicate the selection of clips being positioned. The Timeline ruler shows the start time and end time of the selected clips being positioned. Select Mix mode (circled). Select and drag to position a clip. By default, when you drag to position clips, both Snap to Frame and Edge Snap are enabled in the Clips view. You can change these snap settings in the Timeline Settings menu. You can also move a clip to another track of the same type. Drag the clip off of its current track and a white ghost indicates where the clip will be moved. If you drag a clip to an area where the clip cannot be placed, the ghost changes to red indicating that you cannot release the clip in that area. For example, you cannot drag a clip where there is no track. The ghost of the selection being moved is drawn in red if you attempt to move a clip to an invalid area You can position a selection of clips on the same track, or on different tracks. You are not limited to positioning one clip at a time. The same edge snapping rules and invalid area restrictions apply when positioning a selection of clips on many tracks. Positioning clips with the Inspector window You can use the Inspector window to position clips. To position a clip with the Inspector window, select a clip and use the Clip Timing properties in the Inspector window to change its Start property. Clip Timing properties for an Animation clip The effect that changing the Start value has on adjacent clips depends on the selected Clip Edit mode. Positioning clips in different Clip Edit modes You are not restricted to positioning clips with Mix mode as the selected Clip Edit mode. You can also position clips in Ripple mode and in Replace mode. The difference is the effect each Clip Edit mode has on adjacent clips on the tracks where clips are being moved: Positioning clips in Mix mode creates blends between intersecting clips. Positioning clips in Ripple mode ripples subsequent clips, respecting the gaps between clips. Positioning clips in Replace mode cuts or replaces intersecting clips. Positioning clips with the Timeline Playhead You can position clips by inserting frames at the position of the Timeline Playhead. To do this, move the Timeline Playhead to where you want to insert frames. To insert frames starting at frame 40, move the Timeline Playhead to frame 40 Right-click the Timeline Playhead on the Timeline ruler above the Clips view, choose Insert > Frame, and a number of frames. To insert 25 frames, right-click the Timeline Playhead and select Insert > Frame, then 25 Frames This inserts frames in the Timeline Asset at the position of the Timeline Playhead. Inserting frames only repositions the clips that start after the position of the Timeline Playhead. Only the clips that start after the Timeline Playhead are moved. In this example, inserting 25 frames at frame 40 affects Clip 1B, Clip 2B, and Clip 2C."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_reset.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_reset.html",
    "title": "Resetting clips | FSM Unity Framework",
    "keywords": "Resetting clips You can reset the duration and speed of a clip. Resetting a clip does not reset the following properties: Start Ease In Duration and Ease Out Duration Animation Extrapolation settings Blend Curves To reset a clip, right-click the clip and select Editing from the context menu. Then, select Reset Duration, Reset Speed, or Reset All. Depending on the reset option you select, resetting a clip does the following: Option: Description: Reset Duration Resets the Duration and the Clip In. Reset Speed Resets the Speed Multiplier. Reset All Resets the Duration, Clip In, and Speed Multiplier. If resetting a clip results in two clips overlapping each other, Timeline creates a blend for the overlap, regardless of the selected Clip Edit mode."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_select.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_select.html",
    "title": "Selecting clips | FSM Unity Framework",
    "keywords": "Selecting clips Click to select a single clip. The Clips view displays the selected clip with a white border, including its blends. Selecting a clip deselects all other tracks or clips. Selecting a clip also shows its properties in the Inspector window. The clip properties change depending on the type of clip and whether multiple clips are selected. See Setting Clip properties for details. Hold Shift and click to select contiguous clips vertically on different tracks or horizontally on the same track. For example, to select three contiguous clips on the same track, select the first clip, then hold Shift and click the third clip. All three clips are selected. Click to select the first clip Shift-click the third clip to select contiguous clips on the same track Hold Command/Control and click to select discontiguous clips. Hold Command/Control and click a selected clip to deselect it. Click and drag on an empty area in the Clips view to draw a selection rectangle. This selects all clips inside the rectangle, including the clips that intersect the rectangle. Hold down Shift and draw a selection rectangle to add clips to the current selection. You can also press the Tab key to select clips. The behaviour of the Tab key changes depending on the current selection: If a track is selected, press Tab to select the first clip on the selected track. If many tracks are selected, press Tab to select the first clip on the first selected track. If a clip is selected, press Tab to select its track. If there are no clips or tracks selected, press Tab to select the first clip on the first track. Use the arrow keys to change the selected clips. The behaviour and results depend on the current selection and which modifier keys you press: If nothing is selected in the Timeline window, press the Tab, Up arrow, or Down arrow key to select the first clip on the first track. If a clip is selected, press the Left arrow key to select the previous clip. If the selected clip is the first clip on a track, the Left arrow key selects the track. If a clip is selected, press the Right arrow key to select the next clip. Press the Up arrow key to select the closest clip on a previous track. Press the Down arrow key to select the closest clip on a next track. Hold Shift and press either the Left arrow key or Right arrow key to add or remove clips from the selection of clips. Whether a clip is added to or removed from the selection of clips is relative to the first selected clip. If you zoom into the Clips view, it pans to show either the start or end of the most recently selected clip. For example, if a selected clip is framed in the Clips view and you press the Right arrow key to select the next clip which is outside the Clips view, the Clips view pans to show the start of the selected clip. You can also select clips with the Timeline Playhead. Right-click the Timeline Playhead and choose a selection option. This selects clips that either start after, start before, end after, end before, or intersect the Timeline Playhead. Clips are selected on all tracks. Right-click the Timeline Playhead and choose Select for more clip selection options"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_speed.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_speed.html",
    "title": "Changing clip play speed | FSM Unity Framework",
    "keywords": "Changing clip play speed Change the clip play speed to accelerate or decelerate its audio, motion, animation, or particle effect. Changing the clip play speed affects the duration of the clip. You can only change the play speed for Animation clips, Audio clips, and Control clips. To change the clip play speed, first, select the Clip Edit mode to determine how other clips on the same track are affected: If the change in duration results in two clips that overlap each other: Select Mix mode to create a blend. Select Replace mode to cut or remove intersecting clips. Select Ripple mode to reposition the clips that come after the clip being sped up or slowed down. Ripple mode preserves the gaps between clips. Select the clip and set the Speed Multiplier property in the Inspector window. The Speed Multiplier property shows the play speed as a multiplier of the original clip speed, so 1 plays the clip at the same speed as the original clip. Speed Multiplier in the Inspector window For example, to double the play speed of an Animation clip, change the Speed Multiplier to 2. This changes the duration of an 80 frame Animation clip to 40 frames by doubling its play speed. There are other ways to change the play speed of a clip: Right-click the clip and select Editing > Double Speed to halve the clip duration. The clip plays at twice its current speed. A short-dashed line and a multiplication factor indicates an accelerated clip. Doubling the clip speed sets the Speed Multiplier property to double its current value. Right-click the clip and select Editing > Half Speed to double the clip duration. The clip plays at half its current speed. A long-dashed line and multiplication factor indicates a decelerated clip. Halving the clip speed sets the Speed Multiplier property to half its current value. Right-click the clip and select Editing > Reset Speed to reset the clip to its original speed. This is the original duration of the clip. Resetting the clip speed sets the Speed Multiplier property to 1. A short-dashed line and multiplication factor of 2.00x indicates a clip playing at double its original speed"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_split.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_split.html",
    "title": "Splitting clips | FSM Unity Framework",
    "keywords": "Splitting clips You can split a clip into two identical clips that have different start points, end points, and durations. You can extend the start or end of the clip to include split animation or audio. You can also reset a clip to undo a split and other edits. To split a clip, select the clip, position the playhead where you want to split the clip, and either right-click the clip and select Editing > Split, or press S. Any selected clips that intersect the playhead are split into separate clips. You can position, trim, and edit split clips independently. Select the clips to be split, position the playhead where you want the split to occur, and press S Selected clips are split where each clip intersects the playhead If a split clip is part of a blend, or if the split is performed within a blend, Timeline copies the blend settings to the split clips."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_tile.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_tile.html",
    "title": "Tiling clips | FSM Unity Framework",
    "keywords": "Tiling clips Tile clips to remove gaps and blends between clips on the same track. Tiling clips is useful if you want each clip to begin exactly where the previous clip ends. If you select multiple clips on multiple tracks, you must select at least two clips on the same track for tiling to have an affect. To tile clips, select at least two clips on the same track. Three clips with gaps and blends are selected Right-click on one of the selected clips and select Tile from the context menu. Timeline positions the selected clips based on the position of the first selected clip. The first selected clip does not move, and the duration of each clip remains the same. Tiling removes gaps and blends between the selected clips"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_trim.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/clp_trim.html",
    "title": "Trimming clips | FSM Unity Framework",
    "keywords": "Trimming clips Trimming a clip cuts off a portion of the clip at its start or end. To trim a clip, select the Mix mode as the Clip Edit mode, then drag the start or end of the clip. Dragging the start or end of a clip automatically selects the clip, showing its properties in the Inspector window. Use the Clip Timing properties in the Inspector window to set the start, end, duration, and offset (Clip In) of a clip to exact values. Position and trim a clip by adjusting its Start, End, Duration, and Clip In properties in the Inspector window Trimming the start of a clip Trimming an Animation clip or Audio clip after the start of the Source Asset, selects the part of the Source Asset the clip uses. Trimming the start of an Animation clip trims its key animation, relative to the start of the Source Asset Trimming a clip is non-destructive. Trim the clip again to modify its start to include the animation, or the audio waveform, cut off during a previous trim. You can also reset a clip to undo trims or other edits. To trim the start of a clip to a precise time or frame, use the Clip In property in the Inspector window. Changing the Clip In property is similar to the same effect as trimming the start of a clip after the start of its Source Asset. Trimming the end of a clip As with the start of the clip, trimming an Animation clip or Audio clip before the end of the Source Asset, selects the part of the Source Asset the clip uses. Trimming the end of an Animation clip trims its key animation, relative to the end of the Source Asset If you trim the end of an Animation clip or Audio clip past the end of the Source Asset the clip is based on, the extra clip area either holds or loops, depending on the settings of the Source Asset. For example, an Animation clip named \"End Move\" uses the motion file \"Recorded(2)\" as its Source Asset. The motion file \"Recorded(2)\" is set to loop. Trimming the end of the Animation clip past the end of the \"Recorded(2)\" Source Asset fills the extra clip area by looping \"Recorded(2)\". A white animation curve shows the hold or loop. A white animation curve indicates whether the extra clip area holds or loops data, depending on the Source Asset To choose whether the extra clip area holds or loops, select the Source Asset to change its settings in the Inspector window. Depending on the type of Source Asset, different properties control whether the Source Asset holds or loops. If you are unsure which Source Asset is used by a clip, select the clip in the Clips view, right-click and select Find Source Asset from the context menu. This highlights the Source Asset in the Project window. Trimming the end of looping clips The Timeline window provides special trimming options for Animation clips or Audio clips with loops. These special trim options either remove the last partial loop or complete the last partial loop. For example, the Animation clip named run_away is over three times longer than the Source Asset on which it is based. Since the Source Asset is set to loop, the Animation clip loops the Source Asset until the Animation clip ends which results in a partial loop. L1, L2, and L3 signify complete loops. The clip ends partially through the fourth loop, L4. To extend the end of the clip and complete a partial loop, select the clip, right-click and select Editing > Complete Last Loop. To trim the clip at the last complete loop, select the clip, Right-clip and select Editing > Trim Last Loop. The result of select Editing > Complete Last Loop The result of select Editing > Trim Last Loop Trimming with the Timeline Playhead You can also trim a clip based on the location of the playhead. To trim using the playhead, position the playhead within the clip to be trimmed. Right-click the clip and select either Editing > Trim Start or Editing > Trim End. Trim Start trims the start of the clip to the playhead. Trim End trims the end of the clip to the playhead. _Move the Timeline Playhead within the _ Right-click and select Editing > Trim Start to trim the start of the clip to the playhead If you select clips on multiple tracks, Timeline only trims the selected clips that intersect the playhead."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_about.html",
    "title": "Curves view | FSM Unity Framework",
    "keywords": "Curves view The Curves view shows the animation curves for Infinite clips, or for Animation clips that were converted from Infinite clips. Use the Curves view for basic animation editing such as adding keys, modifying keys, adjusting tangents, and changing the interpolation between keys. To view animation curves for an Infinite clip, click the Curves icon next to the Track name. To view animation curves for an Animation clip, select the Animation clip and click the Curves icon. The Curves view is similar to Curves mode in the Animation window. The Curves icon (circled) shows and hides the Curves view for the selected clip The Curves icon does not appear for Animation tracks with humanoid animation or imported animation. To view and edit key animation for humanoid or imported Animation clips, right-click an Animation clip and select Edit in Animation Window from the context menu. You can also double-click the Animation clip. The Animation window appears, linked to the Timeline window. When in linked mode, the Animation window shows a Linked icon and the name of the Animation clip being edited. Click the Linked icon to stop editing the Animation clip and to release the Animation window from linked mode. Animation window linked to the Timeline window, indicated by the Linked icon and Animation clip name"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_hide.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_hide.html",
    "title": "Hiding and showing curves | FSM Unity Framework",
    "keywords": "Hiding and showing curves For the selected Animation clip, the Curves view includes a hierarchical list of the properties with animation curves. Expand, collapse, select, and deselect the properties in this list to filter which animation curves show in the Curves view. For example, to show only the X-axis animation curves for the position of a GameObject, expand Position, select the Position.x property, and then press F to frame the animation curve for the Position.x property. Curves view showing the animation curve for the Position.x property There are many ways to expand, collapse, select, and deselect animation curves: Click the Triangle icon of a parent property to expand and collapse its list of child properties. Hold Shift and click to select contiguous properties. Hold Command/Control and click to select discontiguous properties. Hold Command/Control and click a selected property to deselect it."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_add.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_add.html",
    "title": "Adding keys | FSM Unity Framework",
    "keywords": "Adding keys The Curves view provides the following methods for adding keys: Right-click on an animation curve and select Add Key. This method adds a key at the location of the right-click. Double-click on an animation curve. This method adds a key at the location of the Double-click."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_del.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_del.html",
    "title": "Deleting keys | FSM Unity Framework",
    "keywords": "Deleting keys The Curves view provides the following methods for deleting keys: Right-click a key and select Delete Key from the context menu. This method does not affect selected keys. Select a key and either press Delete or right-click and select Delete Key from the context menu."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_edit.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_edit.html",
    "title": "Editing keys | FSM Unity Framework",
    "keywords": "Editing keys Edit a key to change its time, value, or both. The Curves view provides the following different methods for editing a key: Right-click a key and select Edit from the context menu to enter specific values for time and value. Select a key and press Enter to enter specific values. Select and drag a key to change its time and value. Drag a key vertically, then press Shift to snap the key on the vertical axis. This changes the value of the key, but not its time. Drag a key horizontally, then press Shift to snap the key on the horizontal axis. This changes the time of the key, but not its value."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_interp.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_interp.html",
    "title": "Changing interpolation and shape | FSM Unity Framework",
    "keywords": "Changing interpolation and shape Every key has one or two tangents that control the interpolation of the animation curve. The term interpolation refers to the estimation of values that determine the shape of the animation curve between two keys. Whether a key has one of two tangents depends on the location of the key on the animation curve. The first key only has a right tangent that controls the interpolation of the animation curve after the key. The last key only has a left tangent that controls the interpolation of the animation curve before the last key. The first key (red) only has a right tangent, and the last key (blue) only has a left tangent All other keys have two tangents where the left tangent controls the interpolation before the key, and the right tangent controls the interpolation after the key. By default, tangents are joined. Dragging one tangent affects the position of both tangents, and the interpolation of the animation curve both before and after the key. Keys that are neither the first key nor last key have joined tangents by default. Dragging either tangent changes the interpolation of the animation curve both before and after the key. Dragging a tangent may also change the interpolation mode of the animation curve. For example, most keys are set to the Clamped Auto interpolation mode which automatically smooths animation curve as it passes through the key. If you drag a tangent of a key set to Clamped Auto, the interpolation mode changes to Free Smooth. The term interpolation mode refers to the interpolation algorithm that determines which shape to use when drawing the animation curve. To view the interpolation mode for a key, select the key and right-click. The context menu shows the interpolation mode. To change the interpolation mode for a key, select the key, right-click and select another interpolation mode. The context menu shows the interpolation mode for the selected key. Use the context menu to change the interpolation mode. Some interpolation modes break the left and right tangents so that you can position them separately. When tangents are broken, you can set a separate interpolation mode for the animation curve before the key and the animation curve after the key. For more details on the different interpolation modes, see Editing Curves. In the Animation window documentation, the interpolation mode is referred to as tangent type."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_sel.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_keys_sel.html",
    "title": "Selecting keys | FSM Unity Framework",
    "keywords": "Selecting keys Click to select a single key. Selecting a key deselects all other selected keys. The Curves view displays the selected key with its tangents. Click to select a single key. A selected key shows its tangents. To select contiguous keys along the same animation curve, click the first key, then hold Shift and click the last key. Hold Shift and click a key to select contiguous keys There are many ways to select and deselect keys in the Curves view: Hold Command/Control and click to select discontiguous keys. Hold Command/Control and click a selected key to deselect it. Click and drag on an empty spot in the Curves view to draw a selection rectangle. This selects all keys within the rectangle. Hold down Shift while drawing the selection rectangle to add keys to the current selection. Double-click a selected key to select all keys on the same animation curve."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_nav.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/crv_nav.html",
    "title": "Navigating the Curves view | FSM Unity Framework",
    "keywords": "Navigating the Curves view Use one of the following methods to pan, zoom, resize, or frame the animation curves and keys in the Curves view: To pan, middle-drag, or hold Alt and drag. To zoom vertically, move the scroll-wheel, or hold Alt and right-drag. To zoom horizontally, hold Command/Control and zoom vertically. To resize the Curves view, drag the double line separating the Curves view from the next track in the Track list. To frame only selected animation curves or selected keys, press F. To frame all animation curves or keys, press A. You can also use the Zoombar to pan, zoom, and resize the Clips view."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_hide.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_hide.html",
    "title": "Collapsing and expanding Track groups | FSM Unity Framework",
    "keywords": "Collapsing and expanding Track groups To collapse the tracks in a Track group, either click the Triangle icon beside the name of the Track group or double-click the Track group. The tracks are collapsed from view in the Timeline window, not muted. To expand the tracks in a Track group, click the Triangle icon or double-click the Track group again. Triangle icon (circled) collapses the tracks in the Game Board Track group. A ghost track visually represents the tracks in the collapsed group. You can also press the Left Arrow key to collapse the tracks in a Track group while the Track group is selected. Press the Right Arrow key to expand the tracks in a Track group. If you press the Right Arrow key with a Track group already selected, the selection switches to the first track in the Track group."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_lock.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_lock.html",
    "title": "Locking Track groups | FSM Unity Framework",
    "keywords": "Locking Track groups You can also lock a Track group to prevent editing its Track sub-groups, tracks, and clips. This is useful when you have finished animating the content within a Track group and you want to avoid inadvertently modifying its tracks or clips. You cannot edit the tracks or select the clips in a locked Track group. The Lock icon identifies a locked Track group. Selected and locked Track group with Lock icon (red circle) To lock a Track group, right-click on the Track group header and select Lock from the context menu. You can also select a Track group and press L. You can select and lock multiple Track groups. To unlock a Track group, click the Lock icon. You can also select a locked Track group and press L, or right-click and select Unlock from the context menu. Tracks in a Track group maintain their individual locked state when you lock a Track group. This means that if you lock a track and then lock its Track group, when you unlock the Track group, the track remains locked. For example, the MovingPieces Track group has its first track locked and its second track unlocked. If you lock the Track group, both the first and second track are locked. If you unlock the Track group, the first track remains locked and the second track is unlocked because the first track was already locked before the Track group was locked."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_use.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/grp_use.html",
    "title": "Using Track groups | FSM Unity Framework",
    "keywords": "Using Track groups Use Track groups to organize tracks when you are working with many tracks. For example, a Timeline Asset contains an Animation track and an Audio track that interacts with the same GameObject. To organize these tracks, move them into their own Track group. To add a Track group, click the Add button and select Track Group from the Add menu. You can also right-click an empty area of the Track list and select Track Group from the context menu. A new Track group appears at the bottom of the Track list. Timeline window with Track group added To rename a Track group, click its name and an I-beam cursor appears. Type the new name for the Track group and press Return. To move tracks into a Track group, select one or more tracks and drag over the Track group. The Track group is highlighted. When dragging a selection of tracks, the last selected track type displays beside the cursor. To drop the tracks before a specific track in the Track group, drag until a white insert line indicates the destination. Release the mouse button when the white insert line appears within the Track group Selected tracks are moved to the location of the insert line A Track group can also have any number of Track sub-groups. To add a Track sub-group, either select a Track group and click the Add button in the Track list, or click the Plus icon beside the Track group name, and select Track Sub-Group. You can also use this menu to add tracks directly to a Track group or a Track sub-group. Click the Plus icon to add Track Sub-Groups and tracks to Track groups"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/index.html",
    "title": "About Timeline | FSM Unity Framework",
    "keywords": "About Timeline Unity's Timeline Use Unity's Timeline to create cinematic content, game-play sequences, audio sequences, and complex particle effects. Each cut-scene, cinematic, or game-play sequence that you create with Unity's Timeline consists of a Timeline Asset and a Timeline instance. The Timeline window creates and modifies Timeline Assets and Timeline instances simultaneously. The Timeline Overview section includes details on the relationship between the Timeline window, Timeline Assets, and Timeline instances. The Using Timeline section shows how to create Timeline Assets and Timeline instances, record basic animation, animate humanoids, and use other Timeline features. The Samples section includes a description of the samples offered by the Timeline package. Installing Timeline Timeline is a Package and is installed through the Packages window in Unity. Consult the Packages window documentation for more information. Technical details Requirements This version of Timeline is compatible with the following versions of the Unity Editor: 2019.1 and later (recommended) Package contents The following table indicates the folder structure of the Timeline package: Location Description <Runtime> Root folder containing the source for the Timeline Runtime. This is the source for what is available in the Player. <Editor> Root folder containing the source for the Timeline Editor used to edit Timeline files inside the Unity Editor. Document revision history Date Reason October 23, 2020 Added documentation for customization samples. October 22, 2020 Added samples section October 10, 2018 Document created. Matches package version 0.0.0"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_about.html",
    "title": "Timeline properties in the Inspector window | FSM Unity Framework",
    "keywords": "Timeline properties in the Inspector window The Inspector window displays information about the selected GameObject including all attached components and their properties. This section documents the properties in the Inspector window that appear when you select one or many Timeline Assets, tracks, clips, or a combination. If you select a single Timeline Asset, track, or clip, the Inspector window displays the properties for the selected Asset, track, or clip. For example, if you select an Animation clip, the Inspector window shows the common properties and Playable Asset properties for the selected Animation clip. Inspector window when selecting an Animation clip in the Timeline window If you select multiple Timeline Assets, tracks, or clips, the Inspector window shows two sections: a section with properties that apply to the entire selection, and a section of common properties that apply to each selected object individually. For example, if you select an Audio clip on one track and two Animation clips on another track, the Inspector window includes Multiple Clip Timing properties and Clip Timing properties: Use the Multiple Clip Timing properties to change the Start or End of the selection as a group. For example, if you change the Start to frame 30, the selection of clips start at frame 30. This moves the start of the first clip to frame 30 and the remaining selected clips are placed relative to the first clip, respecting gaps between selected clips. Use the Clip Timing properties to change the common properties for each selected clip. If the selected clips have different values for the same property, the value is represented with a dash (\"-\"). If you change the dash to a value, it sets the value for all selected clips. For example, if you change the Ease In Duration from a dash to 10 frames, the ease in of each selected clip changes to 10 frames. Inspector window when selecting multiple clips, on multiple tracks, in the Timeline window If your selection does not have common properties, the Inspector window prompts you to narrow the selection. For example, if you select an Animation track and an Audio clip in the Timeline window, you are prompted to narrow the selection: The message in the Inspector window when the selection does not have common properties"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp.html",
    "title": "Setting clip properties | FSM Unity Framework",
    "keywords": "Setting clip properties Use the Inspector window to change the name of a clip and other properties, such as its timing and blend properties. The available properties depend on the type of clip selected. For example, select an Activation clip to change its name and set its Clip Timing. Inspector window when selecting an Activation clip in the Timeline window Not all clips have properties. See the following sections for clips with properties: Activation clip properties Animation clip common properties Animation clip Playable Asset properties Audio clip properties Control clip common properties Control clip Playable Asset properties"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_act.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_act.html",
    "title": "Activation clip properties | FSM Unity Framework",
    "keywords": "Activation clip properties Use the Inspector window to change the name of an Activation clip and its Clip Timing. Inspector window when selecting an Activation clip in the Timeline window Display Name The name of the Activation clip shown in the Timeline window. By default, each Activation clip is named \"Active\". Clip Timing properties Use the Clip Timing properties to change the position and duration of the Activation clip. Most timing properties are expressed in both seconds (s) and frames (f). When specifying seconds to modify a Clip Timing property, all decimal values are accepted. When specifying frames, only integer values are accepted. For example, if you attempt to enter 12.5 in a frames (f) field, it is set to 12 frames. Depending on the selected Clip Edit mode, changing the Start, End, or Duration may ripple or replace Activation clips on the same track. Property Description Start The frame or time (in seconds) when the clip starts. Changing the Start also affects the End. Changing the Start sets the End to the new Start value plus the Duration. End The frame or time (in seconds) when the clip ends. Changing the End also affects the Start. Changing the End sets the Start to the new End value minus the Duration. Duration The duration of the clip in frames or seconds. Changing the Duration also affects the End. Changing the Duration sets the End to the Start value plus the new Duration."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_anim_com.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_anim_com.html",
    "title": "Animation clip common properties | FSM Unity Framework",
    "keywords": "Animation clip common properties Use the Inspector window to change the common properties of an Animation clip. The common properties of an Animation clip include its name, timing, play speed, blend properties, and extrapolation settings. Inspector window when selecting an Animation clip in the Timeline window Display Name The name of the Animation clip shown in the Timeline window. Clip Timing properties Use the Clip Timing properties to position, change the duration, change the ease-in and ease-out duration, choose the extrapolation mode, and adjust the play speed of the Animation clip. Most timing properties are expressed in both seconds (s) and frames (f). When specifying seconds, a Clip Timing property accepts decimal values. When specifying frames, a property only accepts integer values. For example, if you attempt to enter 12.5 in a frames (f) field, the Inspector window sets the value to 12 frames. Depending on the selected Clip Edit mode, changing the Start, End, or Duration may blend, ripple, or replace Animation clips on the same track. Property Description Start The frame or time (in seconds) when the clip starts. Changing the Start changes the position of the clip on its track in the Timeline Asset. Changing the Start also affects the End. Changing the Start sets the End to the new Start value plus the Duration. End The frame or time (in seconds) when the clip ends. Changing the End also affects the Start. Changing the End sets the Start to the new End value minus the Duration. Duration The duration of the clip in frames or seconds. Changing the Duration also affects the End. Changing the Duration sets the End to the Start value plus the new Duration. Ease In Duration Sets the number of seconds or frames that it takes for the clip to ease in. If the beginning of the clip overlaps and blends with another clip, the Ease In Duration cannot be edited and instead shows the duration of the blend between clips. See Blending clips. Ease Out Duration Sets the number of seconds or frames that it takes for the clip to ease out. If the end of the clip overlaps and blends with another clip, the Ease Out Duration cannot be edited and instead shows the duration of the blend between clips. In this case, trim or position the clip to change the duration of the blend between clips. See Blending clips. Clip In Sets the offset of when the source clip should start playing. For example, to play the last 10 seconds of a 30 second Animation clip, set Clip In to 20 seconds. Speed Multiplier A multiplier on the playback speed of the clip. This value must be greater than 0. Changing this value changes the duration of the clip. Animation Extrapolation Use the Animation Extrapolation properties to set the gap extrapolation before and after an Animation clip. The term gap extrapolation refers to how an Animation track approximates or extends animation data in the gaps before, between, and after the Animation clips on a track. There are two properties for setting the gap extrapolation between Animation clips. The Pre-Extrapolate property only appears for Animation clips. Property Description Pre-Extrapolate Controls how animation data is approximated in the gap before an Animation clip. The Pre-Extrapolate property affects the easing-in of an Animation clip. Post-Extrapolate Controls how animation data extends in the gap after an Animation clip. The Post-Extrapolate property affects the easing-out of an Animation clip. Blend Curves Use the Blend Curves to customize the transition between the outgoing and incoming Animation clips. See Blending clips for details on how to blend clips and customize blend curves. When easing-in or easing-out clips, use the Blend Curves to customize the curve that eases-in or eases-out an Animation clip. See Easing-in and Easing-out clips for details."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_anim_plyb.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_anim_plyb.html",
    "title": "Animation clip Playable Asset properties | FSM Unity Framework",
    "keywords": "Animation clip Playable Asset properties Use the Inspector window to change the Playable Asset properties of an Animation clip. These properties include controls for manually applying position and rotation clip offsets, and options for overriding default clip matching. To view the Playable Asset properties for an Animation clip, select an Animation clip in the Timeline window and expand Animation Playable Asset in the Inspector window. Inspector window showing the Animation Playable Asset properties for the selected Animation clip Animation Clip Use the Animation Clip to change the source asset used by the clip on the Animation track. The source asset is either a recorded Infinite clip or an external motion clip. Clip Transform Offsets Use the Clip Transform Offsets area to manually apply position and rotation offsets to the selected Animation clip. The tools and properties underneath the Clip Transform Offsets provide two methods of manually applying offsets based on the selected source: Property: Description: Move tool Shows a Move Gizmo in the Scene view. Use the Move Gizmo to manually position the clip offset for the selected Animation clip. Using the Move Gizmo changes the Position coordinates. Rotate tool Shows a Rotate Gizmo in the Scene view. Use the Rotate Gizmo to manually rotate the clip offset for the selected Animation clip. Using the Rotate Gizmo changes the Rotation coordinates. Position Manually sets the clip offset in X, Y, and Z coordinates. By default, the Position coordinates are set to zero and are relative to the track offsets. Rotation Manually sets the clip rotation offset around the X, Y, and Z axes. By default, the Rotation axes are set to zero and are relative to the track offsets. You can also automatically match the clip offsets based on the end of the previous Animation clip, or the start of the next Animation clip. The transforms that are matched depends on the Offset Match Fields. Offsets Match Fields Use Offsets Match Fields to choose which transforms to match when matching clip offsets. By default, Use Defaults is enabled and uses the default matching options set for the Animation track. Disable Use Defaults to override the track matching options and choose which transformations to match when performing a Match Offsets to Previous Clip or Match Offsets to Next Clip for the selected Animation clip. When you disable Offsets Match Fields, a series of additional checkboxes appear. Use these additional checkboxes to enable or disable matching per coordinate, for both position and rotation. Remove Start Offset Enable Remove Start Offset to make the Animation clip begin at position zero and rotation zero. The rest of the position and rotation keys in the Animation clip follow from zero. Enabling Remove Start Offset makes it easier to match the Animation clip with the previous Animation clip. Disable Remove Start Offset to keep the starting position and rotation. The Animation clip starts from its original position and rotation. Foot IK Enable Foot IK if the Animation clip is animating a humanoid and you want to use inverse kinematics for foot solving. Inverse kinematics attempts to remedy foot sliding by solving and influencing foot placement from the foot to the hip of the humanoid. Disable Foot IK if the Animation clip is animating a non-humanoid object such as a moving platform or a quadruped character with a non-human bone structure."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_aud.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_aud.html",
    "title": "Audio clip properties | FSM Unity Framework",
    "keywords": "Audio clip properties Use the Inspector window to change the properties of an Audio clip. These properties include the name, timing, play speed, blend properties, audio media, and loop option. Inspector window when selecting an Audio clip in the Timeline window Display Name The name of the Audio clip shown in the Timeline window. This is not the name of the audio file that Unity uses for the waveform. For information on audio file properties, see Audio Playable Asset below. Clip Timing properties Use the Clip Timing properties to position, change the duration, change the ease-in and ease-out duration, and adjust the play speed of the Audio clip. Most timing properties are expressed in both seconds (s) and frames (f). When specifying seconds, a Clip Timing property accepts decimal values. When specifying frames, a property only accepts integer values. For example, if you attempt to enter 12.5 in a frames (f) field, the Inspector window sets the value to 12 frames. Depending on the selected Clip Edit mode, changing the Start, End, or Duration may blend, ripple, or replace Audio clips on the same track. Property Description Start The frame or time (in seconds) when the clip starts. Changing the Start property changes the position of the clip on its track in the Timeline Asset. Changing the Start also affects the End. Changing the Start sets the End to the new Start value plus the Duration. End The frame or time (in seconds) when the clip ends. Changing the End also affects the Start. Changing the End sets the Start to the new End value minus the Duration. Duration The duration of the clip in frames or seconds. Changing the Duration also affects the End. Changing the Duration sets the End to the Start value plus the new Duration. Blend Curves Use the Blend Curves to customize the fade-in and fade-out between the outgoing and incoming Audio clips. See Blending clips for details on how to blend clips and customize blend curves. When easing-in or easing-out Audio clips, use the Blend Curves to customize the curve that fades-in or fades-out an Audio clip. See Easing-in and Easing-out clips for details. Audio Playable Asset Use the Audio Playable Asset properties to select the Audio file used by the Audio clip and to set whether the selected Audio clip loops (Loop enabled) or plays once (Loop disabled)."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_ctrl_com.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_ctrl_com.html",
    "title": "Control clip common properties | FSM Unity Framework",
    "keywords": "Control clip common properties Use the Inspector window to change the common properties of a Control clip. You can only create a Control clip in a Control track. A Control clip is a special clip that controls a nested Timeline instance, Particle System, Prefab instance, or ITimeControl Script, depending on how you create the Control clip: If you create the Control clip from a GameObject with a Playable Director component associated with a Timeline Asset, then the Control clip controls a nested Timeline instance. If the GameObject parents other GameObjects associated with many Timeline Assets, then the Control clip controls multiple Timeline instances. If you create the Control clip from a GameObject with a Particle System component, then the Control clip controls a Particle System. If you create the Control clip from a GameObject linked to a Prefab, then the Control clip controls a Prefab instance. If you create the Control clip from a GameObject with a script that implements the ITimeControl interface, then the Control clip controls an ITimeControl Script. The common properties of a Control clip include its name and Clip Timing properties. Not all common properties apply to all types of Control clips. Inspector window when selecting a Control clip in the Timeline window Display Name The name of the Control clip shown in the Timeline window. Clip Timing properties Use the Clip Timing properties to position and change the duration of the Control clip. Most timing properties are expressed in both seconds (s) and frames (f). When specifying seconds, a Clip Timing property accepts decimal values. When specifying frames, a property only accepts integer values. For example, if you attempt to enter 12.5 in a frames (f) field, the Inspector window sets the value to 12 frames. Depending on the selected Clip Edit mode, changing the Start, End or Duration of a Control clip may create an insert or replace clips on the same track. You cannot create a blend between Control clips. Property: Description: Start The frame or time (in seconds) when the Control clip starts. Changing the Start changes the position of the Control clip on its track in the Timeline Asset. Changing the Start also affects the End. Changing the Start sets the End to the new Start value plus the Duration. End The frame or time (in seconds) when the Control clip ends. Changing the End also affects the Start. Changing the End sets the Start to the new End value minus the Duration. Duration The duration of the clip in frames or seconds. Changing the Duration also affects the End. Changing the Duration sets the End to the Start value plus the new Duration. Clip In Sets the offset of when the Control clip starts playing. The Clip In property only affects Particle Systems and nested Timeline instances. Speed Multiplier A speed multiplier that affects the playback speed of the Control clip. This value must be greater than 0. The Speed Multiplier property only affects Particle Systems and nested Timeline instances."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_ctrl_plyb.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_clp_ctrl_plyb.html",
    "title": "Control clip Playable Asset properties | FSM Unity Framework",
    "keywords": "Control clip Playable Asset properties Use the Inspector window to change the playable asset properties of a Control clip. To view the playable asset properties for a Control clip, select a Control clip in the Timeline window and expand Control Playable Asset in the Inspector window. Inspector window showing the Control Playable Asset properties for the selected Control clip Source Game Object Use Source Game Object to select the GameObject with the Particle System, nested Timeline instance, or ITimeControl Script for the selected Control clip. Changing the Source Game Object changes what the Control clip controls. Prefab Use Prefab to select a Prefab to instantiate when the Timeline instance plays in Play Mode. When a Prefab is selected, the label of the Source Game Object property changes to Parent Object. When in Play Mode, the Prefab is instantiated as a child of the Parent Object. Although the Prefab is instantiated at the start of the Timeline instance, the Prefab is only activated during the Control clip. When the Control clip ends, the Prefab instance is deactivated. Control Activation Enable Control Activation to activate the Source Game Object while the Control clip plays. Disable this property to activate the Source Game Object during the entire Timeline instance. The Control Activation property only affects Control clips that control a nested Timeline instance or a Particle System. Post Playback When Control Activation is enabled, use the Post Playback property to set the activation state for the nested Timeline instance when the main Timeline stops playing. The Post Playback property only affects nested Timeline instances. Post-Playback State Description Active Activates the Source Game Object after the nested Timeline instance finishes playing. Inactive Deactivates the Source Game Object after the nested Timeline instance finishes playing. Revert Reverts the Source Game Object to its activation state before the nested Timeline instance began playing. Advanced properties Use the Advanced properties to select additional functionality based on whether the Control clip controls a Playable Director, Particle System, or ITimeControl Script. The Advanced properties do not apply to all Control clips. Property Description Control Playable Directors Enable this property if the Source Game Object is attached to a Playable Director and you want the Control clip to control the nested Timeline instance associated with this Playable Director. Control Particle Systems Enable this property when the Control clip includes a Particle System. Set the value of the Random Seed property to create a unique, repeatable effect. Control ITimeControl Enable this property to control ITimeControl scripts on the Source GameObject. To use this feature, the Source Game Object must have a script that implements the ITimeControl interface. Control Children Enable this property if the Source Game Object has a child GameObject with either a Playable Director, Particle System, or ITimeControl Script, and you want the Control clip to control this child component. For example, if the Source Game Object is a GameObject that parents another GameObject with a Particle System, enable this property to make the Control clip control the Particle system on the child GameObject."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_tl.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_tl.html",
    "title": "Setting Timeline Asset properties | FSM Unity Framework",
    "keywords": "Setting Timeline Asset properties Use the Inspector window to set the frame rate, the duration mode, and a fixed length for the selected Timeline Asset. From the Project window, select a Timeline Asset to view its properties. Inspector window when selecting a Timeline Asset in the Project window Property Description Frame Rate Sets the reference frame rate for the Timeline Asset and its Timeline instances. Change the Frame Rate to align clips at precise frames but changing the Frame Rate is only visual and has no effect on play speed, keys, tracks, or clips. Timeline supports the following standard frame rates: 24 (PAL), 25 (NTSC), 30, 50, and 60. Timeline also supports custom frame rates from 1e-6 to 1000. To set a custom frame rate, enter a non-standard frame rate for the Frame Rate property. In the Timeline Settings menu, the Custom menu item is enabled and automatically selected for the Timeline instance. The Custom menu item shows the custom frame rate in parentheses. Duration Mode Choose whether the duration of the Timeline Asset extends to the end of the last clip or ends at a specific time or frame. Based On Clips Sets the length of the Timeline Asset based on the end of the last clip. Fixed Length Sets the length of the Timeline Asset to a specific number of seconds or frames. Duration Shows the length of the Timeline Asset in seconds and frames when the Duration Mode is set to Based on Clips. Sets the length of the Timeline Asset to a specific number of seconds or frames when the Duration Mode is set to Fixed Length."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk.html",
    "title": "Setting track properties | FSM Unity Framework",
    "keywords": "Setting track properties Use the Inspector window to change the name of a track and its properties. The available properties depend on the type of track selected. For example, select an Animation Track to set how track offsets are applied, to apply an avatar mask, and to select which transforms are modified when matching offsets between Animation clips. Inspector window when selecting an Animation track in the Timeline window Not all tracks have properties. See the following sections for tracks with properties: Activation Track properties Animation Track properties"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk_act.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk_act.html",
    "title": "Activation track properties | FSM Unity Framework",
    "keywords": "Activation track properties Use the Inspector window to change the name of an Activation track and set the state of its bound GameObject when the Timeline Asset finishes playing. Inspector window when selecting an Activation track in the Timeline window Property Description Display Name The name of the Activation track shown in the Timeline window and Playable Director component. The Display Name applies to the Timeline Asset and all of its Timeline instances. Post-playback state Sets the activation state for the bound GameObject when the Timeline Asset stops playing. The Post-playback state applies to the Timeline Asset and all of its Timeline instances. Active Activates the bound GameObject when the Timeline Asset finishes playing. Inactive Deactivates the bound GameObject when the Timeline Asset finishes playing. Revert Reverts the bound GameObject to its activation state before the Timeline Asset began playing. For example, if the Timeline Asset finishes playing with the GameObject set to inactive, and the GameObject was active before the Timeline Asset began playing, then the GameObject reverts to active. Leave As Is Sets the activation state of the bound GameObject to the state the Timeline Asset is at when it finishes playing. For example, if the Timeline Asset finishes playing with the GameObject set to inactive, the GameObject remains inactive."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk_anim.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/insp_trk_anim.html",
    "title": "Animation track properties | FSM Unity Framework",
    "keywords": "Animation track properties Use the Inspector window to change the name of an Animation track, set how track offsets are applied, apply an avatar mask, and set which transforms are modified by default when you match clip offsets. Inspector window when selecting an Animation track in the Timeline window Property Description Display Name The name of the Animation track shown in the Timeline window and in the Playable Director component. The Display Name applies to the Timeline Asset and all of its Timeline instances. Track Offsets Applies a position and rotation offset to the start of each Animation clip on the selected Animation track. The position and rotation offset starts from a specific position and rotation or from the position and rotation relative to a state machine or another Timeline instance. Apply Transform Offsets Starts the animation in each Animation clip from a specific position and rotation offset. Use the Move and Rotate tools, and the Position and Rotation fields, to set the starting position and rotation. Apply Scene Offsets Starts the animated GameObject from its current position and rotation in the Scene. Use this mode to build a Timeline instance that transitions to and from a state machine or to and from another Timeline instance. Auto (deprecated) If you load a Scene or Project that was built before 2018.3, Track Offsets is automatically set to Auto (deprecated). This is a special mode for backwards compatibility. After opening an old Project, choose another Track Offsets mode because the Auto (deprecated) offset disables key animation recording. Move tool Enable the Move tool to show the Move Gizmo in the Scene view. Use the Move Gizmo to visually position the transform offset. Positioning the Move Gizmo changes the Position properties. The Move tool only appears when Track Offsets is set to Apply Transform Offsets. Rotate tool Enable the Rotate tool to show the Rotate Gizmo in the Scene view. Use the Rotate Gizmo to visually rotate the track offset. Rotating the Rotate Gizmo changes the Rotation properties. The Rotate tool only appears when Track Offsets is set to Apply Transform Offsets. Position Sets the track position offset in X, Y, and Z coordinates. The Position fields only appears when Track Offsets is set to Apply Transform Offsets. Rotation Sets the track rotation offset in X, Y, and Z coordinates. The Rotation fields appear when Track Offsets is set to Apply Transform Offsets. Apply Avatar Mask Enables Avatar masking. When enabled, Timeline applies the animation of all Animation clips on the track based on the selected Avatar Mask. Avatar Mask Selects the Avatar Mask applied to all Animation clips on the Animation track. An Avatar Mask defines which humanoid body parts are animated by Animation clips on the selected Animation track. The body parts that are masked are animated by other Animation tracks in the Timeline Asset. For example, you can use an Avatar Mask to combine the lower-body animation on an Animation track with the upper body animation on an Override Animation track. Default Offset Match Fields Expand to display a series of checkboxes that choose which transforms are matched when matching clip offsets between Animation clips. The Default Offset Match Fields set the default matching options for all Animation clips on the same track. Use the Animation Playable Asset properties to override these defaults for each Animation clip."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/play_director.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/play_director.html",
    "title": "Playable Director component | FSM Unity Framework",
    "keywords": "Playable Director component The Playable Director component stores the link between a Timeline instance and a Timeline Asset. The Playable Director component controls when the Timeline instance plays, how the Timeline instance updates its clock, and what happens when the Timeline instance finishes playing. Playable Director component added to the GameObject named Ground. The GameObject is associated with the GroundCTL Timeline Asset. The Playable Director component also shows the list of tracks from the associated Timeline Asset (Playable property) that animate GameObjects in the Scene. The link between Timeline Asset tracks and GameObjects in the Scene is referred to as binding or Track binding. For more on binding and the relationship between Timeline Assets and Timeline instances, see Timeline overview. Property Description Playable Associates a Timeline Asset with a GameObject in the Scene. When you make this association, you create a Timeline instance for the selected Timeline Asset. After you create a Timeline instance, you can use the other properties in the Playable Director component to control the instance and choose which GameObjects in the Scene are animated by the Timeline Asset. Update Method Sets the clock source that the Timeline instance uses to update its timing. DSP Select for sample accurate audio scheduling. When selected, the Timeline instance uses the same clock source that processes audio. DSP stands for digital signal processing. Game Time Select to use the same clock source as the game clock. This clock source is affected by time scaling. Unscaled Game Time Select to use the same clock source as the game clock, but without being affected by time scaling. Manual Select to not use a clock source and to manually set the clock time through scripting. Play on Awake Whether the Timeline instance is played when game play is initiated. By default, a Timeline instance is set to begin as soon as the Scene begins playback. To disable the default behaviour, disable the Play on Awake option in the Playable Director component. Wrap Mode The behaviour when the Timeline instance ends playback. Hold Plays the Timeline instance once and holds on the last frame until playback is interrupted. Loop Plays the Timeline instance repeatedly until playback is interrupted. None Plays the Timeline instance once. Initial Time The time (in seconds) at which the Timeline instance begins playing. The Initial Time adds a delay in seconds before the Timeline instance actually begins. For example, when Play On Awake is enabled and Initial Time is set to five seconds, if you click the Play button in the Unity Toolbar, Play Mode starts and the Timeline instance begins five seconds later. Current Time Views the progression of time according to the Timeline instance in the Timeline window. The Current Time field matches the Playhead Location field. Use the Current Time field when the Timeline window is hidden. The Current Time field appears in the Playable Director Component when in Timeline Playback mode or when Unity is in Game Mode. Bindings Shows the link between GameObjects in the Scene with tracks from the associated Timeline Asset (Playable property). The Bindings area is split into two columns: The first column lists the tracks from the Timeline Asset. Each track is identified by an icon and its track type. The second column lists the GameObject linked (or bound) to each track. The Bindings area does not list Track groups, Track sub-groups, or tracks that do not animate GameObjects. The Timeline window shows the same bindings in the Track list."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_about.html",
    "title": "Samples | FSM Unity Framework",
    "keywords": "Samples Gameplay Sequence Demo This sample demonstrates how Timeline can be used to create a small in-game moment, using built-in tracks. Overview In this example, we have a Player character jogging and then colliding with another character. The Player character represents our gameplay character and is using a looping jog animation. The Timeline then takes control of the player character, collides with the other character, then returns to his original gameplay animation (jog) once the Timeline has finished. Note that the gameplay animation is driven by an Animator and will only play at runtime. The character will be in a T-Stance pose for the runtime portion when using the Timeline preview. Timeline Structure This sample uses the following tracks: Activation, Animation, Audio, Control and Marker track. The GameplaySequence timeline has been organized in the following groups: Building, Lights, Cameras, Characters, Audio and Props. Building group This group contains two Control tracks: Building Spawn which spawns the building prefab, and Building Particles that controls two particle systems in the scene (ElectricalSparks and SandSwirlsEffect). Lights group This group contains two Animation tracks; one for the Sun light, and the other for the flickering light. Both tracks where animated in Unity and the curves can be seen by enabling the curves icon or double clicking on the track to open the Animation window. Sun light is animated in Rotation (sunrise) and the Point light has a spiked Intensity curve. Cameras group This group contains two cameras: Main camera using an Animation track, and Follow camera using an Activation track. Main Camera has an animation curve for the continuous movement (which can be seen by enabling the curves icon) and two override tracks, one for each character. The clips on the override tracks have static values for a fixed camera shot. Follow camera is simply parented child of the Player character's root and activated for the follow-cam shots. Characters group The first track is an Animation track for the Playercharacter. Notice that clip pre and post extrapolations are set to None, meaning the character will not be influenced by the timeline during these gaps. At runtime, these gaps mean the Player character will be using his Animator state, jog. The second track is an Activation track for the second character, making his appear in the scene. The third and last track is an Animation track for the second character. On this track, the second character blends from one clip to another creating a cinematic sequence. Audio group This group contains four Audio tracks; Player, crickets, neon-light & character2. The Player track has a jog/breathing and bump clip. The crickets track has pan and volume animation curves (can be seen by enabling the curves icon). The neon-light sound is for the flickering Point light. The Character2 tracks contains all audio clips for this second character. Props group This group animates Table and Can. The first Activation track makes Table appear in the scene. The second Activation track makes a static version of Can appear in the scene. The third track is an Animation track that animates the table bump animation. The last track is a Control track with a sub-timeline for the can animation. The static version of the can is disabled and replaced with the animated version when the Control clip starts. Double-clicking the Control track clip will enter the Can sub-timeline. Can Sub-Timeline This sub-timeline contains an Animation track for the can rolling off the table and bouncing on the ground, an Audio track for the sounds effects and a Control track for the liquid particles splashing out of the can. Marker track In the Timeline window, under the time ruler, there is a Marker track with one Signal at frame 1200. This is the Signal marker that triggers the jog audio clip on Player once the timeline finishes the gameplay jog. Customization samples This sample includes tracks, clips, markers and actions that demonstrate how to extend and customize timeline in different ways. Annotation: provides a marker that can be used as a bookmark. Video track: provides a track capable of playing video clips. Time dilation track: provides a track that can be used to adjust Unity's global Time.timeScale. Tween track: provides a track that can be used for simple transform movements. Text track: provides a track that can be used to display different messages to the screen using a TextMeshPro Text component. Demo Included is a demo timeline that showcases all the of the above samples."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_annotation.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_annotation.html",
    "title": "Annotation marker sample | FSM Unity Framework",
    "keywords": "Annotation marker sample The Annotation sample provides a marker that can be used as a bookmark for your timeline. Here are the options available on an annotation: Field | Description --- | --- Title | The annotation's title. This will be displayed as a tooltip, when hovering the mouse on the annotation. Color | The annotation's color in the Timeline window. Show line overlay | Use this option to show a vertical line that spans the full height of the Timeline window. Custom marker workflow example This example will demonstrate how to: create a custom marker; customize a marker with MarkerEditor; use a custom USS style to draw a marker; add additional commands with Actions; 1. Create an annotation marker A marker is an item that can be added to a Timeline Asset and is used to represent a point in time. Markers also have a specialization, just like clips (Activation clip, Audio clip, Animation clip, etc). In order to add a new type of marker, all we need to do is to create a class that inherits the Marker class: public class AnnotationMarker : UnityEngine.Timeline.Marker {} This custom marker can now be added to any track or on the timeline marker area: We can add a title, description and color to the annotation: public class AnnotationMarker : Marker { public string title; public Color color; public string description; public bool showLineOverlay; } The annotation marker itself is now complete. But the customization work is not done yet. Timeline offers many customization abilities. 2. Customize the marker's appearance A marker's appearance can be customized using a USS style or with MarkerEditor. Both paths have their advantages and drawbacks. Custom USS style A marker can use a USS style to specify its appearance. For more information on how to create custom USS styles, see how to define custom USS styles. The CustomStyle attribute can be used to specify a style for a given marker: [CustomStyle(\"AnnotationStyle\")] public class AnnotationMarker : Marker { //... } AnnotationStyle is defined in a USS stylesheet and will be used when a marker is displayed on screen: USS styles are useful if the desired appearance is simple (i.e. when only using a texture icon). For more complex stuff (i.e. dynamically changing a marker's color), a MarkerEditor will be needed. Custom editor MarkerEditor can be used to augment the capabilities of a marker in the editor. It works like a custom Inspector; the CustomTimelineEditor attribute is used to tell Timeline that a MarkerEditor class should be associated to a given marker. [CustomTimelineEditor(typeof(AnnotationMarker))] public class AnnotationMarkerEditor : MarkerEditor { //... } Marker information MarkerEditor lets us provide information about the marker by overriding the GetMarkerOptions method. public override MarkerDrawOptions GetMarkerOptions(IMarker marker) { var annotation = marker as AnnotationMarker; if (annotation != null) { return new MarkerDrawOptions { tooltip = annotation.title }; } return base.GetMarkerOptions(marker); } Here the tooltip of an Annotation has been set to use the annotation's title variable. MarkerDrawOptions can also set the error text on a marker, which can be useful if a variable has been incorrectly set and needs attention. Overlay An overlay can be drawn on top of a marker by overriding the DrawOverlay method: public override void DrawOverlay(IMarker marker, MarkerUIStates uiState, MarkerOverlayRegion region) { var annotation = marker as AnnotationMarker; if (annotation != null) { //Draw overlay code... } } An overlay is drawn on top of the marker; the USS style is drawn first and DrawOverlay is called afterwards. For an Annotation, we can use DrawOverlay to change the color of the marker and to draw a line that spans the full Timeline window's height. To do this, we can use the information given in region. Along with the visible time range, MarkerOverlayRegion provides two rectangles that can be used to know where to draw: markerRegion markerRegion is the rectangle that encompasses the marker. This is useful to draw something directly on the marker itself. For Annotation, this rectangle is used to draw the color overlay. timelineRegion timelineRegion is the rectangle that encompasses the clips and markers region of the timeline window. This is useful to draw something out of the marker's region, like the Annotation's line overlay. const float k_LineOverlayWidth = 6.0f; float markerRegionCenter = markerRegion.xMin + (markerRegion.width - k_LineOverlayWidth) / 2.0f; Rect lineRect = new Rect(markerRegionCenter, timelineRegion.y, k_LineOverlayWidth, timelineRegion.height); 3. Create custom Actions Timeline Action Actions can be used to add new menu entries in Timeline's context menus. For an Annotation, we want to add a menu item available in all context menus to create an Annotation with the clipboard's contents. To do this, a TimelineAction is needed, along with the MenuEntry attribute. [MenuEntry(\"Create Annotation from clipboard contents\")] public class CreateAnnotationFromClipboardContents : TimelineAction { //... } MenuEntry lets Timeline know that this action can be added in context menus. Classes inheriting from TimelineAction need to override two methods: Execute and Validate. Validate Validate is used to specify that the action's prerequisites are fulfilled. In the case of CreateAnnotationFromClipboardContents, the action is only valid if there actually is contents in the clipboard. ActionValidity is used to describe the validity state of an action: public override ActionValidity Validate(ActionContext context) { if (!markers.All(marker => marker is AnnotationMarker)) return ActionValidity.NotApplicable; string buffer = EditorGUIUtility.systemCopyBuffer; return buffer.Length == 0 ? ActionValidity.Invalid : ActionValidity.Valid; } ActionValidity.Valid : The action can be executed. ActionValidity.Invalid : The action cannot be executed given the current context and will appear grayed out in context menus. ActionValidity.NotApplicable : The action does not apply to the current context and will not show up in menus. Execute Execute should run the code necessary to execute the action's purpose. public override bool Execute(ActionContext context) { string buffer = EditorGUIUtility.systemCopyBuffer; TrackAsset track = context.tracks.FirstOrDefault(); if (buffer.Length != 0) { // Create the new annotation and add it to the track //... return true; } return false; } The return value should specify if the execution succeeded or not. Marker Action It is also possible to write custom actions that apply only to markers, instead of all Timeline items. This is the purpose of the MarkerEditor class. It works just like TimelineAction, except that action applies to a list of markers. A shortcut can also be assigned to an action. A static method with the TimelineShortcut attribute is needed. Invoker can be used to easily execute a given action: [TimelineShortcut(\"Replace annotation description with clipboard\", KeyCode.G)] public static void InvokeAction() { Invoker.InvokeWithSelectedMarkers<ReplaceAnnotationDescriptionAction>(); } Notes Runtime considerations AnnotationMarker is available at runtime; it can be queried using, for example, TrackAsset.GetMarkers(). However, AnnotationMarkerEditor and custom actions are not available at runtime, since it depends on classes that are not part of the runtime assembly."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_text.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_text.html",
    "title": "Text Track sample | FSM Unity Framework",
    "keywords": "Text Track sample This track requires the TextMeshPro package to be installed in the project. This type of track can be used to display different messages to the screen using a TextMeshPro Text Component. It is ideal for things like subtitles. It demonstrates the following: Registering custom previewable properties in a custom track. Perform custom blending of clips using a mixer PlayableBehaviour. Provide custom clip data that can be animated using the inline curve editor using a PlayableBehaviour template. Using a ClipEditor to react to changes in a clip. Usage To use this custom track, drag a TextMeshPro Text Component into the hierarchy view of the Timeline. A TextTrack will be created, and use the track context menu to create clips. Clip properties such as Message, FontSize and Color can be modified in the inspector by selecting the clip. Clips can be overlapped to create transitions."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_time.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_time.html",
    "title": "Time Dilation Track sample | FSM Unity Framework",
    "keywords": "Time Dilation Track sample This type of track can be used to adjust Unity's global Time.timeScale for the duration of the clip. The most common use would be to create bullet-time style effects. The sample demonstrates the following: Creating a custom TrackMixer PlayableBehaviour that performs custom blending of clip values. Setting and restoring Unity global values in a PlayableBehaviour. How to support blending and extrapolation on custom clips. Provide custom clip data that can be animated using the inline curve editor using a PlayableBehaviour template. Usage Create a TimeDilationTrack in timeline using the Add Track menu, found under the Timeline.Samples submenu. Add clips to the track, and use the inspector to set scale values for the clip, or use the inline curve editor to animate the scale values. Clips can also be overlapped to create transitions between clips."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_tween.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_tween.html",
    "title": "Transform Tween track sample | FSM Unity Framework",
    "keywords": "Transform Tween track sample This track can be used for simple transform movements between two points. Usage This track can be used for simple transform movements. All translation happens in a straight line but the speed can be controlled with an animation curve. The Tween track binds to the scene Transform you wish to move. Field Description Start Location This is a reference to a Transform in the scene that marks the position and/or rotation of the moving Transform when the playable starts. If it is left null the position/rotation of the moving Transform when the playable starts will be used. End Location This is a reference to a Transform in the scene that marks the position and/or rotation of the moving Transform when the playable finishes. Tween Position Whether or not the position of the Transform should change. Tween Rotation Whether or not the rotation of the Transform should change. Custom clip workflow example This example will demonstrate how to: create a custom clip, track and mixer; use the PlayableGraph API to animate an object's transform; customize a clip with ClipEditor; 1. Custom clip when a Timeline begins playing, nodes called Playables are created. They are organized in a tree-like structure called the PlayableGraph. For each frame, Timeline samples this graph to read and mix multiple data sources (animation, audio and more). The first step to create a custom clip is to define a new PlayableBehaviour that will be added to a graph. It will need to store the data needed to implement the transform tween: public class TweenBehaviour : PlayableBehaviour { public Transform startLocation; public Transform endLocation; public bool shouldTweenPosition; public bool shouldTweenRotation; public AnimationCurve curve; } The PlayableBehaviour's data is not serialized and will be lost once its parent graph is destroyed. To save this data, the next step is to define a new PlayableAsset: [Serializable] public class TweenClip : PlayableAsset { public ExposedReference<Transform> startLocation; public ExposedReference<Transform> endLocation; public bool shouldTweenPosition = true; public bool shouldTweenRotation = true; public AnimationCurve curve; //... } Note: The clip needs to store a start and an end location. Since an asset cannot directly reference a scene object, it cannot store a transform object directly. This is why an ExposedReference<Transform> is used. A PlayableAsset's main purpose is to build a PlayableBehaviour. This is done with the CreatePlayable method: public class TweenClip : PlayableAsset { //... public override Playable CreatePlayable(PlayableGraph graph, GameObject owner) { // create a new TweenBehaviour ScriptPlayable<TweenBehaviour> playable = ScriptPlayable<TweenBehaviour>.Create(graph); TweenBehaviour tween = playable.GetBehaviour(); // set the behaviour's data tween.startLocation = startLocation.Resolve(graph.GetResolver()); tween.endLocation = endLocation.Resolve(graph.GetResolver()); tween.curve = curve; tween.shouldTweenPosition = shouldTweenPosition; tween.shouldTweenRotation = shouldTweenRotation; return playable; } } CreatePlayable will initialize a new TweenBehaviour using TweenClip's data. 2. Custom track A custom track is created by defining a TrackAsset subclass. The following attributes can be added to a TrackAsset: TrackBindingType: defines which type of object should be bound to a track; TrackClipType: defines which type of clip should be associated to a track. For this example, the track needs a Transform object binding and can only accepts clips of type TweenClip, which was previously defined in step 1: [TrackBindingType(typeof(Transform))] [TrackClipType(typeof(TweenClip))] public class TweenTrack : TrackAsset { // ... } The data setup is complete; TweenTrack and TweenClip can now be added to a timeline: However, no transform tween has been implemented yet. To do this, a track mixer is needed. 3. Define a track mixer To properly handle blending, or crossfading, between two clips, a track mixer is needed. A track mixer is a PlayableBehaviour that will have access to all clips data and will blend those together. Track mixer setup By default, when a track is added to a timeline, an empty playable is generated and is connected to each clip's playable. For example, this track: will generate the following playable graph: Timeline: this playable is the root playable; all playables related to tracks are connected to this node. Playable: this playable represents the track mixer. Since no track mixer is defined, an empty one is generated. TweenBehaviour: this playable represents a clip. One per clip is generated. All clip playables are connected to the track mixer. In order to define a custom track mixer, a new PlayableBehaviour needs to be defined: public class TweenMixerBehaviour : PlayableBehaviour {} then, in TrackAsset, the CreateTrackMixer method can be used to specify a custom track mixer: public class TweenTrack : TrackAsset { public override Playable CreateTrackMixer(PlayableGraph graph, GameObject go, int inputCount) { return ScriptPlayable<TweenMixerBehaviour>.Create(graph, inputCount); } } Now the playable graph looks like this: The empty playable that used to connect clip playables together is now replaced by TweenMixerBehaviour. Transform tween implementation The implementation of the transform tween resides in the ProcessFrame method from TweenMixerBehaviour. Here are the main steps of that implementation: Initialization: When the timeline is first played, the initial transform of the track binding is fetched. If the start or end transform is null, the initial transform will be used instead. Get clip behaviours & weights: to appropriately blend, the mixer needs to ask information for all of its inputs (clips): // Iterate on all the playable's (mixer) inputs (ie each clip on the track) int inputCount = playable.GetInputCount(); for (int i = 0; i < inputCount; i++) { // get the input connected to the mixer Playable input = playable.GetInput(i); // get the weight of the connection float inputWeight = playable.GetInputWeight(i); // get the clip's behaviour TweenBehaviour tweenInput = GetTweenBehaviour(input); } Calculate and blend: A linear interpolation is used to calculate a transform between two points. Apply result: Once the calculation is done, the transform is written in the track binding object: // Apply the final position and rotation values in the track binding trackBinding.position = accumPosition + m_InitialPosition * (1.0f - totalPositionWeight); trackBinding.rotation = accumRotation.Blend(m_InitialRotation, 1.0f - totalRotationWeight); 4. Customize a clip's appearance ClipEditor can be used to augment the capabilities of a clip in the editor. It works like a custom Inspector; the CustomTimelineEditor attribute is used to tell Timeline that a ClipEditor class should be associated to a given clip. [CustomTimelineEditor(typeof(TweenClip))] public class TweenClipEditor : ClipEditor { //... } It is possible to customize the appearance of a clip with the DrawBackground method: public override void DrawBackground(TimelineClip clip, ClipBackgroundRegion region) { TweenClip asset = clip.asset as TweenClip; if (asset == null) return; // Drawing code here... } Notes Only the portion between (0,1) of the curve will be used. When a clip ends, the object bound to the track will return to its original position."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_video.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/smpl_custom_video.html",
    "title": "VideoTrack sample | FSM Unity Framework",
    "keywords": "VideoTrack sample The Video Track sample provides a track capable of playing video clips in Timeline. It demonstrates how to do the following: Using built-in blending, speed and clip-in capabilities in custom clips. Using ClipEditors to customize clip drawing. Using a mixer PlayableBehaviour to perform look-ahead operations. Managing UnityEngine.Object lifetime (VideoPlayer) with a PlayableBehaviour. Using ExposedReferences to reference components in the scene from a PlayableAsset. Usage Drag and drop an imported video from the project window onto a timeline. The video track and clip will be created. The video clip has several playback options, including the option to specify the camera to render to video to, and an audio source to redirect the audio. If no camera is specified, the main camera in the scene will be used. If no audio source is specified, the audio will play directly (i.e. no 3D audio). Known Issues The video track supports ease-in and ease-out of a video, but blending between videos will not give expected results. Editing a timeline containing video clips may cause the clip to flicker or change unexpectedly. Looping a timeline with video clips may cause the video to be de-synchronized."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_about.html",
    "title": "Timeline overview | FSM Unity Framework",
    "keywords": "Timeline overview Use the Timeline window to create cut-scenes, cinematics, and game-play sequences by visually arranging tracks and clips linked to GameObjects in your Scene. A cinematic sequence in the Timeline window. For each cut-scene, cinematic, or game-play sequence, the Timeline window saves the following: Timeline Asset: Stores the tracks, clips, and recorded animations without links to the specific GameObjects being animated. The Timeline Asset is saved to the Project. Timeline instance: Stores links to the specific GameObjects being animated or affected by the Timeline Asset. These links, referred to as bindings, are saved to the Scene. Timeline Asset The Timeline window saves track and clip definitions as a Timeline Asset. If you record key animations while creating your cinematic, cut-scene, or game-play sequence, the Timeline window saves the recorded clips as children of the Timeline Asset. The Timeline Asset saves tracks and clips (red). Timeline saves recorded clips (blue) as children of the Timeline Asset. Timeline instance To animate a GameObject in your Scene with a Timeline Asset, you must create a Timeline instance. A Timeline instance associates a Timeline Asset with the GameObject in the Scene, through a Playable Director component. When you select a GameObject in a Scene that has a Playable Director component, the Timeline instance appears in the Timeline window. The bindings appear in the Timeline window and in the Playable Director component (Inspector window). The Playable Director component shows the Timeline Asset (blue) with its bound GameObjects (red). The Timeline window shows the same bindings (red) in the Track list. The Timeline window provides an automated method of creating a Timeline instance while creating a Timeline Asset. Reusing Timeline Assets Because Timeline Assets and Timeline instances are separate, you can reuse the same Timeline Asset with many Timeline instances. For example, you could create a Timeline Asset named VictoryTL with the animation, music, and particle effects that play when the main game character (Player) wins. To reuse the VictoryTL Timeline Asset to animate another game character (Enemy) in the same Scene, you can create another Timeline instance for the secondary game character. The Player GameObject (red) is attached to the VictoryTL Timeline Asset] The Enemy GameObject (blue) is also attached to the VictoryTL Timeline Asset] Because you are reusing the Timeline Asset, any modification to the Timeline Asset in the Timeline window results in changes to all Timeline instances. For example, in the previous example, if you delete the Audio track while modifying the Player Timeline instance, the Timeline window removes the track from the VictoryTL Timeline Asset. The Timeline window also removes the Audio track from all instances of the VictoryTL Timeline Asset, including the Enemy Timeline instance."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_gloss.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_gloss.html",
    "title": "Timeline glossary | FSM Unity Framework",
    "keywords": "Timeline glossary This topic provides an alphabetical list of the terminology used throughout the Timeline documentation. animatable property: A property belonging to a GameObject, or belonging to a component added to a GameObject, that can have different values over time. animation: The result of adding two different keys, at two different times, for the same animatable property. animation curve: The curve drawn between keys set for the same animatable property, at different frames or seconds. The position of the tangents and the selected interpolation mode for each key determines the shape of the animation curve. binding or Track binding: Refers to the link between Timeline Asset tracks and the GameObjects in the scene. When you link a GameObject to a track, the track animates the GameObject. Bindings are stored as part of the Timeline instance. blend and blend area: The area where two Animation clips, Audio clips, or Control clips overlap. The overlap creates a transition that is referred to as a blend. The duration of the overlap is referred to as the blend area. The blend area sets the duration of the transition. Blend In curve: In a blend between two Animation clips, Audio clips, or Control clips, there are two blend curves. The blend curve for the incoming clip is referred to as the Blend In curve. Blend Out curve: In a blend between two Animation clips, Audio clips, or Control clips, there are two blend curves. The blend curve for the out-going clip is referred to as the Blend Out curve. clip: A generic term that refers to any clip within the Clips view of the Timeline window. Clips view: The area in the Timeline window where you add, position, and manipulate clips. Control/Command: This term is used when instructing the user to press or hold down the Control key on Windows, or the Command key on Mac. Curves view: The area in the Timeline window that shows the animation curves for Infinite clips or for Animation clips that have been converted from Infinite clips. The Curves view is similar to Curves mode in the Animation window. Gap extrapolation: How an Animation track approximates animation data in the gaps before and after an Animation clip. field: A generic term that describes an editable box that the user clicks and types-in a value. A field is also referred to as a property. incoming clip: The second clip in a blend between two clips. The first clip, the out-going clip, transitions to the second clip, the incoming clip. Infinite clip: A special animation clip that contains basic key animation recorded directly to an Animation track within the Timeline window. An Infinite clip cannot be positioned, trimmed, or split because it does not have a defined duration: it spans the entirety of an Animation track. interpolation: The estimation of values that determine the shape of an animation curve between two keys. interpolation mode: The interpolation algorithm that draws the animation curve between two keys. The interpolation mode also joins or breaks left and right tangents. key: The value of an animatable property, set at a specific point in time. Setting at least two keys for the same property creates an animation. out-going clip: The first clip in a blend between two clips. The first clip, the out-going clip, transitions to the second clip, the incoming clip. Playhead Location field: The field that expresses the location of the Timeline Playhead in either frames or seconds, depending on the Timeline Settings. property: A generic term for the editable fields, buttons, checkboxes, or menus that comprise a component. An editable field is also referred to as a field. tangent: One of two handles that controls the shape of the animation curve before and after a key. Tangents appear when a key is selected in the Curves view, or when a key is selected in the Curve Editor. tangent mode: The selected interpolation mode used by the left tangent, right tangent, or both tangents. Timeline or Unity's Timeline: Generic terms that refer to all features, windows, editors, and components related to creating, modifying, or reusing cut-scenes, cinematics, and game-play sequences. Timeline Asset: Refers to the tracks, clips, and recorded animation that comprise a cinematic, cut-scene, game-play sequence, or other effect created with the Timeline window. A Timeline Asset does not include bindings to the GameObjects animated by the Timeline Asset. The bindings to scene GameObjects are stored in the Timeline instance. The Timeline Asset is project-based. Timeline window: The official name of the window where you create, modify, and preview a Timeline instance. Modifications to a Timeline instance also affects the Timeline Asset. Timeline instance: Refers to the link between a Timeline Asset and the GameObjects that the Timeline Asset animates in the scene. You create a Timeline instance by associating a Timeline Asset to a GameObject through a Playable Director component. The Timeline instance is scene-based. Timeline Playback Controls: The row of buttons and fields in the Timeline window that controls playback of the Timeline instance. The Timeline Playback Controls affect the location of the Timeline Playhead. Timeline Playback mode: The mode that previews the Timeline instance in the Timeline window. Timeline Playback mode is a simulation of Play mode. Timeline Playback mode does not support audio playback. Timeline Playhead: The white marker and line that indicates the exact point in time being previewed in the Timeline window. Timeline Selector: The name of the menu in the Timeline window that selects the Timeline instance to be previewed or modified. track: A generic term that refers to any track within the Track list of the Timeline window. Track groups: The term for a series of tracks organized in an expandable and collapse collection of tracks. Track list: The area in the Timeline window where you add, group, and modify tracks."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_play_cntrls.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_play_cntrls.html",
    "title": "Timeline Playback Controls | FSM Unity Framework",
    "keywords": "Timeline Playback Controls To play the Timeline instance and to control the location of the Timeline Playhead, use the Timeline Playback Controls. Timeline Playback Controls Timeline Start button To move the Timeline Playhead to the start of the Timeline instance, click the Timeline Start button, or hold Shift and press Comma (,). Previous Frame button To move the Timeline Playhead to the previous frame, click the Previous Frame button, or press Comma (,). Timeline Play button To preview the Timeline instance in Timeline Playback mode, click the Timeline Play button, or press the Spacebar. Timeline Playback mode does the following: Begins playback at the current location of the Timeline Playhead and continues to the end of the Timeline instance. If the Play Range button is enabled, playback is restricted to a specified time range. The Timeline Playhead position moves along the Timeline instance. The Playhead Location field shows the position of the Timeline Playhead in either frames, timecode or seconds, depending on the Timeline settings. To pause playback, click the Timeline Play button again, or press the Spacebar. When playback reaches the end of the Timeline instance, the Wrap Mode determines whether playback should hold, repeat, or do nothing. The Wrap Mode setting is a Playable Director component property. Timeline Playback mode provides a preview of the Timeline instance while in the Timeline window. Timeline Playback mode is only a simulation of Play Mode in the Game View. The Timeline Playback mode does not support audio playback. To preview a Timeline instance with audio, enable the Play on Awake option in the Playable Director component and preview game play in Play Mode. Next Frame button To move the Timeline Playhead to the next frame, click the Next Frame button, or press Period (.). Timeline End button To move the Timeline Playhead to the end of the Timeline instance, click the Timeline End button, or hold Shift and press Period (.). Play Range button Enable the Play Range button to restrict playback to a specific range of seconds or frames. You can only set a play range when previewing a Timeline instance within the Timeline window. Unity ignores the play range in Play Mode. The Timeline ruler highlights the play range and indicates its start and end with white markers. To modify the play range, drag either marker. Play Range (red circle) enabled with while markers and highlighted area defining range Timeline Playhead and Playhead Location field The Timeline Playhead indicates the exact point in time being previewed in the Timeline window. The Playhead Location field expresses the location of the Timeline Playhead in either frames or seconds. Playhead Location field and Timeline Playhead (red). The Timeline Playhead also appears on the Zoombar (red arrow). Use the Zoombar to navigate, scroll, and zoom the Clips view. A white line indicates the location of the Timeline Playhead in relation to the entire Timeline instance. To jump the Timeline Playhead to a specific time, click the Timeline ruler. You can also enter the time value in the Playhead Location field and press Enter. When entering a value, frames are converted to seconds or seconds are converted to frames, based on the Timeline settings. For example, if the Timeline ruler is expressed as seconds with a frame rate of 30 frames per second, entering 180 in the Playhead Location field converts 180 frames to seconds and moves the Timeline Playhead to 6:00. To set the time format that the Timeline window uses, configure the Timeline Settings. Switching between Local and Global Use the Local or Global button to change the Timeline ruler from local time to global time. Local time and global time are only relevant when editing a nested Timeline instance. To create a nested Timeline instance, drag a GameObject associated with a Timeline instance into another Timeline instance. The Timeline instance you are dragging into becomes the master Timeline instance. The Timeline instance associated with the GameObject becomes a nested Timeline instance. A nested Timeline instance appears as a Control clip on a Control track (red arrow) To edit a nested Timeline instance, double-click the Control clip that contains the nested Timeline instance. The Timeline window switches to the nested Timeline instance, indicated by the Timeline title which shows the name and GameObject of the master Timeline instance, followed by the name and GameObject of the nested Timeline instance. The Timeline title indicates that you are editing a nested Timeline instance (red outline). The Global button (red arrow) indicates that the nested Timeline instance is shown using global time. When editing a nested Timeline instance, click Global to switch the Timeline ruler to Local time. Local time is relative to the nested Timeline. This means that the Timeline ruler starts at zero. A nested Timeline instance in Local time. Click Local to view the Timeline ruler in relation to the placement of the nested Timeline in the master Timeline instance. This means that if, for example, if the Control clip is placed at frame 70 of the master Timeline then the Timeline ruler starts at 70 at the beginning of the nested Timeline instance. A nested Timeline instance in Global time."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_selector.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_selector.html",
    "title": "Timeline Preview and Timeline Selector | FSM Unity Framework",
    "keywords": "Timeline Preview and Timeline Selector Use the Timeline Selector to select the Timeline instance to view, modify, or preview in the Timeline window. The Timeline Preview button enables or disables previewing the effect that the selected Timeline instance has on your Scene. Timeline Preview button with Timeline Selector and menu. Selecting a Timeline instance automatically enables the Timeline Preview button. To select a Timeline instance, click the Timeline Selector and choose from the list of Timeline instances in the current Scene. Each menu item displays the name of the Timeline Asset and its associated GameObject in the current Scene. For example, the Timeline Asset named GroundATL that is associated with the Ground GameObject, displays as \"GroundATL (Ground).\""
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_settings.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_settings.html",
    "title": "Timeline Settings | FSM Unity Framework",
    "keywords": "Timeline Settings Use the Timeline Settings to choose the Timeline window and Timeline Asset settings such as the unit of measurement, the duration mode, audio waveform, and window snap settings. Click the Cog icon in the Timeline window to view the Timeline Settings menu Time Unit Select either Frames, Timecode or Seconds to set the Timeline window to display time in that format. Timecode will display the time in seconds with sub-second values displayed in frames. Duration Mode Use the Duration Mode to set whether the duration of the Timeline Asset extends to the end of the last clip (Based On Clips), or ends at a specific time or frame (Fixed Length). When the Duration Mode is set to Fixed Length, use one of the following methods to change the length of the Timeline Asset: Select the Timeline Asset in the Project window and use the Inspector window to set the Duration in seconds or frames. In the Timeline window, drag the blue marker on the timeline. The blue marker indicates the end of the Timeline Asset. A blue line indicates the duration of the Timeline Asset. Timeline Asset duration (red rectangle) and end marker (green circle) Frame Rate Select one of the options under Frame Rate to set the unit of measurement for the Timeline ruler. Change the Frame Rate to align clips at precise frames but changing the Frame Rate is only visual and has no effect on play speed, keys, tracks, or clips. The following standard frame rates are listed: Film (24 fps), PAL (25 fps), NTSC (29.97 fps), 30, 50, or 60. Timeline supports custom frame rates from 1e-6 to 1000. To set a custom frame rate, you must use the Frame Rate property in the Timeline Asset settings. When the Timeline Asset is set to a custom frame rate, the Custom menu item is enabled and is automatically selected for the Timeline instance. The Custom menu item shows the custom frame rate in parentheses. Show Audio Waveforms Enable Show Audio Waveforms to draw the waveforms for all audio clips on all audio tracks. For example, use an audio waveform as a guide when manually positioning an Audio clip of footsteps with the Animation clip of a humanoid walking. Disable Show Audio Waveform to hide audio waveforms. Show Audio Waveforms is enabled by default. Enable Audio Scrubbing Enable Audio Scrubbing to play audio while dragging the Timeline Playhead. Disable Enable Audio Scrubbing to stop playing audio while dragging the Timeline Playhead. When disabled, Timeline only plays audio when in Timeline Playback mode. Snap to Frame Enable Snap to Frame to manipulate clips, preview Timeline instances, drag the Timeline Playhead, and position the Timeline Playhead using frames. Disable Snap to Frame to use subframes. Snap to Frame is enabled by default. Disable Snap to Frame to position clips and drag the playhead between frames For example, when Snap to Frame is disabled and you drag the Timeline Playhead, it moves the playhead between frames. The format of Playhead Location displays differently depending on whether the Timeline window is set to Seconds, Timecode or Frames: When the Timeline window is set to Frames, the Playhead Location shows frames and subframes. For example, 8 frames and 34 subframes displays as 8.34. When the Timeline window is set to Timecode, the Playhead Location shows seconds, frames, and subframes. For example, 6 seconds, 17 frames, and 59 subframes displays as 6:17 [.59]. When the Timeline window is set to Seconds, the Playhead Location shows seconds. For example, 6.5 seconds displays as 6:50. Manipulating clips, previewing Timeline instances, and positioning the playhead at the subframes level is useful when attempting to synchronize animation and effects with audio. Many high-end audio processing software products create audio waveforms with subframe accuracy. Edge Snap Enable the Edge Snap option to snap clips when you position, trim, and create blends. When enabled, the Timeline window snaps the start or end of a clip when dragged within 10 pixels of the Timeline Playhead, the start or end of a clip on the same track, the start or end of a clip on another track, or the start or end of the entire Timeline instance. The start guide or end guide is redrawn in white to indicate that the clip has snapped to the edge of another clip or the Timeline Playhead. Disable Edge Snap to create more accurate blends, ease-ins, or ease-outs. Edge Snap is enabled by default."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_window.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/tl_window.html",
    "title": "Timeline window | FSM Unity Framework",
    "keywords": "Timeline window To access the Timeline window, select Sequencing > Timeline from the Window menu. What the Timeline window shows depends on what you select in either the Project window or the Scene view. For example, if you select a GameObject that is associated with a Timeline Asset, the Timeline window shows the tracks and clips from the Timeline Asset and the GameObject bindings from the Timeline instance. Selecting a GameObject associated with a Timeline Asset displays its tracks and clips, and the bindings from the Timeline instance If you haven’t selected a GameObject, the Timeline window informs you that the first step for creating a Timeline Asset and a Timeline instance is to select a GameObject. With no GameObject selected, the Timeline window provides instructions If a GameObject is selected and it is not associated with a Timeline Asset, the Timeline window provides the option for creating a new Timeline Asset, adding the necessary components to the selected GameObject, and creating a Timeline instance. Select a GameObject that is not associated with a Timeline Asset to create a new Timeline Asset, add components, and create a Timeline instance To use the Timeline window to view a previously created Timeline Asset, select the Timeline Asset in the Project window and open the Timeline window. The Timeline window shows the tracks and clips associated with the Timeline Asset, but without the track bindings to GameObjects in the Scene. In addition, the Timeline Playback Controls are disabled and there is no Timeline Playhead. Timeline Asset selected in the Project window shows its tracks and clips, but with no track bindings. The Timeline Playback Controls are disabled. Timeline saves the track bindings to GameObjects in the Scene with the Timeline instance, not the Timeline Asset. For details on the relationship between the Project, Scene, Timeline Assets, and Timeline instances, see Timeline overview."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_add.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_add.html",
    "title": "Adding tracks | FSM Unity Framework",
    "keywords": "Adding tracks The Timeline window supports many different methods of adding tracks to the Track list. Depending on the method you choose, the Timeline window may also add track bindings to the Track header, clips to tracks, and components to GameObjects. Add Track menu The simplest method to add a track is to click the Add button and select the type of track from the Add Track drop-down menu. You can also right-click an empty area of the Track list to make the Add Track menu appear. The Timeline window also supports dragging a GameObject into the Track list. Drag a GameObject into an empty area in the Track list and select the type of track to add from the context menu. Depending on the type of track selected, the Timeline window performs different actions: Select Animation Track and the Timeline window binds the GameObject to the Animation track. If the GameObject doesn't already have an Animator component, the Timeline window creates an Animator component for the GameObject. Select Activation Track and the Timeline window binds the GameObject to the Activation track. There are some limitations when creating an Activation track when dragging a GameObject. For example, the main GameObject with the Playable Directory component should not be bound to an Activation track. Because this is the same GameObject that links the Timeline Asset to the Scene, activating and disabling the GameObject affects the length of Timeline instance. Select Audio Track and the Timeline window adds an Audio Source component to the GameObject and binds this Audio Source component to the Audio track."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_delete.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_delete.html",
    "title": "Deleting tracks | FSM Unity Framework",
    "keywords": "Deleting tracks Delete a track to remove the track, its clips, blends, and properties from the Timeline window. This is a destructive action that modifies a Timeline Asset and affects all Timeline instances based on the Timeline Asset. There are many ways to delete tracks: Select a track and press the Delete key (or hold Command and press Delete). Select a track. Right-click an empty area in the Track list and select Delete from the context menu. Right-click a track and select Delete from the context menu. Deleting an Animation track also deletes the recorded Infinite clips for Animation clips that were converted from Infinite clips. The Project window may still show recorded Infinite clips as children of a Timeline Asset because it is not updated until you save the Scene or Project. You cannot delete a locked track."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_dup.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_dup.html",
    "title": "Duplicating tracks | FSM Unity Framework",
    "keywords": "Duplicating tracks Duplicating a track copies its clips, blends, and Inspector properties. If the duplicated track is bound to a GameObject, the binding is reset to None. Track binding for a duplicated track is reset to None There are many ways to duplicate tracks: Select a track. Right-click an empty area in the Track list and select Duplicate from the context menu. Select a track. Hold Command/Control and press D. Select a track. Hold Command/Control and press C, for copy, then press V, for paste. Right-click a track and either select Duplicate from the context menu or hold Command/Control and press D."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_list_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_list_about.html",
    "title": "Track list and Track headers | FSM Unity Framework",
    "keywords": "Track list and Track headers Use the Track list to add, select, duplicate, delete, lock, mute, and reorder the tracks that comprise a Timeline Asset. You can also organize tracks into Track groups. Track list and Track headers for the Timeline instance named GroundETL Each track has two areas: Track list: Shows a Track header for each track. Clips view: Shows the clips for each track. The Track header contains the name of the track or its binding information. Track bindings are saved to the Playable Director component associated with the GameObject that is linked to the Timeline Asset. This association is referred to as a Timeline instance (see Timeline overview). Each Track header has a colored accent that identifies the track type and its clips: Activation tracks are green. Use Activation tracks to add Activation clips which set when the bound GameObject is active (shown). The GameObject is bound to the Activation track. Animation tracks are blue. Use Animation tracks to add Animation clips that animate the bound GameObject. Use an Animation track and its Animation clips to record basic animation or animate a humanoid. Audio tracks are orange. Use Audio tracks to add Audio clips for playing background music or sound effects. Each Audio clip is bound to an audio waveform. The audio source, that plays each waveform, is bound to the Audio track. Control tracks are turquoise. Use Control tracks to add Control clips which are special clips that control a nested Timeline instance, Particle System, Prefab instance, or ITimeControl Script. How the Control clip is created determines what it controls. Playable tracks are white. Use Playable tracks to add Playable clips. Each Playable clip is bound to a script that uses the Playables API to create custom animation tools, effects or gameplay mechanisms. Each Track header is also identified by an icon. If a track has a binding error or if the bound GameObject is disabled, the icon representing a track changes to an alert icon. For example, if an Animation track is bound to a GameObject that is disabled at the location of the Playhead, the icon switches to an alert icon. An alert icon indicates that the RedCube bound GameObject is disabled at the start of the Timeline instance"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_lock.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_lock.html",
    "title": "Locking tracks | FSM Unity Framework",
    "keywords": "Locking tracks Lock a track to prevent editing of the track and any of the clips used by the track. Use lock when you have finished animating a track and you want to avoid inadvertently modifying the track. You cannot edit or delete a locked track, or select its clips. The Lock icon identifies a locked track. Selected and locked track with Lock icon (red circle) To lock a track, right-click on the track and select Lock from the context menu. You can also select a track and press L. You can select and lock multiple tracks at a time. A track can be both locked and muted. To unlock a track, click the Lock icon. You can also select a locked track and press L, or right-click and select Unlock."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_mute.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_mute.html",
    "title": "Muting tracks | FSM Unity Framework",
    "keywords": "Muting tracks Mute a track to disable its clips and their effect on the Scene. You can also use mute when your Timeline instance includes many tracks with animations and you want to focus on the animation of one or a few tracks. The Mute icon identifies a muted track. Selected and muted track with Mute icon (red circle) To mute a track, right-click on the track and select Mute from the context menu. You can also select a track and press M. You can select and mute multiple tracks at a time. A track can be both muted and locked. To unmute a track, click the Mute icon. You can also select a muted track and press M, or right-click and select Unmute. Note: Muted tracks can be deleted."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_reorder.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_reorder.html",
    "title": "Reordering tracks and rendering priority | FSM Unity Framework",
    "keywords": "Reordering tracks and rendering priority In the Timeline window, the rendering and animation priority is from the last track to the first track, where the last track takes priority. You can reorder tracks to change their rendering or animation priority. For example, a Timeline instance has four Animation tracks, where the second and fourth Animation tracks animate the same GameObject. The fourth track overrides the animation on any of the preceding tracks. This animation priority is the reason why Animation Override tracks are added as child tracks, under Animation tracks. The second track (red arrow) and fourth track (selected, green arrow) animate the same GameObject (GreenCube). The fourth track has priority and overrides the second track.) To reorder tracks, select one or more tracks and drag until a white insert line appears between tracks in the Track list. The white insert line indicates the destination of the tracks you are dragging. The last selected track type displays beside the cursor. Release the mouse button to reorder tracks. For example, the white insert line indicates that the Control track (Storm) will be placed between the first track (Ground) and second track (Audio Src) An Animation Override track is bound to the same GameObject as its parent Animation track. Reordering an Animation Override track converts it to an Animation track and resets its binding to none."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_select.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/trk_select.html",
    "title": "Selecting tracks | FSM Unity Framework",
    "keywords": "Selecting tracks To select a single track, click its Track header in the Track list. You can also click an empty area in the Clips view. When you select a track, Timeline highlights its Track header and Clips view. Selecting a track deselects all other tracks or clips. Selecting a track shows its properties in the Inspector window. The track properties change depending on the type of track and how many tracks you select. See Timeline Inspector for details. To select contiguous tracks, select the first track and then hold Shift and click the last track in the series. For example, to select three contiguous tracks, click the first track, then hold Shift and click the third track. All three tracks are selected. Click to select the first track Hold Shift and click to select contiguous tracks Hold Command/Control and click to select discontiguous tracks. Hold Command/Control and click to deselect a selected track. There are many other ways to select tracks: Hold down Shift and press the Up arrow or Down arrow keys to add and remove tracks from the selection. To deselect all tracks or clips, click on an empty area in the Track list. When a clip is selected on a track, press Tab to select the track. Use the arrow keys to change the selected track. The Up and Down arrow keys select the previous or next track. The Right arrow key selects the first clip on the track. If a Track group is already selected, the Left arrow and Right arrow keys collapse and expand the Track group."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/uss_styles.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/uss_styles.html",
    "title": "Defining custom USS styles | FSM Unity Framework",
    "keywords": "Defining custom USS styles The first step to define a USS style is to create a new stylesheet. Stylesheets can be used to extend the Editor’s visual appearance. This can be done by adding a file named common.uss in an Editor folder in a StyleSheets/Extensions folder hierarchy. For example, the following locations are valid: Assets/Editor/Stylesheets/Extensions/common.uss Assets/Editor/MyFolder/Stylesheets/Extensions/common.uss Assets/Editor/MyFolder1/MyFolder2/Stylesheets/Extensions/common.uss USS files (for Unity Style Sheet) use a CSS-like syntax to describe new styles. Here is an example: myStyle { width:18px; height:18px; background-image: resource(\"Assets/Editor/icon.png\"); } In this style, we specified that we wish to use a custom icon along with size properties. USS styles also support pseudo-states, which works like pseudo-classes in CSS. Timeline markers support the following pseudo-states: Collapsed: .myStyle Normal: .myStyle:checked Selected: .myStyle:hover:focus:checked USS stylesheets also support Unity's light and dark themes. Styles in files named dark.uss and light.uss will be used as an override of the style in common.uss. For example: common.uss myStyle { width:18px; height:18px; color: rgb(125, 125, 125); } dark.uss myStyle { color: rgb(0, 0, 0); background-image: resource(\"icon_dark.png\"); } light.uss myStyle { color: rgb(255, 255, 255); background-image: resource(\"icon_light.png\"); } In the dark theme, myStyle will be resolved to: myStyle { width:18px; height:18px; color: rgb(0, 0, 0); background-image: resource(\"icon_dark.png\"); } and in the light theme: myStyle { width:18px; height:18px; color: rgb(255, 255, 255); background-image: resource(\"icon_light.png\"); }"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_about.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_about.html",
    "title": "Using the Timeline window | FSM Unity Framework",
    "keywords": "Using the Timeline window Use the Timeline window to create Timeline Assets and Timeline instances, record animation, schedule animation, and create cinematic content. This section shows you how to do the following tasks: Create a Timeline Asset and Timeline instance Record basic animation with an Infinite clip Convert an Infinite clip to an Animation clip Animate a humanoid Use Animation Override tracks and Avatar Masking Nest Timeline Instances"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_char_anim.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_char_anim.html",
    "title": "Animating a humanoid | FSM Unity Framework",
    "keywords": "Animating a humanoid This task demonstrates how to use a Timeline instance to animate a humanoid with external motion clips. This workflow also demonstrates how to match clip offsets, manually adjust clip offsets, and create blends between clips to minimize jumping and sliding. Although this workflow uses a humanoid, you can use this animation method for any GameObject. This workflow assumes that you have already created a Timeline instance with an empty Animation track bound to a humanoid. For example, the DefaultMale humanoid is bound to an empty Animation track: DefaultMale humanoid bound to an empty Animation track. From your Project, drag a motion clip into the Animation track to create a new Animation clip. For example, drag an idle pose as the first clip to start the humanoid from an idle stance. To position, resize, or trim your Animation clip in the Clips view, select Mix mode as the Clip Edit mode. There are three different Clip Edit modes that change the editing behaviour of the Timeline window. When the Timeline window is in Mix mode, you can drag and trim clips to create blends. Animation track, bound to the DefaultMale humanoid, with an idle pose (Idle) as its Animation clip. The Mix mode (red circle) is the selected Clip Edit mode. Add a second Animation clip. This example adds a run and turn left clip (named Run_Left) to the Animation track, and then resizes the clip to include one loop, so the DefaultMale runs and turns 180 degrees. Animation track with an Idle clip and a Run_Left clip Play the Timeline instance. In this example, the DefaultMale humanoid jumps between each Animation clip because the position of the humanoid at the end of the first Animation clip (Idle) does not match the position of the humanoid at the start of the next Animation clip (RunLeft). The humanoid jumps between the first Animation clip, which ends at frame 29 (red arrow and box), and the second Animation clip, which starts at frame 30 (ghost with green arrow and box) Matching clips To fix the animation jump between clips, match the offset of each Animation clip. The Timeline window provides different methods for matching offsets. In this example, Timeline matches the second Animation clip with the previous clip. To do this, select the Run_Left clip, right-click and select Match Offsets to Previous Clip. Right-click and select Match Offsets to Previous Clip to match the offsets of the selected Animation clip with the preceding Animation clip After matching offsets, the position and rotation of the humanoid at the start of the second Animation clip (frame 30, ghost with green arrow) matches the position and rotation of the humanoid at the end of the first Animation clip (frame 29, red arrow) Play the Timeline instance again. Although the position and rotation of the humanoid matches, there is still a jump between the two Animation clips because the humanoid is in different poses. At the end of the first Animation clip, the humanoid is standing upright with its feet together. At the start of the second Animation clip, the humanoid is bent forward with its feet apart. Blending clips Create a blend to remove the jump and transition between the two poses. Adjust the size of the clips, the Blend Area, the Clip In, and the shape of each Blend Curve to create a transition between the two poses. For example, in the transition between the Idle clip and the Run_Left clip, the Idle clip was resized to 36 frames and the Run_Left clip was repositioned to start at frame 25. The rest of the clip properties are unchanged from their default values. With Mix mode selected, you can create a blend (red circle) between two clips to create a smooth transition between two animations. As the Idle clip transitions to the Run_Left clip, the blend removes the obvious jump between poses. The transition between most body parts appears natural, however in this example, the blend between the different positions of the foot results in an unnatural foot slide. Reducing foot slide To reduce foot sliding, manually adjust the offset of an Animation clip so that the position of the foot changes less drastically. To manually adjust the offset, select the Animation clip in the Timeline window. In the Inspector window, expand Animation Playable Asset. Select an Animation clip. In the Inspector window, expand Animation Playable Asset (red) to view the Clip Transform Offsets. The rotation and position Clip Transform Offsets are not zero because performing Match Offsets to Previous Clip already set these values to match the root (hips) of the humanoid at the end of the previous Animation clip. Under Clip Transform Offsets, enable the Move tool. The Move Gizmo appears in the Scene view, at the root of the Animation clip. Enable the Move tool (Inspector window, red arrow) to show the Move Gizmo (green arrow) in the Scene view Use one of the following methods to manually adjust the offset position of the Animation clip: In the Scene view, drag the Move Gizmo. In the Inspector window, under Clip Transform Offsets, change the value of the appropriate Position property."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_conv_infinite.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_conv_infinite.html",
    "title": "Converting an Infinite clip to an Animation clip | FSM Unity Framework",
    "keywords": "Converting an Infinite clip to an Animation clip An Infinite clip appears as a dope sheet. An Infinite clip cannot be positioned, trimmed, or split because it does not have a defined duration. To position, trim, split, or perform other clip manipulations on an Infinite clip, you must first convert it to an Animation clip. You cannot convert an Animation clip back to an Infinite clip. To convert an Infinite clip to an Animation clip, click the Track menu icon and select Convert to Clip Track: The Track menu (circled) converts an Infinite clip to an Animation clip. You can also right-click the track and select Convert to Clip Track from the context menu. The Track menu and context menu are the same. An infinite clip after it has been converted to an Animation clip"
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_instance.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_instance.html",
    "title": "Creating a Timeline Asset and Timeline instance | FSM Unity Framework",
    "keywords": "Creating a Timeline Asset and Timeline instance To use a Timeline Asset in your Scene, associate the Timeline Asset with a GameObject using a Playable Director component. Associating a Timeline Asset with a Playable Director component creates a Timeline instance and allows you to specify which objects in the Scene are animated by the Timeline Asset. To animate a GameObject, it must also have an Animator component. The Timeline window automatically creates a Timeline instance while creating a new Timeline Asset. The Timeline window also creates the necessary components. To create a new Timeline Asset and Timeline instance, follow these steps: In your Scene, select the GameObject that you want to use as the focus of your cinematic or other gameplay-based sequence. Open the Timeline window (menu: Window > Sequencing > Timeline). If the GameObject does not yet have a Playable Director component attached to a Timeline Asset, a message in the Timeline window prompts you to click the Create button. Click Create. A dialog box prompts you for the name and location of the Timeline Asset you are creating. You can also specify tags to identify the Timeline Asset. Click Save. The Timeline window does the following: Saves a new Timeline Asset to the Assets directory of your Project. If you did not change the name and location of the Timeline Asset you are creating, the Timeline window creates a name based on the selected GameObject with the \"Timeline\" suffix. For example, selecting the GameObject called \"Enemy\" names the Asset \"EnemyTimeline\". Adds an empty Animation track to the Timeline Asset. Adds a Playable Director component to the selected GameObject, and sets the Playable property to the Timeline Asset. This creates a Timeline instance. Sets the binding on the Animation track in the Playable Director component to the selected GameObject. The Animation track does not have any clips, so the selected GameObject is not animated. Adds an Animator component to the selected GameObject. The Animator component animates the GameObject through the Timeline instance. The GameObject cannot be animated without an Animator component."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_mask.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_mask.html",
    "title": "Using an Animation Override track and an Avatar Mask | FSM Unity Framework",
    "keywords": "Using an Animation Override track and an Avatar Mask This task demonstrates how to use an Animation Override track and an Avatar Mask to replace the upper-body animation of an Animation track. Use this technique to animate a humanoid to, for example, run and carry an object. For information on creating an Avatar mask, see Avatar Mask window. This task assumes that you have already created a Timeline instance with a simple Animation clip on an Animation track bound to a humanoid: This example uses a humanoid bound to a simple run cycle animation (RunForward) that loops once Right-click the Animation track and select Add Override Track from the context menu. An Animation Override track, named Override 0, is linked to the selected Animation track. Notice that the Animation Override track is not bound to a GameObject. Because the Override track is linked to the Animation track above, the Override track is bound to the same GameObject, in this case, the DefaultMale humanoid. To add an Override track, right-click the Animation track and select Add Override Track from the context menu] From your Project, drag an Animation Clip with upper-body animation into the Override track. For example, drag an animation of a humanoid standing still and waving their arms. Position and resize the clip to match the Animation clip that you want to override. The Animation Override track contains an Animation clip of a humanoid standing still, waving their arms (WavingArms). This clip was resized to match the Animation clip (RunForward) of the parent Animation track. Play the Timeline instance. In this example, the WavingArms clip completely overrides the RunForward clip. To combine the lower-body animation from one Animation clip with upper-body animation from another Animation clip, specify an Avatar Mask for the Animation Override track. To specify an Avatar Mask, select the Override track to view its properties in the Inspector window From the Project, drag an Avatar Mask, that masks the lower body animation, into the Avatar Mask property in the Inspector window. Enable the Apply Avatar Mask checkbox. An Avatar Mask icon appears beside the track name. An Avatar Mask, that masks the lower body animation, is specified for the Animation Overview clip in the Inspector window. This allows the upper body animation to pass through. The Avatar Mask icon (red) indicates that the Animation Override track uses an Avatar Mask. Play the Timeline instance. In this example, the DefaultMale humanoid uses upper-body animation from the WavingArms clip and lower-body animation from the RunForward clip. To temporarily disable the Avatar Mask, click the Avatar Mask icon. The Avatar Mask icon (red) is gray when disabled. The WavingArms clip completely overrides the RunForward clip."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_nested.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_nested.html",
    "title": "Nesting Timeline instances | FSM Unity Framework",
    "keywords": "Nesting Timeline instances Timeline supports nesting Timeline instances. A nested Timeline instance refers to a Timeline instance that is added within another Timeline instance. The master Timeline instance refers to the main or parent Timeline instance that includes other Timeline instances. Nesting Timeline instances is useful if you have a large team working on different aspects of a Project and you want them to collaborate on the same cinematic or cut-scene. For example, you are developing a puzzle game that starts with a cinematic of the character walking into frame while the pieces on the game board move into position. You could create a master Timeline instance for the character walking into frame. You could create another Timeline instance for the game board animation. You could then nest the game board animation into the master Timeline instance of the character walking into frame. The master NestTL Timeline instance has a character walking and theme music. A Control track nests the BoardTL Timeline instance for the game board animation. By creating a master Timeline instance with nested Timeline instances, different teams can work on different animations and then combine the animations into one master Timeline. In the puzzle game example, one team could work on the character Timeline instance, and another team could work on the game board animation. When you have two Timeline instances, and you want to nest one instance into the other, open the Timeline instance that you want to be the master Timeline instance. To ensure that the Timeline window does not switch while you select GameObjects, click the lock icon (red arrow): The master NestTL Timeline instance has a character walking and theme music. Find the GameObject in your Scene that is associated with the Timeline instance that you want to nest inside the master Timeline instance. Drag the GameObject into the Clips view of the Timeline window. The Board GameObject is associated with the BoardTL Timeline instance. Drag the Board GameObject into the NestTL master Timeline to nest the BoardTL Timeline instance in the NestTL Timeline instance. The Timeline window creates a Control track and places the Control clip where you drop the GameObject. The Control clip is set to the same size as the Timeline instance. When a Control clip contains a nested Timeline instance, a downward arrow appears beside its name. To edit a nested Timeline instance from the master timeline, double-click its Control clip. Double-click the Control clip to edit the nested BoardTL Timeline instance from within the master A warning icon appears beside the name of the nested Timeline instance because the Timeline Playhead is outside the range of the nested Timeline. Timeline also disables the Timeline Playhead controls. When you edit a nested Timeline instance, you cannot change the duration of the nested Timeline instance. You must return to the master Timeline instance and change the duration of the Control clip to change the duration of the nested Timeline instance. To return to the master, click the name of the master Timeline instance (red arrow): Warning icon (red circle) means the Timeline Playhead Controls are disabled. The Timeline window is in this state because, by default, the size of the Control clip in the master Timeline instance determines when the nested Timeline instance is active. Use one of the following methods to change this state and edit the nested Timeline instance: Click the Timeline ruler to move the Timeline Playhead into the nested Timeline. This enables editing and the Timeline Playback Controls. In the master Timeline instance, move the Timeline Playhead to within the Control clip before you double-click the Control clip. In the master Timeline instance, select the Control clip, and disable the Control Activation property in the Inspector window. Disable the Control Activation property (red outline) to have the nested Timeline instance active throughout the duration of the master Timeline instance."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_rec_anim.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/Documentation~/wf_rec_anim.html",
    "title": "Recording basic animation with an Infinite clip | FSM Unity Framework",
    "keywords": "Recording basic animation with an Infinite clip You can record animation directly to an Animation track. When you record directly to an empty Animation track, you create an Infinite clip. An Infinite clip is a clip that contains basic key animation recorded through the Timeline window. An Infinite clip cannot be positioned, trimmed, or split because it does not have a defined size: it spans the entirety of an Animation track. Before creating an Infinite clip, you must add an empty Animation track for the GameObject that you want to animate. In the Track list, click the red circular Record button for the empty Animation track to enable Record mode. Click the Record button on an empty track to enable Record mode When a track is in Record mode, the clip area of the track is drawn in red with the \"Recording...\" message, and the Record button blinks on and off. Timeline window in Record mode When in Record mode, any modification to an animatable property of the GameObject sets a key at the location of the Timeline Playhead. To start creating an animation, move the Timeline Playhead to the location of the first key, and do one of the following: In the Inspector window, right-click the name of the property and choose Add Key. This adds an animation key for the property without changing its value. A diamond appears in the Infinite clip to show the position of the key. In the Inspector window, change the value of the animatable property of the GameObject. This adds an animation key for the property with its changed value. A diamond appears in the Infinite clip. In the Scene view, either move, rotate, or scale the GameObject. This automatically adds a key for the properties you change. A diamond appears in the Infinite clip. Red background indicates that you’ve added an animation curve for the property to the clip Setting a key adds a diamond to the Infinite clip Move the playhead to a different position on the Timeline and change the animatable properties of the GameObject. At each position, the Timeline window adds a diamond to the Infinite clip for any changed properties and adds a key to its associated animation curves. While in Record mode, you can right-click the name of an animatable property name to perform keying operations such as setting a key without changing its value, jumping to the next or previous keys, and removing keys. For example, to set a key for the position of a GameObject without changing its value, right-click Position and select Add Key from the context menu. Right-click the name of an animatable property to perform keying operations When you finish the animation, click the blinking Record button to disable Record mode. An Infinite clip appears as a dope sheet in the Timeline window, but you cannot edit the keys in this view. Use the Curves view to edit keys. You can also double-click the Infinite clip and edit the keys with the Animation window. An Infinite clip appears as a dope sheet Save the Scene or Project to save the Timeline Asset and the Infinite clip. The Timeline window saves the key animation from the Infinite clip as a source asset. The source asset is named \"Recorded\" and saved as a child of the Timeline Asset in the Project. Recorded clips are saved under the Timeline Asset in the Project For every additional recorded Infinite clip, the Timeline window numbers each clip sequentially, starting at \"(1)\". For example, a Timeline Asset with three recorded Infinite clips are named \"Recorded\", \"Recorded (1)\", and \"Recorded (2)\". If you delete a Timeline Asset, its recorded clips are also removed."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Timeline copyright © 2022 Unity Technologies Licensed under the Unity Companion License for Unity-dependent projects--see Unity Companion License. Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.timeline@1.7.6/README.html": {
    "href": "Library/PackageCache/com.unity.timeline@1.7.6/README.html",
    "title": "About Timeline | FSM Unity Framework",
    "keywords": "About Timeline Use Unity’s Timeline to create cinematic content, game-play sequences, audio sequences, and complex particle effects. Installing Timeline To install this package, follow the instructions in the Package Manager documentation. Using Timeline The Timeline Manual can be found here Technical details Requirements This version of Timeline is compatible with the following versions of the Unity Editor: 2019.3 and later (recommended)"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/CHANGELOG.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/CHANGELOG.html",
    "title": "Changelog | FSM Unity Framework",
    "keywords": "Changelog [1.0.0] - 2019-01-08 This is the first release of Unity UI as a built in package."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/EventSystem.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/EventSystem.html",
    "title": "Event System | FSM Unity Framework",
    "keywords": "Event System The Event System is a way of sending events to objects in the application based on input, be it keyboard, mouse, touch, or custom input. The Event System consists of a few components that work together to send events. When you add an Event System component to a GameObject you will notice that it does not have much functionality exposed, this is because the Event System itself is designed as a manager and facilitator of communication between Event System modules. The primary roles of the Event System are as follows: Manage which GameObject is considered selected Manage which Input Module is in use Manage Raycasting (if required) Updating all Input Modules as required Input Modules An Input Module is where the main logic of how you want the Event System to behave lives, they are used for: Handling Input Managing event state Sending events to scene objects. Only one Input Module can be active in the Event System at a time, and they must be components on the same GameObject as the Event System component. If you want to write a custom Input Module, send events supported by existing UI components in Unity. To extend and write your own events, see the Messaging System documentation. Raycasters Raycasters are used for figuring out what the pointer is over. It is common for Input Modules to use the Raycasters configured in the Scene to calculate what the pointing device is over. There are 3 provided Raycasters that exist by default: Graphic Raycaster - Used for UI elements Physics 2D Raycaster - Used for 2D physics elements Physics Raycaster - Used for 3D physics elements If you have a 2d / 3d Raycaster configured in your Scene, it is easy to make non-UI elements receive messages from the Input Module. Simply attach a script that implements one of the event interfaces. For examples of this, see the IPointerEnterHandler and IPointerClickHandler Scripting Reference pages."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/EventSystemReference.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/EventSystemReference.html",
    "title": "Event System Reference | FSM Unity Framework",
    "keywords": "Event System Reference This section provides details about the following parts of the event system: Event System Manager Graphic Raycaster Physics Raycaster Physics2D Raycaster Standalone Input Module Touch Input Module Event Trigger"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UICreateFromScripting.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UICreateFromScripting.html",
    "title": "Creating UI elements from scripting | FSM Unity Framework",
    "keywords": "Creating UI elements from scripting If you are creating a dynamic UI where UI elements appear, disappear, or change based on user actions or other actions in the game, you may need to make a script that instantiates new UI elements based on custom logic. Creating a prefab of the UI element In order to be able to easily instantiate UI elements dynamically, the first step is to create a prefab for the type of UI element that you want to be able to instantiate. Set up the UI element the way you want it to look in the Scene, and then drag the element into the Project View to make it into a prefab. For example, a prefab for a button could be a Game Object with a Image component and a Button component, and a child Game Object with a Text component. Your setup might be different depending on your needs. You might wonder why we don't have a API methods to create the various types of controls, including visuals and everything. The reason is that there are an infinite number of way e.g. a button could be setup. Does it use an image, text, or both? Maybe even multiple images? What is the text font, color, font size, and alignment? What sprite or sprites should the image use? By letting you make a prefab and instantiate that, you can set it up exactly the way you want. And if you later want to change the look and feel of your UI you can just change the prefab and then it will be reflected in your UI, including the dynamically created UI. Instantiating the UI element Prefabs of UI elements are instantiated as normal using the Instantiate method. When setting the parent of the instantiated UI element, it's recommended to do it using the Transform.SetParent method with the worldPositionStays parameter set to false. Positioning the UI element A UI Element is normally positioned using its Rect Transform. If the UI Element is a child of a Layout Group it will be automatically positioned and the positioning step can be skipped. When positioning a Rect Transform it's useful to first determine it has or should have any stretching behavior or not. Stretching behavior happens when the anchorMin and anchorMax properties are not identical. For a non-stretching Rect Transform, the position is set most easily by setting the anchoredPosition and the sizeDelta properties. The anchoredPosition specifies the position of the pivot relative to the anchors. The sizeDelta is just the same as the size when there's no stretching. For a stretching Rect Transform, it can be simpler to set the position using the offsetMin and offsetMax properties. The offsetMin property specifies the corner of the lower left corner of the rect relative to the lower left anchor. The offsetMax property specifies the corner of the upper right corner of the rect relative to the upper right anchor. Customizing the UI Element If you are instantiating multiple UI elements dynamically, it's unlikely that you'll want them all to look the same and do the same. Whether it's buttons in a menu, items in an inventory, or something else, you'll likely want the individual items to have different text or images and to do different things when interacted with. This is done by getting the various components and changing their properties. See the scripting reference for the Image and Text components, and for how to work with UnityEvents from scripting."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIFitContentSize.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIFitContentSize.html",
    "title": "Making UI elements fit the size of their content | FSM Unity Framework",
    "keywords": "Making UI elements fit the size of their content Normally when positioning a UI element with its Rect Transform, its position and size is specified manually (optionally including behavior to stretch with the parent Rect Transform). However, sometimes you may want the rectangle to be automatically sized to fit the content of the UI element. This can be done by adding a component called Content Size Fitter. Fit to size of Text In order to make a Rect Transform with a Text component on it fit the text content, add a Content Size Fitter component to the same Game Object which has the Text component. Then set both the Horizontal Fit and Vertical Fit dropdowns to the Preferred setting. How does it work? What happens here is that the Text component functions as a Layout Element that can provide information about how big its minimum and preferred size is. In a manual layout this information is not used. A Content Size Fitter is a type of Layout Controller, which listens to layout information provided by Layout Elements and control the size of the Rect Transform according to this. Remember the pivot When UI elements are automatically resized to fit their content, you should pay extra attention to the pivot of the Rect Transform. The pivot will stay in place when the element is resized, so by setting the pivot position you can control in which direction the element will expand or shrink. For example, if the pivot is in the center, then the element will expand equally in all directions, and if the pivot is in the upper left corner, then the element will expand to the right and down. Fit to size of UI element with child Text If you have a UI element, such as a Button, that has a background image and a child Game Object with a Text component on it, you probably want the whole UI element to fit the size of the text - maybe with some padding. In order to do this, first add a Horizontal Layout Group to the UI element, then add a Content Size Fitter too. Set the Horizontal Fit, the Vertical Fit, or both to the Preferred setting. You can add and tweak padding using the padding property in the Horizontal Layout Group. Why use a Horizontal Layout Group? Well, it could have been a Vertical Layout Group as well - as long as there is only a single child, they produce the same result. How does it work? The Horizontal (or Vertical) Layout Group functions both as a Layout Controller and as a Layout Element. First it listens to the layout information provided by the children in the group - in this case the child Text. Then it determines how large the group must be (at minimum, and preferably) in order to be able to contain all the children, and it functions as a Layout Element that provides this information about its minimum and preferred size. The Content Size Fitter listens to layout information provided by any Layout Element on the same Game Object - in this case provided by the Horizontal (or Vertical) Layout Group. Depending on its settings, it then controls the size of the Rect Transform based on this information. Once the size of the Rect Transform has been set, the Horizontal (or Vertical) Layout Group makes sure to position and size its children according to the available space. See the page about the Horizontal Layout Group for more information about how it controls the positions and sizes of its children. Make children of a Layout Group fit their respective sizes If you have a Layout Group (horizontal or vertical) and want each of the UI elements in the group to fit their respective content, what do you do? You can't put a Content Size Fitter on each child. The reason is that the Content Size Fitter wants control over its own Rect Transform, but the parent Layout Group also wants control over the child Rect Transform. This creates a conflict and the result is undefined behavior. However, it isn't necessary either. The parent Layout Group can already make each child fit the size of the content. What you need to do is to disable the Child Force Expand toggles on the Layout Group. If the children are themselves Layout Groups too, you may need to disable the Child Force Expand toggles on those too. Once the children no longer expand with flexible width, their alignment can be specified in the Layout Group using the Child Alignment setting. What if you want some of the children to expand to fill additional available space, but not the other children? You can easily control this by adding a Layout Element component to the children you want to expand and enabling the Flexible Width or Flexible Height properties on those Layout Elements. The parent Layout Group should still have the Child Force Expand toggles disabled, otherwise all the children will expand flexibly. How does it work? A Game Object can have multiple components that each provide layout information about minimum, preferred and flexible sizes. A priority system determines which values take effect over others. The Layout Element component has a higher priority than the Text, Image, and Layout Group components, so it can be used to override any layout information values they provide. When the Layout Group listens to the layout information provided by the children, it will take the overridden flexible sizes into account. Then, when controlling the sizes of the children, it will not make them any bigger than their preferred sizes. However, if the Layout Group has the Child Force Expand option enabled, it will always make the flexible sizes of all the children be at least 1. More information This page has explained solutions to a few common use cases. For a more in depth explanation of the auto layout system, see the UI Auto Layout page."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIMultiResolution.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIMultiResolution.html",
    "title": "Designing UI for Multiple Resolutions | FSM Unity Framework",
    "keywords": "Designing UI for Multiple Resolutions Modern games and applications often need to support a wide variety of different screen resolutions and particularly UI layouts need to be able to adapt to that. The UI System in Unity includes a variety of tools for this purpose that can be combined in various ways. In this how-to we're going to use a simple case study and look at and compare the different tools in the context of that. In our case study we have three buttons in the corners of the screen as shown below, and the goal is to adapt this layout to various resolutions. For this how-to we're going to consider four screen resolutions: Phone HD in portrait (640 x 960) and landscape (960 x 640) and Phone SD in portrait (320 x 480) and landscape (480 x 320). The layout is initially setup in the Phone HD Portrait resolution. Using anchors to adapt to different aspect ratios UI elements are by default anchored to the center of the parent rectangle. This means that they keep a constant offset from the center. If the resolution is changed to a landscape aspect ratio with this setup, the buttons may not even be inside the rectangle of the screen anymore. One way to keep the buttons inside the screen is to change the layout such that the locations of the buttons are tied to their respective corners of the screen. The anchors of the top left button can be set to the upper left corner using the Anchors Preset drop down in the Inspector, or by dragging the triangular anchor handles in the Scene View. It's best to do this while the current screen resolution set in the Game View is the one the layout is initially designed for, where the button placement looks correct. (See the UI Basic Layout page for more information on anchors.) Similarly, the anchors for the lower left and lower right buttons can be set to the lower left corner and lower right corner, respectively. Once the buttons have been anchored to their respective corners, they stick to them when changing the resolution to a different aspect ratio. When the screen size is changed to a larger or smaller resolution, the buttons will also remain anchored to their respective corners. However, since they keep their original size as specified in pixels, they may take up a larger or smaller proportion of the screen. This may or may not be desirable, depending on how you would like your layout to behave on screens of different resolutions. In this how-to, we know that the smaller resolutions of Phone SD Portrait and Landscape don't correspond to screens that are physically smaller, but rather just screens with a lower pixel density. On these lower-density screens the buttons shouldn't appear larger than on the high-density screens - they should instead appear with the same size. This means that the buttons should become smaller by the same percentage as the screen is smaller. In other words, the scale of the buttons should follow the screen size. This is where the Canvas Scaler component can help. Scaling with Screen Size The Canvas Scaler component can be added to a root Canvas - a Game Object with a Canvas component on it, which all the UI elements are children of. It is also added by default when creating a new Canvas through the GameObject menu. In the Canvas Scaler component, you can set its UI Scale Mode to Scale With Screen Size. With this scale mode you can specify a resolution to use as reference. If the current screen resolution is smaller or larger than this reference resolution, the scale factor of the Canvas is set accordingly, so all the UI elements are scaled up or down together with the screen resolution. In our case, we set the Canvas Scaler to be the Phone HD portrait resolution of 640 x 960. Now, when setting the screen resolution to the Phone SD portrait resolution of 320 x 480, the entire layout is scaled down so it appears proportionally the same as in full resolution. Everything is scaled down: The button sizes, their distances to the edges of the screen, the button graphics, and the text elements. This means that the layout will appear the same in the Phone SD portrait resolution as in Phone HD portrait; only with a lower pixel density. One thing to be aware of: After adding a Canvas Scaler component, it's important to also check how the layout looks at other aspect ratios. By setting the resolution back to Phone HD landscape, we can see that the buttons now appear bigger than they should (and used to). The reason for the larger buttons in landscape aspect ratio comes down to how the Canvas Scaler setting works. By default it compares the width or the current resolution with the width of the Canvas Scaler and the result is used as the scale factor to scale everything with. Since the current landscape resolution of 960 x 640 has a 1.5 times larger width than the portrait Canvas Scaler of 640 x 960, the layout is scaled up by 1.5. The component has a property called Match which can be 0 (Width), 1 (Height) or a value in between. By default it's set to 0, which compares the current screen width with the Canvas Scaler width as described. If the Match property is set to 0.5 instead, it will compare both the current width to the reference width and the current height to the reference height, and choose a scale factor that's in between the two. Since in this case the landscape resolution is 1.5 times wider but also 1.5 times shorter, those two factor even out and produce a final scale factor of 1, which means the buttons keep their original size. At this point the layout supports all the four screen resolutions using a combination of appropriate anchoring and the Canvas Scaler component on the Canvas. See the Canvas Scaler reference page for more information on different ways to scale UI elements in relation to different screen sizes."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIScreenTransition.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIScreenTransition.html",
    "title": "Creating Screen Transitions | FSM Unity Framework",
    "keywords": "Creating Screen Transitions The need to transition between multiple UI screens is fairly common. In this page we will explore a simple way to create and manage those transitions using animation and State Machines to drive and control each screen. Overview The high-level idea is that each of our screens will have an Animator Controller with two states (Open and Closed) and a boolean Parameter (Open). To transition between screens you will only need to close the currently open Screen and open the desired one. To make this process easier we will create a small Class ScreenManager that will keep track and take care of closing any already open Screen for us. The button that triggers the transition will only have to ask the ScreenManager to open the desired screen. Thinking about Navigation If you plan to support controller/keyboard navigation of UI elements, then it's important to have a few things in mind. It's important to avoid having Selectable elements outside the screen since that would enable players to select offscreen elements, we can do that by deactivating any off-screen hierarchy. We also need to make sure when a new screen is shown we set a element from it as selected, otherwise the player would not be able to navigate to the new screen. We will take care of all that in the ScreenManager class below. Setting up the Animator Controller Let's take a look at the most common and minimal setup for the Animation Controller to do a Screen transition. The controller will need a boolean parameter (Open) and two states (Open and Closed), each state should have an animation with only one keyframe, this way we let the State Machine do the transition blending for us. Now we need to create the transition between both states, let's start with the transition from Open to Closed and let's set the condition properly, we want to go from Open to Closed when the parameter Open is set to false. Now we create the transition from Closed to Open and set the condition to go from Closed to Open when the parameter Open is true. Managing the screens With all the above set up, the only thing missing is for us to set the parameter Open to true on the screens Animator we want to transition to and Open to false on the currently open screens Animator. To do that, we will create a small script: using UnityEngine; using UnityEngine.UI; using UnityEngine.EventSystems; using System.Collections; using System.Collections.Generic; public class ScreenManager : MonoBehaviour { //Screen to open automatically at the start of the Scene public Animator initiallyOpen; //Currently Open Screen private Animator m_Open; //Hash of the parameter we use to control the transitions. private int m_OpenParameterId; //The GameObject Selected before we opened the current Screen. //Used when closing a Screen, so we can go back to the button that opened it. private GameObject m_PreviouslySelected; //Animator State and Transition names we need to check against. const string k_OpenTransitionName = \"Open\"; const string k_ClosedStateName = \"Closed\"; public void OnEnable() { //We cache the Hash to the \"Open\" Parameter, so we can feed to Animator.SetBool. m_OpenParameterId = Animator.StringToHash (k_OpenTransitionName); //If set, open the initial Screen now. if (initiallyOpen == null) return; OpenPanel(initiallyOpen); } //Closes the currently open panel and opens the provided one. //It also takes care of handling the navigation, setting the new Selected element. public void OpenPanel (Animator anim) { if (m_Open == anim) return; //Activate the new Screen hierarchy so we can animate it. anim.gameObject.SetActive(true); //Save the currently selected button that was used to open this Screen. (CloseCurrent will modify it) var newPreviouslySelected = EventSystem.current.currentSelectedGameObject; //Move the Screen to front. anim.transform.SetAsLastSibling(); CloseCurrent(); m_PreviouslySelected = newPreviouslySelected; //Set the new Screen as then open one. m_Open = anim; //Start the open animation m_Open.SetBool(m_OpenParameterId, true); //Set an element in the new screen as the new Selected one. GameObject go = FindFirstEnabledSelectable(anim.gameObject); SetSelected(go); } //Finds the first Selectable element in the providade hierarchy. static GameObject FindFirstEnabledSelectable (GameObject gameObject) { GameObject go = null; var selectables = gameObject.GetComponentsInChildren<Selectable> (true); foreach (var selectable in selectables) { if (selectable.IsActive () && selectable.IsInteractable ()) { go = selectable.gameObject; break; } } return go; } //Closes the currently open Screen //It also takes care of navigation. //Reverting selection to the Selectable used before opening the current screen. public void CloseCurrent() { if (m_Open == null) return; //Start the close animation. m_Open.SetBool(m_OpenParameterId, false); //Reverting selection to the Selectable used before opening the current screen. SetSelected(m_PreviouslySelected); //Start Coroutine to disable the hierarchy when closing animation finishes. StartCoroutine(DisablePanelDeleyed(m_Open)); //No screen open. m_Open = null; } //Coroutine that will detect when the Closing animation is finished and it will deactivate the //hierarchy. IEnumerator DisablePanelDeleyed(Animator anim) { bool closedStateReached = false; bool wantToClose = true; while (!closedStateReached && wantToClose) { if (!anim.IsInTransition(0)) closedStateReached = anim.GetCurrentAnimatorStateInfo(0).IsName(k_ClosedStateName); wantToClose = !anim.GetBool(m_OpenParameterId); yield return new WaitForEndOfFrame(); } if (wantToClose) anim.gameObject.SetActive(false); } //Make the provided GameObject selected //When using the mouse/touch we actually want to set it as the previously selected and //set nothing as selected for now. private void SetSelected(GameObject go) { //Select the GameObject. EventSystem.current.SetSelectedGameObject(go); //If we are using the keyboard right now, that's all we need to do. var standaloneInputModule = EventSystem.current.currentInputModule as StandaloneInputModule; if (standaloneInputModule != null) return; //Since we are using a pointer device, we don't want anything selected. //But if the user switches to the keyboard, we want to start the navigation from the provided game object. //So here we set the current Selected to null, so the provided gameObject becomes the Last Selected in the EventSystem. EventSystem.current.SetSelectedGameObject(null); } } Let's hook up this script, we do this by creating a new GameObject, we can rename it \"ScreenManager\" for instance, and add the component above to it. You can assign an initial screen to it, this screen will be open at the start of your scene. Now for the final part, let's make the UI buttons work. Select the button that should trigger the screen transition and add a new action under the On Click () list in the Inspector. Drag the ScreenManager GameObject we just created to the ObjectField, on the dropdown select ScreenManager->OpenPanel (Animator) and drag and drop the panel you want to open when the user clicks the button to the las ObjectField. Notes This technique only requires each screen to have an AnimatorController with an Open parameter and a Closed state to work - it doesn't matter how your screen or State Machine are constructed. This technique also works well with nested screens, meaning you only need one ScreenManager for each nested level. The State Machine we set up above has the default state of Closed, so all of the screens that use this controller start as closed. The ScreenManager provides an initiallyOpen property so you can specify which screen is shown first."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIWorldSpace.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/HOWTO-UIWorldSpace.html",
    "title": "Creating a World Space UI | FSM Unity Framework",
    "keywords": "Creating a World Space UI The UI system makes it easy to create UI that is positioned in the world among other 2D or 3D objects in the Scene. Start by creating a UI element (such as an Image) if you don't already have one in your scene by using GameObject > UI > Image. This will also create a Canvas for you. Set the Canvas to World Space Select your Canvas and change the Render Mode to World Space. Now your Canvas is already positioned in the World and can be seen by all cameras if they are pointed at it, but it is probably huge compared to other objects in your Scene. We'll get back to that. Decide on a resolution First you need to decide what the resolution of the Canvas should be. If it was an image, what should the pixel resolution of the image be? Something like 800x600 might be a good starting point. You enter the resolution in the Width and Height values of the Rect Transform of the Canvas. It's probably a good idea to set the position to 0,0 at the same time. Specify the size of the Canvas in the world Now you should consider how big the Canvas should be in the world. You can use the Scale tool to simply scale it down until it has a size that looks good, or you can decide how big it should be in meters. If you want it to have a specific width in meters, you can can calculate the needed scale by using meter_size / canvas_width. For example, if you want it to be 2 meters wide and the Canvas width is 800, you would have 2 / 800 = 0.0025. You then set the Scale property of the Rect Transform on the Canvas to 0.0025 for both X, Y, and Z in order to ensure that it's uniformly scaled. Another way to think of it is that you are controlling the size of one pixel in the Canvas. If the Canvas is scaled by 0.0025, then that is also the size in the world of each pixel in the Canvas. Position the Canvas Unlike a Canvas set to Screen Space, a World Space Canvas can be freely positioned and rotated in the Scene. You can put a Canvas on any wall, floor, ceiling, or slanted surface (or hanging freely in the air of course). Just use the normal Translate and Rotate tools in the toolbar. Create the UI Now you can begin setting up your UI elements and layouts the same way you would with a Screen Space Canvas."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/InputModules.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/InputModules.html",
    "title": "Input Modules | FSM Unity Framework",
    "keywords": "Input Modules An Input Module is where the main logic of an event system can be configured and customized. Out of the box there are two provided Input Modules, one designed for Standalone, and one designed for Touch input. Each module receives and dispatches events as you would expect on the given configuration. Input modules are where the 'business logic' of the Event System take place. When the Event System is enabled it looks at what Input Modules are attached and passes update handling to the specific module. Input modules are designed to be extended or modified based on the input systems that you wish to support. Their purpose is to map hardware specific input (such as touch, joystick, mouse, motion controller) into events that are sent via the messaging system. The built in Input Modules are designed to support common game configurations such as touch input, controller input, keyboard input, and mouse input. They send a variety of events to controls in the application, if you implement the specific interfaces on your MonoBehaviours. All of the UI components implement the interfaces that make sense for the given component."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/MessagingSystem.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/MessagingSystem.html",
    "title": "Messaging System | FSM Unity Framework",
    "keywords": "Messaging System The new UI system uses a messaging system designed to replace SendMessage. The system is pure C# and aims to address some of the issues present with SendMessage. The system works using custom interfaces that can be implemented on a MonoBehaviour to indicate that the component is capable of receiving a callback from the messaging system. When the call is made a target GameObject is specified; the call will be issued on all components of the GameObject that implement the specified interface that the call is to be issued against. The messaging system allows for custom user data to be passed, as well as how far through the GameObject hierarchy the event should propagate; that is should it just execute for the specified GameObject, or should it also execute on children and parents. In addition to this the messaging framework provides helper functions to search for and find GameObjects that implement a given messaging interface. The messaging system is generic and designed for use not just by the UI system but also by general game code. It is relatively trivial to add custom messaging events and they will work using the same framework that the UI system uses for all event handling. Defining A Custom Message If you wish to define a custom message it is relatively simple. In the UnityEngine.EventSystems namespace there is a base interface called 'IEventSystemHandler'. Anything that extends from this can be considered as a target for receiving events via the messaging system. public interface ICustomMessageTarget : IEventSystemHandler { // functions that can be called via the messaging system void Message1(); void Message2(); } Once this interface is defined then it can be implemented by a MonoBehaviour. When implemented it defines the functions that will be executed if the given message is issued against this MonoBehaviours GameObject. public class CustomMessageTarget : MonoBehaviour, ICustomMessageTarget { public void Message1() { Debug.Log (\"Message 1 received\"); } public void Message2() { Debug.Log (\"Message 2 received\"); } } Now that a script exists that can receive the message we need to issue the message. Normally this would be in response to some loosely coupled event that occurs. For example, in the UI system we issue events for such things as PointerEnter and PointerExit, as well as a variety of other things that can happen in response to user input into the application. To send a message a static helper class exists to do this. As arguments it requires a target object for the message, some user specific data, and a functor that maps to the specific function in the message interface you wish to target. ExecuteEvents.Execute<ICustomMessageTarget>(target, null, (x,y)=>x.Message1()); This code will execute the function Message1 on any components on the GameObject target that implement the ICustomMessageTarget interface. The scripting documentation for the ExecuteEvents class covers other forms of the Execute functions, such as Executing in children or in parents."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/Raycasters.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/Raycasters.html",
    "title": "Raycasters | FSM Unity Framework",
    "keywords": "Raycasters The Event System needs a method for detecting where current input events need to be sent to, and this is provided by the Raycasters. Given a screen space position they will collect all potential targets, figure out if they are under the given position, and then return the object that is closest to the screen. There are a few types of Raycasters that are provided: Graphic Raycaster - Used for UI elements, lives on a Canvas and searches within the canvas Physics 2D Raycaster - Used for 2D physics elements Physics Raycaster - Used for 3D physics elements When a Raycaster is present and enabled in the scene it will be used by the Event System whenever a query is issued from an Input Module. If multiple Raycasters are used then they will all have casting happen against them and the results will be sorted based on distance to the elements."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/StyledText.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/StyledText.html",
    "title": "Rich Text | FSM Unity Framework",
    "keywords": "Rich Text The text for UI elements and text meshes can incorporate multiple font styles and sizes. Rich text is supported both for the UI System and the legacy GUI system. The Text, GUIStyle, GUIText and TextMesh classes have a Rich Text setting which instructs Unity to look for markup tags within the text. The Debug.Log function can also use these markup tags to enhance error reports from code. The tags are not displayed but indicate style changes to be applied to the text. Markup format The markup system is inspired by HTML but isn't intended to be strictly compatible with standard HTML. The basic idea is that a section of text can be enclosed inside a pair of matching tags:- We are <b>not</b> amused. As the example shows, the tags are just pieces of text inside the \"angle bracket\" characters, < and >. You place the opening tag at the beginning of the section. The text inside the tag denotes its name (which in this case is just b). You place another tag at the end of the section. This is the closing tag. It has the same name as the opening tag, but the name is prefixed with a slash / character. Every opening tag must have a corresponding closing tag. If you don't close an opening tag, it is rendered as regular text. The tags are not displayed to the user directly but are interpreted as instructions for styling the text they enclose. The b tag used in the example above applies boldface to the word \"not\", so the text appears ons creen as:- We are not amused A marked up section of text (including the tags that enclose it) is referred to as an element. Nested elements It is possible to apply more than one style to a section of text by \"nesting\" one element inside another We are <b><i>definitely not</i></b> amused The <i> tag applies italic style, so this would be presented onscreen as We are definitely not amused Note the ordering of the closing tags, which is in reverse to that of the opening tags. The reason for this is perhaps clearer when you consider that the inner tags need not span the whole text of the outermost element We are <b>absolutely <i>definitely</i> not</b> amused which gives We are absolutely definitely not amused Tag parameters Some tags have a simple all-or-nothing effect on the text but others might allow for variations. For example, the color tag needs to know which color to apply. Information like this is added to tags by the use of parameters:- We are <color=green>green</color> with envy Which produces this result: Note that the ending tag doesn't include the parameter value. Optionally, the value can be surrounded by quotation marks but this isn't required. Tag parameters cannot include blank spaces. For example: We are <color = green>green</color> with envy does not work because of the spaces to either side of the = character. Supported tags The following list describes all the styling tags supported by Unity. Tag Description Example Notes b Renders the text in boldface. We are <b>not</b> amused. i Renders the text in italics. We are <i>usually</i> not amused. size Sets the size of the text according to the parameter value, given in pixels. We are <size=50>largely</size> unaffected. Although this tag is available for Debug.Log, you will find that the line spacing in the window bar and Console looks strange if the size is set too large. color Sets the color of the text according to the parameter value. The color can be specified in the traditional HTML format. #rrggbbaa ...where the letters correspond to pairs of hexadecimal digits denoting the red, green, blue and alpha (transparency) values for the color. For example, cyan at full opacity would be specified by color=#00ffffff... You can specify hexadecimal values in uppercase or lowercase; #FF0000 is equivalent to #ff0000. We are <color=#ff0000ff>colorfully</color> amused Another option is to use the name of the color. This is easier to understand but naturally, the range of colors is limited and full opacity is always assumed. <color=cyan>some text</color> The available color names are given in the table below. material This is only useful for text meshes and renders a section of text with a material specified by the parameter. The value is an index into the text mesh's array of materials as shown by the inspector. We are <material=2>texturally</material> amused quad This is only useful for text meshes and renders an image inline with the text. It takes parameters that specify the material to use for the image, the image height in pixels, and a further four that denote a rectangular area of the image to display. Unlike the other tags, quad does not surround a piece of text and so there is no ending tag - the slash character is placed at the end of the initial tag to indicate that it is \"self-closing\". <quad material=1 size=20 x=0.1 y=0.1 width=0.5 height=0.5> This selects the material at position in the renderer's material array and sets the height of the image to 20 pixels. The rectangular area of image starts at given by the x, y, width and height values, which are all given as a fraction of the unscaled width and height of the texture. Supported colors The following table lists colors for which you can use a name instead of a hexadecimal tag in the <color> rich text tag. Color name Hex value Swatch aqua (same as cyan) #00ffffff black #000000ff blue #0000ffff brown #a52a2aff cyan (same as aqua) #00ffffff darkblue #0000a0ff fuchsia (same as magenta) #ff00ffff green #008000ff grey #808080ff lightblue #add8e6ff lime #00ff00ff magenta (same as fuchsia) #ff00ffff maroon #800000ff navy #000080ff olive #808000ff orange #ffa500ff purple #800080ff red #ff0000ff silver #c0c0c0ff teal #008080ff white #ffffffff yellow #ffff00ff Editor GUI Rich text is disabled by default in the editor GUI system but it can be enabled explicitly using a custom GUIStyle. The richText property should be set to true and the style passed to the GUI function in question: GUIStyle style = new GUIStyle (); style.richText = true; GUILayout.Label(\"<size=30>Some <color=yellow>RICH</color> text</size>\",style);"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/SupportedEvents.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/SupportedEvents.html",
    "title": "Supported Events | FSM Unity Framework",
    "keywords": "Supported Events The Event System supports a number of events, and they can be customized further in user custom user written Input Modules. The events that are supported by the Standalone Input Module and Touch Input Module are provided by interface and can be implemented on a MonoBehaviour by implementing the interface. If you have a valid Event System configured the events will be called at the correct time. IPointerEnterHandler - OnPointerEnter - Called when a pointer enters the object IPointerExitHandler - OnPointerExit - Called when a pointer exits the object IPointerDownHandler - OnPointerDown - Called when a pointer is pressed on the object IPointerUpHandler- OnPointerUp - Called when a pointer is released (called on the GameObject that the pointer is clicking) IPointerClickHandler - OnPointerClick - Called when a pointer is pressed and released on the same object IInitializePotentialDragHandler - OnInitializePotentialDrag - Called when a drag target is found, can be used to initialize values IBeginDragHandler - OnBeginDrag - Called on the drag object when dragging is about to begin IDragHandler - OnDrag - Called on the drag object when a drag is happening IEndDragHandler - OnEndDrag - Called on the drag object when a drag finishes IDropHandler - OnDrop - Called on the object where a drag finishes IScrollHandler - OnScroll - Called when a mouse wheel scrolls IUpdateSelectedHandler - OnUpdateSelected - Called on the selected object each tick ISelectHandler - OnSelect - Called when the object becomes the selected object IDeselectHandler - OnDeselect - Called on the selected object becomes deselected IMoveHandler - OnMove - Called when a move event occurs (left, right, up, down) ISubmitHandler - OnSubmit - Called when the submit button is pressed ICancelHandler - OnCancel - Called when the cancel button is pressed"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/TableOfContents.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/TableOfContents.html",
    "title": "| FSM Unity Framework",
    "keywords": "Unity UI: Unity User Interface Canvas Basic Layout Visual Components Interaction Components Animation Integration Auto Layout Rich Text Events MessagingSystem InputModules SupportedEvents Raycasters Reference Rect Transform Canvas Components Canvas Canvas Scaler Canvas Group Canvas Renderer Visual UIInteractionComponents Text Image Raw Image Mask RectMask2D UI Effect Components Shadow Outline Position as UV1 Interaction Components Selectable Base Class Transition Options Navigation Options Button Toggle Toggle Group Slider Scrollbar Dropdown Input Field Scroll Rect Auto Layout Layout Element Content Size Fitter Aspect Ratio Fitter Horizontal Layout Group Vertical Layout Group Grid Layout Group Events script-EventSystem script-GraphicRaycaster script-PhysicsRaycaster script-Physics2DRaycaster script-StandaloneInputModule script-TouchInputModule script-EventTrigger UI How Tos Designing UI for Multiple Resolutions Making UI elements fit the size of their content Creating a World Space UI Creating UI elements from scripting Creating Screen Transitions"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIAnimationIntegration.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIAnimationIntegration.html",
    "title": "Animation Integration | FSM Unity Framework",
    "keywords": "Animation Integration Animation allows for each transition between control states to be fully animated using Unity's animation system. This is the most powerful of the transition modes due to the number of properties that can be animated simultaneously. To use the Animation transition mode, an Animator Component needs to be attached to the controller element. This can be done automatically by clicking \"Auto Generate Animation\". This also generates an Animator Controller with states already set up, which will need to be saved. The new Animator controller is ready to use straight away. Unlike most Animator Controllers, this controller also stores the animations for the controller's transitions and these can be customised, if desired. For example, if a Button element with an Animator controller attached is selected, the animations for each of the button's states can be edited by opening the Animation window (Window>Animation). There is an Animation Clip pop-up menu to select the desired clip. Choose from \"Normal\", \"Highlighted\", \"Pressed\" and \"Disabled\". The Normal State is set by the values on button element itself and can be left empty. On all other states, the most common configuration is a single keyframe at the start of the timeline. The transition animation between states will be handled by the Animator. As an example, the width of the button in the Highlighted State could be changed by selecting the Highlighted state from the Animation Clip pop up menu and with the playhead at the start of the time line: Select the record Button Change the width of the Button in the inspector Exit the record mode. Change to play mode to see how the button grows when highlighted. Any number of properties can have their parameters set in this one keyframe. Several buttons can share the same behaviour by sharing Animator Controllers. The UI Animation transition mode is not compatible with Unity's legacy animation system. You should only use the Animator Component."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIAutoLayout.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIAutoLayout.html",
    "title": "Auto Layout | FSM Unity Framework",
    "keywords": "Auto Layout The Rect Transform layout system is flexible enough to handle a lot of different types of layouts and it also allows placing elements in a complete freeform fashion. However, sometimes something a bit more structured can be needed. The auto layout system provides ways to place elements in nested layout groups such as horizontal groups, vertical groups, or grids. It also allows elements to automatically be sized according to the contained content. For example a button can be dynamically resized to exactly fit its text content plus some padding. The auto layout system is a system built on top of the basic Rect Transform layout system. It can optionally be used on some or all elements. Understanding Layout Elements The auto layout system is based on a concept of layout elements and layout controllers. A layout element is an Game Object with a Rect Transform and optionally other components as well. The layout element has certain knowledge about which size it should have. Layout elements don't directly set their own size, but other components that function as layout controllers can use the information they provide in order to calculate a size to use for them. A layout element has properties that defines its own: Minimum width Minimum height Preferred width Preferred height Flexible width Flexible height Examples of layout controller components that use the information provided by layout elements are Content Size Fitter and the various Layout Group components. The basic principles for how layout elements in a layout group are sized is as follows: First minimum sizes are allocated. If there is sufficient available space, preferred sizes are allocated. If there is additional available space, flexible size is allocated. Any Game Object with a Rect Transform on it can function as a layout element. They will by default have minimum, preferred, and flexible sizes of 0. Certain components will change these layout properties when added to the Game Object. The Image and Text components are two examples of components that provide layout element properties. They change the preferred width and height to match the sprite or text content. Layout Element Component If you want to override the minimum, preferred, or flexible size, you can do that by adding a Layout Element component to the Game Object. The Layout Element component lets you override the values for one or more of the layout properties. Enable the checkbox for a property you want to override and then specify the value you want to override with. See the reference page for Layout Element for more information. Understanding Layout Controllers Layout controllers are components that control the sizes and possibly positions of one or more layout elements, meaning Game Objects with Rect Transforms on. A layout controller may control its own layout element (the same Game Object it is on itself) or it may control child layout elements. A component that functions as a layout controller may also itself function as a layout element at the same time. Content Size Fitter The Content Size Fitter functions as a layout controller that controls the size of its own layout element. The simplest way to see the auto layout system in action is to add a Content Size Fitter component to a Game Object with a Text component. If you set either the Horizontal Fit or Vertical Fit to Preferred, the Rect Transform will adjust its width and/or height to fit the Text content. See the reference page for Content Size Fitter for more information. Aspect Ratio Fitter The Aspect Ratio Fitter functions as a layout controller that controls the size of its own layout element. It can adjust the height to fit the width or vice versa, or it can make the element fit inside its parent or envelope its parent. The Aspect Ratio Fitter does not take layout information into account such as minimum size and preferred size. See the reference page for Aspect Ratio Fitter for more information. Layout Groups A layout group functions as a layout controller that controls the sizes and positions of its child layout elements. For example, a Horizontal Layout Group places its children next to each other, and a Grid Layout Group places its children in a grid. A layout group doesn't control its own size. Instead it functions as a layout element itself which may be controlled by other layout controllers or be set manually. Whatever size a layout group is allocated, it will in most cases try to allocate an appropriate amount of space for each of its child layout elements based on the minimum, preferred, and flexible sizes they reported. Layout groups can also be nested arbitrarily this way. See the reference pages for Horizontal Layout Group, Vertical Layout Group and Grid Layout Group for more information. Driven Rect Transform properties Since a layout controller in the auto layout system can automatically control the sizes and placement of certain UI elements, those sizes and positions should not be manually edited at the same time through the Inspector or Scene View. Such changed values would just get reset by the layout controller on the next layout calculation anyway. The Rect Transform has a concept of driven properties to address this. For example, a Content Size Fitter which has the Horizontal Fit property set to Minimum or Preferred will drive the width of the Rect Transform on the same Game Object. The width will appear as read-only and a small info box at the top of the Rect Transform will inform that one or more properties are driven by Conten Size Fitter. The driven Rect Transforms properties have other reasons beside preventing manual editing. A layout can be changed just by changing the resolution or size of the Game View. This in turn can change the size or placement of layout elements, which changes the values of driven properties. But it wouldn't be desirable that the Scene is marked as having unsaved changes just because the Game View was resized. To prevent this, the values of driven properties are not saved as part of the Scene and changes to them do not mark the scene as changed. Technical Details The auto layout system comes with certain components built-in, but it is also possible to create new components that controls layouts in custom ways. This is done by having a component implement specific interfaces which are recognized by the auto layout system. Layout Interfaces A component is treated as a layout element by the auto layout system if it implements the interface ILayoutElement. A component is expected to drive the Rect Transforms of its children if it implements the interface ILayoutGroup. A component is expected to drive its own RectTransform if it implements the interface ILayoutSelfController. Layout Calculations The auto layout system evaluates and executes layouts in the following order: The minimum, preferred, and flexible widths of layout elements are calculated by calling CalculateLayoutInputHorizontal on ILayoutElement components. This is performed in bottom-up order, where children are calculated before their parents, such that the parents may take the information in their children into account in their own calculations. The effective widths of layout elements are calculated and set by calling SetLayoutHorizontal on ILayoutController components. This is performed in top-down order, where children are calculated after their parents, since allocation of child widths needs to be based on the full width available in the parent. After this step the Rect Transforms of the layout elements have their new widths. The minimum, preferred, and flexible heights of layout elements are calculated by calling CalculateLayoutInputVertical on ILayoutElement components. This is performed in bottom-up order, where children are calculated before their parents, such that the parents may take the information in their children into account in their own calculations. The effective heights of layout elements are calculated and set by calling SetLayoutVertical on ILayoutController components. This is performed in top-down order, where children are calculated after their parents, since allocation of child heights needs to be based on the full height available in the parent. After this step the Rect Transforms of the layout elements have their new heights. As can be seen from the above, the auto layout system evaluates widths first and then evaluates heights afterwards. Thus, calculated heights may depend on widths, but calculated widths can never depend on heights. Triggering Layout Rebuild When a property on a component changes which can cause the current layout to no longer be valid, a layout recalculation is needed. This can be triggered using the call: LayoutRebuilder.MarkLayoutForRebuild (transform as RectTransform); The rebuild will not happen immediately, but at the end of the current frame, just before rendering happens. The reason it is not immediate is that this would cause layouts to be potentially rebuild many times during the same frame, which would be bad for performance. Guidelines for when a rebuild should be triggered: In setters for properties that can change the layout. In these callbacks: OnEnable OnDisable OnRectTransformDimensionsChange OnValidate (only needed in the editor, not at runtime) OnDidApplyAnimationProperties"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIBasicLayout.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIBasicLayout.html",
    "title": "Basic Layout | FSM Unity Framework",
    "keywords": "Basic Layout In this section we'll look at how you can position UI elements relative to the Canvas and each other. If you want to test yourself while reading, you can create an Image using the menu GameObject -> UI -> Image. The Rect Tool Every UI element is represented as a rectangle for layout purposes. This rectangle can be manipulated in the Scene View using the Rect Tool in the toolbar. The Rect Tool is used both for Unity's 2D features and for UI, and in fact can be used even for 3D objects as well. The Rect Tool can be used to move, resize and rotate UI elements. Once you have selected a UI element, you can move it by clicking anywhere inside the rectangle and dragging. You can resize it by clicking on the edges or corners and dragging. The element can be rotated by hovering the cursor slightly away from the corners until the mouse cursor looks like a rotation symbol. You can then click and drag in either direction to rotate. Just like the other tools, the Rect Tool uses the current pivot mode and space, set in the toolbar. When working with UI it's usually a good idea to keep those set to Pivot and Local. Rect Transform The Rect Transform is a new transform component that is used for all UI elements instead of the regular Transform component. Rect Transforms have position, rotation, and scale just like regular Transforms, but it also has a width and height, used to specify the dimensions of the rectangle. Resizing Versus Scaling When the Rect Tool is used to change the size of an object, normally for Sprites in the 2D system and for 3D objects it will change the local scale of the object. However, when it's used on an object with a Rect Transform on it, it will instead change the width and the height, keeping the local scale unchanged. This resizing will not affect font sizes, border on sliced images, and so on. Pivot Rotations, size, and scale modifications occur around the pivot so the position of the pivot affects the outcome of a rotation, resizing, or scaling. When the toolbar Pivot button is set to Pivot mode, the pivot of a Rect Transform can be moved in the Scene View. Anchors Rect Transforms include a layout concept called anchors. Anchors are shown as four small triangular handles in the Scene View and anchor information is also shown in the Inspector. If the parent of a Rect Transform is also a Rect Transform, the child Rect Transform can be anchored to the parent Rect Transform in various ways. For example, the child can be anchored to the center of the parent, or to one of the corners. The anchoring also allows the child to stretch together with the width or height of the parent. Each corner of the rectangle has a fixed offset to its corresponding anchor, i.e. the top left corner of the rectangle has a fixed offset to the top left anchor, etc. This way the different corners of the rectangle can be anchored to different points in the parent rectangle. The positions of the anchors are defined in fractions (or percentages) of the parent rectangle width and height. 0.0 (0%) corresponds to the left or bottom side, 0.5 (50%) to the middle, and 1.0 (100%) to the right or top side. But anchors are not limited to the sides and middle; they can be anchored to any point within the parent rectangle. You can drag each of the anchors individually, or if they are together, you can drag them together by clicking in the middle in between them and dragging. If you hold down Shift key while dragging an anchor, the corresponding corner of the rectangle will move together with the anchor. A useful feature of the anchor handles is that they automatically snap to the anchors of sibling rectangles to allow for precise positioning. Anchor presets In the Inspector, the Anchor Preset button can be found in the upper left corner of the Rect Transform component. Clicking the button brings up the Anchor Presets dropdown. From here you can quickly select from some of the most common anchoring options. You can anchor the UI element to the sides or middle of the parent, or stretch together with the parent size. The horizontal and vertical anchoring is independent. The Anchor Presets buttons displays the currently selected preset option if there is one. If the anchors on either the horizontal or vertical axis are set to different positions than any of the presets, the custom options is shown. Anchor and position fields in the Inspector You can click the Anchors expansion arrow to reveal the anchor number fields if they are not already visible. Anchor Min corresponds to the lower left anchor handle in the Scene View, and Anchor Max corresponds to the upper right handle. The position fields of rectangle are shown differently depending on whether the anchors are together (which produces a fixed width and height) or separated (which causes the rectangle to stretch together with the parent rectangle). When all the anchor handles are together the fields displayed are Pos X, Pos Y, Width and Height. The Pos X and Pos Y values indicate the position of the pivot relative to the anchors. When the anchors are separated the fields can change partially or completely to Left, Right, Top and Bottom. These fields define the padding inside the rectangle defined by the anchors. The Left and Right fields are used if the anchors are separated horizontally and the Top and Bottom fields are used if they are separated vertically. Note that changing the values in the anchor or pivot fields will normally counter-adjust the positioning values in order to make the rectangle stay in place. In cases where this is not desired, enable Raw edit mode by clicking the R button in the Inspector. This causes the anchor and pivot value to be able to be changed without any other values changing as a result. This will likely cause the rectangle to be visually moved or resized, since its position and size is dependent on the anchor and pivot values."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UICanvas.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UICanvas.html",
    "title": "Canvas | FSM Unity Framework",
    "keywords": "Canvas The Canvas is the area that all UI elements should be inside. The Canvas is a Game Object with a Canvas component on it, and all UI elements must be children of such a Canvas. Creating a new UI element, such as an Image using the menu GameObject > UI > Image, automatically creates a Canvas, if there isn't already a Canvas in the scene. The UI element is created as a child to this Canvas. The Canvas area is shown as a rectangle in the Scene View. This makes it easy to position UI elements without needing to have the Game View visible at all times. Canvas uses the EventSystem object to help the Messaging System. Draw order of elements UI elements in the Canvas are drawn in the same order they appear in the Hierarchy. The first child is drawn first, the second child next, and so on. If two UI elements overlap, the later one will appear on top of the earlier one. To change which element appear on top of other elements, simply reorder the elements in the Hierarchy by dragging them. The order can also be controlled from scripting by using these methods on the Transform component: SetAsFirstSibling, SetAsLastSibling, and SetSiblingIndex. Render Modes The Canvas has a Render Mode setting which can be used to make it render in screen space or world space. Screen Space - Overlay This render mode places UI elements on the screen rendered on top of the scene. If the screen is resized or changes resolution, the Canvas will automatically change size to match this. Screen Space - Camera This is similar to Screen Space - Overlay, but in this render mode the Canvas is placed a given distance in front of a specified Camera. The UI elements are rendered by this camera, which means that the Camera settings affect the appearance of the UI. If the Camera is set to Perspective, the UI elements will be rendered with perspective, and the amount of perspective distortion can be controlled by the Camera Field of View. If the screen is resized, changes resolution, or the camera frustum changes, the Canvas will automatically change size to match as well. World Space In this render mode, the Canvas will behave as any other object in the scene. The size of the Canvas can be set manually using its Rect Transform, and UI elements will render in front of or behind other objects in the scene based on 3D placement. This is useful for UIs that are meant to be a part of the world. This is also known as a \"diegetic interface\"."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIHowTos.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIHowTos.html",
    "title": "UI How Tos | FSM Unity Framework",
    "keywords": "UI How Tos In this section you can learn about solutions to common UI tasks."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIInteractionComponents.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIInteractionComponents.html",
    "title": "Interaction Components | FSM Unity Framework",
    "keywords": "Interaction Components This section covers components in the UI system that handles interaction, such as mouse or touch events and interaction using a keyboard or controller. The interaction components are not visible on their own, and must be combined with one or more visual components in order to work correctly. Common Functionality Most of the interaction components have some things in common. They are selectables, which means they have shared built-in functionality for visualising transitions between states (normal, highlighted, pressed, disabled), and for navigation to other selectables using keyboard or controller. This shared functionality is described on the Selectable page. The interaction components have at least one UnityEvent that is invoked when user interacts with the component in specific way. The UI system catches and logs any exceptions that propagate out of code attached to UnityEvent. Button A Button has an OnClick UnityEvent to define what it will do when clicked. See the Button page for details on using the Button component. Toggle A Toggle has an Is On checkbox that determines whether the Toggle is currently on or off. This value is flipped when the user clicks the Toggle, and a visual checkmark can be turned on or off accordingly. It also has an OnValueChanged UnityEvent to define what it will do when the value is changed. See the Toggle page for details on using the Toggle component. Toggle Group A Toggle Group can be used to group a set of Toggles that are mutually exclusive. Toggles that belong to the same group are constrained so that only one of them can be selected at a time - selecting one of them automatically deselects all the others. See the Toggle Group page for details on using the Toggle Group component. Slider A Slider has a decimal number Value that the user can drag between a minimum and maximum value. It can be either horizontal or vertical. It also has a OnValueChanged UnityEvent to define what it will do when the value is changed. See the Slider page for details on using the Slider component. Scrollbar A Scrollbar has a decimal number Value between 0 and 1. When the user drags the scrollbar, the value changes accordingly. Scrollbars are often used together with a Scroll Rect and a Mask to create a scroll view. The Scrollbar has a Size value between 0 and 1 that determines how big the handle is as a fraction of the entire scrollbar length. This is often controlled from another component to indicate how big a proportion of the content in a scroll view is visible. The Scroll Rect component can automatically do this. The Scrollbar can be either horizontal or vertical. It also has a OnValueChanged UnityEvent to define what it will do when the value is changed. See the Scrollbar page for details on using the Scrollbar component. Dropdown A Dropdown has a list of options to choose from. A text string and optionally an image can be specified for each option, and can be set either in the Inspector or dynamically from code. It has a OnValueChanged UnityEvent to define what it will do when the currently chosen option is changed. See the Dropdown page for details on using the Dropdown component. Input Field An Input Field is used to make the text of a Text Element editable by the user. It has a UnityEvent to define what it will do when the text content is changed, and an another to define what it will do when the user has finished editing it. See the Input Field page for details on using the Input Field component. Scroll Rect (Scroll View) A Scroll Rect can be used when content that takes up a lot of space needs to be displayed in a small area. The Scroll Rect provides functionality to scroll over this content. Usually a Scroll Rect is combined with a Mask in order to create a scroll view, where only the scrollable content inside the Scroll Rect is visible. It can also additionally be combined with one or two Scrollbars that can be dragged to scroll horizontally or vertically. See the Scroll Rect page for details on using the Scroll Rect component."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIReference.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIReference.html",
    "title": "UI Reference | FSM Unity Framework",
    "keywords": "UI Reference This section goes into more depth about Unity’s UI features."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIVisualComponents.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/UIVisualComponents.html",
    "title": "Visual Components | FSM Unity Framework",
    "keywords": "Visual Components With the introduction of the UI system, new Components have been added that will help you create GUI specific functionality. This section will cover the basics of the new Components that can be created. Text The Text component, which is also known as a Label, has a Text area for entering the text that will be displayed. It is possible to set the font, font style, font size and whether or not the text has rich text capability. There are options to set the alignment of the text, settings for horizontal and vertical overflow which control what happens if the text is larger than the width or height of the rectangle, and a Best Fit option that makes the text resize to fit the available space. Image An Image has a Rect Transform component and an Image component. A sprite can be applied to the Image component under the Target Graphic field, and its colour can be set in the Color field. A material can also be applied to the Image component. The Image Type field defines how the applied sprite will appear, the options are: Simple - Scales the whole sprite equally. Sliced - Utilises the 3x3 sprite division so that resizing does not distort corners and only the center part is stretched. Tiled - Similar to Sliced, but tiles (repeats) the center part rather than stretching it. For sprites with no borders at all, the entire sprite is tiled. Filled - Shows the sprite in the same way as Simple does except that it fills in the sprite from an origin in a defined direction, method and amount. The option to Set Native Size, which is shown when Simple or Filled is selected, resets the image to the original sprite size. Images can be imported as UI sprites by selecting Sprite( 2D / UI) from the 'Texture Type' settings. Sprites have extra import settings compared to the old GUI sprites, the biggest difference is the addition of the sprite editor. The sprite editor provides the option of 9-slicing the image, this splits the image into 9 areas so that if the sprite is resized the corners are not stretched or distorted. Raw Image The Image component takes a sprite but Raw Image takes a texture (no borders etc). Raw Image should only be used if necessary otherwise Image will be suitable in the majority of cases. Mask A Mask is not a visible UI control but rather a way to modify the appearance of a control’s child elements. The mask restricts (ie, “masks”) the child elements to the shape of the parent. So, if the child is larger than the parent then only the part of the child that fits within the parent will be visible. Effects Visual components can also have various simple effects applied, such as a simple drop shadow or outline. See the UI Effects reference page for more information."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-Canvas.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-Canvas.html",
    "title": "Canvas | FSM Unity Framework",
    "keywords": "Canvas The Canvas component represents the abstract space in which the UI is laid out and rendered. All UI elements must be children of a GameObject that has a Canvas component attached. When you create a UI element object from the menu (GameObject > Create UI), a Canvas object will be created automatically if there isn't one in the scene already. Properties Property: Function: Render Mode The way the UI is rendered to the screen or as an object in 3D space (see below). The options are Screen Space - Overlay, Screen Space - Camera and World Space. Pixel Perfect (Screen Space modes only) Should the UI be rendered without antialiasing for precision? Render Camera (Screen Space - Camera mode only) The camera to which the UI should be rendered (see below). Plane Distance (Screen Space - Camera mode only) The distance at which the UI plane should be placed in front of the camera. Event Camera (World Space mode only) The camera that will be used to process UI events. Receives Events Are UI events processed by this Canvas? Details A single Canvas for all UI elements is sufficient but multiple Canvases in the scene is possible. It is also possible use nested Canvases, where one Canvas is placed as a child of another for optimization purposes. A nested Canvas uses the same Render Mode as its parent. Traditionally, UIs are rendered as if they were simple graphic designs drawn directly on the screen. That is to say, they have no concept of a 3D space being viewed by a camera. Unity supports this kind of screen space rendering but also allows UIs to rendered as objects in the scene, depending on the value of the Render Mode property. The modes available are Screen Space - Overlay, Screen Space - Camera and World Space. Screen Space - Overlay In this mode, the Canvas is scaled to fit the screen and then rendered directly without reference to the scene or a camera (the UI will be rendered even if there is no camera in the scene at all). If the screen's size or resolution are changed then the UI will automatically rescale to fit. The UI will be drawn over any other graphics such as the camera view. Note: The Screen Space - Overlay canvas needs to be stored at the top level of the hierarchy. If this is not used then the UI may disappear from the view. This is a built-in limitation. Keep the Screen Space - Overlay canvas at the top level of the hierarchy to get expected results. Screen Space - Camera In this mode, the Canvas is rendered as if it were drawn on a plane object some distance in front of a given camera. The onscreen size of the UI does not vary with the distance since it is always rescaled to fit exactly within the camera frustum. If the screen's size or resolution or the camera frustum are changed then the UI will automatically rescale to fit. Any 3D objects in the scene that are closer to the camera than the UI plane will be rendered in front of the UI, while objects behind the plane will be obscured. World Space This mode renders the UI as if it were a plane object in the scene. Unlike Screen Space - Camera mode, however, the plane need not face the camera and can be oriented however you like. The size of the Canvas can be set using its Rect Transform but its onscreen size will depend on the viewing angle and distance of the camera. Other scene objects can pass behind, through or in front of the Canvas. Hints Read more about setting up a World Space Canvas on the Creating a World Space UI page. For information about making your Canvas and UI scale to different resolutions or aspect ratios, see the Designing UI for Multiple Resolutions page as well as the Canvas Scaler page."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-CanvasGroup.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-CanvasGroup.html",
    "title": "Canvas Group | FSM Unity Framework",
    "keywords": "Canvas Group The Canvas Group can be used to control certain aspects of a whole group of UI elements from one place without needing to handle them each individually. The properties of the Canvas Group affect the GameObject it is on as well as all children. Properties Property: Function: Alpha The opacity of the UI elements in this group. The value is between 0 and 1 where 0 is fully transparent and 1 is fully opaque. Note that elements retain their own transparency as well, so the Canvas Group alpha and the alpha values of the individual UI elements are multiplied with each other. Interactable Determines if this component will accept input. When it is set to false interaction is disabled. Block Raycasts Will this component act as a collider for Raycasts? You will need to call the RayCast function on the graphic raycaster attached to the Canvas. This does not apply to Physics.Raycast. Ignore Parent Groups Will this group also be affected by the settings in Canvas Group components further up in the Game Object hierarchy, or will it ignore those and hence override them? Details Typical uses of Canvas Group are: Fading in or out a whole window by adding a Canvas Group on the GameObject of the Window and control its Alpha property. Making a whole set of controls non-interactable (\"grayed out\") by adding a Canvas Group to a parent GameObject and setting its Interactable property to false. Making one or more UI elements not block mouse events by placing a Canvas Group component on the element or one of its parents and setting its Block Raycasts property to false."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-CanvasRenderer.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-CanvasRenderer.html",
    "title": "Canvas Renderer | FSM Unity Framework",
    "keywords": "Canvas Renderer The Canvas Renderer component renders a graphical UI object contained within a Canvas. Properties The Canvas Renderer has no properties exposed in the inspector. Details The standard UI objects available from the menu (GameObject > Create UI) all have Canvas Renderers attached wherever they are required but you may need to add this component manually for custom UI objects. Although there are no properties exposed in the inspector, a few properties and function can be accessed from scripts - see the CanvasRenderer page in the Script Reference for full details."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-RectTransform.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/class-RectTransform.html",
    "title": "Rect Transform | FSM Unity Framework",
    "keywords": "Rect Transform The Rect Transform component is the 2D layout counterpart of the Transform component. Where Transform represents a single point, Rect Transform represent a rectangle that a UI element can be placed inside. If the parent of a Rect Transform is also a Rect Transform, the child Rect Transform can also specify how it should be positioned and sized relative to the parent rectangle. Properties Property: Function: Pos (X, Y, Z) Position of the rectangle's pivot point relative to the anchors. The pivot point is the location around which the rectangle rotates. Width/Height Width and height of the rectangle. Left, Top, Right, Bottom Positions of the rectangle's edges relative to their anchors. This can be thought of as padding inside the rectangle defined by the anchors. Shown in place of Pos and Width/Height when the anchors are separated (see below). To access these options click the square Anchor Presets box at the top left of the RectTransform component. Anchors The anchor points for the lower left corner and the upper right corner of the rectangle. Min The anchor point for the lower left corner of the rectangle defined as a fraction of the size of the parent rectangle. 0,0 corresponds to anchoring to the lower left corner of the parent, while 1,1 corresponds to anchoring to the upper right corner of the parent. Max The anchor point for the upper right corner of the rectangle defined as a fraction of the size of the parent rectangle. 0,0 corresponds to anchoring to the lower left corner of the parent, while 1,1 corresponds to anchoring to the upper right corner of the parent. Pivot Location of the pivot point around which the rectangle rotates, defined as a fraction of the size of the rectangle itself. 0,0 corresponds to the lower left corner while 1,1 corresponds to the upper right corner. Rotation Angle of rotation (in degrees) of the object around its pivot point along the X, Y and Z axis. Scale Scale factor applied to the object in the X, Y and Z dimensions. Blueprint Mode Edit RectTransforms as if they were not rotated and scaled. This enabled snapping too. Raw Edit Mode When enabled, editing pivot and anchor values will not counter adjust the position and size of the rectangle in order to make it stay in one place. Details Note that some RectTransform calculations are performed at the end of a frame, just before calculating UI vertices, in order to ensure that they are up to date with all the latest changes performed throughout the frame. This means that they haven't yet been calculated for the first time in the Start callback and first Update callback. You can work around this by creating a Start() callback and adding Canvas.ForceUpdateCanvases() method to it. This will force Canvas to be updated not at the end of the frame, but when that method is called. See the Basic Layout page for a full introduction and overview of how to use the Rect Transform."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-CanvasComponents.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-CanvasComponents.html",
    "title": "Canvas Components | FSM Unity Framework",
    "keywords": "Canvas Components All UI Components are placed within a Canvas. Canvas Canvas Scaler Canvas Group Canvas Renderer"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIAutoLayout.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIAutoLayout.html",
    "title": "Auto Layout | FSM Unity Framework",
    "keywords": "Auto Layout The auto layout system provides ways to place elements in nested layout groups such as horizontal groups, vertical groups, or grids. It also allows elements to automatically be sized according to the contained content. Content Size Fitter Layout Element Horizontal Layout Group Vertical Layout Group Grid Layout Group"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIEffects.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIEffects.html",
    "title": "UI Effect Components | FSM Unity Framework",
    "keywords": "UI Effect Components The effects components allow adding simple effects to Text and Image graphics, such as shadow and outline. Shadow Outline Position as UV1"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIInteraction.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIInteraction.html",
    "title": "Interaction Components | FSM Unity Framework",
    "keywords": "Interaction Components The interaction components in the UI system handle interaction, such as mouse or touch events and interaction using a keyboard or controller. Selectable Base Class Button Toggle Toggle Group Slider Scrollbar Scroll Rect InputField"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIVisual.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/comp-UIVisual.html",
    "title": "Visual Components | FSM Unity Framework",
    "keywords": "Visual Components The visual components allow for ease of creation and GUI specific functionality. Text Image Raw Image Mask"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/index.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/index.html",
    "title": "Unity UI: Unity User Interface | FSM Unity Framework",
    "keywords": "Unity UI: Unity User Interface Unity UI is a UI toolkit for developing user interfaces for games and applications. It is a GameObject-based UI system that uses Components and the Game View to arrange, position, and style user interfaces. ​ You cannot use Unity UI to create or change user interfaces in the Unity Editor. This documentation describes Unity UI features such as creating a Canvas, positioning and animating elements, defining user interactions, and sizing layouts automatically."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-AspectRatioFitter.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-AspectRatioFitter.html",
    "title": "Aspect Ratio Fitter | FSM Unity Framework",
    "keywords": "Aspect Ratio Fitter Properties Property: Function: Aspect Mode How the rectangle is resized to enforce the aspect ratio. None Do not make the rect fit the aspect ratio. Width Controls Height The height is automatically adjusted based on the width. Height Controls Width The width is automatically adjusted based on the height. Fit In Parent The width, height, position, and anchors are automatically adjusted to make the rect fit inside the rect of the parent while keeping the aspect ratio. The may be some space inside the parent rect which is not covered by this rect. Envelope Parent The width, height, position, and anchors are automatically adjusted to make the rect cover the entire area of the parent while keeping the aspect ratio. This rect may extend further out than the parent rect. Aspect Ratio The aspect ratio to enforce. This is the width divided by the height. Description The Aspect Ratio Fitter functions as a layout controller that controls the size of its own layout element. It can adjust the height to fit the width or vice versa, or it can make the element fit inside its parent or envelope its parent. The Aspect Ratio Fitter does not take layout information into account such as minimum size and preferred size. It's worth keeping in mind that when a Rect Transform is resized - whether by an Aspect Ratio Fitter or something else - the resizing is around the pivot. This means that the pivot can be used to control the alignment of the rectangle. For example, a pivot placed at the top center will make the rectangle grow evenly to both sides, and only grow downwards while the top edge remain at its position."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Button.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Button.html",
    "title": "Button | FSM Unity Framework",
    "keywords": "Button The Button control responds to a click from the user and is used to initiate or confirm an action. Familiar examples include the Submit and Cancel buttons used on web forms. Properties Property: Function: Interactable Enable Interactable if you want this button to accept input. See API documentation on Interactable for more details. Transition Properties that determine the way the control responds visually to user actions. See Transition Options. Navigation Properties that determine the sequence of controls. See Navigation Options. Events Property: Function: On Click A UnityEvent that Unity invokes when a user clicks the button and releases it. Details The button is designed to initiate an action when the user clicks and releases it. If the mouse is moved off the button control before the click is released, the action does not take place. The button has a single event called On Click that responds when the user completes a click. Typical use cases include: Confirming a decision (eg, starting gameplay or saving a game) Moving to a sub-menu in a GUI Cancelling an action in progress (eg, downloading a new scene)"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-CanvasScaler.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-CanvasScaler.html",
    "title": "Canvas Scaler | FSM Unity Framework",
    "keywords": "Canvas Scaler The Canvas Scaler component is used for controlling the overall scale and pixel density of UI elements in the Canvas. This scaling affects everything under the Canvas, including font sizes and image borders. Properties Property: Function: UI Scale Mode Determines how UI elements in the Canvas are scaled. Constant Pixel Size Makes UI elements retain the same size in pixels regardless of screen size. Scale With Screen Size Makes UI elements bigger the bigger the screen is. Constant Physical Size Makes UI elements retain the same physical size regardless of screen size and resolution. Settings for Constant Pixel Size: Property: Function: Scale Factor Scales all UI elements in the Canvas by this factor. Reference Pixels Per Unit If a sprite has this 'Pixels Per Unit' setting, then one pixel in the sprite will cover one unit in the UI. Settings for Scale With Screen Size: Property: Function: Reference Resolution The resolution the UI layout is designed for. If the screen resolution is larger, the UI will be scaled up, and if it's smaller, the UI will be scaled down. Screen Match Mode A mode used to scale the canvas area if the aspect ratio of the current resolution doesn't fit the reference resolution. Match Width or Height Scale the canvas area with the width as reference, the height as reference, or something in between. Expand Expand the canvas area either horizontally or vertically, so the size of the canvas will never be smaller than the reference. Shrink Crop the canvas area either horizontally or vertically, so the size of the canvas will never be larger than the reference. Match Determines if the scaling is using the width or height as reference, or a mix in between. Reference Pixels Per Unit If a sprite has this 'Pixels Per Unit' setting, then one pixel in the sprite will cover one unit in the UI. Settings for Constant Physical Size: Property: Function: Physical Unit The physical unit to specify positions and sizes in. Fallback Screen DPI The DPI to assume if the screen DPI is not known. Default Sprite DPI The pixels per inch to use for sprites that have a 'Pixels Per Unit' setting that matches the 'Reference Pixels Per Unit' setting. Reference Pixels Per Unit If a sprite has this 'Pixels Per Unit' setting, then its DPI will match the 'Default Sprite DPI' setting. Settings for World Space Canvas (shown when Canvas component is set to World Space): Property: Function: Dynamic Pixels Per Unit The amount of pixels per unit to use for dynamically created bitmaps in the UI, such as Text. Reference Pixels Per Unit If a sprite has this 'Pixels Per Unit' setting, then one pixel in the sprite will cover one unit in the world. If the 'Reference Pixels Per Unit' is set to 1, then the 'Pixels Per Unit' setting in the sprite will be used as-is. Details For a Canvas set to 'Screen Space - Overlay' or 'Screen Space - Camera', the Canvas Scaler UI Scale Mode can be set to Constant Pixel Size, Scale With Screen Size, or Constant Physical Size. Constant Pixel Size Using the Constant Pixel Size mode, positions and sizes of UI elements are specified in pixels on the screen. This is also the default functionality of the Canvas when no Canvas Scaler is attached. However, With the Scale Factor setting in the Canvas Scaler, a constant scaling can be applied to all UI elements in the Canvas. Scale With Screen Size Using the Scale With Screen Size mode, positions and sizes can be specified according to the pixels of a specified reference resolution. If the current screen resolution is larger than the reference resolution, the Canvas will keep having only the resolution of the reference resolution, but will scale up in order to fit the screen. If the current screen resolution is smaller than the reference resolution, the Canvas will similarly be scaled down to fit. If the current screen resolution has a different aspect ratio than the reference resolution, scaling each axis individually to fit the screen would result in non-uniform scaling, which is generally undesirable. Instead of this, the ReferenceResolution component will make the Canvas resolution deviate from the reference resolution in order to respect the aspect ratio of the screen. It is possible to control how this deviation should behave using the Screen Match Mode setting. Constant Physical Size Using the Constant Physical Size mode, positions and sizes of UI elements are specified in physical units, such as millimeters, points, or picas. This mode relies on the device reporting its screen DPI correctly. You can specify a fallback DPI to use for devices that do not report a DPI. World Space For a Canvas set to 'World Space' the Canvas Scaler can be used to control the pixel density of UI elements in the Canvas. Hints See the page Designing UI for Multiple Resolutions for a step by step explanation of how Rect Transform anchoring and Canvas Scaler can be used in conjunction to make UI layouts that adapt to different resolutions and aspect ratios."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ContentSizeFitter.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ContentSizeFitter.html",
    "title": "Content Size Fitter | FSM Unity Framework",
    "keywords": "Content Size Fitter Properties Property: Function: Horizontal Fit How the width is controlled. Unconstrained Do not drive the width based on the layout element. Min Size Drive the width based on the minimum width of the layout element. Preferred Size Drive the width based on the preferred width of the layout element. Vertical Fit How the height is controlled. Unconstrained Do not drive the height based on the layout element. Min Size Drive the height based on the minimum height of the layout element. Preferred Size Drive the height based on the preferred height of the layout element. Description The Content Size Fitter functions as a layout controller that controls the size of its own layout element. The size is determined by the minimum or preferred sizes provided by layout element components on the Game Object. Such layout elements can be Image or Text components, layout groups, or a Layout Element component. It's worth keeping in mind that when a Rect Transform is resized - whether by a Content Size Fitter or something else - the resizing is around the pivot. This means that the direction of the resizing can be controlled using the pivot. For example, when the pivot is in the center, the Content Size Fitter will expand the Rect Transform out equally in all directions. And when the pivot is in the upper left corner, the Content Size Fitter will expand the Rect Transform down and to the right."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Dropdown.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Dropdown.html",
    "title": "Dropdown | FSM Unity Framework",
    "keywords": "Dropdown The Dropdown can be used to let the user choose a single option from a list of options. The control shows the currently chosen option. Once clicked, it opens up the list of options so a new option can be chosen. Upon choosing a new option, the list of closed again, and the control shows the new selected option. The list is also closed if the user clicks on the control itself, or anywhere else inside the Canvas. Properties Property: Function: Interactable Will this component will accept input? See Interactable. Transition Properties that determine the way the control responds visually to user actions. See Transition Options. Navigation Properties that determine the sequence of controls. See Navigation Options. Template The Rect Transform of the template for the dropdown list. See instructions below. Caption Text The Text component to hold the text of the currently selected option. (Optional) Caption Image The Image component to hold the image of the currently selected option. (Optional) Item Text The Text component to hold the text of the item. (Optional) Item Image The Image component to hold the image of the item. (Optional) Value The index of the currently selected option. 0 is the first option, 1 is the second, and so on. Options The list of possible options. A text string and an image can be specified for each option. Events Property: Function: On Value Changed A UnityEvent that is invoked when a user has clicked one of the options in the dropdown list. Details The list of options is specified in the Inspector or can be assigned from code. For each option a text string can be specified, and optionally an image as well, if the Dropdown is setup to support it. The button has a single event called On Value Changed that responds when the user completes a click on one of the options in the list. It supports sending an integer number value that is the index of the selected option. 0 is the first option, 1 is the second, and so on. The template system The Dropdown control is designed to have a child GameObject which serves as a template for the dropdown list that is shown when clicking the dropdown control. The template GameObject is inactive by default, but can be made active while editing the template to better see what's going on. A reference to the template object must be specified in the Template property of the Dropdown component. The template must have a single item in it with a Toggle component on. When the actual dropdown list is created upon clicking the dropdown control, this item is duplicated multiple times, with one copy used for each option in the list. The parent of the item is automatically resized so it can fit all the items inside. The template can be setup in many different ways. The setup used by the GameObject > UI > Dropdown menu item includes a scroll view, such that if there are too many options to show at once, a scrollbar will appear and the user can scroll through the options. This is however not a mandatory part of the template setup. (See the ScrollRect page for more information about setup of Scroll Views.) Setup of text and image support The dropdown supports one text content and one image content for each option. Both text and image is optional. They can only be used if the Dropdown is setup to support it. The dropdown supports text for each option when the Caption Text and Item Text properties are both setup. These are setup by default when using the GameObject > UI > Dropdown menu item. The Caption Text is the Text component to hold the text for the currently selected option. It is typically a child to the Dropdown GameObject. The Item Text is the Text component to hold the text for each option. It is typically a child to the Item GameObject. The dropdown supports an image for each option when the Caption Image and Item Image properties are both setup. These are not setup by default. The Caption Image is the Image component to hold the image for the currently selected option. It is typically a child to the Dropdown GameObject. The Item Image is the Image component to hold the image for each option. It is typically a child to the Item GameObject. The actual text and images used for the dropdowns are specified in the Options property of the Dropdown component, or can be set from code. Placement of the dropdown list The placement of the dropdown list in relation to the dropdown control is determined by the anchoring and pivot of the Rect Transform of the Template. By default, the list will appear below the control. This is achieved by anchoring the template to the bottom of the control. The pivot of the template also needs to be at the top, so that as the template is expanded to accommodate a variable number of option items, it only expands downwards. The Dropdown control has simple logic to prevent that the dropdown is displayed outside the bounds of the Canvas, since this would make it impossible to select certain options. If the dropdown at its default position is not fully within the Canvas rectangle, its position in relation to the control is reversed. For example, a list that is shown below the control by default will be shown above it instead. This logic is quite simple and has certain limitations. The dropdown template needs to be no larger than half the Canvas size minus the size of the dropdown control, otherwise there may not be room for the list at either position if the dropdown control is placed in the middle of the Canvas."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-EventSystem.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-EventSystem.html",
    "title": "Event System Manager | FSM Unity Framework",
    "keywords": "Event System Manager This subsystem is responsible for controlling all the other elements that make up eventing. It coordinates which Input Module is currently active, which GameObject is currently considered 'selected', and a host of other high level Event System concepts. Each 'Update' the Event System receives the call, looks through its Input Modules and figures out which is the Input Module that should be used for this tick. It then delegates the processing to the modules. Properties Property: Function: First Selected The GameObject that was selected first. Send Navigation Events Should the EventSystem allow navigation events (move / submit / cancel). Drag Threshold The soft area for dragging in pixels. Beneath the Properties table is the \"Add Default Input Modules\" button."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-EventTrigger.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-EventTrigger.html",
    "title": "Event Trigger | FSM Unity Framework",
    "keywords": "Event Trigger The Event Trigger receives events from the Event System and calls registered functions for each event. The Event Trigger can be used to specify functions you wish to be called for each Event System event. You can assign multiple functions to a single event and whenever the Event Trigger receives that event it will call those functions. Note that attaching an Event Trigger component to a GameObject will make that object intercept all events, and no event bubbling will occur from this object! Events Each of the Supported Events can optionally be included in the Event Trigger by clicking the Add New Event Type button."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-GraphicRaycaster.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-GraphicRaycaster.html",
    "title": "Graphic Raycaster | FSM Unity Framework",
    "keywords": "Graphic Raycaster The Graphic Raycaster is used to raycast against a Canvas. The Raycaster looks at all Graphics on the canvas and determines if any of them have been hit. The Graphic Raycaster can be configured to ignore backfacing Graphics as well as be blocked by 2D or 3D objects that exist in front of it. A manual priority can also be applied if you want processing of this element to be forced to the front or back of the Raycasting. Properties Property: Function: Ignore Reversed Graphics Should graphics facing away from the raycaster be considered? Blocked Objects Type of objects that will block graphic raycasts. Blocking Mask Type of objects that will block graphic raycasts."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-GridLayoutGroup.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-GridLayoutGroup.html",
    "title": "Grid Layout Group | FSM Unity Framework",
    "keywords": "Grid Layout Group The Grid Layout Group component places its child layout elements in a grid. Properties Property: Function: Padding The padding inside the edges of the layout group. Cell Size The size to use for each layout element in the group. Spacing The spacing between the layout elements. Start Corner The corner where the first element is located. Start Axis Which primary axis to place elements along. Horizontal will fill an entire row before a new row is started. Vertical will fill an entire column before a new column is started. Child Alignment The alignment to use for the layout elements if they don't fill out all the available space. Constraint Constraint the grid to a fixed number of rows or columns to aid the auto layout system. Description Unlike other layout groups, the Grid Layout Group ignores the minimum, preferred, and flexible size properties of its contained layout elements and instead assigns a fixed size to all of them which is defined with the Cell Size property of the Grid Layout Group itself. Grid Layout Group and auto layout There are special considerations to be aware of when using the Grid Layout Group as part of an auto layout setup, such as using it with a Content Size Fitter. The auto layout system calculates the horizontal and vertical sizes independently. This can be at odds with the Grid Layout Group, where the number of rows depends on the number of columns and vice versa. For any given number of cells, there are different combinations of row count and column count that can make the grid fit its content. In order to aid the layout system, you can specify that you intent the table to have a fixed number of columns or rows by using the Constraint property. Here are suggested ways of using the Layout System with a Content Size Fitter: Flexible width and fixed height To setup a grid with a flexible width and fixed height, where the grid expands horizontally as more elements are added, you can set these properties as follows: Grid Layout Group Constraint: Fixed Row Count Content Size Fitter Horizontal Fit: Preferred Size Content Size Fitter Vertical Fit: Preferred Size or Unconstrained If unconstrained Vertical Fit is used, it's up to you to give the grid a height that is big enough to fit the specified row count of cells. Fixed width and flexible height To setup a grid with a fixed width and flexible height, where the grid expands vertically as more elements are added, you can set these properties as follows: Grid Layout Group Constraint: Fixed Column Count Content Size Fitter Horizontal Fit: Preferred Size or Unconstrained Content Size Fitter Vertical Fit: Preferred Size If unconstrained Horizontal Fit is used, it's up to you to give the grid a width that is big enough to fit the specified column count of cells. Both flexible width and height If you want a grid with both a flexible width and height you can do that, but you will have no control over the specific number of rows and columns. The grid will attempt to make the row and column count approximately the same. You can set these properties as follows: Grid Layout Group Constraint: Flexible Content Size Fitter Horizontal Fit: Preferred Size Content Size Fitter Vertical Fit: Preferred Size"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-HorizontalLayoutGroup.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-HorizontalLayoutGroup.html",
    "title": "Horizontal Layout Group | FSM Unity Framework",
    "keywords": "Horizontal Layout Group The Horizontal Layout Group component places its child layout elements next to each other, side by side. Their widths are determined by their respective minimum, preferred, and flexible widths according to the following model: The minimum widths of all the child layout elements are added together and the spacing between them is added as well. The result is the mimimum width of the Horizontal Layout Group. The preferred widths of all the child layout elements are added together and the spacing between them is added as well. The result is the preferred width of the Horizontal Layout Group. If the Horizontal Layout Group is at its minimum width or smaller, all the child layout elements will also have their minimum width. The closer the Horizontal Layout group is to its preferred width, the closer each child layout element will also get to their preferred width. If the Horizontal Layout Group is wider than its preferred width, it will distribute the extra available space proportionally to the child layout elements according to their respective flexible widths. For more information about minimum, preferred, and flexible width, see the documentation on Auto Layout. Properties Property: Function: Padding The padding inside the edges of the layout group. Spacing The spacing between the layout elements. Child Alignment The alignment to use for the child layout elements if they don't fill out all the available space. Control Child Size Whether the Layout Group controls the width and height of its child layout elements. Use Child Scale Whether the Layout Group considers the scale of its child layout elements when sizing and laying out elements. Width and Height correspond to the Scale > X and Scale > Y values in each child layout element's Rect Transform component. You cannot animate the Scale values using the Animator Controller Child Force Expand Whether to force the child layout elements to expand to fill additional available space."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Image.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Image.html",
    "title": "Image | FSM Unity Framework",
    "keywords": "Image The Image control displays a non-interactive image to the user. You can use this for purposes such as decorations or icons, and you can change the image from a script to reflect changes in other controls. The control is similar to the Raw Image control, but offers more options for animating the image and accurately filling the control rectangle. However, the Image control requires its Texture to be a Sprite, while the Raw Image can accept any Texture. Properties Property: Function: Source Image The Texture that represents the image to display (which must be imported as a Sprite). Color The color to apply to the image. Material The Material to use for rendering the image. Raycast Target Enable Raycast Target if you want Unity to consider the image a target for raycasting. Preserve Aspect Ensure the image retains its existing dimension. Set Native Size Set the dimensions of the image box to the original pixel size of the Texture. You must import the image to display as a Sprite to work with the Image control."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-InputField.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-InputField.html",
    "title": "Input Field | FSM Unity Framework",
    "keywords": "Input Field An Input Field is a way to make the text of a Text Control editable. Like the other interaction controls, it's not a visible UI element in itself and must be combined with one or more visual UI elements in order to be visible. Properties Property: Function: Interactable A boolean that determines if the Input Field can be interacted with or not. |Transition ||Transitions are used to set how the input field transitions when Normal, Highlighted, Pressed or Disabled. | |Navigation || Properties that determine the sequence of controls. See Navigation Options.| |TextComponent ||A reference to the Text element used as the contents of the Input Field| |Text ||Starting Value. The initial text placed in the field before editing begins. | |Character Limit ||The value of the maximum number of characters that can be entered into the input field.| |Content Type ||Define the type(s) of characters that your input field accepts| | |Standard |Any character can be entered.| | |Autocorrected |The autocorrection determines whether the input tracks unknown words and suggests a more suitable replacement candidate to the user, replacing the typed text automatically unless the user explicitly overrides the action.| | |Integer Number |Allow only whole numbers to be entered.| | |Decimal Number |Allow only numbers and a single decimal point to be entered.| | |Alphanumeric |Allow both letters and numbers. Symbols cannot be entered.| | |Name |Automatically capitalizes the first letter of each word. Note that the user can circumvent the capitalization rules using the Delete key.| | |Email Address |Allows you to enter an Alphanumeric string consisting of a maximum of one @ sign. periods/baseline dots cannot be entered next to each other. | | |Password* |Conceals the characters inputed with an asterisk. Allows symbols.| | |Pin |Conceals the characters inputed with an asterisk. Only allows only whole numbers to be entered.| | |Custom |Allows you to customise the Line Type, Input Type, Keyboard Type and Character Validation.| |Line Type ||Defines how text is formatted inside the text field.| | |Single Line |Only allows text to be on a single line.| | |Multi Line Submit |Allows text to use multiple lines. Only uses a new line when needed.| | |Multi Line Newline |Allows text to use multiple lines. User can use a newline by pressing the return key.| |Placeholder ||This is an optional ‘empty’ Graphic to show that the Input Field is empty of text. Note that this ‘empty' graphic still displays even when the Input Field is selected (that is; when there is focus on it). eg; \"Enter text...\".| |Caret Blink Rate ||Defines the blink rate for the mark placed on the line to indicate a proposed insertion of text.| |Selection Color ||The background color of the selected portion of text.| Hide Mobile Input Hides the native input field attached to the onscreen keyboard on mobile devices. Note that this only works on iOS and Android devices. Events Property: Function: On Value Change A UnityEvent that is invoked when the text content of the Input Field changes. The event can send the current text content as a string type dynamic argument. End Edit A UnityEvent that is invoked when the user finishes editing the text content either by submitting or by clicking somewhere that removes the focus from the Input Field. The event can send the current text content as a string type dynamic argument. Details The Input Field script can be added to any existing Text control object from the menu (Component > UI > Input Field). Having done this, you should also drag the object to the Input Field's Text property to enable editing. The Text property of the Text control itself will change as the user types and the value can be retrieved from a script after editing. Note that Rich Text is intentionally not supported for editable Text controls; the field will apply any Rich Text markup instantly when typed but the markup essentially \"disappears\" and there is no subsequent way to change or remove the styling. Hints To obtain the text of the Input Field, use the text property on the InputField component itself, not the text property of the Text component that displays the text. The text property of the Text component may be cropped or may consist of asterisks for passwords. Limitations On iOS when an external keyboard is connected, the onscreen keyboard will be hidden by the OS but the caret will not appear in the InputField. This is due to a lack of external keyboard support on iOS 13 and older."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-LayoutElement.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-LayoutElement.html",
    "title": "Layout Element | FSM Unity Framework",
    "keywords": "Layout Element If you want to override the minimum, preferred, or flexible size of a layout element, you can do that by adding a Layout Element component to the GameObject. A layout controller allocates width or height to a layout element in the following order: First, the layout controller allocates the minimum size properties (Min Width, Min Height). If there is sufficient available space, the layout controller allocates the preferred size properties (Preferred Width, Preferred Height). If there is additional available space, the layout controller allocates the flexible size properties (Flexible Width, Flexible Height). For more information about minimum, preferred, and flexible size, see documentation on Auto Layout. Properties When you enable a width or height property, a value field appears next to it. Use this value field to enter the exact value for the width or height. Min and Preferred sizes are in regular units, while the Flexible sizes are in relative units. Property: Function: Ignore Layout When enabled, the layout system ignores this layout element. Min Width The minimum width this layout element should have. Min Height The minimum height this layout element should have. Preferred Width The preferred width this layout element should have before additional available width is allocated. Preferred Height The preferred height this layout element should have before additional available height is allocated. Flexible Width The relative amount of additional available width this layout element should fill out relative to its siblings. Flexible Height The relative amount of additional available height this layout element should fill out relative to its siblings. Layout Priority The layout priority for this component. If a GameObject has more than one component with layout properties (for example, an Image component and a LayoutElement component), the layout system uses the property values from the component with the highest Layout Priority. If the components have the same Layout Priority, the layout system uses the highest value for each property, regardless of which component it comes from. Description The Layout Element component lets you override the values for one or more of the layout properties. Enable the checkbox for a property you want to override and then specify the value you want to override with. Minimum and preferred sizes are defined in regular units, while the flexible sizes are defined in relative units. If any layout element has flexible size greater than zero, it means that all the available space will be filled out. The relative flexible size values of the siblings determines how big a proportion of the available space each sibling fills out. Most commonly, flexible width and height is set to just 0 or 1. Specifying both a preferred size and a flexible size can make sense in certain cases. Flexible sizes are only allocated after all preferred sizes have been fully allocated. Thus, a layout element which has a flexible size specified but no preferred size will keep its minimum size until other layout elements have grown to their full preferred size, and only then begin to grow based on additional available space. By also specifying a flexible size, this can be avoided and the element can grow to its preferred size in tandem with the other layout elements that have preferred sizes, and then grow further once all flexible sizes have been allocated."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Mask.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Mask.html",
    "title": "Mask | FSM Unity Framework",
    "keywords": "Mask A Mask is not a visible UI control but rather a way to modify the appearance of a control's child elements. The mask restricts (ie, \"masks\") the child elements to the shape of the parent. So, if the child is larger than the parent then only the part of the child that fits within the parent will be visible. Properties Property: Function: Show Graphic Should the graphic of the masking (parent) object be drawn with alpha over the child object? Description A common use of a Mask is to show a small section of a large Image, using say a Panel object (menu: GameObject > Create UI > Panel) as a \"frame\". You can achieve this by firstly making the Image a child of the Panel object. You should position the Image so that the area that should be visible is directly behind the Panel area. Then, add a Mask component to the Panel. The areas of the child Image outside the panel will become invisible since they are masked by the shape of the Panel. If the image is then moved around then only the part revealed by the Panel will be visible. The movement could be controlled by Scrollbars to create a scrollable viewer for a map, say. Implementation Masking is implemented using the stencil buffer of the GPU. *The first Mask element writes a 1 to the stencil buffer *All elements below the mask check when rendering, and only render to areas where there is a 1 in the stencil buffer *Nested Masks will write incremental bit masks into the buffer, this means that renderable children need to have the logical & of the stencil values to be rendered."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Outline.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Outline.html",
    "title": "Outline | FSM Unity Framework",
    "keywords": "Outline The Outline component adds a simple outline effect to graphic components such as Text or Image. It must be on the same GameObject as the graphic component. Properties Property: Function: Effect Color The color of the outline. Effect Distance The distance of the outline effect horizontally and vertically. Use Graphic Alpha Multiplies the color of the graphic onto the color of the effect."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Physics2DRaycaster.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Physics2DRaycaster.html",
    "title": "Physics 2D Raycaster | FSM Unity Framework",
    "keywords": "Physics 2D Raycaster The 2D Raycaster raycasts against 2D objects in the scene. This allows messages to be sent to 2D physics objects that implement event interfaces. The Camera GameObject needs to be used and will be added to the GameObject if the Physics 3D Raycaster is not added to the Camera GameObject. For more Raycaster information see Raycasters. Properties Property: Function: Event Camera The camera that will generate rays for this raycaster. Priority Priority of the caster relative to other casters. Sort Order Priority Priority of the raycaster based upon sort order. Render Order Priority Priority of the raycaster based upon render order."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-PhysicsRaycaster.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-PhysicsRaycaster.html",
    "title": "Physics Raycaster | FSM Unity Framework",
    "keywords": "Physics Raycaster The Raycaster raycasts against 3D objects in the scene. This allows messages to be sent to 3D physics objects that implement event interfaces. Properties Property: Function: Depth Get the depth of the configured camera. Event Camera Get the camera that is used for this module. Event Mask Logical and of Camera mask and eventMask. Final Event Mask Logical and of Camera mask and eventMask."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-PositionAsUV1.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-PositionAsUV1.html",
    "title": "Position as UV1 | FSM Unity Framework",
    "keywords": "Position as UV1 This adds a simple Position as UV1 effect to text and image graphics. Properties Property: Function: Script"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-RawImage.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-RawImage.html",
    "title": "Raw Image | FSM Unity Framework",
    "keywords": "Raw Image The Raw Image control displays a non-interactive image to the user. You can use this for purposes such as decorations or icons, and you can change the image from a script to reflect changes in other controls. The control is similar to the Image control, but offers more options for animating the image and accurately filling the control rectangle. However, the Image control requires its Texture to be a Sprite, while the Raw Image can accept any Texture. Properties Property: Function: Texture The texture that represents the image to display. Color The color to apply to the image. Material The Material to use for rendering the image. Raycast Target Enable Raycast Target if you want Unity to consider the image a target for raycasting. UV Rectangle The image's offset and size within the control rectangle, given in normalized coordinates (range 0.0 to 1.0). The edges of the image are stretched to fill the space around the UV rectangle. Details Since the Raw Image does not require a sprite texture, you can use it to display any texture available to the Unity player. For example, you might show an image downloaded from a URL using the WWW class or a texture from an object in a game. The UV Rectangle properties allow you to display a small section of a larger image. The X and Y coordinates specify which part of the image is aligned with the bottom left corner of the control. For example, an X coordinate of 0.25 will cut off the leftmost quarter of the image. The W and H (ie, width and height) properties indicate the width and height of the section of image that will be scaled to fit the control rectangle. For example, a width and height of 0.5 will scale a quarter of the image area up to the control rectangle. By changing these properties, you can zoom and scale the image as desired (see also the Scrollbar control)."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-RectMask2D.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-RectMask2D.html",
    "title": "RectMask2D | FSM Unity Framework",
    "keywords": "RectMask2D A RectMask2D is a masking control similar to the Mask control. The mask restricts the child elements to the rectangle of the parent element. Unlike the standard Mask control it has some limitations, but it also has a number of performance benefits. Description A common use of a RectMask2D is to show small sections of a larger area. Using the RectMask2D to frame this area. The limitations of RectMask2D control are: It only works in 2D space It will not properly mask elements that are not coplanar The advantages of RectMask2D are: It does not use the stencil buffer No extra draw calls No material changes Fast performance"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ScrollRect.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ScrollRect.html",
    "title": "Scroll Rect | FSM Unity Framework",
    "keywords": "Scroll Rect A Scroll Rect can be used when content that takes up a lot of space needs to be displayed in a small area. The Scroll Rect provides functionality to scroll over this content. Usually a Scroll Rect is combined with a Mask in order to create a scroll view, where only the scrollable content inside the Scroll Rect is visible. It can also additionally be combined with one or two Scrollbars that can be dragged to scroll horizontally or vertically. Properties Property: Function: Content This is a reference to the Rect Transform of the UI element to be scrolled, for example a large image. Horizontal Enables horizontal scrolling Vertical Enables vertical scrolling Movement Type Unrestricted, Elastic or Clamped. Use Elastic or Clamped to force the content to remain within the bounds of the Scroll Rect. Elastic mode bounces the content when it reaches the edge of the Scroll Rect Elasticity This is the amount of bounce used in the elasticity mode. Inertia When Inertia is set the content will continue to move when the pointer is released after a drag. When Inertia is not set the content will only move when dragged. Deceleration Rate When Inertia is set the deceleration rate determines how quickly the contents stop moving. A rate of 0 will stop the movement immediately. A value of 1 means the movement will never slow down. Scroll Sensitivity The sensitivity to scroll wheel and track pad scroll events. Viewport Reference to the viewport Rect Transform that is the parent of the content Rect Transform. Horizontal Scrollbar Optional reference to a horizontal scrollbar element. Visibility Whether the scrollbar should automatically be hidden when it isn't needed, and optionally expand the viewport as well. Spacing The space between the scrollbar and the viewport. Vertical Scrollbar Optional reference to a vertical scrollbar element. Visibility Whether the scrollbar should automatically be hidden when it isn't needed, and optionally expand the viewport as well. Spacing The space between the scrollbar and the viewport. Events Property: Function: On Value Changed A UnityEvent that is invoked when the scroll position of the Scroll Rect changes. The event can send the current scroll position as a Vector2 type dynamic argument. Details The important elements in a scroll view are the viewport, the scrolling content, and optionally one or two scrollbars. The root GameObject has the Scroll Rect component. The viewport has a Mask component. The viewport can either be the root GameObject, or a separate GameObject that's a child to the root. If auto-hiding scrollbars are used, it must be a child. The viewport Rect Transform needs to be referenced in the Viewport property of the Scroll Rect. All the scrolling content must be children of a single content GameObject that is a child to the viewport. The content Rect Transform needs to be referenced in the Content property of the Scroll Rect. The scrollbars - if used - are children to the root GameObject. See the Scrollbar page for more details on the setup of a scrollbar and see the section Scrollbar setup below for information about setup of scrollbars with a scroll view. This image shows a setup where the viewport is a child to the scroll view root. This is the default used when using the GameObject > UI > Scroll View menu option. To scroll content, the input must be received from inside the bounds of the ScrollRect, not on the content itself. Take care when using Unrestricted scrolling movement as it is possible to lose control of the content in an irretrievable way. When using Elastic or Constrained movement it is best to position the content so that it starts within the bounds of the ScrollRect, or undesirable behaviour may occur as the RectTransform tries to bring the content back within its bounds. Scrollbar setup Optionally, the Scroll Rect can be linked to a horizontal and/or a vertical Scrollbar. These are typically placed in the hierarchy as siblings to the viewport, and when present, should be dragged into the Horizontal Scrollbar and Vertical Scrollbar properties of the Scroll Rect, respectively. Note that the Direction property on such a horizontal Scrollbar should be set to Left To Right, and on the vertical Scrollbar to Bottom To Top. The scrollbars can optionally have auto-hiding behaviour that hides the scrollbars if the content doesn't need to scroll because it isn't larger than the viewport. Note that the auto-hiding only ever happens in Play Mode. In Edit Mode the scrollbars are always shown. This prevents marking the scene as dirty when it shouldn't be, and also help authoring content with proportions that there's room for even when the scrollbars are shown. If one or both scrollbars have their visibility behaviour set to Auto Hide And Expand View, the viewport is automatically expanded when the scrollbars are hidden in order to take up the extra room where the scrollbars would otherwise have been. With this setup, the position and size of the view is driven by the Scroll Rect, and the width of the horizontal scrollbar as well as the height of the vertical scrollbar is driven as well. With this setup the viewport as well as the scrollbars must be children to the Scroll Rect root GameObject. Hints The pivot and anchors of the content RectTransform can be used to determine how the content is aligned inside the scroll view if the content grows or shrinks. If the content should stay aligned with the top, set the anchors to the top of the parent, and set the pivot to the top position. See the page Making UI elements fit the size of their content for information about how to make the content Rect Transform automatically resize to fit the content."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Scrollbar.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Scrollbar.html",
    "title": "Scrollbar | FSM Unity Framework",
    "keywords": "Scrollbar The Scrollbar control allows the user to scroll an image or other view that is too large to see completely. Note that the similar Slider control is used for selecting numeric values rather than scrolling. Familiar examples include the vertical Scrollbar at the side of a text editor and the vertical and horizontal pair of bars for viewing a section of a large image or map. Properties Property: Function: Interactable Will this component accept input? See Interactable. Transition Properties that determine the way the control responds visually to user actions. See Transition Options. Navigation Properties that determine the sequence of controls. See Navigation Options. Fill Rect The graphic used for the background area of the control. Handle Rect The graphic used for the sliding \"handle\" part of the control Direction The direction in which the Scrollbar's value will increase when the handle is dragged. The options are Left To Right, Right To Left, Bottom To Top and Top To Bottom. Value Initial position value of the Scrollbar, in the range 0.0 to 1.0. Size Fractional size of the handle within the Scrollbar, in the range 0.0 to 1.0. Number Of Steps The number of distinct scroll positions allowed by the Scrollbar. Events Property: Function: On Value Changed A UnityEvent that is invoked when the current value of the Scrollbar changes. The event can send the value as a float type dynamic argument. Details The value of a Scrollbar is determined by the position of the handle along its length with the value being reported as a fraction between the extreme ends. For example, the default left-to-right bar has a value of 0.0 at the left end, 1.0 at the right end and 0.5 indicates the halfway point. A scrollbar can be oriented vertically by choosing Top To Bottom or Bottom To Top for the Direction property. A significant difference between the Scrollbar and the similar Slider control is that the Scrollbar's handle can change in size to represent the distance of scrolling available; when the view can scroll only a short way, the handle will fill up most of the bar and only allow a slight shift either direction. The Scrollbar has a single event called On Value Changed that responds as the user drags the handle. The current value is passed to the even function as a float parameter. Typical use cases for a scrollbar include: Scrolling a piece of text vertically. Scrolling a timeline horizontally. Used as a pair, scrolling a large image both horizontally and vertically to view a zoomed section. The size of the handle changes to indicate the degree of zooming and therefore the available distance for scrolling."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Selectable.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Selectable.html",
    "title": "Selectable Base Class | FSM Unity Framework",
    "keywords": "Selectable Base Class The Selectable Class is the base class for all the interaction components and it handles the items that are in common. Property: Function: Interactable This determines if this component will accept input. When it is set to false interaction is disabled and the transition state will be set to the disabled state. Transition Within a selectable component there are several Transition Options depending on what state the selectable is currently in. The different states are: normal, highlighted, pressed and disabled. Navigation There are also a number of Navigation Options to control how keyboard navigation of the controls is implemented."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-SelectableNavigation.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-SelectableNavigation.html",
    "title": "Navigation Options | FSM Unity Framework",
    "keywords": "Navigation Options Property: Function: Navigation The Navigation options refers to how the navigation of UI elements in play mode will be controlled. None No keyboard navigation. Also ensures that it does not receive focus from clicking/tapping on it. Horizontal Navigates Horizontally. Vertical Navigates Vertically. Automatic Automatic Navigation. Explicit In this mode you can explicitly specify where the control navigates to for different arrow keys. Visualize Selecting Visualize gives you a visual representation of the navigation you have set up in the scene window. See below. In the above visualization mode, the arrows indicate how the change of focus is set up for the collection of controls as a group. That means - for each individual UI control - you can see which UI control will get focus next, if the user presses an arrow key when the given control has focus. So in the example shown above, If the \"button\" has focus and the user presses the right arrow key, the first (left-hand) vertical slider will then become focused. Note that the vertical sliders can't be focused-away-from using up or down keys, because they control the value of the slider. The same is true of the horizontal sliders and the left/right arrow keys."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-SelectableTransition.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-SelectableTransition.html",
    "title": "Transition Options | FSM Unity Framework",
    "keywords": "Transition Options Within a selectable component there are several transition options depending on what state the selectable is currently in. The different states are: normal, highlighted, pressed and disabled. Transition Options: Function: None This option is for the button to have no state effects at all. Color Tint Changes the colour of the button depending on what state it is in. It is possible to select the colour for each individual state. It is also possible to set the Fade Duration between the different states. The higher the number is, the slower the fade between colors will be. Sprite Swap Allows different sprites to display depending on what state the button is currently in, the sprites can be customised. Animation Allows animations to occur depending on the state of the button, an animator component must exist in order to use animation transition. It’s important to make sure root motion is disabled. To create an animation controller click on generate animation (or create your own) and make sure that an animation controller has been added to the animator component of the button. Each Transition option (except None) provides additional options for controlling the transitions. We'll go into details with those in each of the sections below. Color Tint Property: Function: Target Graphic The graphic used for the interaction component. Normal Color The normal color of the control Highlighted Color The color of the control when it is highlighted Pressed Color The color of the control when it is pressed Disabled Color The color of the control when it is disabled Color Multiplier This multiplies the tint color for each transition by its value. With this you can create colors greater than 1 to brighten the colors (or alpha channel) on graphic elements whose base color is less than white (or less then full alpha). Fade Duration The time taken, in seconds, to fade from one state to another Sprite Swap Property: Function: Target Graphic The normal sprite to use Highlighted Sprite Sprite to use when the control is highlighted Pressed Sprite Sprite to use when the control is pressed Disabled Sprite Sprite to use when the control is disabled Animation Property: Function: Normal Trigger The normal animation trigger to use Highlighted Trigger Trigger to use when the control is highlighted Pressed Trigger Trigger to use when the control is pressed Disabled Trigger Trigger to use when the control is disabled"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Shadow.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Shadow.html",
    "title": "Shadow | FSM Unity Framework",
    "keywords": "Shadow The Shadow component adds a simple outline effect to graphic components such as Text or Image. It must be on the same GameObject as the graphic component. Properties Property: Function: Effect Color The color of the shadow. Effect Distance The offset of the shadow expressed as a vector. Use Graphic Alpha Multiplies the color of the graphic onto the color of the effect."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Slider.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Slider.html",
    "title": "Slider | FSM Unity Framework",
    "keywords": "Slider The Slider control allows the user to select a numeric value from a predetermined range by dragging the mouse. Note that the similar ScrollBar control is used for scrolling rather than selecting numeric values. Familiar examples include difficulty settings in games and brightness settings in image editors. Properties Property: Function: Interactable Will this component accept input? See Interactable. Transition Properties that determine the way the control responds visually to user actions. See Transition Options. Navigation Properties that determine the sequence of controls. See Navigation Options. Fill Rect The graphic used for the fill area of the control. Handle Rect The graphic used for the sliding \"handle\" part of the control Direction The direction in which the slider's value will increase when the handle is dragged. The options are Left To Right, Right To Left, Bottom To Top and Top To Bottom. Min Value The value of the slider when the handle is at its extreme lower end (determined by the Direction property). Max Value The value of the slider when the handle is at its extreme upper end (determined by the Direction property). Whole Numbers Should the slider be constrained to integer values? Value Current numeric value of the slider. If the value is set in the inspector it will be used as the initial value, but this will change at runtime when the value changes. Events Property: Function: On Value Changed A UnityEvent that is invoked when the current value of the Slider has changed. The event can send the current value as a float type dynamic argument. The value is passed as a float type regardless of whether the Whole Numbers property is enabled. Details The value of a Slider is determined by the position of the handle along its length. The value increases from the Min Value up to the Max Value in proportion to the distance the handle is dragged. The default behaviour is for the slider to increase from left to right but it is also possible to reverse this behavior using the Direction property. You can also set the slider to increase vertically by selecting Bottom To Top or Top To Bottom for the Direction property. The slider has a single event called On Value Changed that responds as the user drags the handle. The current numeric value of the slider is passed to the function as a float parameter. Typical use cases include: Choosing a level of difficulty in a game, brightness of a light, etc. Setting a distance, size, time or angle."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-StandaloneInputModule.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-StandaloneInputModule.html",
    "title": "Standalone Input Module | FSM Unity Framework",
    "keywords": "Standalone Input Module The module is designed to work as you would expect a controller / mouse input to work. Events for button presses, dragging, and similar are sent in response to input. The module sends pointer events to components as a mouse / input device is moved around, and uses the Graphics Raycaster and Physics Raycaster to calculate which element is currently pointed at by a given pointer device. You can configure these raycasters to detect or ignore parts of your Scene, to suit your requirements. The module sends move events and submit / cancel events in response to Input tracked via the Input window. This works for both keyboard and controller input. The tracked axis and keys can be configured in the module's inspector. Properties Property: Function: Horizontal Axis Type the desired manager name for the horizontal axis button. Vertical Axis Type the desired manager name for the vertical axis. Submit Button Type the desired manager name for the Submit button. Cancel Button Type the desired manager name for the Cancel button. Input Actions Per Second Number of keyboard/controller inputs allowed per second. Repeat Delay Delay in seconds before the input actions per second repeat rate takes effect. Force Module Active Enable this property to force this Standalone Input Module to be active. Details The module uses: Vertical / Horizontal axis for keyboard and controller navigation Submit / Cancel button for sending submit and cancel events Has a timeout between events to only allow a maximum number of events a second. The flow for the module is as follows Send a Move event to the selected object if a valid axis from the Input window is entered Send a submit or cancel event to the selected object if a submit or cancel button is pressed Process Mouse input If it is a new press Send PointerEnter event (sent to every object up the hierarchy that can handle it) Send PointerPress event Cache the drag handler (first element in the hierarchy that can handle it) Send BeginDrag event to the drag handler Set the 'Pressed' object as Selected in the event system If this is a continuing press Process movment Send DragEvent to the cached drag handler Handle PointerEnter and PointerExit events if touch moves between objects If this is a release Send PointerUp event to the object that received the PointerPress If the current hover object is the same as the PointerPress object send a PointerClick event Send a Drop event if there was a drag handler cached Send a EndDrag event to the cached drag handler Process scroll wheel events"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Text.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Text.html",
    "title": "Text | FSM Unity Framework",
    "keywords": "Text The Text control displays a non-interactive piece of text to the user. This can be used to provide captions or labels for other GUI controls or to display instructions or other text. Properties Property: Function: Text The text displayed by the control. Character Font The Font used to display the text. Font Style The style applied to the text. The options are Normal, Bold, Italic and Bold And Italic. Font Size The size of the displayed text. Line Spacing The vertical separation between lines of text. Rich Text Should markup elements in the text be interpreted as Rich Text styling? Paragraph Alignment The horizontal and vertical alignment of the text. Align by Geometry Use the extents of glyph geometry to perform horizontal alignment rather than glyph metrics. Horizontal Overflow The method used to handle the situation where the text is too wide to fit in the rectangle. The options are Wrap and Overflow. Vertical Overflow The method used to handle the situation where wrapped text is too tall to fit in the rectangle. The options are Truncate and Overflow. Best Fit Should Unity ignore the size properties and simply try to fit the text to the control's rectangle? Color The color used to render the text. Material The Material used to render the text. A default text element looks like this: Details Some controls (such as Buttons and Toggles) have textual descriptions built-in. For controls that have no implicit text (such as Sliders), you can indicate the purpose using a label created with a Text control. Text is also useful for lists of instructions, story text, conversations and legal disclaimers. The Text control offers the usual parameters for font size, style, etc, and text alignment. When the Rich Text option is enabled, markup elements within the text will be treated as styling information, so you can have just a single word or short section in boldface or in a different color, say (see the page about Rich Text for details of the markup scheme). Hints See the Effects page for how to apply a simple shadow or outline effect to the text."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Toggle.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-Toggle.html",
    "title": "Toggle | FSM Unity Framework",
    "keywords": "Toggle The Toggle control is a checkbox that allows the user to switch an option on or off. Properties Property: Function: Interactable Will this component will accept input? See Interactable. Transition Properties that determine the way the control responds visually to user actions. See Transition Options. Navigation Properties that determine the sequence of controls. See Navigation Options. Is On Is the toggle switched on from the beginning? Toggle Transition The way the toggle reacts graphically when its value is changed. The options are None (ie, the checkmark simply appears or disappears) and Fade (ie, the checkmark fades in or out). Graphic The image used for the checkmark. Group The Toggle Group (if any) that this Toggle belongs to. Events Property: Function: On Value Changed A UnityEvent that is invoked when the Toggle is clicked. The event can send the current state as a bool type dynamic argument. Details The Toggle control allows the user to switch an option on or off. You can also combine several toggles into a Toggle Group in cases where only one of a set of options should be on at once. The Toggle has a single event called On Value Changed that responds when the user changes the current value. The new value is passed to the event function as a boolean parameter. Typical use cases for Toggles include: Switching an option on or off (eg, playing music during a game). Letting the user confirm they have read a legal disclaimer. Choosing one of a set of options (eg, a day of the week) when used in a Toggle Group. Note that the Toggle is a parent that provides a clickable area to children. If the Toggle has no children (or they are disabled) then it is not clickable."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ToggleGroup.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-ToggleGroup.html",
    "title": "Toggle Group | FSM Unity Framework",
    "keywords": "Toggle Group A Toggle Group is not a visible UI control but rather a way to modify the behavior of a set of Toggles. Toggles that belong to the same group are constrained so that only one of them can switched on at a time - pressing one of them to switch it on automatically switches the others off. Properties Property: Function: Allow Switch Off Is it allowed that no toggle is switched on? If this setting is enabled, pressing the toggle that is currently switched on will switch it off, so that no toggle is switched on. If this setting is disabled, pressing the toggle that is currently switched on will not change its state. Description The Toggle Group is setup by dragging the Toggle Group object to the Group property of each of the Toggles in the group. Toggle Groups are useful anywhere the user must make a choice from a mutually exclusive set of options. Common examples include selecting player character types, speed settings (slow, medium, fast, etc), preset colors and days of the week. You can have more than one Toggle Group object in the scene at a time, so you can create several separate groups if necessary. Unlike other UI elements, an object with a Toggle Group component does not need to be a child of a Canvas object, although the Toggles themselves still do. Note that the Toggle Group will not enforce its constraint right away if multiple toggles in the group are switched on when the scene is loaded or when the group is instantiated. Only when a new toggle is swicthed on are the others switched off. This means it's up to you to ensure that only one toggle is switched on from the beginning."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-TouchInputModule.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-TouchInputModule.html",
    "title": "Touch Input Module | FSM Unity Framework",
    "keywords": "Touch Input Module Note: TouchInputModule is obsolete. Touch input is now handled in StandaloneInputModule. This module is designed to work with touch devices. It sends pointer events for touching and dragging in response to user input. The module supports multitouch. The module uses the scene configured Raycasters to calculate what element is currently being touched over. A raycast is issued for each current touch. Properties Property: Function: Force Module Active Forces this module to be active. Details The flow for the module is as follows: For each touch event If it is a new press Send PointerEnter event (sent to every object up the hierarchy that can handle it) Send PointerPress event Cache the drag handler (first element in the hierarchy that can handle it) Send BeginDrag event to the drag handler Set the 'Pressed' object as Selected in the event system If this is a continuing press Process movement Send DragEvent to the cached drag handler Handle PointerEnter and PointerExit events if touch moves between objects If this is a release Send PointerUp event to the object that received the PointerPress If the current hover object is the same as the PointerPress object send a PointerClick event Send a Drop event if there was a drag handler cached Send a EndDrag event to the cached drag handler"
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-VerticalLayoutGroup.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/script-VerticalLayoutGroup.html",
    "title": "Vertical Layout Group | FSM Unity Framework",
    "keywords": "Vertical Layout Group The Vertical Layout Group component places its child layout elements on top of each other. Their heights are determined by their respective minimum, preferred, and flexible heights according to the following model: The minimum heights of all the child layout elements are added together and the spacing between them is added as well. The result is the mimimum height of the Vertical Layout Group. The preferred heights of all the child layout elements are added together and the spacing between them is added as well. The result is the preferred height of the Vertical Layout Group. If the Vertical Layout Group is at its minimum height or smaller, all the child layout elements will also have their minimum height. The closer the Vertical Layout group is to its preferred height, the closer each child layout element will also get to their preferred height. If the Vertical Layout Group is taller than its preferred height, it will distribute the extra available space proportionally to the child layout elements according to their respective flexible heights. For more information about minimum, preferred, and flexible height, see the documentation on Auto Layout. Properties Property: Function: Padding The padding inside the edges of the layout group. Spacing The spacing between the layout elements. Child Alignment The alignment to use for the child layout elements if they don't fill out all the available space. Control Child Size Whether the Layout Group controls the width and height of its child layout elements. Use Child Scale Whether the Layout Group considers the scale of its child layout elements when sizing and laying out elements. Width and Height correspond to the Scale > X and Scale > Y values in each child layout element's Rect Transform component. You cannot animate the Scale values using the Animator Controller Child Force Expand Whether to force the child layout elements to expand to fill additional available space."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/ugui.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/Documentation~/ugui.html",
    "title": "About Unity UI | FSM Unity Framework",
    "keywords": "About Unity UI Unity UI is a UI toolkit for developing user interfaces for games and applications. It is a GameObject-based UI system that uses Components and the Game View to arrange, position, and style user interfaces. You cannot use Unity UI to create or change user interfaces within the Unity Editor. Installing Unity UI Unity UI is a core package. A version of it is included in each Unity release. To remove this package, or reinstall it after removal, follow the instructions in the Package Manager documentation. Getting documentation User documentation The Unity UI user documentation is in the Unity Manual. It provides a basic overview of the available components, and a few how-tos. API documentation You can find Class descriptions and API compatibility information in the Scripting API section of this documentation. Getting support For questions and assistance, visit the Unity UI section of the Unity Forum."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/LICENSE.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/LICENSE.html",
    "title": "| FSM Unity Framework",
    "keywords": "Unity UI Copyright © 2015-2020 Unity Technologies ApS (\"Unity\") Licensed under the Unity Companion License for Unity-dependent projects (see https://unity3d.com/legal/licenses/unity_companion_license). _Unless expressly provided otherwise, the Software under this license is made available strictly on an “AS IS” BASIS WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED. Please review the license for details on these and other terms and conditions."
  },
  "Library/PackageCache/com.unity.ugui@1.0.0/README.html": {
    "href": "Library/PackageCache/com.unity.ugui@1.0.0/README.html",
    "title": "Unity UI | FSM Unity Framework",
    "keywords": "Unity UI The Unity UI package allows you to create in-game user interfaces fast and intuitively. Prerequisites Unity 2019.2 This package is in development, and requires Unity 2019.2. Getting Started The Unity UI user manual can be found here."
  },
  "README.html": {
    "href": "README.html",
    "title": "Unity_FSM_ToolKit | FSM Unity Framework",
    "keywords": "Unity_FSM_ToolKit"
  },
  "api/EditorWindow.FSMSystem.BehaviorScripts.BehaviorScript.html": {
    "href": "api/EditorWindow.FSMSystem.BehaviorScripts.BehaviorScript.html",
    "title": "Class BehaviorScript | FSM Unity Framework",
    "keywords": "Class BehaviorScript Namespace EditorWindow.FSMSystem.BehaviorScripts Assembly EditorWindow.dll Represents an abstract behavior script for managing state options. public abstract class BehaviorScript : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour BehaviorScript Fields CurrentState protected FsmStates CurrentState Field Value FsmStates options public List<StateScript> options Field Value List<StateScript> selectedOptionIndex public int selectedOptionIndex Field Value int Methods GetVariables() Retrieves variables and their values from the behavior script. public virtual Dictionary<string, object> GetVariables() Returns Dictionary<string, object> A dictionary containing variable names and their values. SetVariableValue(string, object) Sets the value of a specified variable in the behavior script. public virtual void SetVariableValue(string variableName, object newValue) Parameters variableName string The name of the variable to set. newValue object The new value to assign to the variable."
  },
  "api/EditorWindow.FSMSystem.BehaviorScripts.chasingai.html": {
    "href": "api/EditorWindow.FSMSystem.BehaviorScripts.chasingai.html",
    "title": "Class chasingai | FSM Unity Framework",
    "keywords": "Class chasingai Namespace EditorWindow.FSMSystem.BehaviorScripts Assembly Assembly-CSharp.dll [Serializable] public class chasingai : BehaviorScript Inheritance object Object Component Behaviour MonoBehaviour BehaviorScript chasingai Inherited Members BehaviorScript.options BehaviorScript.selectedOptionIndex BehaviorScript.CurrentState BehaviorScript.GetVariables() BehaviorScript.SetVariableValue(string, object) Fields chase [Header(\"Chase\")] [SerializeField] public ChaseStateScript chase Field Value ChaseStateScript distance0 [Header(\"Distance 0\")] [SerializeField] public DistanceConditionScript distance0 Field Value DistanceConditionScript distance1 [Header(\"Distance 1\")] [SerializeField] public DistanceConditionScript distance1 Field Value DistanceConditionScript nextState [Header(\"NextState\")] [SerializeField] public NextStateConditionScript nextState Field Value NextStateConditionScript search [Header(\"Search\")] [SerializeField] public SearchStateScript search Field Value SearchStateScript Methods UpdateChaseState() public void UpdateChaseState() UpdateDieState() public void UpdateDieState() UpdateHitState() public void UpdateHitState() UpdateSearchState() public void UpdateSearchState()"
  },
  "api/EditorWindow.FSMSystem.BehaviorScripts.html": {
    "href": "api/EditorWindow.FSMSystem.BehaviorScripts.html",
    "title": "Namespace EditorWindow.FSMSystem.BehaviorScripts | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.BehaviorScripts Classes BehaviorScript Represents an abstract behavior script for managing state options."
  },
  "api/EditorWindow.FSMSystem.BehaviorScripts.meleeAttack.html": {
    "href": "api/EditorWindow.FSMSystem.BehaviorScripts.meleeAttack.html",
    "title": "Class meleeAttack | FSM Unity Framework",
    "keywords": "Class meleeAttack Namespace EditorWindow.FSMSystem.BehaviorScripts Assembly Assembly-CSharp.dll [Serializable] public class meleeAttack : BehaviorScript Inheritance object Object Component Behaviour MonoBehaviour BehaviorScript meleeAttack Inherited Members BehaviorScript.options BehaviorScript.selectedOptionIndex BehaviorScript.CurrentState BehaviorScript.GetVariables() BehaviorScript.SetVariableValue(string, object) Fields attack [Header(\"Attack\")] [SerializeField] public AttackStateScript attack Field Value AttackStateScript chase [Header(\"Chase\")] [SerializeField] public ChaseStateScript chase Field Value ChaseStateScript distance0 [Header(\"Distance 0\")] [SerializeField] public DistanceConditionScript distance0 Field Value DistanceConditionScript distance1 [Header(\"Distance 1\")] [SerializeField] public DistanceConditionScript distance1 Field Value DistanceConditionScript Methods UpdateAttackState() public void UpdateAttackState() UpdateChaseState() public void UpdateChaseState() UpdateDieState() public void UpdateDieState() UpdateHitState() public void UpdateHitState()"
  },
  "api/EditorWindow.FSMSystem.Data.Error.FsmErrorData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Error.FsmErrorData.html",
    "title": "Class FsmErrorData | FSM Unity Framework",
    "keywords": "Class FsmErrorData Namespace EditorWindow.FSMSystem.Data.Error Assembly EditorWindow.dll Represents data for FSM errors. public class FsmErrorData Inheritance object FsmErrorData Constructors FsmErrorData() Initializes a new instance of the FsmErrorData class. public FsmErrorData() Properties Color Gets the color associated with the FSM error. public Color Color { get; } Property Value Color"
  },
  "api/EditorWindow.FSMSystem.Data.Error.FsmNodeErrorData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Error.FsmNodeErrorData.html",
    "title": "Class FsmNodeErrorData | FSM Unity Framework",
    "keywords": "Class FsmNodeErrorData Namespace EditorWindow.FSMSystem.Data.Error Assembly EditorWindow.dll Represents error data associated with FSM nodes. public class FsmNodeErrorData Inheritance object FsmNodeErrorData Properties ErrorData Gets the error data associated with the FSM node error. public FsmErrorData ErrorData { get; } Property Value FsmErrorData Nodes Gets the list of FSM nodes associated with the error. public List<FsmNode> Nodes { get; } Property Value List<FsmNode>"
  },
  "api/EditorWindow.FSMSystem.Data.Error.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Error.html",
    "title": "Namespace EditorWindow.FSMSystem.Data.Error | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Data.Error Classes FsmErrorData Represents data for FSM errors. FsmNodeErrorData Represents error data associated with FSM nodes."
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmAnimatorSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmAnimatorSaveData.html",
    "title": "Class FsmAnimatorSaveData | FSM Unity Framework",
    "keywords": "Class FsmAnimatorSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll Represents save data for FSM Animator parameters. [Serializable] public class FsmAnimatorSaveData Inheritance object FsmAnimatorSaveData Properties AnimationTrigger Gets or sets a value indicating whether the trigger for setting animations is enabled. public bool AnimationTrigger { get; set; } Property Value bool ParameterName Gets or sets the name of the parameter. public string ParameterName { get; set; } Property Value string ParameterType Gets or sets the type of the parameter. public string ParameterType { get; set; } Property Value string Value Gets or sets the value of the parameter. public string Value { get; set; } Property Value string Methods Initialize(bool, string, string, string) Initializes the save data with specified parameters. public void Initialize(bool triggerEnable, string parameterName, string parameterType, string value) Parameters triggerEnable bool Whether the trigger for setting animations is enabled. parameterName string The name of the parameter. parameterType string The type of the parameter. value string The value of the parameter."
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmConnectionSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmConnectionSaveData.html",
    "title": "Class FsmConnectionSaveData | FSM Unity Framework",
    "keywords": "Class FsmConnectionSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll Represents save data for FSM connections. [Serializable] public class FsmConnectionSaveData Inheritance object FsmConnectionSaveData Properties NodeId Gets or sets the ID of the connected node. public string NodeId { get; set; } Property Value string Text Gets or sets the text associated with the connection. public string Text { get; set; } Property Value string"
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmGraphSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmGraphSaveData.html",
    "title": "Class FsmGraphSaveData | FSM Unity Framework",
    "keywords": "Class FsmGraphSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll Represents save data for FSM graphs. public class FsmGraphSaveData : ScriptableObject Inheritance object Object ScriptableObject FsmGraphSaveData Properties FileName Gets or sets the name of the file associated with the graph. public string FileName { get; set; } Property Value string GameObject Gets or sets the name of the GameObject associated with the graph. public string GameObject { get; set; } Property Value string HitData Gets or sets the save data for hit events in the graph. public FsmHitSaveData HitData { get; set; } Property Value FsmHitSaveData InitialState Gets or sets the initial state of the graph. public string InitialState { get; set; } Property Value string Nodes Gets or sets the list of nodes in the graph. public List<FsmNodeSaveData> Nodes { get; set; } Property Value List<FsmNodeSaveData> OldNodeNames Gets or sets the list of old node names in the graph. public List<string> OldNodeNames { get; set; } Property Value List<string> Methods Initialize(string, string, FsmHitSaveData) Initializes the save data with specified parameters. public void Initialize(string fileName, string initialState, FsmHitSaveData hitSaveData) Parameters fileName string The name of the file associated with the graph. initialState string The initial state of the graph. hitSaveData FsmHitSaveData The save data for hit events in the graph."
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmHitNodeSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmHitNodeSaveData.html",
    "title": "Class FsmHitNodeSaveData | FSM Unity Framework",
    "keywords": "Class FsmHitNodeSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll Represents save data for hit events in the nodes of the FSM graphs. [Serializable] public class FsmHitNodeSaveData Inheritance object FsmHitNodeSaveData Properties CanDie Gets or sets a value indicating whether the enemy can die. public bool CanDie { get; set; } Property Value bool CanGetHit Gets or sets a value indicating whether the enemy can get hit. public bool CanGetHit { get; set; } Property Value bool HasHitOverride Gets or sets a value indicating whether the hit node has the hit override option enabled. public bool HasHitOverride { get; set; } Property Value bool TimeToWait Gets or sets the time to wait for the enemy to be stunned. public float TimeToWait { get; set; } Property Value float Methods Initialize(bool, bool, float, bool) Initializes the hit save data with specified parameters. public void Initialize(bool hasHitOverride, bool canGetHit, float timeToWait, bool canDie) Parameters hasHitOverride bool Whether the hit node has the hit override option enabled. canGetHit bool Whether the enemy can get hit. timeToWait float The time to wait for the enemy to be stunned. canDie bool Whether the enemy can die."
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmHitSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmHitSaveData.html",
    "title": "Class FsmHitSaveData | FSM Unity Framework",
    "keywords": "Class FsmHitSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll Represents save data for hit events in FSM graphs. [Serializable] public class FsmHitSaveData Inheritance object FsmHitSaveData Properties CanDie Gets or sets a value indicating whether the enemy can die. public bool CanDie { get; set; } Property Value bool HitEnable Gets or sets a value indicating whether the enemy can get hit. public bool HitEnable { get; set; } Property Value bool TimeToWait Gets or sets the time to wait for the enemy to be stunned. public float TimeToWait { get; set; } Property Value float Methods Initialize(bool, float, bool) Initializes the save data with specified parameters. public void Initialize(bool hitEnable, float timeToWait, bool canDie) Parameters hitEnable bool Whether the enemy can get hit. timeToWait float The time to wait for the enemy to be stunned. canDie bool Whether the enemy can die."
  },
  "api/EditorWindow.FSMSystem.Data.Save.FsmNodeSaveData.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.FsmNodeSaveData.html",
    "title": "Class FsmNodeSaveData | FSM Unity Framework",
    "keywords": "Class FsmNodeSaveData Namespace EditorWindow.FSMSystem.Data.Save Assembly EditorWindow.dll [Serializable] public class FsmNodeSaveData Inheritance object FsmNodeSaveData Properties AnimatorSaveData Gets or sets the save data about the animation option selected associated with the node. public FsmAnimatorSaveData AnimatorSaveData { get; set; } Property Value FsmAnimatorSaveData Connections Gets or sets the list of connections associated with the node. public List<FsmConnectionSaveData> Connections { get; set; } Property Value List<FsmConnectionSaveData> DataObject Gets or sets the data object saved in a json associated with the node. public string DataObject { get; set; } Property Value string HitNodeSaveData Gets or sets the save data for hit override values associated with the node. public FsmHitNodeSaveData HitNodeSaveData { get; set; } Property Value FsmHitNodeSaveData Id Gets or sets the ID of the node. public string Id { get; set; } Property Value string Name Gets or sets the name of the node. public string Name { get; set; } Property Value string NodeType Gets or sets the type of the node. public FsmNodeType NodeType { get; set; } Property Value FsmNodeType Position Gets or sets the position of the node. public Vector2 Position { get; set; } Property Value Vector2"
  },
  "api/EditorWindow.FSMSystem.Data.Save.html": {
    "href": "api/EditorWindow.FSMSystem.Data.Save.html",
    "title": "Namespace EditorWindow.FSMSystem.Data.Save | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Data.Save Classes FsmAnimatorSaveData Represents save data for FSM Animator parameters. FsmConnectionSaveData Represents save data for FSM connections. FsmGraphSaveData Represents save data for FSM graphs. FsmHitNodeSaveData Represents save data for hit events in the nodes of the FSM graphs. FsmHitSaveData Represents save data for hit events in FSM graphs. FsmNodeSaveData"
  },
  "api/EditorWindow.FSMSystem.Elements.FsmCustomConditionNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmCustomConditionNode.html",
    "title": "Class FsmCustomConditionNode | FSM Unity Framework",
    "keywords": "Class FsmCustomConditionNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents a custom condition node in an FSM graph, inheriting from FsmNode. With this node you are able to add and execute a function from any GameObject. public class FsmCustomConditionNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmCustomConditionNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding ports and custom state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM custom state node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmCustomStateNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmCustomStateNode.html",
    "title": "Class FsmCustomStateNode | FSM Unity Framework",
    "keywords": "Class FsmCustomStateNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents a custom state node in an FSM graph, inheriting from FsmNode. With this node you are able to add and execute a function from any GameObject. public class FsmCustomStateNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmCustomStateNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding ports and custom state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM custom state node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmDualTransitionNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmDualTransitionNode.html",
    "title": "Class FsmDualTransitionNode | FSM Unity Framework",
    "keywords": "Class FsmDualTransitionNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents a dual transition node in an FSM graph, inheriting from FsmNode. In this case, there are two possible transitions, one for true and one for false. public class FsmDualTransitionNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmDualTransitionNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding ports and custom state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM dual transition node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmExtensionNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmExtensionNode.html",
    "title": "Class FsmExtensionNode | FSM Unity Framework",
    "keywords": "Class FsmExtensionNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents an FSM extension node, inheriting from FsmNode. This type of node is used to draw more comprehensive connections between states. public class FsmExtensionNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmExtensionNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding the input and output ports and styling the containers. This draw does not inherit from the base class, as the extension node has a different layout. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM extension node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph. SetStateName(string) Sets the name of the state. public override void SetStateName(string stateName) Parameters stateName string The new name of the state."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmInitialNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmInitialNode.html",
    "title": "Class FsmInitialNode | FSM Unity Framework",
    "keywords": "Class FsmInitialNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents the initial node in an FSM graph, inheriting from FsmNode. Used to know the first state to execute in the FSM. public class FsmInitialNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmInitialNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding a label for the state name and configuring output ports. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM initial node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmNode.html",
    "title": "Class FsmNode | FSM Unity Framework",
    "keywords": "Class FsmNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents the base class for all FSM node elements. public class FsmNode : Node Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode Derived FsmCustomConditionNode FsmCustomStateNode FsmDualTransitionNode FsmExtensionNode FsmInitialNode FsmStateNode FsmTransitionNode FsmVariableNode Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Fields DataObjects protected List<StateScriptData> DataObjects Field Value List<StateScriptData> HasAnimatorTrigger protected bool HasAnimatorTrigger Field Value bool HasHitStateOverride protected bool HasHitStateOverride Field Value bool InputPort protected Port InputPort Field Value Port OutputPort protected List<Port> OutputPort Field Value List<Port> Properties Connections Gets or sets the list of connections associated with the FSM node. public List<FsmConnectionSaveData> Connections { get; set; } Property Value List<FsmConnectionSaveData> Id Gets or sets the unique identifier of the FSM node. public string Id { get; set; } Property Value string NodeType Gets or sets the type of the FSM node. public FsmNodeType NodeType { get; set; } Property Value FsmNodeType StateName Gets or sets the name of the state associated with the FSM node. public string StateName { get; set; } Property Value string StateScript Gets or sets the script data associated with the behaviour of the FSM node. public StateScriptData StateScript { get; set; } Property Value StateScriptData Methods BuildContextualMenu(ContextualMenuPopulateEvent) Overrides the method to build a contextual menu for the FSM node. In this case, the menu will contain options to disconnect input and output ports. public override void BuildContextualMenu(ContextualMenuPopulateEvent evt) Parameters evt ContextualMenuPopulateEvent DisconnectAllPorts() public void DisconnectAllPorts() Draw() Virtual method to draw the FSM node. This method will be overridden by the derived classes. Acts as a base node and the derived classes will implement their own logic. public virtual void Draw() GetAnimatorSaveData() Retrieves the saved animator data. public FsmAnimatorSaveData GetAnimatorSaveData() Returns FsmAnimatorSaveData The saved animator data. GetHitNodeSaveData() Gets the save data from the hit information. public FsmHitNodeSaveData GetHitNodeSaveData() Returns FsmHitNodeSaveData The hit node save data. GetScriptableObject() Retrieves the StateScriptData object associated with the current state. protected void GetScriptableObject() Initialize(string, FsmGraphView, Vector2) Initializes the FSM node with the provided name, FSM graph view, and position. public virtual void Initialize(string nodeName, FsmGraphView graphView, Vector2 vectorPos) Parameters nodeName string The name of the FSM node. graphView FsmGraphView The FSM graph view where the node will be placed. vectorPos Vector2 The position of the node within the FSM graph view. ResetStyle() Resets the style of the main container to its default color. public void ResetStyle() SetAnimatorSaveData(FsmAnimatorSaveData) Sets the animator save data. public void SetAnimatorSaveData(FsmAnimatorSaveData animatorSaveData) Parameters animatorSaveData FsmAnimatorSaveData The animator save data to set. SetErrorStyle(Color) Sets the error style of the main container. public void SetErrorStyle(Color color) Parameters color Color The color to set as the background color. SetHitNodeSaveData(FsmHitNodeSaveData) Sets the hit node save data. public void SetHitNodeSaveData(FsmHitNodeSaveData animatorSaveData) Parameters animatorSaveData FsmHitNodeSaveData The hit node save data to set. SetPortColor(Color, Direction, string) Sets the color of a port based on its direction (input or output). public void SetPortColor(Color color, Direction direction, string portName = \"\") Parameters color Color The color to set. direction Direction The direction of the port (input or output). portName string The name of the port. SetStateName(string) Sets the name of the state and updates the displayed state name field. public virtual void SetStateName(string stateName) Parameters stateName string The new name of the state. ShowAnimatorParameterDropdown(Toggle) Shows a dropdown menu for selecting an animator parameter and provides fields for configuring its value based on the parameter type. protected void ShowAnimatorParameterDropdown(Toggle toggle) Parameters toggle Toggle The toggle representing the state of showing the dropdown. ShowHitStateOverrideToggle(Toggle) If enabled adds the parameters to be able to override the hit values. protected void ShowHitStateOverrideToggle(Toggle toggle) Parameters toggle Toggle The toggle to show override hit parameters. UpdateHitEnable(bool) public virtual void UpdateHitEnable(bool canGetHit) Parameters canGetHit bool UpdateNameStyle(string) Updates the naming style of a string from camel case to a readable format with spaces between words. protected string UpdateNameStyle(string newName) Parameters newName string The string to be formatted. Returns string The formatted string."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmStateNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmStateNode.html",
    "title": "Class FsmStateNode | FSM Unity Framework",
    "keywords": "Class FsmStateNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents a state node in an FSM graph, inheriting from FsmNode. public class FsmStateNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmStateNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding ports, toggles, and any state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM state node. public override void Initialize(string nodeName, FsmGraphView graphView, Vector2 posVector2) Parameters nodeName string The name of the node. graphView FsmGraphView The graph view this node belongs to. posVector2 Vector2 The position of the node in the graph. UpdateHitEnable(bool) public override void UpdateHitEnable(bool value) Parameters value bool"
  },
  "api/EditorWindow.FSMSystem.Elements.FsmTransitionNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmTransitionNode.html",
    "title": "Class FsmTransitionNode | FSM Unity Framework",
    "keywords": "Class FsmTransitionNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents a transition node in an FSM graph, inheriting from FsmNode. public class FsmTransitionNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmTransitionNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods Draw() Draws the node, adding ports and custom state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM transition node. public override void Initialize(string nodeName, FsmGraphView graphView, Vector2 vectorPos) Parameters nodeName string The name of the node. graphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.FsmVariableNode.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.FsmVariableNode.html",
    "title": "Class FsmVariableNode | FSM Unity Framework",
    "keywords": "Class FsmVariableNode Namespace EditorWindow.FSMSystem.Elements Assembly EditorWindow.dll Represents an override node in an FSM graph, inheriting from FsmNode. With this node you are able to override the variable of a state node. public class FsmVariableNode : FsmNode Inheritance object CallbackEventHandler Focusable VisualElement GraphElement Node FsmNode FsmVariableNode Inherited Members FsmNode.Id FsmNode.StateName FsmNode.Connections FsmNode.NodeType FsmNode.StateScript FsmNode.DataObjects FsmNode.InputPort FsmNode.OutputPort FsmNode.HasAnimatorTrigger FsmNode.HasHitStateOverride FsmNode.BuildContextualMenu(ContextualMenuPopulateEvent) FsmNode.DisconnectAllPorts() FsmNode.SetErrorStyle(Color) FsmNode.ResetStyle() FsmNode.SetStateName(string) FsmNode.GetScriptableObject() FsmNode.UpdateNameStyle(string) FsmNode.SetPortColor(Color, Direction, string) FsmNode.ShowAnimatorParameterDropdown(Toggle) FsmNode.GetAnimatorSaveData() FsmNode.SetAnimatorSaveData(FsmAnimatorSaveData) FsmNode.ShowHitStateOverrideToggle(Toggle) FsmNode.GetHitNodeSaveData() FsmNode.SetHitNodeSaveData(FsmHitNodeSaveData) FsmNode.UpdateHitEnable(bool) Extension Methods FsmElementUtility.CreatePort(FsmNode, string, Orientation, Direction, Port.Capacity) FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Methods ConnectToStateScript(StateScriptData) public void ConnectToStateScript(StateScriptData stateScript) Parameters stateScript StateScriptData Draw() Draws the node, adding ports and custom state attributes. public override void Draw() Initialize(string, FsmGraphView, Vector2) Initializes the FSM override node. public override void Initialize(string nodeName, FsmGraphView fsmGraphView, Vector2 vectorPos) Parameters nodeName string The name of the node. fsmGraphView FsmGraphView The graph view this node belongs to. vectorPos Vector2 The position of the node in the graph."
  },
  "api/EditorWindow.FSMSystem.Elements.html": {
    "href": "api/EditorWindow.FSMSystem.Elements.html",
    "title": "Namespace EditorWindow.FSMSystem.Elements | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Elements Classes FsmCustomConditionNode Represents a custom condition node in an FSM graph, inheriting from FsmNode. With this node you are able to add and execute a function from any GameObject. FsmCustomStateNode Represents a custom state node in an FSM graph, inheriting from FsmNode. With this node you are able to add and execute a function from any GameObject. FsmDualTransitionNode Represents a dual transition node in an FSM graph, inheriting from FsmNode. In this case, there are two possible transitions, one for true and one for false. FsmExtensionNode Represents an FSM extension node, inheriting from FsmNode. This type of node is used to draw more comprehensive connections between states. FsmInitialNode Represents the initial node in an FSM graph, inheriting from FsmNode. Used to know the first state to execute in the FSM. FsmNode Represents the base class for all FSM node elements. FsmStateNode Represents a state node in an FSM graph, inheriting from FsmNode. FsmTransitionNode Represents a transition node in an FSM graph, inheriting from FsmNode. FsmVariableNode Represents an override node in an FSM graph, inheriting from FsmNode. With this node you are able to override the variable of a state node."
  },
  "api/EditorWindow.FSMSystem.Inspectors.FsmGraph.html": {
    "href": "api/EditorWindow.FSMSystem.Inspectors.FsmGraph.html",
    "title": "Class FsmGraph | FSM Unity Framework",
    "keywords": "Class FsmGraph Namespace EditorWindow.FSMSystem.Inspectors Assembly EditorWindow.dll Represents the finite state machine graph attached to a GameObject. [RequireComponent(typeof(Animator))] [RequireComponent(typeof(NavMeshAgent))] [RequireComponent(typeof(CapsuleCollider))] [RequireComponent(typeof(AudioSource))] [RequireComponent(typeof(EnemyHealthSystem))] [AddComponentMenu(\"FSM AI/FSM AI\")] public class FsmGraph : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour FsmGraph Methods UpdateComponentOfGameObject() Updates the components of the GameObject based on the FSM graph. public void UpdateComponentOfGameObject()"
  },
  "api/EditorWindow.FSMSystem.Inspectors.FsmInspector.html": {
    "href": "api/EditorWindow.FSMSystem.Inspectors.FsmInspector.html",
    "title": "Class FsmInspector | FSM Unity Framework",
    "keywords": "Class FsmInspector Namespace EditorWindow.FSMSystem.Inspectors Assembly EditorWindow.dll Custom inspector for FSM graphs, enabling interaction with FSM graph data in the Unity Editor. [CustomEditor(typeof(FsmGraph))] public class FsmInspector : Editor Inheritance object Object ScriptableObject Editor FsmInspector Methods OnInspectorGUI() Overrides the default Inspector GUI to add custom drawing logic. This way you can add your created FSM Graphs to the GameObject. If the FSM Graph has saved data, the behavior will be added to the GameObject as well. You can swap graph or edit the current one by clicking the button \"Open FSM Graph\". public override void OnInspectorGUI()"
  },
  "api/EditorWindow.FSMSystem.Inspectors.chasingaiEditor.html": {
    "href": "api/EditorWindow.FSMSystem.Inspectors.chasingaiEditor.html",
    "title": "Class chasingaiEditor | FSM Unity Framework",
    "keywords": "Class chasingaiEditor Namespace EditorWindow.FSMSystem.Inspectors Assembly Assembly-CSharp.dll [CustomEditor(typeof(chasingai))] public class chasingaiEditor : Editor Inheritance object Object ScriptableObject Editor chasingaiEditor Methods OnInspectorGUI() Implement this function to make a custom inspector. public override void OnInspectorGUI()"
  },
  "api/EditorWindow.FSMSystem.Inspectors.html": {
    "href": "api/EditorWindow.FSMSystem.Inspectors.html",
    "title": "Namespace EditorWindow.FSMSystem.Inspectors | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Inspectors Classes FsmGraph Represents the finite state machine graph attached to a GameObject. FsmInspector Custom inspector for FSM graphs, enabling interaction with FSM graph data in the Unity Editor."
  },
  "api/EditorWindow.FSMSystem.Inspectors.meleeAttackEditor.html": {
    "href": "api/EditorWindow.FSMSystem.Inspectors.meleeAttackEditor.html",
    "title": "Class meleeAttackEditor | FSM Unity Framework",
    "keywords": "Class meleeAttackEditor Namespace EditorWindow.FSMSystem.Inspectors Assembly Assembly-CSharp.dll [CustomEditor(typeof(meleeAttack))] public class meleeAttackEditor : Editor Inheritance object Object ScriptableObject Editor meleeAttackEditor Methods OnInspectorGUI() Implement this function to make a custom inspector. public override void OnInspectorGUI()"
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmCreationNodesUtilities.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmCreationNodesUtilities.html",
    "title": "Class FsmCreationNodesUtilities | FSM Unity Framework",
    "keywords": "Class FsmCreationNodesUtilities Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll public static class FsmCreationNodesUtilities Inheritance object FsmCreationNodesUtilities Methods CreateConditionNode(string) public static void CreateConditionNode(string stateName) Parameters stateName string CreateStateNode(string) public static void CreateStateNode(string stateName) Parameters stateName string"
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmElementUtility.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmElementUtility.html",
    "title": "Class FsmElementUtility | FSM Unity Framework",
    "keywords": "Class FsmElementUtility Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll Utility class for creating various UI elements for the FSM editor. public static class FsmElementUtility Inheritance object FsmElementUtility Methods CreateButton(string, Action) Creates a button. public static Button CreateButton(string text, Action onClick = null) Parameters text string Text of the button. onClick Action Action to be performed on button click. Returns Button Returns the created Button. CreateFoldout(string, bool) Creates a foldout element. public static Foldout CreateFoldout(string title, bool collapsed = false) Parameters title string Title of the foldout. collapsed bool Whether the foldout is collapsed initially. Returns Foldout Returns the created Foldout. CreateLabel(string, EventCallback<ChangeEvent<string>>) Creates a label. public static Label CreateLabel(string text = null, EventCallback<ChangeEvent<string>> onValueChanged = null) Parameters text string Text of the label. onValueChanged EventCallback<ChangeEvent<string>> Event callback for value change. Returns Label Returns the created Label. CreateObjectField<T>(string, T, EventCallback<ChangeEvent<Object>>) Creates a custom object field. public static CustomObjectField CreateObjectField<T>(string label = null, T value = null, EventCallback<ChangeEvent<Object>> onValueChanged = null) where T : Object Parameters label string Label for the object field. value T Initial value of the object field. onValueChanged EventCallback<ChangeEvent<Object>> Event callback for value change. Returns CustomObjectField Returns the created CustomObjectField. Type Parameters T Type of the object field. CreatePopupField(List<string>, int, EventCallback<ChangeEvent<string>>) Creates a popup field. public static PopupField<string> CreatePopupField(List<string> choices, int index = 0, EventCallback<ChangeEvent<string>> onValueChanged = null) Parameters choices List<string> List of choices for the popup field. index int Index of the default choice. onValueChanged EventCallback<ChangeEvent<string>> Event callback for value change. Returns PopupField<string> Returns the created PopupField. CreatePort(FsmNode, string, Orientation, Direction, Capacity) Creates a port for a node. public static Port CreatePort(this FsmNode node, string portName = \"\", Orientation orientation = Orientation.Horizontal, Direction direction = Direction.Input, Port.Capacity capacity = Capacity.Single) Parameters node FsmNode The node to which the port is added. portName string Name of the port. orientation Orientation Orientation of the port. direction Direction Direction of the port. capacity Port.Capacity Capacity of the port. Returns Port Returns the created Port. CreateTextArea(string, string, EventCallback<ChangeEvent<string>>) Creates a text area. public static TextField CreateTextArea(string value = null, string label = null, EventCallback<ChangeEvent<string>> onValueChanged = null) Parameters value string Initial value of the text area. label string Label for the text area. onValueChanged EventCallback<ChangeEvent<string>> Event callback for value change. Returns TextField Returns the created TextField as a text area. CreateTextField(string, string, EventCallback<ChangeEvent<string>>) Creates a text field. public static TextField CreateTextField(string value = null, string label = null, EventCallback<ChangeEvent<string>> onValueChanged = null) Parameters value string Initial value of the text field. label string Label for the text field. onValueChanged EventCallback<ChangeEvent<string>> Event callback for value change. Returns TextField Returns the created TextField. CreateToggle(string, bool, EventCallback<ChangeEvent<bool>>) Creates a toggle. public static Toggle CreateToggle(string label = null, bool value = false, EventCallback<ChangeEvent<bool>> onValueChanged = null) Parameters label string Label for the toggle. value bool Initial value of the toggle. onValueChanged EventCallback<ChangeEvent<bool>> Event callback for value change. Returns Toggle Returns the created Toggle."
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmEnemyStateMachineEditor.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmEnemyStateMachineEditor.html",
    "title": "Class FsmEnemyStateMachineEditor | FSM Unity Framework",
    "keywords": "Class FsmEnemyStateMachineEditor Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll Utility class for generating enemy state machine scripts. public static class FsmEnemyStateMachineEditor Inheritance object FsmEnemyStateMachineEditor Methods GenerateScript(FsmGraphSaveData) Generates and saves the script files based on the FSM graph data. public static void GenerateScript(FsmGraphSaveData saveData) Parameters saveData FsmGraphSaveData The save data containing FSM graph information."
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmIOUtility.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmIOUtility.html",
    "title": "Class FsmIOUtility | FSM Unity Framework",
    "keywords": "Class FsmIOUtility Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll Utility class for saving and loading FSM graphs and associated data. public static class FsmIOUtility Inheritance object FsmIOUtility Methods CreateFolder(string, string) Creates a folder in the specified path if it doesn't already exist. public static void CreateFolder(string path, string folderName) Parameters path string Path where the folder should be created. folderName string Name of the folder to create. CreateJson(StateScript, string) Creates a JSON file for the specified state script. public static void CreateJson(StateScript stateScript, string className) Parameters stateScript StateScript State script to serialize. className string Name of the class associated with the state script. FindGameObjectWithId<T>(string) Finds a GameObject with a specified ID in its IDGenerator component. public static GameObject FindGameObjectWithId<T>(string id) where T : MonoBehaviour Parameters id string The ID to search for. Returns GameObject The GameObject with the specified ID, if found; otherwise, null. Type Parameters T Type of MonoBehaviour to search for. Initialize(string, FsmGraphView, string, FsmHitSaveData) Initializes the FSM IO utility with necessary parameters. public static void Initialize(string graphName, FsmGraphView fsmGraphView, string initialState, FsmHitSaveData hitData) Parameters graphName string Name of the FSM graph. fsmGraphView FsmGraphView Reference to the FSM graph view. initialState string Name of the initial state. hitData FsmHitSaveData Hit data associated with the FSM. Load() Loads the FSM graph and associated data. public static void Load() LoadNode(FsmNodeSaveData, string) Loads a node from its saved data. public static FsmNode LoadNode(FsmNodeSaveData nodeData, string fileName) Parameters nodeData FsmNodeSaveData Node save data object. fileName string Name of the file. Returns FsmNode The loaded FSM node. Save() Saves the FSM graph and associated data. public static bool Save() Returns bool True if saving was successful, false otherwise. UpdateHitEnableOverrides(bool) public static void UpdateHitEnableOverrides(bool enableHitState) Parameters enableHitState bool"
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmInspectorUtility.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmInspectorUtility.html",
    "title": "Class FsmInspectorUtility | FSM Unity Framework",
    "keywords": "Class FsmInspectorUtility Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll Utility class for drawing custom inspector elements in the Unity Editor. public static class FsmInspectorUtility Inheritance object FsmInspectorUtility Methods DrawDisabledFields(Action) Draws fields in a disabled state. public static void DrawDisabledFields(Action action) Parameters action Action The action that draws the fields. DrawHeader(string) Draws a header label with bold styling. public static void DrawHeader(string label) Parameters label string The label text. DrawHelpBox(string, MessageType, bool) Draws a help box with the specified message and type. public static void DrawHelpBox(string message, MessageType messageType = MessageType.Info, bool wide = true) Parameters message string The help box message. messageType MessageType The type of the message. wide bool Whether the help box should be wide. DrawPropertyField(SerializedProperty) Draws a property field for the given serialized property. public static bool DrawPropertyField(this SerializedProperty serializedProperty) Parameters serializedProperty SerializedProperty The serialized property to draw. Returns bool True if the property has been modified; otherwise, false. DrawSpace(int) Draws a space of the specified amount. public static void DrawSpace(int amount = 4) Parameters amount int The amount of space to draw."
  },
  "api/EditorWindow.FSMSystem.Utilities.FsmStyleUtility.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.FsmStyleUtility.html",
    "title": "Class FsmStyleUtility | FSM Unity Framework",
    "keywords": "Class FsmStyleUtility Namespace EditorWindow.FSMSystem.Utilities Assembly EditorWindow.dll Utility class for adding classes and style sheets to VisualElements in Unity's UI Toolkit. public static class FsmStyleUtility Inheritance object FsmStyleUtility Methods AddClasses(VisualElement, params string[]) Adds one or more classes to a VisualElement. public static VisualElement AddClasses(this VisualElement element, params string[] classNames) Parameters element VisualElement The VisualElement to which classes will be added. classNames string[] The names of the classes to add. Returns VisualElement The modified VisualElement. AddStyleSheets(VisualElement, params string[]) Adds one or more style sheets to a VisualElement. public static VisualElement AddStyleSheets(this VisualElement element, params string[] styleSheetNames) Parameters element VisualElement The VisualElement to which style sheets will be added. styleSheetNames string[] The names of the style sheets to add. Returns VisualElement The modified VisualElement."
  },
  "api/EditorWindow.FSMSystem.Utilities.html": {
    "href": "api/EditorWindow.FSMSystem.Utilities.html",
    "title": "Namespace EditorWindow.FSMSystem.Utilities | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Utilities Classes FsmCreationNodesUtilities FsmElementUtility Utility class for creating various UI elements for the FSM editor. FsmEnemyStateMachineEditor Utility class for generating enemy state machine scripts. FsmIOUtility Utility class for saving and loading FSM graphs and associated data. FsmInspectorUtility Utility class for drawing custom inspector elements in the Unity Editor. FsmStyleUtility Utility class for adding classes and style sheets to VisualElements in Unity's UI Toolkit."
  },
  "api/EditorWindow.FSMSystem.Windows.FsmCreateScriptableObjectWindow.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmCreateScriptableObjectWindow.html",
    "title": "Class FsmCreateScriptableObjectWindow | FSM Unity Framework",
    "keywords": "Class FsmCreateScriptableObjectWindow Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll Editor window for creating FSM Graph scriptable objects and configuring enemy setup. public class FsmCreateScriptableObjectWindow : EditorWindow Inheritance object Object ScriptableObject EditorWindow FsmCreateScriptableObjectWindow Methods ShowWindow() [MenuItem(\"Window/FSM/FSM Graph\")] public static void ShowWindow()"
  },
  "api/EditorWindow.FSMSystem.Windows.FsmEditorWindow.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmEditorWindow.html",
    "title": "Class FsmEditorWindow | FSM Unity Framework",
    "keywords": "Class FsmEditorWindow Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll Represents an editor window for managing finite state machines. This window contains a graph view for creating and editing FSMs. The window also contains a toolbar with buttons for saving, reloading, clearing, and toggling the minimap. public class FsmEditorWindow : EditorWindow Inheritance object Object ScriptableObject EditorWindow FsmEditorWindow Fields initialState public string initialState Field Value string Methods DisableSaving() public void DisableSaving() EnableSaving() public void EnableSaving() GetHitStatePopup() public FsmHitStatePopup GetHitStatePopup() Returns FsmHitStatePopup GetWindow() public static FsmEditorWindow GetWindow() Returns FsmEditorWindow OpenWithSaveData(FsmGraphSaveData) Opens the FSM editor window with the provided save data. public static void OpenWithSaveData(FsmGraphSaveData saveData) Parameters saveData FsmGraphSaveData The save data containing information about the FSM graph."
  },
  "api/EditorWindow.FSMSystem.Windows.FsmGraphView.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmGraphView.html",
    "title": "Class FsmGraphView | FSM Unity Framework",
    "keywords": "Class FsmGraphView Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll The graph view that hold the content for the FSM system. public class FsmGraphView : GraphView Inheritance object CallbackEventHandler Focusable VisualElement GraphView FsmGraphView Extension Methods FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Constructors FsmGraphView(FsmEditorWindow) Constructor for the FSM graph view. Uses the FsmEditorWindow as a parameter, and loads the content of the graph view inside the window. public FsmGraphView(FsmEditorWindow window) Parameters window FsmEditorWindow Methods ClearGraph() public void ClearGraph() CreateNode(string, Vector2, FsmNodeType, bool, bool) Creates a new node in the graph view. public FsmNode CreateNode(string nodeName, Vector2 position, FsmNodeType nodeT, bool fixName = true, bool shouldDraw = true) Parameters nodeName string The name of the node. position Vector2 The position of the node. nodeT FsmNodeType The type of the node. fixName bool Should the name be fixed? shouldDraw bool Should the node be drawn? Returns FsmNode Returns the FsmNode that you just created. GetCompatiblePorts(Port, NodeAdapter) Get all ports compatible with given port. public override List<Port> GetCompatiblePorts(Port startPort, NodeAdapter nodeAdapter) Parameters startPort Port Start port to validate against. nodeAdapter NodeAdapter Node adapter. Returns List<Port> List of compatible ports. GetLocalMousePosition(Vector2, bool) public Vector2 GetLocalMousePosition(Vector2 mousePosition, bool isSearchWindow = false) Parameters mousePosition Vector2 isSearchWindow bool Returns Vector2 GetWindow() public FsmEditorWindow GetWindow() Returns FsmEditorWindow ToggleMiniMap() public void ToggleMiniMap()"
  },
  "api/EditorWindow.FSMSystem.Windows.FsmHitStatePopup.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmHitStatePopup.html",
    "title": "Class FsmHitStatePopup | FSM Unity Framework",
    "keywords": "Class FsmHitStatePopup Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll Popup window content for configuring hit state settings. public class FsmHitStatePopup : PopupWindowContent Inheritance object PopupWindowContent FsmHitStatePopup Methods CanDie() Checks if the entity can die from hit state. public bool CanDie() Returns bool True if the entity can die, otherwise false. GetTimeToWait() Gets the time to wait for hit state. public float GetTimeToWait() Returns float The time to wait for hit state. GetWindowSize() Gets the size of the popup window. public override Vector2 GetWindowSize() Returns Vector2 The size of the popup window. Initialize(FsmHitSaveData) Initializes the hit state popup with the specified hit data. public void Initialize(FsmHitSaveData hitData) Parameters hitData FsmHitSaveData The hit data to initialize the popup with. IsHitStateEnabled() Checks if hit state is enabled. public bool IsHitStateEnabled() Returns bool True if hit state is enabled, otherwise false. OnGUI(Rect) Renders the GUI for the popup window. public override void OnGUI(Rect rect) Parameters rect Rect The position and size of the popup window."
  },
  "api/EditorWindow.FSMSystem.Windows.FsmHoverPopupWindow.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmHoverPopupWindow.html",
    "title": "Class FsmHoverPopupWindow | FSM Unity Framework",
    "keywords": "Class FsmHoverPopupWindow Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll Class representing a popup window with instructions on enabling hit state override. public class FsmHoverPopupWindow : PopupWindowContent Inheritance object PopupWindowContent FsmHoverPopupWindow Methods GetWindowSize() Gets the size of the popup window. public override Vector2 GetWindowSize() Returns Vector2 The size of the popup window. OnGUI(Rect) Draws the GUI for the popup window. public override void OnGUI(Rect rect) Parameters rect Rect The rectangle within which to draw the GUI."
  },
  "api/EditorWindow.FSMSystem.Windows.FsmNodeCreationWindow.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmNodeCreationWindow.html",
    "title": "Class FsmNodeCreationWindow | FSM Unity Framework",
    "keywords": "Class FsmNodeCreationWindow Namespace EditorWindow.FSMSystem.Windows Assembly Assembly-CSharp.dll public class FsmNodeCreationWindow : EditorWindow Inheritance object Object ScriptableObject EditorWindow FsmNodeCreationWindow Methods ShowWindow() [MenuItem(\"Window/FSM/Create State or Condition\")] public static void ShowWindow()"
  },
  "api/EditorWindow.FSMSystem.Windows.FsmSearchWindow.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.FsmSearchWindow.html",
    "title": "Class FsmSearchWindow | FSM Unity Framework",
    "keywords": "Class FsmSearchWindow Namespace EditorWindow.FSMSystem.Windows Assembly EditorWindow.dll ScriptableObject used to create the search window for adding nodes to the FSM graph. public class FsmSearchWindow : ScriptableObject Inheritance object Object ScriptableObject FsmSearchWindow Methods CreateSearchTree(SearchWindowContext) Creates the search tree for the search window. public List<SearchTreeEntry> CreateSearchTree(SearchWindowContext context) Parameters context SearchWindowContext The search window context. Returns List<SearchTreeEntry> The search tree entries. Initialize(FsmGraphView) Initializes the search window with the specified FSM graph view. public void Initialize(FsmGraphView graphView) Parameters graphView FsmGraphView The FSM graph view to initialize the search window with. OnSelectEntry(SearchTreeEntry, SearchWindowContext) Handles the selection of an entry in the search window. Creating a node based on the selected entry. public bool OnSelectEntry(SearchTreeEntry searchTreeEntry, SearchWindowContext context) Parameters searchTreeEntry SearchTreeEntry The selected search tree entry. context SearchWindowContext The search window context. Returns bool True if the entry was successfully selected, otherwise false."
  },
  "api/EditorWindow.FSMSystem.Windows.html": {
    "href": "api/EditorWindow.FSMSystem.Windows.html",
    "title": "Namespace EditorWindow.FSMSystem.Windows | FSM Unity Framework",
    "keywords": "Namespace EditorWindow.FSMSystem.Windows Classes FsmCreateScriptableObjectWindow Editor window for creating FSM Graph scriptable objects and configuring enemy setup. FsmEditorWindow Represents an editor window for managing finite state machines. This window contains a graph view for creating and editing FSMs. The window also contains a toolbar with buttons for saving, reloading, clearing, and toggling the minimap. FsmGraphView The graph view that hold the content for the FSM system. FsmHitStatePopup Popup window content for configuring hit state settings. FsmHoverPopupWindow Class representing a popup window with instructions on enabling hit state override. FsmSearchWindow ScriptableObject used to create the search window for adding nodes to the FSM graph."
  },
  "api/FSM.Enemy.EnemyHealthBar.html": {
    "href": "api/FSM.Enemy.EnemyHealthBar.html",
    "title": "Class EnemyHealthBar | FSM Unity Framework",
    "keywords": "Class EnemyHealthBar Namespace FSM.Enemy Assembly FSM.dll Class responsible for updating the health bar of an enemy. public class EnemyHealthBar : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour EnemyHealthBar Methods UpdateHealthBar(float, float) Updates the health bar based on the provided max and current health values. public void UpdateHealthBar(float maxHealth, float currentHealth) Parameters maxHealth float Maximum health of the enemy. currentHealth float Current health of the enemy."
  },
  "api/FSM.Enemy.EnemyHealthSystem.html": {
    "href": "api/FSM.Enemy.EnemyHealthSystem.html",
    "title": "Class EnemyHealthSystem | FSM Unity Framework",
    "keywords": "Class EnemyHealthSystem Namespace FSM.Enemy Assembly FSM.dll Class responsible for managing the health of an enemy. public class EnemyHealthSystem : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour EnemyHealthSystem Fields maxHealth public float maxHealth Field Value float Methods Die() Destroys the enemy GameObject. public void Die() GetCurrentHealth() Returns the current health of the enemy. public float GetCurrentHealth() Returns float GetMaxHealth() Returns the maximum health of the enemy. public float GetMaxHealth() Returns float GetPreviousHealth() Returns the previous health of the enemy. public float GetPreviousHealth() Returns float SetPreviousHealth(float) Sets the previous health of the enemy. public void SetPreviousHealth(float health) Parameters health float The previous health value to set. TakeDamage(float) Inflicts damage to the enemy and updates the health bar. public void TakeDamage(float amount) Parameters amount float Amount of damage to inflict."
  },
  "api/FSM.Enemy.html": {
    "href": "api/FSM.Enemy.html",
    "title": "Namespace FSM.Enemy | FSM Unity Framework",
    "keywords": "Namespace FSM.Enemy Classes EnemyHealthBar Class responsible for updating the health bar of an enemy. EnemyHealthSystem Class responsible for managing the health of an enemy."
  },
  "api/FSM.Enumerations.FsmNodeType.html": {
    "href": "api/FSM.Enumerations.FsmNodeType.html",
    "title": "Enum FsmNodeType | FSM Unity Framework",
    "keywords": "Enum FsmNodeType Namespace FSM.Enumerations Assembly FSM.dll Enumeration representing different types of FSM nodes. public enum FsmNodeType Fields CustomCondition = 6 CustomState = 5 DualTransition = 3 Extension = 4 Initial = 2 State = 0 Transition = 1 Variable = 7"
  },
  "api/FSM.Enumerations.FsmOperands.html": {
    "href": "api/FSM.Enumerations.FsmOperands.html",
    "title": "Enum FsmOperands | FSM Unity Framework",
    "keywords": "Enum FsmOperands Namespace FSM.Enumerations Assembly FSM.dll Enumeration representing different comparison operands. public enum FsmOperands Fields EqualTo = 2 GreaterThan = 0 LessThan = 1 NotEqualTo = 3"
  },
  "api/FSM.Enumerations.FsmStates.html": {
    "href": "api/FSM.Enumerations.FsmStates.html",
    "title": "Enum FsmStates | FSM Unity Framework",
    "keywords": "Enum FsmStates Namespace FSM.Enumerations Assembly FSM.dll Enumeration representing different states in a Finite State Machine. public enum FsmStates Fields Alert = 10 Attack = 3 Chase = 2 Custom = 9 Die = 5 Flee = 7 Hit = 4 Idle = 0 Patrol = 1 Search = 6 Variable = 8"
  },
  "api/FSM.Enumerations.html": {
    "href": "api/FSM.Enumerations.html",
    "title": "Namespace FSM.Enumerations | FSM Unity Framework",
    "keywords": "Namespace FSM.Enumerations Enums FsmNodeType Enumeration representing different types of FSM nodes. FsmOperands Enumeration representing different comparison operands. FsmStates Enumeration representing different states in a Finite State Machine."
  },
  "api/FSM.Nodes.Data.FsmNodeConnectionData.html": {
    "href": "api/FSM.Nodes.Data.FsmNodeConnectionData.html",
    "title": "Class FsmNodeConnectionData | FSM Unity Framework",
    "keywords": "Class FsmNodeConnectionData Namespace FSM.Nodes.Data Assembly FSM.dll Data class representing a connection between FSM nodes. [Serializable] public class FsmNodeConnectionData Inheritance object FsmNodeConnectionData Properties NextNode The next node connected to this connection. public FsmNodeSo NextNode { get; set; } Property Value FsmNodeSo Text The text associated with the connection. public string Text { get; set; } Property Value string"
  },
  "api/FSM.Nodes.Data.html": {
    "href": "api/FSM.Nodes.Data.html",
    "title": "Namespace FSM.Nodes.Data | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes.Data Classes FsmNodeConnectionData Data class representing a connection between FSM nodes."
  },
  "api/FSM.Nodes.FsmGraph.html": {
    "href": "api/FSM.Nodes.FsmGraph.html",
    "title": "Class FsmGraph | FSM Unity Framework",
    "keywords": "Class FsmGraph Namespace FSM.Nodes Assembly Assembly-CSharp.dll Represents the finite state machine graph attached to a GameObject. [RequireComponent(typeof(Animator))] [RequireComponent(typeof(NavMeshAgent))] [RequireComponent(typeof(CapsuleCollider))] [RequireComponent(typeof(AudioSource))] [RequireComponent(typeof(EnemyHealthSystem))] [AddComponentMenu(\"FSM AI/FSM AI\")] public class FsmGraph : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour FsmGraph Methods UpdateComponentOfGameObject() Updates the components of the GameObject based on the FSM graph. public void UpdateComponentOfGameObject()"
  },
  "api/FSM.Nodes.ScriptableObjects.FsmNodeSo.html": {
    "href": "api/FSM.Nodes.ScriptableObjects.FsmNodeSo.html",
    "title": "Class FsmNodeSo | FSM Unity Framework",
    "keywords": "Class FsmNodeSo Namespace FSM.Nodes.ScriptableObjects Assembly FSM.dll public class FsmNodeSo : ScriptableObject Inheritance object Object ScriptableObject FsmNodeSo Properties Connections public List<FsmNodeConnectionData> Connections { get; set; } Property Value List<FsmNodeConnectionData> DataObject public string DataObject { get; set; } Property Value string NodeName public string NodeName { get; set; } Property Value string NodeType public FsmNodeType NodeType { get; set; } Property Value FsmNodeType Methods Initialize(string, string, List<FsmNodeConnectionData>, FsmNodeType, string) public void Initialize(string nodeName, string text, List<FsmNodeConnectionData> connections, FsmNodeType nodeType, string dataObject) Parameters nodeName string text string connections List<FsmNodeConnectionData> nodeType FsmNodeType dataObject string"
  },
  "api/FSM.Nodes.ScriptableObjects.FsmNodesContainerSo.html": {
    "href": "api/FSM.Nodes.ScriptableObjects.FsmNodesContainerSo.html",
    "title": "Class FsmNodesContainerSo | FSM Unity Framework",
    "keywords": "Class FsmNodesContainerSo Namespace FSM.Nodes.ScriptableObjects Assembly FSM.dll public class FsmNodesContainerSo : ScriptableObject Inheritance object Object ScriptableObject FsmNodesContainerSo Properties FileName public string FileName { get; set; } Property Value string UngroupedNodes public List<FsmNodeSo> UngroupedNodes { get; set; } Property Value List<FsmNodeSo> Methods GetUngroupedStateNames() public List<string> GetUngroupedStateNames() Returns List<string> Initialize(string) public void Initialize(string fileName) Parameters fileName string"
  },
  "api/FSM.Nodes.ScriptableObjects.html": {
    "href": "api/FSM.Nodes.ScriptableObjects.html",
    "title": "Namespace FSM.Nodes.ScriptableObjects | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes.ScriptableObjects Classes FsmNodeSo FsmNodesContainerSo"
  },
  "api/FSM.Nodes.States.IAction.html": {
    "href": "api/FSM.Nodes.States.IAction.html",
    "title": "Interface IAction | FSM Unity Framework",
    "keywords": "Interface IAction Namespace FSM.Nodes.States Assembly FSM.dll Interface for defining actions. public interface IAction Methods Execute() Executes the action. void Execute()"
  },
  "api/FSM.Nodes.States.ICondition.html": {
    "href": "api/FSM.Nodes.States.ICondition.html",
    "title": "Interface ICondition | FSM Unity Framework",
    "keywords": "Interface ICondition Namespace FSM.Nodes.States Assembly FSM.dll Interface for defining conditions. public interface ICondition Methods Condition() Evaluates the condition. bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScript.html": {
    "href": "api/FSM.Nodes.States.StateScript.html",
    "title": "Class StateScript | FSM Unity Framework",
    "keywords": "Class StateScript Namespace FSM.Nodes.States Assembly FSM.dll Abstract base class for state scripts. [Serializable] public abstract class StateScript : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour StateScript Derived AttackStateScript ChaseStateScript CustomConditionScript CustomStateScript DistanceConditionScript FleeStateScript HealthConditionScript HearingConditionScript IdleStateScript NextStateConditionScript PatrolStateScript SearchStateScript SeeingConditionScript VariableConditionScript Fields Agent Reference to the NavMeshAgent component. protected NavMeshAgent Agent Field Value NavMeshAgent Player Reference to the player GameObject. protected GameObject Player Field Value GameObject Methods GetStateName() Gets the name of the state. public string GetStateName() Returns string The name of the state. GetVariables() Retrieves public variables of the StateScript and returns them in a dictionary. public Dictionary<string, object> GetVariables() Returns Dictionary<string, object> A dictionary containing variable names and their values. HideFlagsInspector() Hides the StateScript from the inspector. public void HideFlagsInspector() InspectVariables() Inspects public variables of the StateScript and returns their values. public List<string> InspectVariables() Returns List<string> A list of strings representing the inspected variables. SetStateName(string) Sets the name of the state. public void SetStateName(string newName) Parameters newName string The new name of the state. SetVariableValue(string, object) Sets the value of a public variable in the StateScript. public void SetVariableValue(string variableName, object newValue) Parameters variableName string The name of the variable to set. newValue object The new value to assign to the variable."
  },
  "api/FSM.Nodes.States.StateScriptData.html": {
    "href": "api/FSM.Nodes.States.StateScriptData.html",
    "title": "Class StateScriptData | FSM Unity Framework",
    "keywords": "Class StateScriptData Namespace FSM.Nodes.States Assembly FSM.dll Data class for storing state script variables and metadata. public class StateScriptData Inheritance object StateScriptData Derived AttackData ChaseData CustomConditionData CustomData DistanceData FleeData HealthData HearingData IdleData NextStateData PatrolData SearchData SeeingData VariableData Methods GetStateName() Gets the name of the state. public string GetStateName() Returns string The name of the state. GetVariables() Retrieves public variables of the StateScriptData and returns them in a dictionary. public Dictionary<string, object> GetVariables() Returns Dictionary<string, object> A dictionary containing variable names and their values. InspectVariables() Inspects public variables of the StateScriptData and returns their values. public List<string> InspectVariables() Returns List<string> A list of strings representing the inspected variables. RemoveVariable(string, object) Removes a variable from the StateScriptData. public void RemoveVariable(string variableName, object pastValue) Parameters variableName string The name of the variable to remove. pastValue object The value of the variable to remove. SetStateName(string) Sets the name of the state. protected void SetStateName(string name) Parameters name string The new name of the state. SetVariableValue(string, object) Sets the value of a public variable in the StateScriptData. public void SetVariableValue(string variableName, object newValue) Parameters variableName string The name of the variable to set. newValue object The new value to assign to the variable."
  },
  "api/FSM.Nodes.States.StateScripts.AttackStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.AttackStateScript.html",
    "title": "Class AttackStateScript | FSM Unity Framework",
    "keywords": "Class AttackStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class AttackStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript AttackStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors AttackStateScript() public AttackStateScript() Fields attackDamage public float attackDamage Field Value float attackFrequency public float attackFrequency Field Value float Methods Execute() Executes the action. public void Execute()"
  },
  "api/FSM.Nodes.States.StateScripts.ChaseStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.ChaseStateScript.html",
    "title": "Class ChaseStateScript | FSM Unity Framework",
    "keywords": "Class ChaseStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class ChaseStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript ChaseStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors ChaseStateScript() public ChaseStateScript() Fields chaseRange public float chaseRange Field Value float chaseSpeed public float chaseSpeed Field Value float Methods Execute() Executes the action. public void Execute()"
  },
  "api/FSM.Nodes.States.StateScripts.CustomConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.CustomConditionScript.html",
    "title": "Class CustomConditionScript | FSM Unity Framework",
    "keywords": "Class CustomConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class CustomConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript CustomConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors CustomConditionScript() public CustomConditionScript() Fields selectedComponent public string selectedComponent Field Value string selectedFunction public string selectedFunction Field Value string selectedGameObject public string selectedGameObject Field Value string Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.CustomStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.CustomStateScript.html",
    "title": "Class CustomStateScript | FSM Unity Framework",
    "keywords": "Class CustomStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class CustomStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript CustomStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors CustomStateScript() public CustomStateScript() Fields selectedComponent public string selectedComponent Field Value string selectedFunction public string selectedFunction Field Value string selectedGameObject public string selectedGameObject Field Value string Methods Execute() Executes the action. public void Execute()"
  },
  "api/FSM.Nodes.States.StateScripts.DistanceConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.DistanceConditionScript.html",
    "title": "Class DistanceConditionScript | FSM Unity Framework",
    "keywords": "Class DistanceConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class DistanceConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript DistanceConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors DistanceConditionScript() public DistanceConditionScript() Fields distanceFromPlayer public float distanceFromPlayer Field Value float operand public FsmOperands operand Field Value FsmOperands Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.FleeStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.FleeStateScript.html",
    "title": "Class FleeStateScript | FSM Unity Framework",
    "keywords": "Class FleeStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class FleeStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript FleeStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors FleeStateScript() public FleeStateScript() Fields detectionRange public float detectionRange Field Value float fleeDistance public float fleeDistance Field Value float fleeSpeed public float fleeSpeed Field Value float Methods Execute() Executes the action. public void Execute() HasReachedDestination() public bool HasReachedDestination() Returns bool"
  },
  "api/FSM.Nodes.States.StateScripts.HealthConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.HealthConditionScript.html",
    "title": "Class HealthConditionScript | FSM Unity Framework",
    "keywords": "Class HealthConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class HealthConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript HealthConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors HealthConditionScript() public HealthConditionScript() Fields health public float health Field Value float operand public FsmOperands operand Field Value FsmOperands Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.HearingConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.HearingConditionScript.html",
    "title": "Class HearingConditionScript | FSM Unity Framework",
    "keywords": "Class HearingConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class HearingConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript HearingConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors HearingConditionScript() public HearingConditionScript() Fields hearingRange public float hearingRange Field Value float minPlayerSpeed public float minPlayerSpeed Field Value float Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.IdleStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.IdleStateScript.html",
    "title": "Class IdleStateScript | FSM Unity Framework",
    "keywords": "Class IdleStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class IdleStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript IdleStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors IdleStateScript() public IdleStateScript() Methods Execute() Executes the action. public void Execute()"
  },
  "api/FSM.Nodes.States.StateScripts.NextStateConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.NextStateConditionScript.html",
    "title": "Class NextStateConditionScript | FSM Unity Framework",
    "keywords": "Class NextStateConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class NextStateConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript NextStateConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors NextStateConditionScript() public NextStateConditionScript() Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.PatrolStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.PatrolStateScript.html",
    "title": "Class PatrolStateScript | FSM Unity Framework",
    "keywords": "Class PatrolStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class PatrolStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript PatrolStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors PatrolStateScript() public PatrolStateScript() Fields patrolPoints public List<string> patrolPoints Field Value List<string> Methods Execute() Executes the action. public void Execute() FindGameObjectWithId<T>(string) public GameObject FindGameObjectWithId<T>(string id) where T : MonoBehaviour Parameters id string Returns GameObject Type Parameters T RemovePatrolPoint(string) public void RemovePatrolPoint(string patrolPoint) Parameters patrolPoint string"
  },
  "api/FSM.Nodes.States.StateScripts.SearchStateScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.SearchStateScript.html",
    "title": "Class SearchStateScript | FSM Unity Framework",
    "keywords": "Class SearchStateScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class SearchStateScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript SearchStateScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors SearchStateScript() public SearchStateScript() Fields exploreRadius public float exploreRadius Field Value float Methods Execute() Executes the action. public void Execute() HasReachedDestination() public bool HasReachedDestination() Returns bool"
  },
  "api/FSM.Nodes.States.StateScripts.SeeingConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.SeeingConditionScript.html",
    "title": "Class SeeingConditionScript | FSM Unity Framework",
    "keywords": "Class SeeingConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class SeeingConditionScript : StateScript, ICondition Inheritance object Object Component Behaviour MonoBehaviour StateScript SeeingConditionScript Implements ICondition Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors SeeingConditionScript() public SeeingConditionScript() Fields distance public float distance Field Value float Methods Condition() Evaluates the condition. public bool Condition() Returns bool True if the condition is met, otherwise false."
  },
  "api/FSM.Nodes.States.StateScripts.VariableConditionScript.html": {
    "href": "api/FSM.Nodes.States.StateScripts.VariableConditionScript.html",
    "title": "Class VariableConditionScript | FSM Unity Framework",
    "keywords": "Class VariableConditionScript Namespace FSM.Nodes.States.StateScripts Assembly FSM.dll public class VariableConditionScript : StateScript, IAction Inheritance object Object Component Behaviour MonoBehaviour StateScript VariableConditionScript Implements IAction Inherited Members StateScript.Player StateScript.Agent StateScript.InspectVariables() StateScript.GetVariables() StateScript.SetVariableValue(string, object) StateScript.SetStateName(string) StateScript.GetStateName() StateScript.HideFlagsInspector() Constructors VariableConditionScript() public VariableConditionScript() Fields selectedVariable public string selectedVariable Field Value string value public string value Field Value string Methods Execute() Executes the action. public void Execute() SetStateScript(StateScript) public void SetStateScript(StateScript stateScript) Parameters stateScript StateScript"
  },
  "api/FSM.Nodes.States.StateScripts.html": {
    "href": "api/FSM.Nodes.States.StateScripts.html",
    "title": "Namespace FSM.Nodes.States.StateScripts | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes.States.StateScripts Classes AttackStateScript ChaseStateScript CustomConditionScript CustomStateScript DistanceConditionScript FleeStateScript HealthConditionScript HearingConditionScript IdleStateScript NextStateConditionScript PatrolStateScript SearchStateScript SeeingConditionScript VariableConditionScript"
  },
  "api/FSM.Nodes.States.StatesData.AttackData.html": {
    "href": "api/FSM.Nodes.States.StatesData.AttackData.html",
    "title": "Class AttackData | FSM Unity Framework",
    "keywords": "Class AttackData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class AttackData : StateScriptData Inheritance object StateScriptData AttackData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors AttackData() public AttackData() Fields attackDamage public float attackDamage Field Value float attackFrequency public float attackFrequency Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.ChaseData.html": {
    "href": "api/FSM.Nodes.States.StatesData.ChaseData.html",
    "title": "Class ChaseData | FSM Unity Framework",
    "keywords": "Class ChaseData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class ChaseData : StateScriptData Inheritance object StateScriptData ChaseData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors ChaseData() public ChaseData() Fields chaseRange public float chaseRange Field Value float chaseSpeed public float chaseSpeed Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.CustomConditionData.html": {
    "href": "api/FSM.Nodes.States.StatesData.CustomConditionData.html",
    "title": "Class CustomConditionData | FSM Unity Framework",
    "keywords": "Class CustomConditionData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class CustomConditionData : StateScriptData Inheritance object StateScriptData CustomConditionData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors CustomConditionData() public CustomConditionData() Fields selectedComponent public string selectedComponent Field Value string selectedFunction public string selectedFunction Field Value string selectedGameObject public string selectedGameObject Field Value string"
  },
  "api/FSM.Nodes.States.StatesData.CustomData.html": {
    "href": "api/FSM.Nodes.States.StatesData.CustomData.html",
    "title": "Class CustomData | FSM Unity Framework",
    "keywords": "Class CustomData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class CustomData : StateScriptData Inheritance object StateScriptData CustomData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors CustomData() public CustomData() Fields selectedComponent public string selectedComponent Field Value string selectedFunction public string selectedFunction Field Value string selectedGameObject public string selectedGameObject Field Value string"
  },
  "api/FSM.Nodes.States.StatesData.DistanceData.html": {
    "href": "api/FSM.Nodes.States.StatesData.DistanceData.html",
    "title": "Class DistanceData | FSM Unity Framework",
    "keywords": "Class DistanceData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class DistanceData : StateScriptData Inheritance object StateScriptData DistanceData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors DistanceData() public DistanceData() Fields distanceFromPlayer public float distanceFromPlayer Field Value float operand public FsmOperands operand Field Value FsmOperands"
  },
  "api/FSM.Nodes.States.StatesData.FleeData.html": {
    "href": "api/FSM.Nodes.States.StatesData.FleeData.html",
    "title": "Class FleeData | FSM Unity Framework",
    "keywords": "Class FleeData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class FleeData : StateScriptData Inheritance object StateScriptData FleeData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors FleeData() public FleeData() Fields detectionRange public float detectionRange Field Value float fleeDistance public float fleeDistance Field Value float fleeSpeed public float fleeSpeed Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.HealthData.html": {
    "href": "api/FSM.Nodes.States.StatesData.HealthData.html",
    "title": "Class HealthData | FSM Unity Framework",
    "keywords": "Class HealthData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class HealthData : StateScriptData Inheritance object StateScriptData HealthData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors HealthData() public HealthData() Fields health public float health Field Value float operand public FsmOperands operand Field Value FsmOperands"
  },
  "api/FSM.Nodes.States.StatesData.HearingData.html": {
    "href": "api/FSM.Nodes.States.StatesData.HearingData.html",
    "title": "Class HearingData | FSM Unity Framework",
    "keywords": "Class HearingData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class HearingData : StateScriptData Inheritance object StateScriptData HearingData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors HearingData() public HearingData() Fields hearingRange public float hearingRange Field Value float minPlayerSpeed public float minPlayerSpeed Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.IdleData.html": {
    "href": "api/FSM.Nodes.States.StatesData.IdleData.html",
    "title": "Class IdleData | FSM Unity Framework",
    "keywords": "Class IdleData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class IdleData : StateScriptData Inheritance object StateScriptData IdleData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors IdleData() public IdleData()"
  },
  "api/FSM.Nodes.States.StatesData.NextStateData.html": {
    "href": "api/FSM.Nodes.States.StatesData.NextStateData.html",
    "title": "Class NextStateData | FSM Unity Framework",
    "keywords": "Class NextStateData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class NextStateData : StateScriptData Inheritance object StateScriptData NextStateData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors NextStateData() public NextStateData()"
  },
  "api/FSM.Nodes.States.StatesData.PatrolData.html": {
    "href": "api/FSM.Nodes.States.StatesData.PatrolData.html",
    "title": "Class PatrolData | FSM Unity Framework",
    "keywords": "Class PatrolData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class PatrolData : StateScriptData Inheritance object StateScriptData PatrolData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors PatrolData() public PatrolData() Fields patrolPoints public List<string> patrolPoints Field Value List<string>"
  },
  "api/FSM.Nodes.States.StatesData.SearchData.html": {
    "href": "api/FSM.Nodes.States.StatesData.SearchData.html",
    "title": "Class SearchData | FSM Unity Framework",
    "keywords": "Class SearchData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class SearchData : StateScriptData Inheritance object StateScriptData SearchData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors SearchData() public SearchData() Fields exploreRadius public float exploreRadius Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.SeeingData.html": {
    "href": "api/FSM.Nodes.States.StatesData.SeeingData.html",
    "title": "Class SeeingData | FSM Unity Framework",
    "keywords": "Class SeeingData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class SeeingData : StateScriptData Inheritance object StateScriptData SeeingData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors SeeingData() public SeeingData() Fields distance public float distance Field Value float"
  },
  "api/FSM.Nodes.States.StatesData.VariableData.html": {
    "href": "api/FSM.Nodes.States.StatesData.VariableData.html",
    "title": "Class VariableData | FSM Unity Framework",
    "keywords": "Class VariableData Namespace FSM.Nodes.States.StatesData Assembly FSM.dll public class VariableData : StateScriptData Inheritance object StateScriptData VariableData Inherited Members StateScriptData.InspectVariables() StateScriptData.GetVariables() StateScriptData.SetVariableValue(string, object) StateScriptData.RemoveVariable(string, object) StateScriptData.SetStateName(string) StateScriptData.GetStateName() Constructors VariableData() public VariableData() Fields selectedVariable public string selectedVariable Field Value string value public string value Field Value string"
  },
  "api/FSM.Nodes.States.StatesData.html": {
    "href": "api/FSM.Nodes.States.StatesData.html",
    "title": "Namespace FSM.Nodes.States.StatesData | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes.States.StatesData Classes AttackData ChaseData CustomConditionData CustomData DistanceData FleeData HealthData HearingData IdleData NextStateData PatrolData SearchData SeeingData VariableData"
  },
  "api/FSM.Nodes.States.html": {
    "href": "api/FSM.Nodes.States.html",
    "title": "Namespace FSM.Nodes.States | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes.States Classes StateScript Abstract base class for state scripts. StateScriptData Data class for storing state script variables and metadata. Interfaces IAction Interface for defining actions. ICondition Interface for defining conditions."
  },
  "api/FSM.Nodes.html": {
    "href": "api/FSM.Nodes.html",
    "title": "Namespace FSM.Nodes | FSM Unity Framework",
    "keywords": "Namespace FSM.Nodes Classes FsmGraph Represents the finite state machine graph attached to a GameObject."
  },
  "api/FSM.Player.HealthBar.html": {
    "href": "api/FSM.Player.HealthBar.html",
    "title": "Class HealthBar | FSM Unity Framework",
    "keywords": "Class HealthBar Namespace FSM.Player Assembly FSM.dll Manages the health bar UI component. public class HealthBar : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour HealthBar Methods UpdateHealthBar(float, float) Updates the health bar UI based on the current health and maximum health. public void UpdateHealthBar(float maxHealth, float currentHealth) Parameters maxHealth float Maximum health value. currentHealth float Current health value."
  },
  "api/FSM.Player.HealthSystem.html": {
    "href": "api/FSM.Player.HealthSystem.html",
    "title": "Class HealthSystem | FSM Unity Framework",
    "keywords": "Class HealthSystem Namespace FSM.Player Assembly FSM.dll Manages the health system of a player character. public class HealthSystem : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour HealthSystem Fields maxHealth public float maxHealth Field Value float Methods GetCurrentHealth() Returns the current health value. public float GetCurrentHealth() Returns float GetMaxHealth() Returns the maximum health value. public float GetMaxHealth() Returns float Heal() Heals the player to full health. public void Heal() TakeDamage(float) Inflicts damage to the player. public void TakeDamage(float amount) Parameters amount float Amount of damage to inflict."
  },
  "api/FSM.Player.MouseLook.html": {
    "href": "api/FSM.Player.MouseLook.html",
    "title": "Class MouseLook | FSM Unity Framework",
    "keywords": "Class MouseLook Namespace FSM.Player Assembly FSM.dll Controls the player's mouse look behavior. public class MouseLook : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour MouseLook"
  },
  "api/FSM.Player.PlayerMovement.html": {
    "href": "api/FSM.Player.PlayerMovement.html",
    "title": "Class PlayerMovement | FSM Unity Framework",
    "keywords": "Class PlayerMovement Namespace FSM.Player Assembly FSM.dll Controls the movement of the player character. public class PlayerMovement : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour PlayerMovement Fields speed public float speed Field Value float"
  },
  "api/FSM.Player.html": {
    "href": "api/FSM.Player.html",
    "title": "Namespace FSM.Player | FSM Unity Framework",
    "keywords": "Namespace FSM.Player Classes HealthBar Manages the health bar UI component. HealthSystem Manages the health system of a player character. MouseLook Controls the player's mouse look behavior. PlayerMovement Controls the movement of the player character."
  },
  "api/FSM.Utilities.CustomObjectField.html": {
    "href": "api/FSM.Utilities.CustomObjectField.html",
    "title": "Class CustomObjectField | FSM Unity Framework",
    "keywords": "Class CustomObjectField Namespace FSM.Utilities Assembly FSM.dll Custom ObjectField class to display object thumbnails. public class CustomObjectField : ObjectField Inheritance object CallbackEventHandler Focusable VisualElement BindableElement BaseField<Object> ObjectField CustomObjectField Extension Methods FsmStyleUtility.AddClasses(VisualElement, params string[]) FsmStyleUtility.AddStyleSheets(VisualElement, params string[]) Constructors CustomObjectField(string) Constructor to initialize the custom object field. public CustomObjectField(string label) Parameters label string Label for the object field."
  },
  "api/FSM.Utilities.HierarchyChecker.html": {
    "href": "api/FSM.Utilities.HierarchyChecker.html",
    "title": "Class HierarchyChecker | FSM Unity Framework",
    "keywords": "Class HierarchyChecker Namespace FSM.Utilities Assembly FSM.dll Class for loading unique ID's to every object everytime the hierarchy changes. [InitializeOnLoad] public static class HierarchyChecker Inheritance object HierarchyChecker"
  },
  "api/FSM.Utilities.IDGenerator.html": {
    "href": "api/FSM.Utilities.IDGenerator.html",
    "title": "Class IDGenerator | FSM Unity Framework",
    "keywords": "Class IDGenerator Namespace FSM.Utilities Assembly FSM.dll Class for generating unique IDs for GameObjects. public class IDGenerator : MonoBehaviour Inheritance object Object Component Behaviour MonoBehaviour IDGenerator Fields uniqueID public string uniqueID Field Value string Methods GenerateNewID() Call this method to generate a new unique ID. public string GenerateNewID() Returns string GetUniqueID() Call this method to get the unique ID. If the unique ID is not generated yet, it will generate it. Else, it will return the existing unique ID. public string GetUniqueID() Returns string The generated unique ID."
  },
  "api/FSM.Utilities.SerializableDictionary-2.SerializableKeyValuePair.html": {
    "href": "api/FSM.Utilities.SerializableDictionary-2.SerializableKeyValuePair.html",
    "title": "Struct SerializableDictionary<TKey, TValue>.SerializableKeyValuePair | FSM Unity Framework",
    "keywords": "Struct SerializableDictionary<TKey, TValue>.SerializableKeyValuePair Namespace FSM.Utilities Assembly FSM.dll [Serializable] public struct SerializableDictionary<TKey, TValue>.SerializableKeyValuePair Constructors SerializableKeyValuePair(TKey, TValue) public SerializableKeyValuePair(TKey key, TValue value) Parameters key TKey value TValue Fields Key public TKey Key Field Value TKey Value public TValue Value Field Value TValue Methods SetValue(TValue) public void SetValue(TValue value) Parameters value TValue"
  },
  "api/FSM.Utilities.SerializableDictionary-2.html": {
    "href": "api/FSM.Utilities.SerializableDictionary-2.html",
    "title": "Class SerializableDictionary<TKey, TValue> | FSM Unity Framework",
    "keywords": "Class SerializableDictionary<TKey, TValue> Namespace FSM.Utilities Assembly FSM.dll Serializable dictionary implementation using a list of key-value pairs. [Serializable] public class SerializableDictionary<TKey, TValue> : SerializableDictionary Type Parameters TKey Type of the keys. TValue Type of the values. Inheritance object SerializableDictionary SerializableDictionary<TKey, TValue> Constructors SerializableDictionary() Constructor for SerializableDictionary. public SerializableDictionary() SerializableDictionary(IDictionary<TKey, TValue>) Constructor for SerializableDictionary with initial dictionary. public SerializableDictionary(IDictionary<TKey, TValue> dictionary) Parameters dictionary IDictionary<TKey, TValue> Initial dictionary to populate. Properties Count public int Count { get; } Property Value int IsReadOnly public bool IsReadOnly { get; } Property Value bool this[TKey] public TValue this[TKey key] { get; set; } Parameters key TKey Property Value TValue Keys public ICollection<TKey> Keys { get; } Property Value ICollection<TKey> Values public ICollection<TValue> Values { get; } Property Value ICollection<TValue> Methods Add(KeyValuePair<TKey, TValue>) public void Add(KeyValuePair<TKey, TValue> kvp) Parameters kvp KeyValuePair<TKey, TValue> Add(TKey, TValue) public void Add(TKey key, TValue value) Parameters key TKey value TValue Clear() public void Clear() Contains(KeyValuePair<TKey, TValue>) public bool Contains(KeyValuePair<TKey, TValue> kvp) Parameters kvp KeyValuePair<TKey, TValue> Returns bool ContainsKey(TKey) public bool ContainsKey(TKey key) Parameters key TKey Returns bool CopyTo(KeyValuePair<TKey, TValue>[], int) public void CopyTo(KeyValuePair<TKey, TValue>[] array, int arrayIndex) Parameters array KeyValuePair<TKey, TValue>[] arrayIndex int GetEnumerator() public IEnumerator<KeyValuePair<TKey, TValue>> GetEnumerator() Returns IEnumerator<KeyValuePair<TKey, TValue>> GetValue(TKey) public TValue GetValue(TKey key) Parameters key TKey Returns TValue OnAfterDeserialize() Implement this method to receive a callback after Unity deserializes your object. public void OnAfterDeserialize() OnBeforeSerialize() Implement this method to receive a callback before Unity serializes your object. public void OnBeforeSerialize() Remove(KeyValuePair<TKey, TValue>) public bool Remove(KeyValuePair<TKey, TValue> kvp) Parameters kvp KeyValuePair<TKey, TValue> Returns bool Remove(TKey) public bool Remove(TKey key) Parameters key TKey Returns bool TryGetValue(TKey, out TValue) public bool TryGetValue(TKey key, out TValue value) Parameters key TKey value TValue Returns bool UpdateKey(TKey, TKey) public void UpdateKey(TKey oldKey, TKey newKey) Parameters oldKey TKey newKey TKey"
  },
  "api/FSM.Utilities.SerializableDictionary.html": {
    "href": "api/FSM.Utilities.SerializableDictionary.html",
    "title": "Class SerializableDictionary | FSM Unity Framework",
    "keywords": "Class SerializableDictionary Namespace FSM.Utilities Assembly FSM.dll Base class for serializable dictionaries. public class SerializableDictionary Inheritance object SerializableDictionary Derived SerializableDictionary<TKey, TValue>"
  },
  "api/FSM.Utilities.html": {
    "href": "api/FSM.Utilities.html",
    "title": "Namespace FSM.Utilities | FSM Unity Framework",
    "keywords": "Namespace FSM.Utilities Classes CustomObjectField Custom ObjectField class to display object thumbnails. HierarchyChecker Class for loading unique ID's to every object everytime the hierarchy changes. IDGenerator Class for generating unique IDs for GameObjects. SerializableDictionary Base class for serializable dictionaries. SerializableDictionary<TKey, TValue> Serializable dictionary implementation using a list of key-value pairs. Structs SerializableDictionary<TKey, TValue>.SerializableKeyValuePair"
  },
  "examples.html": {
    "href": "examples.html",
    "title": "Introduction | FSM Unity Framework",
    "keywords": "Introduction"
  },
  "index.html": {
    "href": "index.html",
    "title": "FSM Unity Framework Documentation | FSM Unity Framework",
    "keywords": "FSM Unity Framework Documentation Welcome to the FSM Unity Framework documentation. This framework provides tools for creating Finite State Machines (FSMs) in Unity. Installation The Installation section will guide you through the process of setting up and using the FSM Unity Framework in your Unity projects. Installation Guide Usage The Usage section contains the information for you to be able to use the framework to its maximum potencial. Usage API Reference The API Reference contains detailed documentation for all the classes, methods, and components available in the FSM Unity Framework. API Reference Features & Examples Explore these examples to gain a deeper understanding of how to implement state-based behaviors using the FSM Unity Framework. We provide you with some examples of multiple situations that you may encountar while working with this tool, so you will be able how to handle them properly. Examples Additional Resources GitHub Repository: Visit our GitHub repository for the latest updates, issues, and contributions. FAQ: Check out the frequently asked questions for answers to common queries. Support: Need help or have questions? Contact our support team."
  },
  "installation.html": {
    "href": "installation.html",
    "title": "Getting Started with FSM Unity Framework | FSM Unity Framework",
    "keywords": "Getting Started with FSM Unity Framework Welcome to the FSM Unity Framework! This framework is designed to help developers implement finite state machines in their Unity projects efficiently. With a user-friendly interface and powerful features, you can easily manage complex game states and transitions. Installation Process For the installations process you have to follow this steps: In this GitHubRepository you will find the lates release of the package. You just need to download the file called FsmUnityFramework.unitypackage. Go to this GitHub Repository: https://github.com/arnaucarbonell/fsm-unity-framework/releases Once this is done open your unity project. Next, you have to execute the unitypackage by double clicking the downloaded element. In you Unity project, a new package will appear with all the folders inside the package and select all the components you want to import. The final step is to install URP to your Unity project and add the _URP render pipeline. If your project already has URP, you can skip this last step. Now you have the FSM Unity Framework imported!!!"
  },
  "usage.html": {
    "href": "usage.html",
    "title": "Usage Guide | FSM Unity Framework",
    "keywords": "Usage Guide Introduction Welcome to the Unity Finite State Machine (FSM) Tool usage guide. This document provides detailed information on the various types of nodes available in the FSM tool and outlines the features of the tool to help you get started with creating and managing FSMs in your Unity projects. Types of Nodes The FSM tool includes several types of nodes, each serving a specific purpose within the state machine. Understanding these nodes is crucial for effectively using the tool. FsmStateNode Description: The FsmStateNode represents a state within the FSM. It initializes the node with a name and position and sets the type of node as a state. Features: Predefined States: Includes states such as Idle, Patrol, Chase, Flee, Attack, and Search. Custom Variables: Displays variables specific to the state for customization. Animation Override: Allows overriding an animation to run while the state is active. Hit State Override: Modifies global enemy values during this state. Unique Instance: Only one instance of a state behavior can exist simultaneously. Connections: Can receive multiple connections from non-action nodes and can only connect to condition nodes. FsmCustomStateNode Description: The FsmCustomStateNode allows for custom behavior by attaching any scene component and selecting its functions to execute during the state. Features: Component Selection: Add any component from the scene and choose a function to execute. Animation and Hit State Override: Similar to FsmStateNode. Unique Instance: Only one instance of a state behavior can exist simultaneously. Connections: Can receive multiple connections from non-action nodes and can only connect to condition nodes. FsmVariableNode Description: The FsmVariableNode is used to override the value of a variable when connected to another state. Features: Variable Override: Change the value of a variable for connected states. Connections: Can only connect to other states. FsmTransitionNode Description: The FsmTransitionNode manages transitions between states based on specified conditions. Features: Conditions: Includes conditions based on distance, direct vision, health, noise, and a default forward execution condition. Connections: Can connect to both states and conditions. Repeatable: Can be used multiple times with different values. FsmDualTransitionNode Description: The FsmDualTransitionNode is similar to FsmTransitionNode but has two output ports for true and false conditions. Features: Dual Conditions: Allows different behaviors based on whether the condition is true or false. Connections: Can connect to both states and conditions. Repeatable: Can be used multiple times with different values. FsmCustomTransitionNode Description: The FsmCustomTransitionNode allows for custom transitions by selecting functions from scene components to execute during the transition. Features: Component Selection: Add any component from the scene and choose a function to execute. Custom Conditions: Similar to FsmDualTransitionNode but with custom component functions. FsmInitialNode Description: The FsmInitialNode marks the starting point of the FSM. Features: Initial State: Defines the initial state when the FSM is initialized. Unique Instance: Only one initial node can exist in each FSM graph. Automatic Creation: Created automatically and cannot be deleted. FsmExtensionNode Description: The FsmExtensionNode is used for organizing the graph visually. Features: Visual Organization: Helps in arranging the graph and its connections for better clarity. Creation: Created by clicking the middle mouse button on a connection line. Features of the Unity FSM Tool The Unity FSM Tool provides several features designed to streamline the creation and management of finite state machines within Unity. Visual Editor Description: The visual editor offers a user-friendly interface for designing FSMs. It allows users to create, connect, and configure nodes using a drag-and-drop system. Features: Node Creation: Easily create and place nodes on the canvas. Connections: Draw connections between nodes to define transitions. Configuration: Configure node properties and conditions directly within the editor. Serialization and Deserialization Description: The tool supports saving and loading FSM configurations, allowing for persistent FSMs across sessions. Features: Save FSM: Save the current FSM configuration to a file. Load FSM: Load an FSM configuration from a file. Inspector Integration Description: The tool integrates with the Unity Inspector, allowing for seamless editing of FSM properties. Features: Property Fields: Edit FSM properties directly in the Inspector. Live Updates: See real-time updates to FSM configurations within the Inspector. Debugging Tools Description: Debugging tools help identify and fix issues within the FSM. Features: State Tracking: Monitor the current state and transitions during runtime. Condition Evaluation: Evaluate and display condition results to ensure they work as expected. Scripting API Description: The FSM tool provides a scripting API for advanced users who prefer to configure FSMs programmatically. Features: Create States and Transitions: Programmatically create and connect states and transitions. Access FSM Properties: Access and modify FSM properties through code."
  }
}